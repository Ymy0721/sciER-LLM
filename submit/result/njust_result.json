{
    "dataset": "njust",
    "result_data": [
        {
            "id": "2020.acl-main.21",
            "result": [
                {
                    "value": {
                        "start": 302,
                        "end": 305,
                        "text": "SQG",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E0"
                },
                {
                    "value": {
                        "start": 4994,
                        "end": 4998,
                        "text": "CoQA",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "2020.acl-main.21:E1"
                },
                {
                    "value": {
                        "start": 5631,
                        "end": 5654,
                        "text": "semi-autoregressive SQG",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E2"
                },
                {
                    "value": {
                        "start": 6572,
                        "end": 6579,
                        "text": "Seq2seq",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E3"
                },
                {
                    "value": {
                        "start": 6760,
                        "end": 6777,
                        "text": "pointer mechanism",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E4"
                },
                {
                    "value": {
                        "start": 6804,
                        "end": 6818,
                        "text": "self-attention",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E5"
                },
                {
                    "value": {
                        "start": 6925,
                        "end": 6938,
                        "text": "Seq2seq model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E6"
                },
                {
                    "value": {
                        "start": 145,
                        "end": 148,
                        "text": "TQG",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E7"
                },
                {
                    "value": {
                        "start": 146,
                        "end": 148,
                        "text": "QG",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E8"
                },
                {
                    "value": {
                        "start": 1732,
                        "end": 1734,
                        "text": "QA",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E9"
                },
                {
                    "value": {
                        "start": 8282,
                        "end": 8291,
                        "text": "SQG model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E10"
                },
                {
                    "value": {
                        "start": 1035,
                        "end": 1067,
                        "text": "answer-aware attention mechanism",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E11"
                },
                {
                    "value": {
                        "start": 8692,
                        "end": 8697,
                        "text": "SQuAD",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "2020.acl-main.21:E12"
                },
                {
                    "value": {
                        "start": 8729,
                        "end": 8735,
                        "text": "NewsQA",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "2020.acl-main.21:E13"
                },
                {
                    "value": {
                        "start": 8971,
                        "end": 8975,
                        "text": "QuAC",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "2020.acl-main.21:E14"
                },
                {
                    "value": {
                        "start": 9039,
                        "end": 9049,
                        "text": "SQG models",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E15"
                },
                {
                    "value": {
                        "start": 9060,
                        "end": 9072,
                        "text": "CoQA dataset",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "2020.acl-main.21:E16"
                },
                {
                    "value": {
                        "start": 8283,
                        "end": 8291,
                        "text": "QG model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E17"
                },
                {
                    "value": {
                        "start": 10972,
                        "end": 10977,
                        "text": "kappa",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "2020.acl-main.21:E18"
                },
                {
                    "value": {
                        "start": 11713,
                        "end": 11721,
                        "text": "accuracy",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "2020.acl-main.21:E19"
                },
                {
                    "value": {
                        "start": 12342,
                        "end": 12366,
                        "text": "Transformer architecture",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E20"
                },
                {
                    "value": {
                        "start": 14525,
                        "end": 14544,
                        "text": "Transformer-encoder",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E21"
                },
                {
                    "value": {
                        "start": 16636,
                        "end": 16662,
                        "text": "bi-directional GRU network",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E22"
                },
                {
                    "value": {
                        "start": 20670,
                        "end": 20689,
                        "text": "Transformer-decoder",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E23"
                },
                {
                    "value": {
                        "start": 21409,
                        "end": 21433,
                        "text": "multi-head selfattention",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E24"
                },
                {
                    "value": {
                        "start": 20750,
                        "end": 20778,
                        "text": "multi-head encoder-attention",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E25"
                },
                {
                    "value": {
                        "start": 22310,
                        "end": 22317,
                        "text": "CoreNQG",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E26"
                },
                {
                    "value": {
                        "start": 22347,
                        "end": 22355,
                        "text": "CorefNet",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "2020.acl-main.21:E27"
                },
                {
                    "value": {
                        "start": 6544,
                        "end": 6555,
                        "text": "NN-based QG",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E28"
                },
                {
                    "value": {
                        "start": 22784,
                        "end": 22791,
                        "text": "CopyNet",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E29"
                },
                {
                    "value": {
                        "start": 22990,
                        "end": 23004,
                        "text": "copy mechanism",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E30"
                },
                {
                    "value": {
                        "start": 23157,
                        "end": 23224,
                        "text": "latent variable hierarchical recurrent encoder-decoder architecture",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E31"
                },
                {
                    "value": {
                        "start": 23225,
                        "end": 23230,
                        "text": "VHRED",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E32"
                },
                {
                    "value": {
                        "start": 23267,
                        "end": 23312,
                        "text": "hierarchical recurrent attention architecture",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E33"
                },
                {
                    "value": {
                        "start": 23313,
                        "end": 23317,
                        "text": "HRAN",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E34"
                },
                {
                    "value": {
                        "start": 23430,
                        "end": 23440,
                        "text": "ReDR model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E35"
                },
                {
                    "value": {
                        "start": 23513,
                        "end": 23527,
                        "text": "CorefNet model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E36"
                },
                {
                    "value": {
                        "start": 23541,
                        "end": 23546,
                        "text": "CFNet",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E37"
                },
                {
                    "value": {
                        "start": 24150,
                        "end": 24154,
                        "text": "BLEU",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "2020.acl-main.21:E38"
                },
                {
                    "value": {
                        "start": 24185,
                        "end": 24192,
                        "text": "ROUGE-L",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "2020.acl-main.21:E39"
                },
                {
                    "value": {
                        "start": 24212,
                        "end": 24218,
                        "text": "METEOR",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "2020.acl-main.21:E40"
                },
                {
                    "value": {
                        "start": 24372,
                        "end": 24397,
                        "text": "semi-autoregressive model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E41"
                },
                {
                    "value": {
                        "start": 22697,
                        "end": 22707,
                        "text": "TQG models",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E42"
                },
                {
                    "value": {
                        "start": 23430,
                        "end": 23434,
                        "text": "ReDR",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E43"
                },
                {
                    "value": {
                        "start": 25902,
                        "end": 25908,
                        "text": "n-gram",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E44"
                },
                {
                    "value": {
                        "start": 26044,
                        "end": 26049,
                        "text": "BLEU4",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "2020.acl-main.21:E45"
                },
                {
                    "value": {
                        "start": 27263,
                        "end": 27270,
                        "text": "fluency",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "2020.acl-main.21:E46"
                },
                {
                    "value": {
                        "start": 27385,
                        "end": 27394,
                        "text": "coherence",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "2020.acl-main.21:E47"
                },
                {
                    "value": {
                        "start": 25810,
                        "end": 25821,
                        "text": "SQG dataset",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "2020.acl-main.21:E48"
                },
                {
                    "value": {
                        "start": 28130,
                        "end": 28141,
                        "text": "TQG dataset",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "2020.acl-main.21:E49"
                },
                {
                    "value": {
                        "start": 28475,
                        "end": 28490,
                        "text": "uni-graph model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E50"
                },
                {
                    "value": {
                        "start": 28687,
                        "end": 28702,
                        "text": "uni-heads model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E51"
                },
                {
                    "value": {
                        "start": 28725,
                        "end": 28741,
                        "text": "no co2fine model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E52"
                },
                {
                    "value": {
                        "start": 28812,
                        "end": 28826,
                        "text": "non-auto model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E53"
                },
                {
                    "value": {
                        "start": 8262,
                        "end": 8291,
                        "text": "semi-autoregressive SQG model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.21:E54"
                }
            ],
            "data": {
                "text": "Learning to Ask More: Semi-Autoregressive Sequential Question Generation under Dual-Graph Interaction Abstract Traditional Question Generation ( TQG ) aims to generate a question given an input passage and an answer. When there is a sequence of answers, we can perform Sequential Question Generation ( SQG ) to produce a series of interconnected questions. Since the frequently occurred information omission and coreference between questions, SQG is rather challenging. Prior works regarded SQG as a dialog generation task and recurrently produced each question. However, they suffered from problems caused by error cascades and could only capture limited context dependencies. To this end, we generate questions in a semi-autoregressive way. Our model divides questions into different groups and generates each group of them in parallel. During this process, it builds two graphs focusing on information from passages, answers respectively and performs dual-graph interaction to get information for generation. Besides , we design an answer-aware attention mechanism and the coarse-to-fine generation scenario . Experiments on our new dataset containing 81.9K questions show that our model substantially outperforms prior works. 1 Introduction Question Generation ( QG ) aims to teach machines to ask human-like questions from a range of inputs such as natural language texts ( Du et al., 2017 ), images ( Mostafazadeh et al., 2016 ) and knowledge bases ( Serban et al., 2016 ). In recent years, QG has received increasing attention due to its wide applications. Asking questions in dialog systems can enhance the interactiveness and persistence of humanmachine interactions ( Wang et al., 2018 ). QG benefits Question Answering ( QA ) models through data augmentation ( Duan et al., 2017 ) and joint learning ( Sun et al., 2019 ). It also plays an important role in education ( Heilman and Smith, 2010 ) and clinical ( Weizenbaum et al., 1966 ) systems. Traditional Question Generation ( TQG ) is defined as the reverse task of QA, i.e., a passage and an answer ( often a certain span from the passage ) are provided as inputs, and the output is a question grounded in the input passage targeting on the given answer. When there is a sequence of answers, we can perform Sequential Question Generation ( SQG ) to produce a series of interconnected questions. Table 1 shows an example comparing the two tasks. Intuitively, questions in SQG are much more concise and we can regard them with given answers as QA-style conversations. Since it is more natural for human beings to test knowledge or seek information through coherent questions ( Reddy et al., 2019 ), SQG has wide applications, e.g., enabling virtual assistants to ask questions based on previous discussions to get better user experiences. SQG is a challenging task in two aspects. First, information omissions between questions lead to complex context dependencies. Second, there are frequently occurred coreference between questions. Prior works regarded SQG as a dialog generation task ( namely conversational QG ) where questions are generated autoregressively ( recurrently ), i.e., a new question is produced based on previous outputs. Although many powerful dialog generation models can be adopted to address the challenges mentioned above, there are two major obstacles. First, these models suffer from problems caused by error cascades. Empirical results from experiments reveal that the later generated questions tend to become shorter with lower quality, especially becoming more irrelevant to given answers, e.g., “Why?”, “What else? ”. Second, models recurrently generating each question struggle to capture complex context dependencies, e.g., long-distance coreference. Essentially, SQG is rather different from dialog generation since all answers are given in advance and they act as strict semantic constraints during text generation. To deal with these problems, we perform SQG in a semi-autoregressive way. More specifically, we divide target questions into different groups ( questions in the same group are closely-related ) and generate all groups in parallel. Especially, our scenario becomes non-autoregressive if each group only contains a single question. Since we eliminate the recurrent dependencies between questions in different groups, the generation process is much faster and our model can better deal with the problems caused by error cascades. To get information for the generation process, we perform dualgraph interaction where a passage-info graph and an answer-info graph are constructed and iteratively updated with each other. The passage-info graph is used for better capturing context dependencies , and the answer-info graph is used to make generated questions more relevant to given answers with the help of our answer-aware attention mechanism . Besides, a coarse-to-fine text generation scenario is adopted for the coreference resolution between questions. Prior works performed SQG on CoQA ( Reddy et al . , 2019 ) , a high-quality dataset for conversational QA . As will be further illustrated , a number of data in CoQA are not suitable for SQG . Some researchers ( Gao et al., 2019 ) directly discarded these data, but the remaining questions may become incoherent, e.g., the antecedent words for many pronouns are unclear. To this end , we build a new dataset from CoQA containing 81.9K relabeled questions . Above all, the main contributions of our work are: We build a new dataset containing 7.2K passages and 81.9K questions from CoQA . It is the first dataset specially built for SQG as far as we know. We perform semi-autoregressive SQG under dual-graph interaction. This is the first time that SQG is not regarded as a dialog generation task. We also propose an answer-aware attention mechanism and a coarse-to-fine generation scenario for better performance . We use extensive experiments to show that our model outperforms previous work by a substantial margin. Further analysis illustrated the impact of different components. Dataset for this paper is available at https:// github.com/ChaiZ-pku/Sequential-QG. 2 Related Work 2.1 Traditional Question Generation TQG was traditionally tackled by rule-based methods ( Lindberg et al., 2013; Mazidi and Nielsen, 2014; Hussein et al., 2014; Labutov et al., 2015 ), e.g., filling handcrafted templates under certain transformation rules. With the rise of data-driven learning approaches, neural networks ( NN ) have gradually taken the mainstream. Du et al . ( 2017 ) pioneered NN-based QG by adopting the Seq2seq architecture ( Sutskever et al . , 2014 ) . Many ideas were proposed since then to make it more powerful , including answer position features ( Zhou et al . , 2017 ) , specialized pointer mechanism ( Zhao et al . , 2018 ) , self-attention ( Scialom et al . , 2019 ) , answer separation ( Kim et al . , 2019 ) , etc . In addition , enhancing the Seq2seq model into more complicated structures using variational inference , adversarial training and reinforcement learning ( Yao et al . , 2018 ; Kumar et al . , 2019 ) have also gained much attention . There are also some works performing TQG under certain constraints, e.g., controlling the topic ( Hu et al., 2018 ) and difficulty ( Gao et al., 2018 ) of questions. Besides, combining QG with QA ( Wang et al., 2017; Tang et al., 2017; Sun et al., 2019 ) is also focused by many researchers. 2.2 Sequential Question Generation As human beings tend to use coherent questions for knowledge testing or information seeking, SQG plays an important role in many applications. Prior works regarded SQG as a dialog generation task ( namely conversational QA ). Pan et al . ( 2019 ) pretrained a model performing dialog generation , and then fine-tuned its parameters by reinforcement learning to make generated questions relevant to given answers . Gao et al. ( 2019 ) iteratively generated questions from previous outputs and leveraged off-the-shelf coreference resolution models to introduce a coreference loss. Besides, additional human annotations were performed on sentences from input passages for conversation flow modeling. Since SQG is essentially different from dialog generation, we discard its dialog view and propose the first semi-autoregressive SQG model. Compared with using the additional human annotation in Gao et al. ( 2019 ), our dual-graph interaction deals with context dependencies automatically. Besides , our answer-aware attention mechanism is much simpler than the fine-tuning process in Pan et al . ( 2019 ) to make outputs more answer-relevant . 3 Dataset As the reverse task of QA , QG is often performed on existing QA datasets , e.g . , SQuAD ( Rajpurkar et al . , 2016 ) , NewsQA ( Trischler et al . , 2016 ) , etc . However, questions are independent in most QA datasets, making TQG the only choice. In recent years , the appearance of large-scale conversational QA datasets like CoQA ( Reddy et al . , 2019 ) and QuAC ( Choi et al . , 2018 ) makes it possible to train data-driven SQG models , and the CoQA dataset was widely adopted by prior works . Since the test set of CoQA is not released to the public , its training set ( 7.2K passages with 108.6K questions ) was split into new training and validation set , and its validation set ( 0.5K passages with 8.0K questions ) was used as the new test set . Different from traditional QA datasets where the answers are certain spans from given passages , answers in CoQA are free-form text1 with corresponding evidence highlighted in the passage . This brings a big trouble for QG. As an example, consider the yes/no questions counting for 19.8% among all questions. Given the answer “yes” and a corresponding evidence “...the group first met on July 5 , 1967 on the campus of the Ohio state university...”, there are many potential outputs, e.g., “Did the group first met in July?”, “Was the group first met in Ohio state? ”. When considering the context formed by previous questions, the potential outputs become even more ( the original question in CoQA is “Was it founded the same year?” ). When there are too many potential outputs with significantly different semantic meanings, training a converged QG model becomes extremely difficult. For this reason, Gao et al. ( 2019 ) directly discarded questions that cannot be answered by spans from passages. However, the remaining questions can become incoherent, e.g., antecedent words for many pronouns become unclear. To this end , we build a new dataset from CoQA by preserving all 7.7K passages and rewriting all questions and answers . More specifically, we first discarded questions that are unsuitable for SQG. To do so, three annotators were hired to vote for the preservation/deletion of each question. A question is preserved if and only if it can be answered by a certain span from the input passage2. As a result, most deleted questions were yes/no questions and unanswerable questions. Besides , the kappa score between results given by different annotators was 0.83 , indicating that there was a strong interagreement between annotators . For the remaining QA-pairs, we preserved their original order and replaced all answers by spans from input passages. After that, we rewrote all questions to make them coherent. To avoid over-editing, annotators were asked to modify as little as possible. It turned out that in most cases, they only needed to deal with coreference since the prototype of pronouns were no longer existed. To further guarantee the annotation quality, we hired another project manager who daily examined 10% of the annotations from each annotator and provided feedbacks. The annotation was considered valid only when the accuracy of examined results surpasses 95% . Our annotation process took 2 months, and we finally got a dataset containing 7.7K passage with 81.9K QA-pairs. 4 Model In this section, we formalize the SQG task and introduce our model in details. As shown in Figure 1, the model first builds a passage-info graph and an answer-info graph by its passage-info encoder and answer-info encoder respectively. After that, it performs dual-graph interaction to get representations for the decoder. Finally, different groups of questions are generated in parallel under a coarse-to-fine scenario. Both encoders and decoder take the form of Transformer architecture ( Vaswani et al . , 2017 ) . 4.1 Problem Formalization In SQG, we input a passage composed by n sentences P = {Si}ni=1 and a sequence of l answers {Ai}li=1, each Ai is a certain span of P . The target output is a series of questions {Qi}li=1, where Qi can be answered by Ai according to the input passage P and previous QA-pairs. As mentioned above, we perform SQG in an semi-autoregressive way, i.e., target questions are divided into into different groups. Ideally, questions in the same group are expected to be closelyrelated, while questions in different groups should be as independent as possible. Our model takes a simple but effective unsupervised question clustering method. The intuition is: if two answers come from the same sentence, the two corresponding questions are likely to be closely-related. More specifically, if the k-th sentence Sk contains p answers from {Ai}li=1, we cluster them into an answer-group Gansk = {Aj1 , Aj2 , ..., Ajp} where j1 < j2 < ... < jp are continuous indexes from {1, 2, ..., l}. By replacing each answer in Gansk with its corresponding question, we get a questiongroup Gquesk = {Qj1 , Qj2 , ..., Qjp}, and we further define a corresponding target-output Tk as “Qj1 [sep]Qj2 [sep] ... [sep]Qjp” where “[sep]” is a special token. In Table 1, there are four target outputs T1, T2, T4, T5 ( no T3 since the third sentence in Table 1 do not contain any answer ), T2 is “What was he doing there? [sep] On What? [sep] ... [sep] What was Tim doing?” corresponding with the second sentence, and T5 is “What did he say?” corresponding with the last sentence. Supposing there are m answer- and question-groups, then our model generates all the m target-outputs in parallel, i.e., all questions are generated in a semi-autoregressive way. 4.2 Passage-Info encoder As shown in Figure 1, our passage-info encoder maps input sentences {Si}ni=1 into their sentence representations {si}ni=1 where every si ∈ R2ds . We regard each sentence as a sequence of words and replace each word by its pre-trained word embeddings ( Mikolov et al., 2013 ) which is a dense vector. After that , the sequence of word embeddings is sent to a Transformer-encoder that outputs a corresponding sequence of vectors . By averaging these vectors, we get the local representation slocali ∈ Rds of Si. After we get the local representations of all sentences { Si } ni = 1 in passage P , another Transformer-encoder is adopted to map the sequence { slocali } ni = 1 into { sglobali } ni = 1 , where sglobali ∈ Rds is called the global representation for Si . In other words, the passage-info encoder takes a hiarachical structure. We expect the local and global representations capture intra- and inter- sentence context dependencies respectively, and the final representation for Si is . 4.3 Answer-Info Encoder As described in Section 4.1, the input answers are split into m answer-groups. For Gansk corresponding with the k-th sentence of the input passage , we define { Gansk , Sk } as a “ rationale ” Rk , and further obtain its representation rk ∈ R2dr by our answerinfo encoder , which is based on a Transformer-encoder regarding sentence Sk as its input . To further consider information from Gansk , two more components are added into the answer-info encoder, as shown in Figure 2. First, we adopt the answer-tag features. For each word wi in sentence Sk, the embedding layer computes [xwi ;x a i ] ∈ Rdr as its final embedding, where xwi is the pre-trained word embedding and xai contains answer-tag features. More specifically, we give wi a label from {O, B, I} if it is “outside”, “the beginning of”, “inside of” any answer from Gansk , and use a vector corresponding with this label as xai . Second , we design the answer-aware attention mechanism . In the multi-head attention layer , there are not only lh vanilla “ self-attention heads ” , but also la “ answer-aware heads ” for each answer in Gansk . In an answer-aware head corresponding with answer A , words not belonging to A are masked out during the attention mechanism . The output of the Transformer-encoder is a sequence of vectors Henck = { henck } ( henck ∈ Rdr ) corresponding with the input word sequence from Sk . After getting Henck , we further send the sequence of vectors to a bi-directional GRU network ( Chung et al . , 2014 ) and take its last hidden state as the final rationale embedding rk ∈ R2dr . 4.4 Graph Construction In our SQG task, the input passage contain n sentences, which can be represented by {si}ni=1 ∈ R2ds leveraging the passage-info encoder. Among all input sentences, only m of them contain certain answers ( m ≤ n ), and we further define m rationales based on these sentences, {GansF ( j ), SF ( j )}mj=1, where the j-th rationale ( j ∈ {1, 2, ...,m} ) corresponds with the F ( j )-th sentence of the input passage ( F ( j ) ∈ {1, 2, ..., n} ). For the example in Table 1, n = 5,m = 4, F ( j ) maps {1, 2, 3, 4} into {1, 2, 4, 5} respectively. Using the answer-info encoder, we can get representations {rF ( j )}mj=1 ∈ R2ds for all rationales. We further build a passage-info graph V and an answer-info graph U based on these representations. For the rationale corresponding with the k-th sentence of the input passage, we add node uk, vk in graph U ,V respectively. For the example in Table 1, U is compused by {u1, u2, u4, u5} and V is compused by {v1, v2, v4, v5}, as shown in Figure 1. The initial representation for uk is computed by: where rk ∈ R2dr is the rationale representation, ek ∈ Rde is the embedding of index k, and Wu ∈ R( de+2dr )×dg , bu ∈ Rdg are trainable parameters. And the initial representation for vk is: where sk ∈ R2ds is the sentence representation and Wv ∈ R( de+2ds )×dg , bv ∈ Rdg are parameters. After adding these points, there arem nodes in U and V respectively. For ui, uj ∈ U corresponding with the i-th, j-th input sentences respectively, we add an edge between them if |i − j| < δ ( δ is a hyper-parameter ). Similarly, we add edges into V and the two graphs are isomorphic. 4.5 Dual-Graph Interaction In our answer-info graph U , node representations contain information focused on input answers. In the passage-info graph V , node representations capture inter- and intra-sentence context dependencies. As mentioned above, a good question should be answer-relevant as well as capturing complex context dependencies. So we should combine information in both U and V . Our dual-graph interaction is a process where U and V iteratively update node representations with each other. At time step t, representations are updated into respectively under three steps. First, we introduce the information transfer step. Taking U as an example. Each u( t−1 )i receives a( t )i from its neighbors ( two nodes are neighbors if there is an edge between them ) by: whereN ( ui ) is composed by all neighbors of node ui and Wij ∈ Rdg×dg , bij ∈ Rdg are parameters controlling the information transfer. For ui, uj and ui′ , uj′ whose |i− j| = |i′ − j′|, we use the same W and b. In other words, we can first create a sequence of matrices {W1,W2, ...} ∈ Rdg×dg and vectors {b1, b2, ...} ∈ Rdg , and then use |i − j| as the index to retrieve the corresponding Wij , bij . For graph V , we similarly compute In the second step, we compute multiple gates. For each u( t−1 )i in U , we compute an “update gate” y ( t ) i and a “reset gate” z ( t ) i by: where Wy,Wz ∈ R2dg×dg are paramenters. Similarly, for each v( t−1 )i in V we compute: Finally, we perform the information interaction, where each graph updates its node representations under the control of gates computed by the other graph. More specifically, node representations are updated by: The idea of using gates computed by the other graph to update node representations in each graph enables the information in input passage and answers interact more frequently, both of which act as strong constraints to the output questions. By iteratively performing the three steps for T times, we get the final representations u( T  )i and v ( T  ) i for ui ∈ U and vi ∈ V . 4.6 Decoder For the k-th input sentence Sk containing certain answers, our decoder generates the corresponding target-output Tk. As mentioned above, the generation process of all target-outputs are independent. The decoder is based on the Transformer-decoder containing a ( masked ) multi-head self-attention layer , a multi-head encoder-attention layer , a feed-forward projection layer and the softmax layer . To compute keys and values for the multi-head encoder-attention layer , it leverages the outputs from our answer-info encoder , i.e . , it usesHenck described in Section 4.3 to generate Tk corresponding with the k-th sentence . To generate coherent questions, we need to capture the context dependencies between input answers and passages. To this end, both u( T  )k and v ( T  ) k , which comes from the dual-graph interaction process, are used as additional inputs for generating Tk. First , they are concatenated with the output of each head from both ( masked ) multi-head selfattention layer and multi-head encoder-attention layer before sending to the next layer . Second, they are concatenated with inputs of the feed-forward projection layer. The two representations are also expected to make generated questions more relevant to given inputs. 4.7 Coarse-To-Fine Generation Since the semi-autoregressive generation scenario makes it more challenging to deal with coreferences between questions ( especially questions in different groups ), we perform question generation in a coarse-to-fine manner. The decoder only needs to generate “coarse questions” where all pronouns are replaced by a placeholder “[p]”. To get final results, we use an additional pre-trained coreference resolution model to fill pronouns into different placeholders. To make a fair comparison , we use the coreference resolution model ( Clark and Manning , 2016 ) adopted by prior works CoreNQG ( Du and Cardie , 2018 ) and CorefNet ( Gao et al . , 2019 ) . 5 Experiments In this section, we first introduce the three kinds of baselines. After that, we compare and analyse the results of different models under both automatic and human evaluation metrics. 5.1 Baselines We compared our model with seven baselines that can be divided into three groups. First , we used three TQG models : the Seq2seq ( Du et al . , 2017 ) model which pioneered NN-based QG , the CopyNet ( See et al . , 2017 ) model that introduced pointer mechanism , and CoreNQG ( Du and Cardie , 2018 ) which used hybrid features ( word , answer and coreference embeddings ) for encoder and adopted copy mechanism for decoder . Second , since prior works regarded SQG as a conversation generation task , we directly used two powerful multi-turn dialog systems : the latent variable hierarchical recurrent encoder-decoder architecture VHRED ( Serban et al . , 2017 ) , and the hierarchical recurrent attention architecture HRAN ( Xing et al . , 2018 ) . Third, we used prior works mentioned above. For Pan et al . ( 2019 ) , we adopted the ReDR model which had the best performance . For Gao et al . ( 2019 ) , we used the CorefNet model . Although a CFNet in this paper got better results , it required additional human annotations denoting the relationship between input sentences and target questions . So it is unfair to compare CFNet with other methods . It is worth mentioning that when generating questions using the second and third groups of baselines, only previously generated outputs were used as dialog history, i.e., the gold standard questions are remain unknown ( in some prior works, they were directly used as dialog history, which we think is inappropriate in practice ). 5.2 Automatic Evaluation Metrics Following the conventions , we used BLEU ( Papineni et al . , 2002 ) , ROUGE-L ( Lin , 2004 ) and METEOR ( Lavie and Agarwal , 2007 ) as automatic evaluation metrics . We also computed the average word-number of generated questions. As shown in Table 2, our semi-autoregressive model outperformed other methods substantially. When we focus on the second and third groups of baselines regarding SQG as multi-turn dialog generation tasks, we can find that models from the third group are more powerful since they make better use of information from input passages. Besides, models from the second group tend to generate shortest questions. Finally, similar to the problem that dialog systems often generate dull and responses, these models also suffer from producing general but meaningless questions like “What?”, “How?”, “And else? ”. When we compare the first and third groups of baselines ( which are all QG models ), it is not surprising that SQG models show more advantages than TQG models, as they take the relationships between questions into consideration. Besides , CorefNet gets better performance among all baselines , especially ReDR . This indicates that comparing with implicitly performing reinforcement learning through QA models, explicitly using target answers as inputs can be more effective. Note that if we directly compare the performance between SQG task and TQG task under the same model ( e.g . , the Seq2seq model ) , evaluation scores for TQG tasks are much higher , which is not surprising since SQG is harder than TQG dealing with dependencies between questions . Another fact lies in the computation of automatic evaluation metrics. As shown in Table 2, questions in SQG datasets are much shorter than TQG. Since our automatic evaluation metrics are based on n-gram overlaps between generated and gold standard questions , the scores significantly go down with the growth of n ( for this reason , the BLEU4 scores are not listed in Table 2 ) . This also illustrates the importance of performing human evaluation. 5.3 Human Evaluation It is generally acknowledged that automatic evaluation metrics are far from enough for SQG. So we perform human evaluation in five aspects. Fluency measures if a question is grammatically correct and is fluent to read. Coherence measures if a question is coherent with previous ones. Coreference measures if a question uses correct pronouns. Answerability measures if a question is targeting on the given answer. Relevance measures if a question is grounded in the given passage. Since performing human evaluation is rather expensive and time-consuming , we picked up the best TQG model ( CoreNQG ) , SQG model ( CorefNet ) to compare with our model . We randomly selected 20 passages from the test set with 207 given answers and asked 10 native speakers to evaluate the outputs of each model independently. Under each aspect, reviewers are asked to choose a score from {1, 2, 3}, where 3 indicates the best quality. The average scores for each evaluation metric are shown in Table 4. We can find that our model gets the best or competitive performance in each metric. When it comes to fluency , all models get high performance , and the CorefNet that outputs shortest questions gets the best score . As for coherence , CoreNQG gets poor results since it generates questions independently . When it comes to coreference , our model only slightly lower than CorefNet , which added direct supervision to attention weights by a coreference resolution model . Finally, our model gets the best performance on both answerabity and relevance. However, it is worth noticing that all models get rather poor performances under these two aspects, indicating that making a concise question meaningful ( i.e., targeting on given answers ) with more information from input passage ( i.e., performing proper information elimination ) is a major challenge in SQG. Besides, as pointed out by Table 3, questions in our SQG dataset are significantly shorter compared with TQG dataset, making subtle errors much easier to be noticed. 6 Analysis 6.1 Ablation Test In this section, we perform ablation test to verify the influence of different components in our model. First, we modify Equation 7 into to get the no interact model, i.e., two graphs are independently updated without any interaction. Second , we build a uni-graph model by removing the passage-info encoder ( the remaining rationale graph is updated similarly to Li et al . ( 2015 ) ) . Third , we discard the attention-aware heads in the rationale encoder to get a uni-heads model . Then , we build the no co2fine model without the coarse-to-fine generation scenario . Finally , we build a non-auto model that performs SQG in an non-autoregressive way , i.e . , each question is generated in parallel . As shown in Table 5, each component in our model plays an important part. Results for the no interact model indicate that compared with independently updating the passage-info graph and answer-info graph , making these information more interacted by our dual-graph interaction scenario is more powerful . Not surprisingly , the uni-graph model removing the passage encoder ( i.e . , less focusing on context dependencies between sentences from input passage ) , and the uni-heads model discarding our answer-aware attention mechanism ( i.e . , less focusing on given answers ) get significant worse performance compared with our full model . Besides, our coarse-to-fine scenario helps to better deal with the dependencies between questions since there are widespread coreferences. Finally , although the architecture of non-auto model is a special case of our model where each group only contains a single question , the performance drops significantly , indicating the importance of using semi-autoregressive generation . However , the dualgraph interaction still makes its performance better than the Seq2seq and CopyNet in Table 2 . 6.2 Running Examples In Table 6 , we present some generated examples comparing our model and the strongest baseline CorefNet . On the one hand , our model performs better than CorefNet , especially that the output questions are more targeting on given answers ( turn 2 , 6 , 7 ) . It also correctly deals with coreferences ( e.g., distinguishing “Peter” and “Sammie” ). On the other hand, the generated questions have poor quality when gold standard questions involve more reasoning ( turn 2, 6 ). Besides, the gold standard questions are more concise as well ( turn 4, 6 ). 7 Conclusion In this paper, we focus on SQG which is an important yet challenging task. Different from prior works regarding SQG as a dialog generation task, we propose the first semi-autoregressive SQG model, which divides questions into different groups and further generates each group of closely-related questions in parallel. During this process, we first build a passage-info graph, an answer-info graph, and then perform dual-graph interaction to get representations capturing the context dependencies between passages and questions. These representations are further used during our coarse-to-fine generation process. To perform experiments, we analyze the limitation of existing datasets and create the first dataset specially used for SQG containing 81.9K questions. Experimental results show that our model outperforms previous works by a substantial margin. For future works, the major challenge is generating more meaningful, informative but concise questions. Besides, more powerful question clustering and coarse-to-fine generation scenarios are also worth exploration. Finally, performing SQG on other types of inputs, e.g., images and knowledge graphs, is an interesting topic. "
            }
        },
        {
            "id": "2020.acl-main.25",
            "result": [
                {
                    "value": {
                        "start": 4513,
                        "end": 4516,
                        "text": "GPT",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.25:E0"
                },
                {
                    "value": {
                        "start": 4519,
                        "end": 4526,
                        "text": "GPT - 2",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.25:E1"
                },
                {
                    "value": {
                        "start": 4563,
                        "end": 4567,
                        "text": "CTRL",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.25:E2"
                },
                {
                    "value": {
                        "start": 4598,
                        "end": 4604,
                        "text": "Grover",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.25:E3"
                },
                {
                    "value": {
                        "start": 4743,
                        "end": 4765,
                        "text": "auto-regressive models",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.25:E4"
                },
                {
                    "value": {
                        "start": 5084,
                        "end": 5088,
                        "text": "GLTR",
                        "labels": [
                            "Tool"
                        ]
                    },
                    "id": "2020.acl-main.25:E5"
                },
                {
                    "value": {
                        "start": 5227,
                        "end": 5246,
                        "text": "energy-based models",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.25:E6"
                },
                {
                    "value": {
                        "start": 8620,
                        "end": 8634,
                        "text": "self-attention",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.25:E7"
                },
                {
                    "value": {
                        "start": 8899,
                        "end": 8902,
                        "text": "BoW",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.25:E8"
                },
                {
                    "value": {
                        "start": 8925,
                        "end": 8945,
                        "text": "bag-of-words ( BoW )",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.25:E9"
                },
                {
                    "value": {
                        "start": 8905,
                        "end": 8911,
                        "text": "Linear",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.25:E10"
                },
                {
                    "value": {
                        "start": 9091,
                        "end": 9102,
                        "text": "BoW ( MLP )",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.25:E11"
                },
                {
                    "value": {
                        "start": 9097,
                        "end": 9100,
                        "text": "MLP",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.25:E12"
                },
                {
                    "value": {
                        "start": 9280,
                        "end": 9287,
                        "text": "ConvNet",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.25:E13"
                },
                {
                    "value": {
                        "start": 9473,
                        "end": 9477,
                        "text": "LSTM",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.25:E14"
                },
                {
                    "value": {
                        "start": 9495,
                        "end": 9504,
                        "text": "CNN model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.25:E15"
                },
                {
                    "value": {
                        "start": 9624,
                        "end": 9635,
                        "text": "Transformer",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.25:E16"
                },
                {
                    "value": {
                        "start": 9732,
                        "end": 9757,
                        "text": "multi-head self-attention",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.25:E17"
                },
                {
                    "value": {
                        "start": 9872,
                        "end": 9899,
                        "text": "CNN / Dailymail news corpus",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "2020.acl-main.25:E18"
                },
                {
                    "value": {
                        "start": 10023,
                        "end": 10045,
                        "text": "CNN / Dailymail corpus",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "2020.acl-main.25:E19"
                },
                {
                    "value": {
                        "start": 11023,
                        "end": 11031,
                        "text": "accuracy",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "2020.acl-main.25:E20"
                },
                {
                    "value": {
                        "start": 13287,
                        "end": 13307,
                        "text": "BoW ( bag of words )",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.25:E21"
                },
                {
                    "value": {
                        "start": 13568,
                        "end": 13580,
                        "text": "Transformers",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "2020.acl-main.25:E22"
                }
            ],
            "data": {
                "text": "Reverse Engineering Configurations of Neural Text Generation Models Abstract This paper seeks to develop a deeper understanding of the fundamental properties of neural text generations models. The study of artifacts that emerge in machine generated text as a result of modeling choices is a nascent research area. Previously, the extent and degree to which these artifacts surface in generated text has not been well studied. In the spirit of better understanding generative text models and their artifacts, we propose the new task of distinguishing which of several variants of a given model generated a piece of text, and we conduct an extensive suite of diagnostic tests to observe whether modeling choices ( e.g., sampling methods, top-k probabilities, model architectures, etc. ) leave detectable artifacts in the text they generate. Our key finding, which is backed by a rigorous set of experiments, is that such artifacts are present and that different modeling choices can be inferred by observing the generated text alone. This suggests that neural text generators may be more sensitive to various modeling choices than previously thought. 1 Introduction The task of generating plausible sounding text from large generative neural networks has garnered significant attention recently ( Zellers et al., 2019; Radford et al., 2019; Keskar et al., 2019 ). The study of these models has been a keen area of interest for many, resulting in research pertaining to the behavior of generation methods ( Holtzman et al., 2019; Fan et al., 2018; Gu et al., 2017 ) as well as modeling techniques ( Radford et al., 2019; Welleck et al., 2019; Dai et al., 2019; Radford et al., 2018 ). This paper presents a focused empirical study of text generation artifacts, i.e., detectable ‘signatures’ that originate from certain modeling or decoding choices. There is a growing body of research that has focused on discriminating between human and machine generated texts ( Gehrmann et al., 2019; Bakhtin et al., 2019; Ippolito et al., 2019 ). There is also extensive past research on authorship attribution ( Sanderson and Guenter, 2006; Stamatatos, 2009; Stamatatos et al., 2018 ), for which it was always assumed that the authors were humans. This work takes a much more fine-grained approach by learning to distinguish between text generated by different machine variants. Do certain modeling choices leave more artifacts than others? In short, given a piece of generated text, can we determine the model configuration that generated this text? The utility of our study manifests in multiple ways. First, the unraveling of artifacts in generated text enables better understanding of neural text generators, revealing potential fundamental weaknesses in modeling or generation schemes. Our study provides relative comparisons of the extent to which artifacts emerge from different modeling choices. Second, this research advances tracking the provenance and origination of machine generated texts, which has a range of useful applications pertaining to online trust and safety, thereby helping to mitigate the overall risk of these models in the wild. To the best of our knowledge, this is the first systematic and fine-grained study of detectable artifacts present in neural generated text. Our contributions The overall contributions of this work can be summarized as follows: We present a largescale analysis of generated text with a special focus on studying artifacts produced by large generative models. We propose the new task of distinguishing between different fine-grained configurations based on the generated text alone. The key idea is that classifiers performing better than random can capture configurationspecific artifacts. Our findings show that ( 1 ) modeling choices can be captured by simple classifiers through artifacts that are present in generated text alone, ( 2 ) the ease of prediction varies across different hyperparameter configurations, ( 3 ) word order is not that important in unraveling artifacts, i.e., artifacts are probably more related to word choice than syntax and composition and ( 4 ) distinguishing between model variants is much harder than predicting between human-or-machine only. 2 Related Work There are many research efforts related to machine generated text. The work in this area can be characterized into two broad categories - ( 1 ) learning to generate better text and ( 2 ) learning to mitigate against generated text. In the former , large generative models such as GPT / GPT - 2 ( Radford et al . , 2018 , 2019 ) , CTRL ( Keskar et al . , 2019 ) and Grover ( Welleck et al . , 2019 ) have recently demonstrated the possibility of generating high quality text . The study of sampling methods for auto-regressive models has also been active where sampling methods such as top-k ( Fan et al . , 2018 ) and nucleus sampling ( Holtzman et al . , 2019 ) have been proposed . Likewise, there have also been recent ongoing efforts that are targeted at distinguishing human text from machine generated text. ( Gehrmann et al . , 2019 ) proposed GLTR , a visual and statistical tool for aiding the detection of machine generated text . In a similar vein, ( Bakhtin et al., 2019 ) proposed energy-based models. Statistical detection of machine generated text is possible largely due to the the presence of artifacts. To this end, the race between generators and discriminators is not entirely de-coupled. ( Welleck et al., 2019 ) showed that a good generator is also a good discriminator. Concurrent work ( Ippolito et al., 2019 ) investigates the performance of human raters on the task of detecting machine generated text. Similarly, they also investigate the effect of model hyperparameters with respect to the ease of being detected by human raters. Our work is also related to the field of authorship attribution ( Stamatatos, 2009 ) which tries to identify the author behind a piece of text. A series of shared tasks have been proposed over the years ( Stamatatos et al., 2018; Tschuggnall et al., 2017 ). The tasks have primarily focused on stylometry and text-based forensics. A key assumption is that authors leave behind distinguishable signatures ( or artifacts ) in their writings. Along a similar vein, our work re-imagines this task by considering different instances of generative models as authors. The emergence of artifacts left behind by machine generated text is a peculiar and interesting phenomena. This work takes this direction further by studying the fine-grained artifacts produced by different modeling choices in hopes of better understanding machine generation in general. 3 Methodology In this section, we introduce our experimental settings and setup. 3.1 Generative Model Configuration Our experiments employ Grover ( Zellers et al . , 2019 ) as the text generator . We consider three generation configurations in our experiments. They are described as follows: Model Sizes - Generative models often come with pre-defined sizes that refer to the layer widths and parameterization. For Grover , the model size options include Base , Large , and Mega . Sampling Method - The sampling function controls the decoding process used to generate text. We explore variants of top-k ( Fan et al . , 2018 ) , top-p nucleus sampling ( Holtzman et al . , 2019 ) , and associated p / k values . Conditioning - Length of initial article conditioning. We define ` which is the amount of text given to the model. The initial ` tokens is concatenated at the end of the title sequence for the model to start generating. In the design of our experiments, while there are countless possibilities to search for, we deliberately sought out settings that are most general and/or are considered fine-grained subtle changes. Such subtle changes are likely to be more challenging to detect compared to larger changes. For example , predicting Grover parameterization subsumes the task of distinguishing Grover versus GPT - 2 . We assume that if a model is able to solve the former, the latter becomes relatively trivial. 3.2 Classifier Models We train a classifier model to discriminate between different model configurations. Generally, the task is framed as a multi-class classification problem where each model configuration is a class that is predicted. Models accept a sequence of tokens as an input. Sequences pass through a parameterized or non-parameterized encoder which are finally passed as input to a softmax classification layer. In this work , we explore and benchmark the effectiveness of various encoding inductive biases such as recurrent , convolutional , and self-attention based models . This is primarily motivated as a probe into the problem domain, i.e., by witnessing the behaviour of different encoder architectures, we may learn more about the nature of these tasks/datasets. We consider the following encoding architectures ( 1 ) BoW ( Linear ) - a simple bag-of-words ( BoW ) baseline that averages the word embeddings and passes the average representation into a single linear classifier . Y = Softmax( W ( X ) ). ( 2 ) BoW ( MLP ) - another simple baseline that builds on top of the Linear baseline . We add a single nonlinear layer with ReLU activation function, i.e., Y = Softmax( W2σr( W1( X ) ) ). ( 3 ) ConvNet - We consider a 1D Convolution layer of filter width 3 . We convolve over the input embeddings and pass the average ( representation ) into a linear Softmax classification layer. ( 4 ) LSTM - Similar to the CNN model , we encode the input sequence with an LSTM layer and pass the mean-pooled representation into a Softmax layer . ( 4 ) Transformer Encoders - We use 4 - layered multi-headed Transformer ( Vaswani et al . , 2017 ) encoders with multi-head self-attention . 3.3 Experimental Setup This section outlines our experimental setup. News Corpora As a seed corpus , we use the CNN / Dailymail news corpus . This corpus is widely used in other NLP tasks ( Hermann et al., 2015 ) such as question answering and summarization. The CNN / Dailymail corpus comprises approximately 90K news articles . Given an initial seed corpora of N news articles, we generate an additional collection of N machine generated articles for each configuration. Tasks We define ten tasks as described in Table 1. These tasks aim at predicting the correct model configuration given the generated text. For all tasks, we use a maximum sequence length of 500 and split the dataset into 80%/10%/10% train, development, and testing splits. We include an additional variant +h which denotes that we add the humanwritten article as an additional class to the mix. Model Training For all models, we fix the word embeddings to d = 64. Embeddings are trained from scratch. All encoder hidden unit size is also set to 64. We tuned the dimensions of models in the range of d ∈ {16, 32, 64, 128, 256} and found no noticable improvement beyond d = 64. We train all models for 50 epochs with a batch size of 64. We employ early stopping with patience 3 if validation accuracy does not improve . Final test accuracy is reported based on the best results on the validation set . 4 Insights and Findings This section presents the insights and findings uncovered by our experiments. Table 2 and Table 3 present the core of our experimental results. ( 1 ) Artifacts are found. Our experiments show that simple classifiers are able to distinguish finegrained and subtle differences between modeling choices ( e.g., top-p probabilities or condition length ` ) in generated texts. In Table 2 , we observe that all classifiers have an accuracy much higher than random chance ( almost double in some cases ) , which suggests that distinguishing between different classes is relatively straightforward . In short, we are able to empirically conclude that all modeling choices leave behind some form of detectable artifacts. ( 2 ) Different generating choices leave behind different amounts of artifacts. From Table 2, the difficulty of each task generally depends on the specific modeling choice. For example, distinguishing between model size ( S1 ) is much harder than the top-p value. Overall, we observe that methods that directly operate at the generation level ( sampling p or k values ) are much easier to predict ( i.e., leave more artifacts ) than condition length ( C1, C2 ) or model size ( S1 ). It is a somewhat surprising result that varying the initial condition length leaves artifacts in the generated text. A secondary finding is that discriminating p or k values that are close together is a significantly more challenging task than those that are far apart ( i.e., task P1 vs P2 ). This empirically shows that generated text moves along some form of ordering and magnitude , i.e . , s ( a , b ) ≤ s ( b , c ) if a − b > b − c where a , b , c ∈ R and s ( x , y ) is the accuracy score obtained by classifying between configurations x , y . ( 3 ) Word order does not matter too much. The key observation when pitting various sequence encoding inductive biases against each other is to observe if modeling sequential interactions ( shortterm or long-range dependencies ) and/or word order helps in any of the MCD tasks. The observation is that most complex encoders that takes into account word order do not outperform simple BoW ( bag of words ) with linear classifiers . This suggests that artifacts found in the text are mostly related to style ( e.g., word choices ), as opposed to compositional dependencies ( e.g., word order ). Occasionally , we observe some marginal gains when utilizing ConvNet or Transformers . We hypothesize that considering some amount of token interaction is indeed useful, albeit very marginally. Moreover , the recurrent model ( LSTM ) performs worse in most cases , suggesting that complex compositional relations are not necessary to capture artifacts . ( 4 ) Discriminating between machines is harder than human and machine. Table 3 report the results of MCD tasks with an additional human article class. By adding human generated articles into the mix , the classification accuracy increases ( ≈ 10% ) across all tasks . Upon inspection , we find that the model separates the human written articles at beyond 90% accuracy , which leads to an overall increase in performance . Hence, the task of distinguishing between machine-machine text is much harder than distinguishing between human-machine text. 5 Discussion This section discusses the implications of our results and findings. ( 1 ) The sensitivity of neural text generation models emerge as artifacts in the generated text. Our results show that a state-of-the-art text generation model produces significant amounts of artifacts even when making small hyperparameter changes ( such as sampling probabilities ). It is also relatively surprising that the amount of article conditioning and model size can also be predicted to a certain degree. We feel that this might arise from limitations in the design of neural generation models which may warrant further study. ( 2 ) Tracing the provenance and origination of text generation models is easier than expected. Given that minor changes to decoding settings leave distinguishable signatures, we hypothesize that it is relatively easy to trace and cluster content produced by specific generative models. 6 Conclusion We studied machine generated text and found that modeling choices leave artifacts, i.e., it is possible to predict modeling choices such as parameterization/sampling choices by looking at generated text alone. We proposed the novel task of machine configuration detection ( MCD ) which aided in the discovery of these artifacts. We believe our work paves the way for better understanding of neural text generation models and understanding that modeling choices reveals the model configurations is a first crucial step. "
            }
        },
        {
            "id": "P08-1019",
            "result": [
                {
                    "value": {
                        "start": 620,
                        "end": 643,
                        "text": "MDLbased tree cut model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E0"
                },
                {
                    "value": {
                        "start": 873,
                        "end": 899,
                        "text": "Vector Space Model ( VSM )",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E1"
                },
                {
                    "value": {
                        "start": 904,
                        "end": 953,
                        "text": "Language Model for Information Retrieval ( LMIR )",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E2"
                },
                {
                    "value": {
                        "start": 2085,
                        "end": 2103,
                        "text": "vector space model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E3"
                },
                {
                    "value": {
                        "start": 2106,
                        "end": 2111,
                        "text": "Okapi",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E4"
                },
                {
                    "value": {
                        "start": 552,
                        "end": 566,
                        "text": "language model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E5"
                },
                {
                    "value": {
                        "start": 2135,
                        "end": 2158,
                        "text": "translation-based model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E6"
                },
                {
                    "value": {
                        "start": 26470,
                        "end": 26494,
                        "text": "community-based Q&A data",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P08-1019:E7"
                },
                {
                    "value": {
                        "start": 26651,
                        "end": 26672,
                        "text": "language model ( LM )",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E8"
                },
                {
                    "value": {
                        "start": 3734,
                        "end": 3789,
                        "text": "MDL-based ( Minimum Description Length ) tree cut model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E9"
                },
                {
                    "value": {
                        "start": 894,
                        "end": 897,
                        "text": "VSM",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E10"
                },
                {
                    "value": {
                        "start": 947,
                        "end": 951,
                        "text": "LMIR",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E11"
                },
                {
                    "value": {
                        "start": 5856,
                        "end": 5880,
                        "text": "MDL-based tree cut model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E12"
                },
                {
                    "value": {
                        "start": 629,
                        "end": 643,
                        "text": "tree cut model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E13"
                },
                {
                    "value": {
                        "start": 629,
                        "end": 637,
                        "text": "tree cut",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E14"
                },
                {
                    "value": {
                        "start": 6968,
                        "end": 6971,
                        "text": "MLE",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E15"
                },
                {
                    "value": {
                        "start": 9535,
                        "end": 9547,
                        "text": "bag-of-words",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E16"
                },
                {
                    "value": {
                        "start": 9830,
                        "end": 9838,
                        "text": "WH-ngram",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E17"
                },
                {
                    "value": {
                        "start": 9833,
                        "end": 9838,
                        "text": "ngram",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E18"
                },
                {
                    "value": {
                        "start": 15170,
                        "end": 15188,
                        "text": "multinomial models",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E19"
                },
                {
                    "value": {
                        "start": 15246,
                        "end": 15278,
                        "text": "unigram document language models",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E20"
                },
                {
                    "value": {
                        "start": 15549,
                        "end": 15563,
                        "text": "MLE estimators",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E21"
                },
                {
                    "value": {
                        "start": 16574,
                        "end": 16581,
                        "text": "TRL-TST",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P08-1019:E22"
                },
                {
                    "value": {
                        "start": 16634,
                        "end": 16640,
                        "text": "CI-TST",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P08-1019:E23"
                },
                {
                    "value": {
                        "start": 17732,
                        "end": 17791,
                        "text": "LMIR ( language modeling method for information retrieval )",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E24"
                },
                {
                    "value": {
                        "start": 17916,
                        "end": 17919,
                        "text": "MAP",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P08-1019:E25"
                },
                {
                    "value": {
                        "start": 17922,
                        "end": 17933,
                        "text": "R-precision",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P08-1019:E26"
                },
                {
                    "value": {
                        "start": 17940,
                        "end": 17943,
                        "text": "MRR",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P08-1019:E27"
                },
                {
                    "value": {
                        "start": 4675,
                        "end": 4690,
                        "text": "Yahoo ! Answers",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P08-1019:E28"
                },
                {
                    "value": {
                        "start": 18748,
                        "end": 18771,
                        "text": "5-fold cross-validation",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E29"
                },
                {
                    "value": {
                        "start": 19069,
                        "end": 19076,
                        "text": "LMIRCUT",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E30"
                },
                {
                    "value": {
                        "start": 21156,
                        "end": 21164,
                        "text": "LMIR-CUT",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E31"
                },
                {
                    "value": {
                        "start": 21537,
                        "end": 21541,
                        "text": "PVSM",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E32"
                },
                {
                    "value": {
                        "start": 19369,
                        "end": 19375,
                        "text": "t-test",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E33"
                },
                {
                    "value": {
                        "start": 14736,
                        "end": 14749,
                        "text": "mixture model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E34"
                },
                {
                    "value": {
                        "start": 24698,
                        "end": 24709,
                        "text": "IBM model 1",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E35"
                },
                {
                    "value": {
                        "start": 25043,
                        "end": 25050,
                        "text": "SMT-CUT",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E36"
                },
                {
                    "value": {
                        "start": 25651,
                        "end": 25670,
                        "text": "vector space models",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E37"
                },
                {
                    "value": {
                        "start": 25724,
                        "end": 25731,
                        "text": "WordNet",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P08-1019:E38"
                },
                {
                    "value": {
                        "start": 25825,
                        "end": 25861,
                        "text": "template based FAQ retrieval systems",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P08-1019:E39"
                }
            ],
            "data": {
                "text": "Searching Questions by Identifying Question Topic and Question Focus Abstract This paper is concerned with the problem of question search . In question search , given a question as query , we are to return questions semantically equivalent or close to the queried question . In this paper , we propose to conduct question search by identifying question topic and question focus . More specifically , we first summarize questions in a data structure consisting of question topic and question focus . Then we model question topic and question focus in a language modeling framework for search . We also propose to use the MDLbased tree cut model for identifying question topic and question focus automatically . Experimental results indicate that our approach of identifying question topic and question focus for search significantly outperforms the baseline methods such as Vector Space Model ( VSM ) and Language Model for Information Retrieval ( LMIR ) . 1 Introduction Over the past few years , online services have been building up very large archives of questions and their answers , for example , traditional FAQ services and emerging community-based Q&A services ( e.g. , Yahoo! Answers1 , Live QnA2 , and Baidu Zhidao3 ) . To make use of the large archives of questions and their answers , it is critical to have functionality facilitating users to search previous answers . Typically , such functionality is achieved by first retrieving questions expected to have the same answers as a queried question and then returning the related answers to users . For example , given question Q1 in Table 1 , question Q2 can be re turned and its answer will then be used to answer Q1 because the answer of Q2 is expected to partially satisfy the queried question Q1 . This is what we called question search . In question search , returned questions are semantically equivalent or close to the queried question . Many methods have been investigated for tackling the problem of question search . For example , Jeon et al . have compared the uses of four different retrieval methods , i.e . vector space model , Okapi , language model , and translation-based model , within the setting of question search ( Jeon et al . , 2005b ) . However , all the existing methods treat questions just as plain texts ( without considering question structure ) . For example , obviously , Q2 can be considered semantically closer to Q1 than Q3-Q5 although all questions ( Q2-Q5 ) are related to Q1 . The existing methods are not able to tell the difference between question Q2 and questions Q3 , Q4 , and Q5 in terms of their relevance to question Q1 . We will clarify this in the following . In this paper , we propose to conduct question search by identifying question topic and question focus . The question topic usually represents the major context/constraint of a question ( e.g. , Berlin , Hamburg ) which characterizes users interests . In contrast , question focus ( e.g. , cool club , cheap hotel ) presents certain aspect ( or descriptive features ) of the question topic . For the aim of retrieving semantically equivalent ( or close ) questions , we need to assure that returned questions are related to the queried question with respect to both question topic and question focus . For example , in Table 1 , Q2 preserves certain useful information of Q1 in the aspects of both question topic ( Berlin ) and question focus ( fun club ) although it loses some useful information in question topic ( Hamburg ) . In contrast , questions Q3-Q5 are not related to Q1 in question focus ( although being related in question topic , e.g. Hamburg , Berlin ) , which makes them unsuitable as the results of question search . We also propose to use the MDL-based ( Minimum Description Length ) tree cut model for automatically identifying question topic and question focus . Given a question as query , a structure called question tree is constructed over the question collection including the queried question and all the related questions , and then the MDL principle is applied to find a cut of the question tree specifying the question topic and the question focus of each question . In a summary , we summarize questions in a data structure consisting of question topic and question focus . On the basis of this , we then propose to model question topic and question focus in a language modeling framework for search . To the best of our knowledge , none of the existing studies addressed question search by modeling both question topic and question focus . We empirically conduct the question search with questions about travel and computers & internet . Both kinds of questions are from Yahoo ! Answers . Experimental results show that our approach can significantly improve traditional methods ( e.g . VSM , LMIR ) in retrieving relevant questions . The rest of the paper is organized as follow . In Section 2 , we present our approach to question search which is based on identifying question topic and question focus . In Section 3 , we empirically verify the effectiveness of our approach to question search . In Section 4 , we employ a translation-based retrieval framework for extending our approach to fix the issue called lexical chasm . Section 5 surveys the related work . Section 6 concludes the paper by summarizing our work and discussing the future directions . 2 Our Approach to Question Search Our approach to question search consists of two steps : ( a ) summarize questions in a data structure consisting of question topic and question focus ; ( b ) model question topic and question focus in a language modeling framework for search . In the step ( a ) , we employ the MDL-based ( Minimum Description Length ) tree cut model for automatically identifying question topic and question focus . Thus , this section will begin with a brief review of the MDL-based tree cut model and then follow that by an explanation of steps ( a ) and ( b ) . 2.1 The MDL-based tree cut model Formally , a tree cut model ( Li and Abe , 1998 ) can be represented by a pair consisting of a tree cut , and a probability parameter vector of the same length , that is , where and are where , , . . . are classes determined by a cut in the tree and 1 any set of nodes in the tree that defines a partition of all the nodes , viewing each node as representing the set of child nodes as well as itself . For example , the cut indicated by the dash line in Figure 1 corresponds to three classes : , , , , and , , , . A straightforward way for determining a cut of a tree is to collapse the nodes of less frequency into their parent nodes . However , the method is too heuristic for it relies much on manually tuned frequency threshold . In our practice , we turn to use a theoretically well-motivated method based on the MDL principle . MDL is a principle of data compression and statistical estimation from information theory ( Rissanen , 1978 ) . Given a sample and a tree cut , we employ MLE to estimate the parameters of the corresponding tree cut model , , where denotes the estimated parameters . According to the MDL principle , the description length ( Li and Abe , 1998 ) , of the tree cut model and the sample is the sum of the model description length L ( r ) , the parameter description length L ( B | r ) , and the data description length L ( S | F , B ) , i.e . . The model description length L ( r ) is a subjective quantity which depends on the coding scheme employed . Here , we simply assume that each tree cut model is equally likely a priori . The parameter description length L ( B | r ) is calculated as where | S | denotes the sample size and k denotes the number of free parameters in the tree cut model , i.e . k equals the number of nodes in r minus one . The data description length L ( S | F , B ) is calculated as where where C is the class that n belongs to and f ( C ) denotes the total frequency of instances in class C in the sample S. With the description length defined as ( 3 ) , we wish to select a tree cut model with the minimum description length and output it as the result . Note that the model description length L ( r ) can be ignored because it is the same for all tree cut models . The MDL-based tree cut model was originally introduced for handling the problem of generalizing case frames using a thesaurus ( Li and Abe , 1998 ) . To the best of our knowledge , no existing work utilizes it for question search . This may be partially because of the unavailability of the resources ( e.g. , thesaurus ) which can be used for embodying the questions in a tree structure . In Section 2.2 , we will introduce a tree structure called question tree for representing questions . 2.2 Identifying question topic and question focus In principle , it is possible to identify question topic and question focus of a question by only parsing the question itself ( for example , utilizing a syntactic parser ) . However , such a method requires accurate parsing results which can not be obtained from the noisy data from online services . Instead , we propose using the MDL-based tree cut model which identifies question topics and question foci for a set of questions together . More specifically , the method consists of two phases : 2.2.1 Constructing a question tree In the following , with a series of definitions , we will describe how a question tree is constructed from a collection of questions . Lets begin with explaining the representation of a question . A straightforward method is to represent a question as a bag-of-words ( possibly ignoring stop words ) . However , this method can not discern the hotels in Paris from the Paris hotel . Thus , we turn to use the linguistic units carrying on more semantic information . Specifically , we make use of two kinds of units : BaseNP ( Base Noun Phrase ) and WH-ngram . A BaseNP is defined as a simple and non-recursive noun phrase ( Cao and Li , 2002 ) . A WH-ngram is an ngram beginning with WH-words . The WH-words that we consider include when , what , where , which , and how . We refer to these two kinds of units as topic terms . With topic terms , we represent a question as a topic chain and a set of questions as a question tree . Definition 1 ( Topic Profile ) The topic profile Bt of a topic term t in a categorized question collection is a probability distribution of categories [ p ( c | t ) ] cEc where C is a set of categories . where count ( c , t ) is the frequency of the topic term t within category c . Clearly , we have ZcEcp ( c | t ) = 1 . By categorized questions , we refer to the questions that are organized in a tree of taxonomy . For example , at Yahoo! Answers , the question How do I install my wireless router is categorized as Computers & Internet Computer Networking . Actually , we can find categorized questions at other online services such as FAQ sites , too . Definition 2 ( Specificity ) The specificity s ( t ) of a topic term t is the inverse of the entropy of the topic profile Bt . More specifically , where 8 is a smoothing parameter used to cope with the topic terms whose entropy is 0 . In our experiments , the value of 8 was set 0.001 . We use the term specificity to denote how specific a topic term is in characterizing information needs of users who post questions . A topic term of high specificity ( e.g. , Hamburg , Berlin ) usually specifies the question topic corresponding to the main context of a question because it tends to occur only in a few categories . A topic term of low specificity is usually used to represent the question focus ( e.g. , cool club , where to see ) which is relatively volatile and might occur in many categories . Definition 3 ( Topic Chain ) A topic chain qc of a question q is a sequence of ordered topic terms For example , the topic chain of any cool clubs in Berlin or Hamburg ? is Hamburg Berlin cool club because the specificities for Hamburg , Berlin , and cool club are 0.99 , 0.62 , and 0.36 . Definition 4 ( Question Tree ) A question tree of a question set Q = [ q is a prefix tree built over the topic chains Qc = [ q c of the question set Q. Clearly , if a question set contains only one question , its question tree will be exactly same as the topic chain of the question . Note that the root node of a question tree is associated with empty string as the definition of prefix tree requires ( Fredkin , 1960 ) . Given the topic chains with respect to the questions in Table 1 as follow , 2.2.2 Determining the tree cut According to the definition of a topic chain , the topic terms in a topic chain of a question are ordered by their specificity values . Thus , a cut of a topic chain naturally separates the topic terms of low specificity ( representing question focus ) from the topic terms of high specificity ( representing question topic ) . Given a topic chain of a question consisting of topic terms , there exist ( 1 ) possible cuts . The question is : which cut is the best ? We propose using the MDL-based tree cut model for the search of the best cut in a topic chain . Instead of dealing with each topic chain individually , the proposed method handles a set of questions together . Specifically , given a queried question , we construct a question tree consisting of both the queried question and the related questions , and then apply the MDL principle to select the best cut of the question tree . For example , in Figure 2 , we hope to get the cut indicated by the dashed line . The topic terms on the left of the dashed line represent the question topic and those on the right of the dashed line represent the question focus . Note that the tree cut yields a cut for each individual topic chain ( each path ) within the question tree accordingly . A cut of a topic chain qc of a question q separates the topic chain in two parts : HEAD and TAIL . HEAD ( denoted as ( qc ) ) is the subsequence of the original topic chain qc before the cut . TAIL ( denoted as ( qc ) ) is the subsequence of qc after the cut . Thus , qc = ( qc ) ( qc ) . For instance , given the tree cut specified in Figure 2 , for the topic chain of Q1 Hamburg Berlin cool club , the HEAD and TAIL are Hamburg Berlin and cool club respectively . 2.3 Modeling question topic and question focus for search We employ the framework of language modeling ( for information retrieval ) to develop our approach to question search . In the language modeling approach to information retrieval , the relevance of a targeted question q to a queried question q is given by the probability ( q | q ) of generating the queried question q from the language model formed by the targeted question . The targeted question is from a collection of questions . Following the framework , we propose a mixture model for modeling question structure ( namely , question topic and question focus ) within the process of searching questions : In the mixture model , it is assumed that the process of generating question topics and the process of generating question foci are independent from each other . In traditional language modeling , a single multinomial model | over terms is estimated for each targeted question . In our case , two multinomial models and need to be estimated for each targeted question . If unigram document language models are used , the equation ( 9 ) can then be re-written as , where , is the frequency of within . To avoid zero probabilities and estimate more accurate language models , the HEAD and TAIL of questions are smoothed using background collection , where | , | , and | are the MLE estimators with respect to the HEAD of , the TAIL of , and the collection . 3 Experimental Results We have conducted experiments to verify the effectiveness of our approach to question search . Particularly , we have investigated the use of identifying question topic and question focus for search . 3.1 Dataset and evaluation measures We made use of the questions obtained from Yahoo ! Answers for the evaluation . More specifically , we utilized the resolved questions under two of the top-level categories at Yahoo ! Answers , namely travel and computers & internet . The questions include 314,616 items from the travel category and 210,785 items from the computers & internet category . Each resolved question consists of three fields : title , description , and answers . For search we use only the title field . It is assumed that the titles of the questions already provide enough semantic information for understanding users information needs . We developed two test sets , one for the category travel denoted as TRL-TST , and the other for computers & internet denoted as CI-TST . In order to create the test sets , we randomly selected 200 questions for each category . To obtain the ground-truth of question search , we employed the Vector Space Model ( VSM ) ( Salton et al . , 1975 ) to retrieve the top 20 results and obtained manual judgments . The top 20 results dont include the queried question itself . Given a returned result by VSM , an assessor is asked to label it with relevant or irrelevant . If a returned result is considered semantically equivalent ( or close ) to the queried question , the assessor will label it as relevant ; otherwise , the assessor will label it as irrelevant . Two assessors were involved in the manual judgments . Each of them was asked to label 100 questions from TRL-TST and 100 from CI-TST . In the process of manually judging questions , the assessors were presented only the titles of the questions ( for both the queried questions and the returned questions ) . Table 2 provides the statistics on the final test set . We utilized two baseline methods for demonstrating the effectiveness of our approach , the VSM and the LMIR ( language modeling method for information retrieval ) ( Ponte and Croft , 1998 ) . We made use of three measures for evaluating the results of question search methods . They are MAP , R-precision , and MRR . 3.2 Searching questions about ‘travel’ In the experiments , we made use of the questions about travel to test the performance of our approach to question search . More specifically , we used the 200 queries in the test set TRL-TST to search for relevant questions from the 314,616 questions categorized as travel . Note that only the questions occurring in the test set can be evaluated . We made use of the taxonomy of questions provided at Yahoo ! Answers for the calculation of specificity of topic terms . The taxonomy is organized in a tree structure . In the following experiments , we only utilized as the categories of questions the leaf nodes of the taxonomy tree ( regarding travel ) , which includes 355 categories . We randomly divided the test queries into five even subsets and conducted 5-fold cross-validation experiments . In each trial , we tuned the parameters A , a , and / 3 in the equation ( 10 ) - ( 12 ) with four of the five subsets and then applied it to one remaining subset . The experimental results reported below are those averaged over the five trials . In Table 3 , our approach denoted by LMIRCUT is implemented exactly as equation ( 10 ) . Neither VSM nor LMIR uses the data structure composed of question topic and question focus . From Table 3 , we see that our approach outperforms the baseline approaches VSM and LMIR in terms of all the measures . We conducted a significance test ( t-test ) on the improvements of our approach over VSM and LMIR . The result indicates that the improvements are statistically significant ( p-value < 0.05 ) in terms of all the evaluation measures . tion Focus In equation ( 9 ) , we use the parameter X to balance the contribution of question topic and the contribution of question focus . Figure 3 illustrates how influential the value of X is on the performance of question search in terms of MRR . The result was obtained with the 200 queries directly , instead of 5-fold cross-validation . From Figure 3 , we see that our approach performs best when X is around 0.7 . That is , our approach tends to emphasize question topic more than question focus . We also examined the correctness of question topics and question foci of the 200 queried questions . The question topics and question foci were obtained with the MDL-based tree cut model automatically . In the result , 69 questions have incorrect question topics or question foci . Further analysis shows that the errors came from two categories : ( a ) 59 questions have only the HEAD parts ( that is , none of the topic terms fall within the TAIL part ) , and ( b ) 10 have incorrect orders of topic terms because the specificities of topic terms were estimated inaccurately . For questions only having the HEAD parts , our approach ( equation ( 9 ) ) reduces to traditional language modeling approach . Thus , even when the errors of category ( a ) occur , our approach can still work not worse than the traditional language modeling approach . This also explains why our approach performs best when X is around 0.7 . The error category ( a ) pushes our model to emphasize more in question topic . Table 4 provides the TOP - 3 search results which are given by VSM , LMIR , and LMIR-CUT ( our approach ) respectively . The questions in bold are labeled as relevant in the evaluation set . The queried question seeks for the weather information about Alaska . Both VSM and LMIR rank certain irrelevant questions higher than relevant questions . The irrelevant questions are not about Alaska although they are about weather . The reason is that neither VSM nor PVSM is aware that the query consists of the two aspects weather ( how cold , winter ) and Alaska . In contrast , our approach assures that both aspects are matched . Note that the HEAD part of the topic chain of the queried question given by our approach is Alaska and the TAIL part is winter - + how cold . 3.3 Searching questions about ‘computers & internet’ In the experiments , we made use of the questions about computers & internet to test the performance of our proposed approach to question search . More specifically , we used the 200 queries in the test set CI-TST to search for relevant questions from the 210,785 questions categorized as computers & internet . For the calculation of specificity of topic terms , we utilized as the categories of questions the leaf nodes of the taxonomy tree regarding computers & Internet , which include 23 categories . We conducted 5-fold cross-validation for the parameter tuning . The experimental results reported in Table 5 are averaged over the five trials . Again , we see that our approach outperforms the baseline approaches VSM and LMIR in terms of all the measures . We conducted a significance test ( t-test ) on the improvements of our approach over VSM and LMIR . The result indicates that the improvements are statistically significant ( p-value < 0.05 ) in terms of all the evaluation measures . We also conducted the experiment similar to that in Figure 3 . Figure 4 provides the result . The trend is consistent with that in Figure 3 . We examined the correctness of ( automatically identified ) question topics and question foci of the 200 queried questions , too . In the result , 65 questions have incorrect question topics or question foci . Among them , 47 fall in the error category ( a ) and 18 in the error category ( b ) . The distribution of errors is also similar to that in Section 3.2 , which also justifies the trend presented in Figure 4 . tion Focus 4 Using Translation Probability In the setting of question search , besides the topic what we address in the previous sections , another research topic is to fix lexical chasm between questions . Sometimes , two questions that have the same meaning use very different wording . For example , the questions where to stay in Hamburg ? and the best hotel in Hamburg ? have almost the same meaning but are lexically different in question focus ( where to stay vs. best hotel ) . This is the socalled lexical chasm . Jeon and Bruce ( 2007 ) proposed a mixture model for fixing the lexical chasm between questions . The model is a combination of the language modeling approach ( for information retrieval ) and translation-based approach ( for information retrieval ) . Our idea of modeling question structure for search can naturally extend to Jeon et al. s model . More specifically , by using translation probabilities , we can rewrite equation ( 11 ) and ( 12 ) as follow : where Tr ( t | t ' ) denotes the probability that topic term t is the translation of t ' . In our experiments , to estimate the probability Tr ( t | t ' ) , we used the collections of question titles and question descriptions as the parallel corpus and the IBM model 1 ( Brown et al . , 1993 ) as the alignment model . Usually , users reiterate or paraphrase their questions ( already described in question titles ) in question descriptions . We utilized the new model elaborated by equation ( 13 ) and ( 14 ) for searching questions about travel and computers & internet . The new model is denoted as SMT-CUT . Table 6 provides the evaluation results . The evaluation was conducted with exactly the same setting as in Section 3 . From Table 6 , we see that the performance of our approach can be further boosted by using translation probability . 5 Related Work The major focus of previous research efforts on question search is to tackle the lexical chasm problem between questions . The research of question search is first conducted using FAQ data . FAQ Finder ( Burke et al . , 1997 ) heuristically combines statistical similarities and semantic similarities between questions to rank FAQs . Conventional vector space models are used to calculate the statistical similarity and WordNet ( Fellbaum , 1998 ) is used to estimate the semantic similarity . Sneiders ( 2002 ) proposed template based FAQ retrieval systems . Lai et al. ( 2002 ) proposed an approach to automatically mine FAQs from the Web . Jijkoun and Rijke ( 2005 ) used supervised learning methods to extend heuristic extraction of Q/A pairs from FAQ pages , and treated Q/A pair retrieval as a fielded search task . Harabagiu et al . ( 2005 ) used a Question Answer Database ( known as QUAB ) to support interactive question answering . They compared seven different similarity metrics for selecting related questions from QUAB and found that the conceptbased metric performed best . Recently , the research of question search has been further extended to the community-based Q&A data . For example , Jeon et al . ( Jeon et al . , 2005a ; Jeon et al . , 2005b ) compared four different retrieval methods , i.e . vector space model , Okapi , language model ( LM ) , and translation-based model , for automatically fixing the lexical chasm between questions of question search . They found that the translation-based model performed best . However , all the existing methods treat questions just as plain texts ( without considering question structure ) . In this paper , we proposed to conduct question search by identifying question topic and question focus . To the best of our knowledge , none of the existing studies addressed question search by modeling both question topic and question focus . Question answering ( e.g. , Pasca and Harabagiu , 2001 ; Echihabi and Marcu , 2003 ; Voorhees , 2004 ; Metzler and Croft , 2005 ) relates to question search . Question answering automatically extracts short answers for a relatively limited class of question types from document collections . In contrast to that , question search retrieves answers for an unlimited range of questions by focusing on finding semantically similar questions in an archive . 6 Conclusions and Future Work In this paper , we have proposed an approach to question search which models question topic and question focus in a language modeling framework . The contribution of this paper can be summarized in 4 - fold : ( 1 ) A data structure consisting of question topic and question focus was proposed for summarizing questions ; ( 2 ) The MDL-based tree cut model was employed to identify question topic and question focus automatically ; ( 3 ) A new form of language modeling using question topic and question focus was developed for question search ; ( 4 ) Extensive experiments have been conducted to evaluate the proposed approach using a large collection of real questions obtained from Yahoo ! Answers . Though we only utilize data from communitybased question answering service in our experiments , we could also use categorized questions from forum sites and FAQ sites . Thus , as future work , we will try to investigate the use of the proposed approach for other kinds of web services . "
            }
        },
        {
            "id": "P09-1023",
            "result": [
                {
                    "value": {
                        "start": 28,
                        "end": 37,
                        "text": "Wikipedia",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P09-1023:E0"
                },
                {
                    "value": {
                        "start": 546,
                        "end": 576,
                        "text": "document concept lattice model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P09-1023:E1"
                },
                {
                    "value": {
                        "start": 2486,
                        "end": 2493,
                        "text": "WordNet",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P09-1023:E2"
                },
                {
                    "value": {
                        "start": 2496,
                        "end": 2506,
                        "text": "Gazetteers",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P09-1023:E3"
                },
                {
                    "value": {
                        "start": 2511,
                        "end": 2518,
                        "text": "Googles",
                        "labels": [
                            "Tool"
                        ]
                    },
                    "id": "P09-1023:E4"
                },
                {
                    "value": {
                        "start": 3485,
                        "end": 3493,
                        "text": "WordNets",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P09-1023:E5"
                },
                {
                    "value": {
                        "start": 448,
                        "end": 461,
                        "text": "wiki concepts",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P09-1023:E6"
                },
                {
                    "value": {
                        "start": 6129,
                        "end": 6134,
                        "text": "ROUGE",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P09-1023:E7"
                },
                {
                    "value": {
                        "start": 6197,
                        "end": 6200,
                        "text": "DUC",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P09-1023:E8"
                },
                {
                    "value": {
                        "start": 6263,
                        "end": 6285,
                        "text": "WordNet-based approach",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P09-1023:E9"
                },
                {
                    "value": {
                        "start": 857,
                        "end": 864,
                        "text": "TREC-QA",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P09-1023:E10"
                },
                {
                    "value": {
                        "start": 11056,
                        "end": 11063,
                        "text": "Pyramid",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P09-1023:E11"
                },
                {
                    "value": {
                        "start": 14067,
                        "end": 14072,
                        "text": "TFIDF",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P09-1023:E12"
                },
                {
                    "value": {
                        "start": 14222,
                        "end": 14227,
                        "text": "CFIDF",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P09-1023:E13"
                },
                {
                    "value": {
                        "start": 14776,
                        "end": 14784,
                        "text": "DUC 2005",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P09-1023:E14"
                },
                {
                    "value": {
                        "start": 20302,
                        "end": 20334,
                        "text": "iterative reinforcement approach",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P09-1023:E15"
                },
                {
                    "value": {
                        "start": 14658,
                        "end": 14661,
                        "text": "DCL",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P09-1023:E16"
                },
                {
                    "value": {
                        "start": 21209,
                        "end": 21251,
                        "text": "extended document concept lattice ( EDCL )",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P09-1023:E17"
                },
                {
                    "value": {
                        "start": 17874,
                        "end": 17878,
                        "text": "EDCL",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P09-1023:E18"
                },
                {
                    "value": {
                        "start": 22582,
                        "end": 22593,
                        "text": "TRECQA task",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P09-1023:E19"
                },
                {
                    "value": {
                        "start": 19405,
                        "end": 19411,
                        "text": "TRECQA",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P09-1023:E20"
                },
                {
                    "value": {
                        "start": 820,
                        "end": 833,
                        "text": "wiki articles",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P09-1023:E21"
                },
                {
                    "value": {
                        "start": 22746,
                        "end": 22758,
                        "text": "TREC 12 - 14",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P09-1023:E22"
                },
                {
                    "value": {
                        "start": 23664,
                        "end": 23678,
                        "text": "TREC-QA corpus",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P09-1023:E23"
                },
                {
                    "value": {
                        "start": 23794,
                        "end": 23806,
                        "text": "wiki nuggets",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P09-1023:E24"
                },
                {
                    "value": {
                        "start": 24690,
                        "end": 24710,
                        "text": "TREC-Iiki collection",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P09-1023:E25"
                },
                {
                    "value": {
                        "start": 25440,
                        "end": 25455,
                        "text": "TREC-QA 12 - 14",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P09-1023:E26"
                },
                {
                    "value": {
                        "start": 25495,
                        "end": 25509,
                        "text": "AQUAINT corpus",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P09-1023:E27"
                },
                {
                    "value": {
                        "start": 25565,
                        "end": 25582,
                        "text": "TREC-QA questions",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P09-1023:E28"
                },
                {
                    "value": {
                        "start": 23748,
                        "end": 23760,
                        "text": "TREC nuggets",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P09-1023:E29"
                },
                {
                    "value": {
                        "start": 25495,
                        "end": 25502,
                        "text": "AQUAINT",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P09-1023:E30"
                },
                {
                    "value": {
                        "start": 26869,
                        "end": 26903,
                        "text": "MMR ( maximal marginal relevance )",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P09-1023:E31"
                },
                {
                    "value": {
                        "start": 188,
                        "end": 192,
                        "text": "wiki",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P09-1023:E32"
                },
                {
                    "value": {
                        "start": 28583,
                        "end": 28591,
                        "text": "accuracy",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P09-1023:E33"
                },
                {
                    "value": {
                        "start": 28616,
                        "end": 28635,
                        "text": "TRECWiki collection",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P09-1023:E34"
                },
                {
                    "value": {
                        "start": 28992,
                        "end": 29005,
                        "text": "recall ( NR )",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P09-1023:E35"
                },
                {
                    "value": {
                        "start": 29037,
                        "end": 29053,
                        "text": "precision ( NP )",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P09-1023:E36"
                },
                {
                    "value": {
                        "start": 29109,
                        "end": 29111,
                        "text": "F1",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P09-1023:E37"
                },
                {
                    "value": {
                        "start": 29116,
                        "end": 29118,
                        "text": "F3",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P09-1023:E38"
                },
                {
                    "value": {
                        "start": 29490,
                        "end": 29498,
                        "text": "F-scores",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P09-1023:E40"
                },
                {
                    "value": {
                        "start": 29585,
                        "end": 29596,
                        "text": "Cuis system",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P09-1023:E41"
                },
                {
                    "value": {
                        "start": 29626,
                        "end": 29646,
                        "text": "bigram soft patterns",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P09-1023:E42"
                },
                {
                    "value": {
                        "start": 29692,
                        "end": 29701,
                        "text": "TREC - 12",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P09-1023:E43"
                },
                {
                    "value": {
                        "start": 29716,
                        "end": 29723,
                        "text": "TREC 13",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P09-1023:E44"
                },
                {
                    "value": {
                        "start": 29037,
                        "end": 29046,
                        "text": "precision",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P09-1023:E45"
                },
                {
                    "value": {
                        "start": 29982,
                        "end": 30003,
                        "text": "soft-pattern approach",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P09-1023:E46"
                },
                {
                    "value": {
                        "start": 188,
                        "end": 200,
                        "text": "wiki article",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P09-1023:E47"
                },
                {
                    "value": {
                        "start": 30252,
                        "end": 30272,
                        "text": "TREC-Wiki collection",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P09-1023:E48"
                },
                {
                    "value": {
                        "start": 30466,
                        "end": 30473,
                        "text": "recalls",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P09-1023:E49"
                },
                {
                    "value": {
                        "start": 30478,
                        "end": 30488,
                        "text": "precisions",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P09-1023:E50"
                },
                {
                    "value": {
                        "start": 28992,
                        "end": 28998,
                        "text": "recall",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P09-1023:E51"
                }
            ],
            "data": {
                "text": "Summarizing Definition from Wikipedia Abstract Wikipedia provides a wealth of knowledge , where the first sentence , infobox ( and relevant sentences ) , and even the entire document of a wiki article could be considered as diverse versions of summaries ( definitions ) of the target topic . We explore how to generate a series of summaries with various lengths based on them . To obtain more reliable associations between sentences , we introduce wiki concepts according to the internal links in Wikipedia . In addition , we develop an extended document concept lattice model to combine wiki concepts and non-textual features such as the outline and infobox . The model can concatenate representative sentences from non-overlapping salient local topics for summary generation . We test our model based on our annotated wiki articles which topics come from TREC-QA 2004 - 2006 evaluations . The results show that the model is effective in summarization and definition QA . 1 Introduction Nowadays , ask Wikipedia has become as popular as Google it during Internet surfing , as Wikipedia is able to provide reliable information about the concept ( entity ) that the users want . As the largest online encyclopedia , Wikipedia assembles immense human knowledge from thousands of volunteer editors , and exhibits significant contributions to NLP problems such as semantic relatedness , word sense disambiguation and question answering ( QA ) . For a given definition query , many search engines ( e.g. , specified by define : in Google ) often place the first sentence of the corresponding wiki1 article at the top of the returned list . The use of one-sentence snippets provides a brief and concise description of the query . However , users often need more information beyond such a one-sentence definition , while feeling that the corresponding wiki article is too long . Thus , there is a strong demand to summarize wiki articles as definitions with various lengths to suite different user needs . The initial motivation of this investigation is to find better definition answer for TREC-QA task using Wikipedia ( Kor and Chua , 2007 ) . According to past results on TREC-QA ( Voorhees , 2004 ; Voorhees and Dang , 2005 ) , definition queries are usually recognized as being more difficult than factoid and list queries . Wikipedia could help to improve the quality of answer finding and even provide the answers directly . Its results are better than other external resources such as WordNet , Gazetteers and Googles define operator , especially for definition QA ( Lita et al . , 2004 ) . Different from the free text used in QA and summarization , a wiki article usually contains valuable information like infobox and wiki link . Infobox tabulates the key properties about the target , such as birth place/date and spouse for a person as well as type , founder and products for a company . Infobox , as a form of thumbnail biography , can be considered as a mini version of a wiki articles summary . In addition , the relevant concepts existing in a wiki article usually refer to other wiki pages by wiki internal links , which will form a close set of reference relations . The current Wikipedia recursively defines over 2 million concepts ( in English ) via wiki links . Most of these concepts are multiword terms , whereas WordNet has only 50,000 plus multi-word terms . Any term could appear in the definition of a concept if necessary , while the total vocabulary existing in WordNets glossary definition is less than 2000 . Wikipedia addresses explicit semantics for numerous concepts . These special knowledge representations will provide additional information for analysis and summarization . We thus need to extend existing summarization technologies to take advantage of the knowledge representations in Wikipedia . wiki ( pedia ) articles and on ( the ) Wikipedia , the latter referring to the entire Wikipedia . The goal of this investigation is to explore summaries with different lengths in Wikipedia . Our main contribution lies in developing a summarization method that can ( i ) explore more reliable associations between passages ( sentences ) in huge feature space represented by wiki concepts ; and ( ii ) effectively combine textual and non-textual features such as infobox and outline in Wikipedia to generate summaries as definition . The rest of this paper is organized as follows : In the next section , we discuss the background of summarization using both textual and structural features . Section 3 presents the extended document concept lattice model for summarizing wiki articles . Section 4 describes corpus construction and experiments are described ; while Section 5 concludes the paper . 2 Background Besides some heuristic rules such as sentence position and cue words , typical summarization systems measure the associations ( links ) between sentences by term repetitions ( e.g. , LexRank ( Erkan and Radev , 2004 ) ) . However , sophisticated authors usually utilize synonyms and paraphrases in various forms rather than simple term repetitions . Furnas et al. ( 1987 ) reported that two people choose the same main key word for a single well-known object less than 20 % of the time . A case study by Ye et al. ( 2007 ) showed that 61 different words existing in 8 relevant sentences could be mapped into 16 distinctive concepts by means of grouping terms with close semantic ( such as [ British , Britain , UK ] and [ war , fought , conflict , military ] ) . However , most existing summarization systems only consider the repeated words between sentences , where latent associations in terms of inter-word synonyms and paraphrases are ignored . The incomplete data likely lead to unreliable sentence ranking and selection for summary generation . To recover the hidden associations between sentences , Ye et al . ( 2007 ) compute the semantic similarity using WordNet . The term pairs with semantic similarity higher than a predefined threshold will be grouped together . They demonstrated that collecting more links between sentences will lead to better summarization as measured by ROUGE scores , and such systems were rated among the top systems in DUC ( document understanding conference ) in 2005 and 2006 . This WordNet-based approach has several shortcomings due to the problems of data deficiency and word sense ambiguity , etc . . Wikipedia already defined millions of multiword concepts in separate articles . Its definition is much larger than that of WordNet . For instance , more than 20 kinds of songs and movies called Butterfly , such as Butterfly ( Kumi Koda song ) , Butterfly ( 1999 film ) and Butterfly ( 2004 film ) , are listed in Wikipedia . When people say something about butterfly in Wikipedia , usually , a link is assigned to refer to a particular butterfly . Following this link , we can acquire its explicit and exact semantic ( Gabrilovich and Markovitch , 2007 ) , especially for multi-word concepts . Phrases are more important than individual words for document retrieval ( Liu et al. , 2004 ) . We hope that the wiki concepts are appropriate text representation for summarization . Generally , wiki articles have little redundancy in their contents as they utilize encyclopedia style . Their authors tend to use wiki links and See Also links to refer to the involved concepts rather than expand these concepts . In general , the guideline for composing wiki articles is to avoid overlong and over-complicated styles . Thus , the strategy of split it into a series of articles is recommended ; so wiki articles are usually not too long and contain limited number of sentences . These factors lead to fewer links between sentences within a wiki article , as compared to normal documents . However , the principle of typical extractive summarization approaches is that the sentences whose contents are repeatedly emphasized by the authors are most important and should be included ( Silber and McCoy , 2002 ) . Therefore , it is challenging to summarize wiki articles due to low redundancy ( and links ) between sentences . To overcome this problem , we seek ( i ) more reliable links between passages , ( ii ) appropriate weighting metric to emphasize the salient concepts about the topic , and ( iii ) additional guideline on utilizing non-textual features such as outline and infobox . Thus , we develop wiki concepts to replace bag-of-words approach for better link measurements between sentences , and extend an existing summarization model on free text to integrate structural information . By analyzing rhetorical discourse structure of aim , background , solution , etc. or citation context , we can obtain appropriate abstracts and the most influential contents from scientific articles ( Teufel and Moens , 2002 ; Mei and Zhai , 2008 ) . Similarly , we believe that the structural information such as infobox and outline is able to improve summarization as well . The outline of a wiki article using inner links will render the structure of its definition . In addition , infobox could be considered as topic signature ( Lin and Hovy , 2000 ) or keywords about the topic . Since keywords and summary of a document can be mutually boosted ( Wan et al. , 2007 ) , infobox is capable of summarization instruction . When Ahn ( 2004 ) and Kor ( 2007 ) utilize Wikipedia for TREC-QA definition , they treat the Wikipedia as the Web and perform normal search on it . High-frequency terms in the query snippets returned from wiki index are used to extend query and rank ( re-rank ) passages . These snippets usually come from multiple wiki articles . Here the useful information may be beyond these snippets but existing terms are possibly irrelevant to the topic . On the contrary , our approach concentrates on the wiki article having the exact topic only . We assume that every sentence in the article is used to define the query topic , no matter whether it contains the term ( s ) of the topic or not . In order to extract some salient sentences from the article as definition summaries , we will build a summarization model that describes the relations between the sentences , where both textual and structural features are considered . 3 Our Approach 3.1 Wiki Concepts In this subsection , we address how to find reasonable and reliable links between sentences using wiki concepts . Consider a sentence : After graduating from Boston University in 1988 , she went to work at a Calvin Klein store in Boston . from a wiki article Carolyn Bessette Kennedy2 , we can find 11 distinctive terms , such as after , graduate , Boston , University ,1988 , go , work , Calvin , Klein , store , Boston , if stop words are ignored . However , multi-word terms such as Boston University and Calvin Klein are linked to the corresponding wiki articles , where their definitions are given . Clearly , considering the anchor texts as two wiki concepts rather than four words is more reasonable . Their granularity are closer to semantic content units in a summarization evaluation method Pyramid ( Nenkova et al . , 2007 ) and nuggets in TREC-QA . When the text is represented by wiki concepts , whose granularity is similar to the evaluation units , it is possibly easy to detect the matching output using a model . Here , an identical concept since they refer to the same wiki article . In wiki articles , the first occurrence of a wiki concept is tagged by a wiki link , but there is no such a link to its subsequent occurrences in the remaining parts of the text in most cases . To alleviate this problem , a set of heuristic rules is proposed to unify the subsequent occurrences of concepts in normal text with previous wiki concepts in the anchor text . These heuristic rules include : ( i ) edit distance between linked wiki concept and candidates in normal text is larger than a predefined threshold ; and ( ii ) partially overlapping words beginning with capital letter , etc. . After filtering out wiki concepts , the words remaining in wiki articles could be grouped into two sets : close-class terms like pronouns and prepositions as well as open-class terms like nouns and verbs . For example , in the sentence She died at age 33 , along with her husband and sister , the openclass terms include die , age , 33 , husband and sister . Even though most open-class terms are defined in Wikipedia as well , the authors of the article do not consider it necessary to present their references using wiki links . Hence , we need to extend wiki concepts by concatenating them with these open-class terms to form an extended vector . In addition , we ignore all close-class terms , since we can not find efficient method to infer reliable links across them . As a result , texts are represented as a vector of wiki concepts . Once we introduce wiki concepts to replace typical bag-of-words approach , the dimensions of concept space will reach six order of magnitudes . We can not ignore the data spareness issue and computation cost when the concept space is so huge . Actually , for a wiki article and a set of relevant articles , the involved concepts are limited , and we need to explore them in a small sub-space . For instance , 59 articles about Kennedy family in Wikipedia have 10,399 distinctive wiki concepts only , where 5,157 wiki concepts exist twice and more . Computing the overlapping among them is feasible . Furthermore , we need to merge the wiki concepts with identical or close semantic ( namely , building links between these synonyms and paraphrases ) . We measure the semantic similarity between two concepts by using cosine distance between their wiki articles , which are represented as the vectors of wiki concepts as well . For computation efficiency , we calculate semantic similarities between all promising concept pairs beforehand , and then retrieve the value in a Hash table directly . We spent CPU time of about 12.5 days preprocessing the se mantic calculation . Details are available at our technical report ( Lu et al. , 2008 ) . Following the principle of TFIDF , we define the weighing metric for the vector represented by wiki concepts using the entire Wikipedia as the observation collection . We define the CFIDF weight of wiki concept i in article j as : where cfi , j is the frequency of concept i in article j ; idfi is the inverse frequency of concept i in Wikipedia ; and D is the number of articles in Wikipedia . Here , sparse wiki concepts will have more contribution . In brief , we represent articles in terms of wiki concepts using the steps below . 3.2 Document Concept Lattice Model Next , we build the document concept lattice ( DCL ) for articles represented by wiki concepts . For illustration on how DCL is built , we consider 8 sentences from DUC 2005 Cluster d324e ( Ye et al . , 2007 ) as case study . 8 sentences , represented by 16 distinctive concepts A-P , are considered as the base nodes 1-8 as shown in Figure 1 . Once we group nodes by means of the maximal common concepts among base nodes hierarchically , we can obtain the derived nodes 11 - 41 , which form a DCL . A derived node will annotate a local topic through a set of shared concepts , and define a sub concept space that contains the covered base nodes under proper projection . The derived node , accompanied with its base nodes , is apt to interpret a particular argument ( or statement ) about the involved concepts . Furthermore , one base node among them , coupled with the corresponding sentence , is capable of this interpretation and could represent the other base nodes to some degree . In order to Extract a set of sentences to cover key distinctive local topics ( arguments ) as much as possible , we need to select a set of important nonoverlapping derived nodes . We measure the importance of node N in DCL of article j in term of representative power ( RP ) as : where concept ci in node N is weighted by wi , j according to Eqn ( 1 ) , and | N | denotes the concept number in N ( if N is a base node ) or the number of distinct concepts in | N | ( if N is a derived node ) , respectively . Here , | ci | represents the cs frequency in N , and log ( | N | ) reflects Ns cost if N is selected ( namely , how many concepts are used in N ) . For example , 7 concepts in sentence 1 lead to the total | c | of 34 if their weights are set to 1 equally . Its By selecting a set of non-overlapping derived nodes with maximal RP , we are able to obtain a set of local topics with highest representativeness and diversity . Next , a representative sentence with maximal RP in each of such derived nodes is chosen to represent the local topics in observation . When the length of the required summary changes , the number of the local topics needed will also be modified . Consequently , we are able to select the sets of appropriate derived nodes in diverse generalization levels , and obtain various versions of summaries containing the local topics with appropriate granularities . In the DCL example shown in Figure 1 , if we expect to have a summary with two sentences , we will select the derived nodes 31 and 32 with highest RP . Nodes 31 and 32 will infer sentences 4 and 2 , and they will be concatenated to form a summary . If the summary is increased to three sentences , then three derived nodes 31 , 23 and 33 with maximal RP will render representative sentences 4 , 5 and 6 . Hence , the different number of actual sentences ( 4 +5 +6 vs. 4 +2 ) will be selected depending on the length of the required summary . The uniqueness of DCL is that the sentences used in a shorter summary may not appear in a longer summary for the same source text . According to the distinctive derived nodes in diverse levels , the sentences with different generalization abilities are chosen to generate various summaries . 3.3 Model of Extended Document Concept Lattice (EDCL) Different from free text and general web documents , wiki articles contain structural features , such as infoboxes and outlines , which correlate strongly to nuggets in definition TREC-QA . By integrating these structural features , we will generate better RP measures in derived topics which facilitates better priority assignment in local topics . 3.3.1 Outline: Wiki Macro Structure A long wiki article usually has a hierarchical outline using inner links to organize its contents . For example , wiki article Cat consists of a set of hierarchical sections under the outline of mouth , legs , Metabolism , genetics , etc. . This outline provides a hierarchical clustering of sub-topics assigned by its author ( s ) , which implies that selecting sentences from diverse sections of outline is apt to obtain a balanced summary . Actually , DCL could be considered as the composite of many kinds of clusterings ( Ye et al . , 2007 ) . Importing the clustering from outline into DCL will be helpful for the generation of a balanced summary . We thus incorporate the structure of outline into DCL as follows : ( i ) treat section titles as concepts in the pseudo derived nodes ; ( ii ) link these pseudo nodes and the base nodes in this section if they share concepts ; and ( iii ) revise base nodes RP in Eqn ( 2 ) ( see Section 3.3.3 ) . 3.3.2 Infobox: a Mini Version of Summary Infobox tabulates the key properties about the topic concept of a wiki article . It could be considered as a mini summary , where many nuggets in TRECQA are included . As properties in infobox are not complete sentences and do not present relevant arguments , it is inappropriate to concatenate them as a summary . However , they are good indicators for summary generation . Following the terms in a property ( e.g. , spouse name and graduation school ) , we can find the corresponding sentences in the body of the text that contains such terms4 . It describes the details about the involved property and provides the relevant arguments . We call it support sentence . Now , again , we have a hierarchy : Infobox + properties + support sentences . This hierarchy can be used to render a summary by concatenating the support sentences . This summary is inferred from hand-crafted infobox directly and is a full version of infobox ; so its quality is guaranteed . However , it is possibly inapplicable due to its improper length . Following the iterative reinforcement approach for summarization and keyword extraction ( Wan et al. , 2007 ) , it could be used to refine other versions of summaries . Hence , we utilize infobox and its support sentences to modify nodes RPs in DCL so that the priority of local topics has bias to infobox . To achieve it , we extend DCL by inserting a hierarchy from infobox : ( i ) generate a pseudo derived node for each property ; ( ii ) link every derived node to its support sentences ; and ( iii ) cover these pseudo nodes by a virtual derived node called infobox . 3.3.3 Summary Generation from EDCL In DCL , sentences with common concepts form local topics by autonomous approach , where shared concepts are depicted in derived nodes . Now we introduce two additional hierarchies derived from outline and infobox into DCL to refine RPs of salient local topics for summarization , which will render a model named extended document concept lattice ( EDCL ) . As shown in Figure 3 , base nodes in EDCL covered by pseudo derived nodes will increase their RPs when they receive influence from outline and infobox . Also , if RPs of their covered base nodes changes , the original derived nodes will modify their RPs as well . Therefore , the new 4Sometimes , we can find more than one appropriate sentence for a property . In our investigation , we select top two sentences with the occurrence of the particular term if available . RPs in derived nodes and based nodes will lead to better priority of ranking derived nodes , which is likely to result in a better summary . One important direct consequence of introducing the extra hierarchies is to increase the RP of nodes relevant to outline and infobox so that the summaries from EDCL are likely to follow human-crafted ones . The influence of human effects are transmitted in a V curve approach . We utilize the following steps to generate a summary with a given length ( say m sentences ) from EDCL . 7 . If one representative sentence is covered by more than one derived node in step 5 , the output will be less than m sentences . In this case , we need to increase m and repeat step 5-6 until m sentences are selected . 4 Experiments The purposes of our experiment are two-fold : ( i ) evaluate the effects of wiki definition to the TRECQA task ; and ( ii ) examine the characteristics and summarization performance of EDCL . 4.1 Corpus Construction We adopt the tasks of TREC-QA in 2004 - 2006 ( TREC 12 - 14 ) as test scope . We retrieve articles with identical topic names from Wikipedia6 . Non-letter transformations are permitted ( e.g. , from Carolyn Bessette-Kennedy to Carolyn BessetteKennedy ) . Because our focus is summarization evaluation , we ignore the cases in TRECQA where the exact topics do not exist in Wikipedia , even though relevant topics are available ( e.g . , France wins World Cup in soccer in TREC-QA vs . France national football team and 2006 FIFA World Cup in Wikipedia ) . Finally , among the 215 topics in TREC 12 - 14 , we obtain 180 wiki articles with the same topics . We ask 15 undergraduate and graduate students from the Department of English Literature in National University of Singapore to choose 7-14 sentences in the above wiki articles as extractive summaries . Each wiki article is annotated by 3 persons separately . In order for the volunteers to avoid the bias from TREC-QA corpus , we do not provide queries and nuggets used in TREC-QA . Similar to TREC nuggets , we call the selected sentences wiki nuggets . Wiki nuggets provides the ground truth of the performance evaluation , since some TREC nuggets are possibly unavailable in Wikipedia . Here , we did not ask the volunteers to create snippets ( like TREC-QA ) or compose an abstractive summary ( like DUC ) . This is because of the special style of wiki articles : the entire document is a long summary without trivial stuff . Usually , we do not need to concatenate key phrases from diverse sentences to form a recapitulative sentence . Meanwhile , selecting a set of salient sentences to form a concise version is a relatively less time-consuming but applicable approach . Snippets , by and large , lead to bad readability , and therefore we do not employ this approach . In addition , the volunteers also annotate 7 - 10 pairs of question / answer for each article for further research on QA using Wikipedia . The corpus , called TREC-Iiki collection , is available at our site ( http : / / nuscu.ddns.comp.nus.edu.sg ) . The system of Wikipedia summarization using EDCL is launched on the Web as well . 4.2 Corpus Exploration 4.2.1 Answer availability The availability of answers in Wikipedia for TRECQA could be measured in two aspects : ( i ) how many TREC-QA topics have been covered by Wikipedia ? and ( ii ) how many nuggets could be found in the corresponding wiki article ? We find that ( i ) over 80 % of topics ( 180 / 215 ) in the TREC 12 - 14 are available in Wikipedia , and ( ii ) about 47 % TREC nuggets could be detected directly from Wikipedia ( examining applet modified from Pourpre ( Lin and Demner-Fushman , 2006 ) ) . In contrast , 6,463 nuggets existing in TREC-QA 12 - 14 are distributed in 4,175 articles from AQUAINT corpus . We can say that Wikipedia is the answer goldmine for TREC-QA questions . When we look into these TREC nuggets in wiki articles closely , we find that most of them are embedded in wiki links or relevant to infobox . It suggests that they are indicators for sentences having nuggets . 4.2.2 Correlation between TREC nuggets and non-text features Analyzing the features used could let us understand summarization better ( Nenkova and Louis , 2008 ) . Here , we focus on the statistical analysis between TREC / wiki nuggets and non-textual features such as wiki links , infobox and outline . The features used are introduced in Table 1 . The correlation coefficients are listed in Table 2 . Observation : ( 1 ) On the whole , wiki nuggets exhibit higher correlation to non-textual features than TREC nuggets do . The possible reason is that TREC nuggets are extracted from AQUAINT rather than Wikipedia . ( 2 ) As compared to other features , infobox and wiki links strongly relate to nuggets . They are thus reliable features beyond text for summarization . ( 3 ) Sentence positions exhibit weak correlation to nuggets , even though the first sentence of an article is a good one-sentence definition . 4.3 Statistical Characteristics of EDCL We design four runs with various configurations as shown in Table 3 . We implement a sentence reranking program using MMR ( maximal marginal relevance ) ( Carbonell and Goldstein , 1998 ) in Run 1 , which is considered as the test baseline . We apply standard DCL in Run 2 , where concepts are determined according to their definitions in WordNet ( Ye et al . , 2007 ) . We introduce wiki concepts for standard DCL in Run 3 . Run 4 is the full version of EDCL , which considers both outline and infobox . Observations : ( 1 ) In Run 1 , the average number of distinctive words per article is near to 1200 after stop words are filtered out . When we merge diverse words having similar semantic according to WordNet concepts , we obtain 873 concepts per article on average in Run 2 . The word number decreases by about 28 % as a result of the omission of close-class terms and the merging of synonyms and paraphrases . ( 2 ) When wiki concepts are introduced in Run 3 , the number of concepts continues to decrease . Here , some adjacent single-word terms are merged into wiki concepts if they are annotated by wiki links . Even though the reduction of total concepts is limited , these new wiki concepts will group the terms that can not be detected by WordNet . ( 3 ) DCL based on WordNet concepts has less derived nodes ( Run 3 ) than DCL based on wiki concepts does , although the former has more concepts . It implies that wiki concepts lead to higher link density in DCL as more links between concepts can be detected . ( 4 ) Outline and infobox will bring additional 54 derived nodes ( from 1695 to 1741 ) . Additional computation cost is limited when they are introduced into EDCL . 4.4 Summarization Performance of EDCL We evaluate the performance of EDCL from two aspects such as contribution to TREC-QA definition task and accuracy of summarization in our TRECWiki collection . Since factoid/list questions are about the most essential information of the target as well , like Cuis approach ( 2005 ) , we treat factoid/list answers as essential nuggets and add them to the gold standard list of definition nuggets . We set the sentence number of summaries generated by the system to 12 . We examine the definition quality by nugget recall ( NR ) and an approximation to nugget precision ( NP ) on answer length . These scores are combined using the F1 and F3 measures . The recall in F3 is weighted three times as important as precision . The evaluation is automatically conducted by Pourpre v1 .1 ( Lin and Demner-Fushman , 2006 ) . Based on the performance of EDCL for TRECQA definition task listed in Table 5 , we observe that : ( i ) When EDCL considers wiki concepts and structural features such as outline and infobox , its F-scores increase significantly ( Run 3 and Run 4 ) . ( ii ) Table 5 also lists the results of Cuis system ( marked by asterisk ) using bigram soft patterns ( Cui et al . , 2005 ) , which is trained by TREC - 12 and tested on TREC 13 . Our EDCL can achieve comparable or better F-scores on the 180 topics in TREC 12 - 14 . It suggests that Wikipedia could provide high-quality definition directly even though we do not use AQUAINT . ( iii ) The precision of EDCL in Run 4 outperforms that of soft-pattern approach remarkably ( from 0.34 to 0.497 ) . One possible reason is that all sentences in a wiki article are oriented to its topic , and the sentence irrelevant to its topic hardly occurs . We also test the performance of EDCL using extractive summaries in TREC-Wiki collection . By means of comparing to each set of sentences selected by a volunteer , we examine how many exact annotated sentences are selected by the system using different configurations . The average recalls and precisions as well as their F-scores are shown in Figure 4 . Observations : ( i ) The structural information of Wikipeida has significant contribution to EDCL for summarization . We manually examine some summaries and find that the sentences containing more wiki links are apt to be chosen when wiki concepts are introduced in EDCL . Most sentences in output summaries in Run 4 usually have 1-3 links and relevant to infobox or outline . ( ii ) When using wiki concepts , infobox and outline to enrich DCL , we find that the precision of sentence selection has improved more than the recall . It reaffirms the conclusion in the previous TREC-QA test in this subsection . ( iii ) In addition , we manually examine the summaries on some wiki articles with common topics , such as car , house , money , etc. . We find that the summaries generated by EDCL could effectively grasp the key information about the topics when the sentence number of summaries exceeds 10 . 5 Conclusion and Future Work Wikipedia recursively defines enormous concepts in huge vector space of wiki concepts . The explicit semantic representation via wiki concepts allows us to obtain more reliable links between passages . Wikipedias special structural features , such as wiki links , infobox and outline , reflect the hidden human knowledge . The first sentence of a wiki article , infobox ( and its support sentences ) , outline ( and its relevant sentences ) , as well as the entire document could be considered as diverse summaries with various lengths . In our proposed model , local topics are autonomously organized in a lattice structure according to their overlapping relations . The hierarchies derived from infobox and outline are imported to refine the representative powers of local topics by emphasizing the concepts relevant to infobox and outline . Experiments indicate that our proposed model exhibits promising performance in summarization and QA definition tasks . Of course , there are rooms to further improve the model . Possible improvements includes : ( a ) using advanced semantic and parsing technologies to detect the support and relevant sentences for infobox and outline ; ( b ) summarizing multiple articles in a wiki category ; and ( c ) exploring the mapping from close-class terms to open-class terms for more links between passages is likely to forward some interesting results . More generally , the knowledge hidden in nontextual features of Wikipedia allow the model to harvest better definition summaries . It is challenging but possibly fruitful to recast the normal documents with wiki styles so as to adopt EDCL for free text and enrich the research efforts on other NLP tasks . phrases . In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval , pages 266272 , New York , NY , USA . ACM . "
            }
        },
        {
            "id": "P11-1006",
            "result": [
                {
                    "value": {
                        "start": 472,
                        "end": 488,
                        "text": "log linear model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P11-1006:E0"
                },
                {
                    "value": {
                        "start": 1033,
                        "end": 1041,
                        "text": "accuracy",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P11-1006:E1"
                },
                {
                    "value": {
                        "start": 2883,
                        "end": 2898,
                        "text": "trie structures",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P11-1006:E2"
                },
                {
                    "value": {
                        "start": 2998,
                        "end": 3021,
                        "text": "Brill and Moores method",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P11-1006:E3"
                },
                {
                    "value": {
                        "start": 3066,
                        "end": 3091,
                        "text": "logistic regression model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P11-1006:E4"
                },
                {
                    "value": {
                        "start": 3904,
                        "end": 3939,
                        "text": "log-linear ( discriminative ) model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P11-1006:E5"
                },
                {
                    "value": {
                        "start": 5512,
                        "end": 5518,
                        "text": "n-gram",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P11-1006:E6"
                },
                {
                    "value": {
                        "start": 6064,
                        "end": 6070,
                        "text": "ngrams",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P11-1006:E7"
                },
                {
                    "value": {
                        "start": 7759,
                        "end": 7781,
                        "text": "Brill and Moores model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P11-1006:E8"
                },
                {
                    "value": {
                        "start": 7890,
                        "end": 7930,
                        "text": "Li-regularized logistic regression model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P11-1006:E9"
                },
                {
                    "value": {
                        "start": 8510,
                        "end": 8531,
                        "text": "maximum entropy model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P11-1006:E10"
                },
                {
                    "value": {
                        "start": 14417,
                        "end": 14432,
                        "text": "gradient ascent",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P11-1006:E11"
                },
                {
                    "value": {
                        "start": 14601,
                        "end": 14607,
                        "text": "L-BFGS",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P11-1006:E12"
                },
                {
                    "value": {
                        "start": 15875,
                        "end": 15904,
                        "text": "Aho-Corasick tree ( AC tree )",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P11-1006:E13"
                },
                {
                    "value": {
                        "start": 15895,
                        "end": 15902,
                        "text": "AC tree",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P11-1006:E14"
                },
                {
                    "value": {
                        "start": 16282,
                        "end": 16320,
                        "text": "Aho-Corasick string matching algorithm",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P11-1006:E15"
                },
                {
                    "value": {
                        "start": 16339,
                        "end": 16361,
                        "text": "Aho-Corasick algorithm",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P11-1006:E16"
                },
                {
                    "value": {
                        "start": 22017,
                        "end": 22032,
                        "text": "logistic method",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P11-1006:E17"
                },
                {
                    "value": {
                        "start": 8043,
                        "end": 8063,
                        "text": "discriminative model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P11-1006:E18"
                },
                {
                    "value": {
                        "start": 2822,
                        "end": 2838,
                        "text": "generative model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P11-1006:E19"
                },
                {
                    "value": {
                        "start": 3066,
                        "end": 3074,
                        "text": "logistic",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P11-1006:E20"
                },
                {
                    "value": {
                        "start": 23569,
                        "end": 23579,
                        "text": "accuracies",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P11-1006:E21"
                },
                {
                    "value": {
                        "start": 24674,
                        "end": 24695,
                        "text": "AhoCorasick algorithm",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P11-1006:E22"
                }
            ],
            "data": {
                "text": "A Fast and Accurate Method for Approximate String Search Abstract This paper proposes a new method for approximate string search , specifically candidate generation in spelling error correction , which is a task as follows . Given a misspelled word , the system finds words in a dictionary , which are most similar to the misspelled word . The paper proposes a probabilistic approach to the task , which is both accurate and efficient . The approach includes the use of a log linear model , a method for training the model , and an algorithm for finding the top k candidates . The log linear model is defined as a conditional probability distribution of a corrected word and a rule set for the correction conditioned on the misspelled word . The learning method employs the criterion in candidate generation as loss function . The retrieval algorithm is efficient and is guaranteed to find the optimal k candidates . Experimental results on large scale data show that the proposed approach improves upon existing methods in terms of accuracy in different settings . 1 Introduction This paper addresses the following problem , referred to as approximate string search . Given a query string , a dictionary of strings ( vocabulary ) , and a set of operators , the system returns the top k strings in the dictionary that can be transformed from the query string by applying several operators in the operator set . Here each operator is a rule that can replace a substring in the query string with another substring . The top k results are defined in Contribution during internship at Microsoft Research Asia . terms of an evaluation measure employed in a specific application . The requirement is that the task must be conducted very efficiently . Approximate string search is useful in many applications including spelling error correction , similar terminology retrieval , duplicate detection , etc. . Although certain progress has been made for addressing the problem , further investigation on the task is still necessary , particularly from the viewpoint of enhancing both accuracy and efficiency . Without loss of generality , in this paper we address candidate generation in spelling error correction . Candidate generation is to find the most possible corrections of a misspelled word . In such a problem , strings are words , and the operators represent insertion , deletion , and substitution of characters with or without surrounding characters , for example , a __ + e and lly * ly . Note that candidate generation is concerned with a single word ; after candidate generation , the words surrounding it in the text can be further leveraged to make the final candidate selection , e.g. , Li et al. ( 2006 ) , Golding and Roth ( 1999 ) . In spelling error correction , Brill and Moore ( 2000 ) proposed employing a generative model for candidate generation and a hierarchy of trie structures for fast candidate retrieval . Our approach is a discriminative approach and is aimed at improving Brill and Moores method . Okazaki et al . ( 2008 ) proposed using a logistic regression model for approximate dictionary matching . Their method is also a discriminative approach , but it is largely different from our approach in the following points . It formalizes the problem as binary classification and assumes that there is only one rule applicable each time in candidate generation . Efficiency is also not a major concern for them , because it is for offline text mining . There are two fundamental problems in research on approximate string search : ( 1 ) how to build a model that can archive both high accuracy and efficiency , and ( 2 ) how to develop a data structure and algorithm that can facilitate efficient retrieval of the top k candidates . In this paper , we propose a probabilistic approach to the task . Our approach is novel and unique in the following aspects . It employs ( a ) a log-linear ( discriminative ) model for candidate generation , ( b ) an effective algorithm for model learning , and ( c ) an efficient algorithm for candidate retrieval . The log linear model is defined as a conditional probability distribution of a corrected word and a rule set for the correction given the misspelled word . The learning method employs , in the training process , a criterion that represents the goal of making both accurate and efficient prediction ( candidate generation ) . As a result , the model is optimally trained toward its objective . The retrieval algorithm uses special data structures and efficiently performs the top k candidates finding . It is guaranteed to find the best k candidates without enumerating all the possible ones . We empirically evaluated the proposed method in spelling error correction of web search queries . The experimental results have verified that the accuracy of the top candidates given by our method is significantly higher than those given by the baseline methods . Our method is more accurate than the baseline methods in different settings such as large rule sets and large vocabulary sizes . The efficiency of our method is also very high in different experimental settings . 2 Related Work Approximate string search has been studied by many researchers . Previous work mainly focused on efficiency rather than model . Usually , it is assumed that the model ( similarity distance ) is fixed and the goal is to efficiently find all the strings in the collection whose similarity distances are within a threshold . Most existing methods employ n-gram based algorithms ( Behm et al . , 2009 ; Li et al . , 2007 ; Yang et al . , 2008 ) or filtering algorithms ( Mihov and Schulz , 2004 ; Li et al . , 2008 ) . Instead of finding all the candidates in a fixed range , methods for finding the top k candidates have also been developed . For example , the method by Vernica and Li ( 2009 ) utilized n-gram based inverted lists as index structure and a similarity function based on n-gram overlaps and word frequencies . Yang et al . ( 2010 ) presented a general framework for top k retrieval based on ngrams . In contrast , our work in this paper aims to learn a ranking function which can achieve both high accuracy and efficiency . Spelling error correction normally consists of candidate generation and candidate final selection . The former task is an example of approximate string search . Note that candidate generation is only concerned with a single word . For single-word candidate generation , rule-based approach is commonly used . The use of edit distance is a typical example , which exploits operations of character deletion , insertion and substitution . Some methods generate candidates within a fixed range of edit distance or different ranges for strings with different lengths ( Li et al. , 2006 ; Whitelaw et al. , 2009 ) . Other methods make use of weighted edit distance to enhance the representation power of edit distance ( Ristad and Yianilos , 1998 ; Oncina and Sebban , 2005 ; McCallum et al. , 2005 ; Ahmad and Kondrak , 2005 ) . Conventional edit distance does not take in consideration context information . For example , people tend to misspell c to s or k depending on contexts , and a straightforward application of edit distance can not deal with the problem . To address the challenge , some researchers proposed using a large number of substitution rules containing context information ( at character level ) . For example , Brill and Moore ( 2000 ) developed a generative model including contextual substitution rules ; and Toutanova and Moore ( 2002 ) further improved the model by adding pronunciation factors into the model . Schaback and Li ( 2007 ) proposed a multi-level feature-based framework for spelling error correction including a modification of Brill and Moores model ( 2000 ) . Okazaki et al . ( 2008 ) utilized substring substitution rules and incorporated the rules into a Li-regularized logistic regression model . Okazaki et al. s model is largely different from the model proposed in this paper , although both of them are discriminative models . Their model is a binary classification model and it is assumed that only a single rule is applied in candidate generation . Since users behavior of misspelling and correction can be frequently observed in web search log data , it has been proposed to mine spelling-error and correction pairs by using search log data . The mined pairs can be directly used in spelling error correction . Methods of selecting spelling and correction pairs with maximum entropy model ( Chen et al . , 2007 ) or similarity functions ( Islam and Inkpen , 2009 ; Jones et al . , 2006 ) have been developed . The mined pairs can only be used in candidate generation of high frequency typos , however . In this paper , we work on candidate generation at the character level , which can be applied to spelling error correction for both high and low frequency words . 3 Model for Candidate Generation As an example of approximate string search , we consider candidate generation in spelling correction . Suppose that there is a vocabulary V and a misspelled word , the objective of candidate generation is to select the best corrections from the vocabulary V . We care about both accuracy and efficiency of the process . The problem is very challenging when the size of vocabulary is large , because there are a large number of potential candidates to be verified . In this paper , we propose a probabilistic approach to candidate generation , which can achieve both high accuracy and efficiency , and is particularly powerful when the scale is large . In our approach , it is assumed that a large number of misspelled words and their best corrections are given as training data . A probabilistic model is then trained by using the training data , which can assign ranking scores to candidates . The best candidates for correction of a misspelled word are thus defined as those candidates having the highest probabilistic scores with respect to the training data and the operators . Hereafter , we will describe the probabilistic model for candidate generation , as well as training and exploitation of the model . 3.1 Model The operators ( rules ) represent insertion , deletion , and substitution of characters in a word with or without surrounding context ( characters ) , which are similar to those defined in ( Brill and Moore , 2000 ; Okazaki et al. , 2008 ) . An operator is formally represented a rule * Q that replaces a substring in a misspelled word with Q , where , Q E [ s | s = t , s = ^ t , or s = t $ ] and t E E * is the set of all possible strings over the alphabet . Obviously , V C E * . We actually derive all the possible rules from the training data using a similar approach to ( Brill and Moore , 2000 ) as shown in Fig. 1 . First we conduct the letter alignment based on the minimum edit-distance , and then derive the rules from the alignment . Furthermore we expand the derived rules with surrounding words . Without loss of generality , we only consider using +2 , +1,0 , 1 , 2 characters as contexts in this paper . If we can apply a set of rules to transform the misspelled word wm to a correct word wc in the vocabulary , then we call the rule set a transformation for the word pair wm and wc . Note that for a given word pair , it is likely that there are multiple possible transformations for it . For example , both n * m and ni * mi can transform nicrosoft to microsoft . Without loss of generality , we set the maximum number of rules applicable to a word pair to be a fixed number . As a result , the number of possible transformations for a word pair is finite , and usually limited . This is equivalent to the assumption that the number of spelling errors in a word is small . Given word pair ( wm , wc ) , let R ( wm , wc ) denote one transformation ( a set of rules ) that can rewrite wm to wc . We consider that there is a probabilistic mapping between the misspelled word wm and correct word wc plus transformation R ( wm , wc ) . We define the conditional probability distribution of wc and R ( wm , wc ) given wm as the following log linear model : where r or o denotes a rule in rule set R , Ar or Ao denotes a weight , and the normalization is carried over i ( wm ) , all pairs of word w ' in V and transformation R ( wm , w ' ) , such that wm can be transformed to w ' by R ( wm , w ' ) . The log linear model actually uses binary features indicating whether or not a rule is applied . In general , the weights in Equ . ( 1 ) can be any real numbers . To improve efficiency in retrieval , we further assume that all the weights are non-positive , i.e. , bAr < 0 . It introduces monotonicity in rule application and implies that applying additional rules can not lead to generation of better candidates . For example , both office and officer are correct candidates of ofice . We view office a better candidate ( with higher probability ) than officer , as it needs one less rule . The assumption is reasonable because the chance of making more errors should be lower than that of making less errors . Our experimental results have shown that the change in accuracy by making the assumption is negligible , but the gain in efficiency is very large . 3.2 Training of Model Training data is given as a set of pairs T = [ ( wim , wic ) ] Ni1 , where wim is a misspelled word and wic E V is a correction of wi m . The objective of training would be to maximize the conditional probability P ( wic , R ( wi m , wic ) | wim ) over the training data . This is not a trivial problem , however , because the true transformation R * ( wi m , wic ) for each word pair wi m and wic is not given in the training data . It is often the case that there are multiple transformations applicable , and it is not realistic to assume that such information can be provided by humans or automatically derived . ( It is relatively easy to automatically find the pairs wi m and wi c as explained in Section 5.1 ) . In this paper , we assume that the transformation that actually generates the correction among all the possible transformations is the one that can give the maximum conditional probability ; the exactly same criterion is also used for fast prediction . Therefore we have the following objective function where A denotes the weight parameters and the max is taken over the set of transformations that can transform wi m to wic . We employ gradient ascent in the optimization in Equ . ( 2 ) . At each step , we first find the best transformation for each word pair based on the current In this paper , we employ the bounded L-BFGS ( Behm et al . , 2009 ) algorithm for the optimization task , which works well even when the number of weights A is large . 3.3 Candidate Generation In candidate generation , given a misspelled word wm , we find the k candidates from the vocabulary , that can be transformed from wm and have the largest probabilities assigned by the learned model . We only need to utilize the following ranking function to rank a candidate wc given a misspelled word wm , by taking into account Equs . ( 1 ) and ( 2 ) For each possible transformation , we simply take summation of the weights of the rules used in the transformation . We then choose the sum as a ranking score , which is equivalent to ranking candidates based on their largest conditional probabilities . 4 Efficient Retrieval Algorithm In this section , we introduce how to efficiently perform top k candidate generation . Our retrieval algorithm is guaranteed to find the optimal k candidates with some pruning techniques . We first introduce the data structures and then the retrieval algorithm . 4.1 Data Structures We exploit two data structures for candidate generation . One is a trie for storing and matching words in the vocabulary , referred to as vocabulary trie , and the other based on what we call an Aho-Corasick tree ( AC tree ) ( Aho and Corasick , 1975 ) , which is used for storing and applying correction rules , referred to as rule index . The vocabulary trie is the same as that used in existing work and it will be traversed when searching the top k candidates . Our rule index is unique because it indexes all the rules based on an AC tree . The AC tree is a trie with failure links , on which the Aho-Corasick string matching algorithm can be executed . Aho-Corasick algorithm is a well known dictionary-matching algorithm which can quickly locate all the words in a dictionary within an input string . Time complexity of the algorithm is of linear order in length of input string plus number of matched entries . We index all the s in the rules on the AC tree . Each corresponds to a leaf node , and the , ( is of the are stored in an associated list in decreasing order of rule weights A , as illustrated in Fig. 2 . 1 ` One may further improve the index structure by using a trie rather than a ranking list to store , Qs associated with the same . However the improvement would not be significant because the number of , Qs associated with each is usually very small . 4.2 Algorithm One could employ a naive algorithm that applies all the possible combinations of rules ( s ) to the current word wm , verifies whether the resulting words ( candidates ) are in the vocabulary , uses the function in Equ . ( 5 ) to calculate the ranking scores of the candidates , and find the top k candidates . This algorithm is clearly inefficient . Our algorithm first employs the Aho-Corasick algorithm to locate all the applicable s within the input word wm , from the rule index . The corresponding , ( is are retrieved as well . Then all the applicable rules are identified and indexed by the applied positions of word wm. Our algorithm next traverses the vocabulary trie and searches the top k candidates with some pruning techniques . The algorithm starts from the root node of the vocabulary trie . At each step , it has multiple search branches . It tries to match at the next position of wm , or apply a rule at the current position of wm. The following two pruning criteria are employed to significantly accelerate the search process . 1 ) If the current sum of weights of applied rules is smaller than the smallest weight in the top k list , the search branch is pruned . This criterion is derived from the non-negative constraint on rule weights A . It is easy to verify that the sum of weights will not become larger if one continues to search the branch because all the weights are non-positive . 2 ) If two search branches merge at the same node in the vocabulary trie as well as the same position on wm , the search branches with smaller sum of weights will be pruned . It is based on the dynamic programming technique because we take max in the ranking function in Equ . 5 . It is not difficult to prove that our algorithm is guaranteed to find the best k candidates in terms of the ranking scores , because we only prune those candidates that can not give better scores than the ones in the current top k list . Due to the limitation of space , we omit the proof of the theorem that if the weights of rules A are non-positive and the ranking function is defined as in Equ . 5 , then the top k candidates obtained with the pruning criteria are the same as the top k candidates obtained without pruning . 5 Experimental Results We have experimentally evaluated our approach in spelling error correction of queries in web search . The problem is more challenging than usual due to the following reasons . ( 1 ) The vocabulary of queries in web search is extremely large due to the scale , diversity , and dynamics of the Internet . ( 2 ) Efficiency is critically important , because the response time of top k candidate retrieval for web search must be kept very low . Our approach for candidate generation is in fact motivated by the application . 5.1 Word Pair Mining In web search , a search session is comprised of a sequence of queries from the same user within a time period . It is easy to observe from search session data that there are many spelling errors and their corrections occurring in the same sessions . We employed heuristics to automatically mine training pairs from search session data at a commercial search engine . First , we segmented the query sequence from each user into sessions . If two queries were issued more than 5 minutes apart , then we put a session boundary between them . We used short sessions here because we found that search users usually correct their misspelled queries very quickly after they find the misspellings . Then the following heuristics were employed to identify pairs of misspelled words and their corrections from two consecutive queries within a session : Finally , we aggregated the identified training pairs across sessions and users and discarded the pairs with low frequencies . Table 1 shows some examples of the mined word pairs . 5.2 Experiments on Accuracy Two representative methods were used as baselines : the generative model proposed by ( Brill and Moore , 2000 ) referred to as generative and the logistic regression model proposed by ( Okazaki et al . , 2008 ) referred to as logistic . Note that Okazaki et al. ( 2008 ) s model is not particularly for spelling error correction , but it can be employed in the task . When using their method for ranking , we used outputs of the logistic regression model as rank scores . We compared our method with the two baselines in terms of top k accuracy , which is ratio of the true corrections among the top k candidates generated by a method . All the methods shared the same settings : 973,902 words in the vocabulary , 10,597 rules for correction , and up to two rules used in one transformation . We made use of 100,000 word pairs mined from query sessions for training , and 10,000 word pairs for testing . The experimental results are shown in Fig. 3 . We can see that our method always performs the best when compared with the baselines and the improvements are statistically significant ( p < 0.01 ) . The logistic method works better than generative , when k is small , but its performance becomes saturated , when k is large . Usually a discriminative model works better than a generative model , and that seems to be what happens with small ks . However , logistic can not work so well for large ks , because it only allows the use of one rule each time . We observe that there are many word pairs in the data that need to be transformed with multiple rules . Next , we conducted experiments to investigate how the top k accuracy changes with different sizes of vocabularies , maximum numbers of applicable rules and sizes of rule set for the three methods . The experimental results are shown in Fig. 4 , Fig. 5 and Fig. 6 . For the experiment in Fig. 4 , we enlarged the vocabulary size from 973,902 ( smallVocab ) to 2,206,948 ( largeVocab ) and kept the other settings the same as in the previous experiment . Because more candidates can be generated with a larger vocabulary , the performances of all the methods de cline . However , the drop of accuracy by our method is much smaller than that by generative , which means our method is more powerful when the vocabulary is large , e.g . , for web search . For the experiment in Fig. 5 , we changed the maximum number of rules that can be applied to a transformation from 2 to 3 . Because logistic can only use one rule at a time , it is not included in this experiment . When there are more applicable rules , more candidates can be generated and thus ranking of them becomes more challenging . The accuracies of both methods drop , but our method is constantly better than generative . Moreover , the decrease in accuracy by our method is clearly less than that by generative . For the experiment in Fig. 6 , we enlarged the number of rules from 10,497 ( smallRuleNum ) to 24,054 ( largeRuleNum ) . The performance of our method and those of the two baselines did not change so much , and our method still visibly outperform the baselines when more rules are exploited . 5.3 Experiments on Efficiency We have also experimentally evaluated the efficiency of our approach . Because most existing work uses a predefined ranking function , it is not fair to make a comparison with them . Moreover , Okazaki et al. method does not consider efficiency , and Brill and Moores method is based a complicated retrieve algorithm which is very hard to implement . Instead of making comparison with the existing methods in terms of efficiency , we evaluated the efficiency of our method by looking at how efficient it becomes with its data structure and pruning technique . First , we tested the efficiency of using AhoCorasick algorithm ( the rule index ) . Because the time complexity of Aho-Corasick algorithm is determined by the lengths of query strings and the number of matches , we examined how the number of matches on query strings with different lengths changes when the number of rules increases . The experimental results are shown in Fig. 7 . We can see that the number of matches is not largely affected by the number of rules in the rule index . It implies that the time for searching applicable rules is close to a constant and does not change much with different numbers of rules . Next , since the running time of our method is proportional to the number of visited nodes on the vocabulary trie , we evaluated the efficiency of our method in terms of number of visited nodes . The result reported here is that when k is 10 . Specifically , we tested how the number of visited nodes changes according to three factors : maximum number of applicable rules in a transformation , vocabulary size and rule set size . The experimental results are shown in Fig. 8 , Fig. 9 and Fig. 10 respectively . From Fig. 8 , with increasing maximum number of applicable rules in a transformation , number of visited nodes increases first and then stabilizes , especially when the words are long . Note that pruning becomes even more effective because number of visited nodes without pruning grows much faster . It demonstrates that our method is very efficient when compared to the non-pruning method . Admittedly , the efficiency of our method also deteriorates somewhat . This would not cause a noticeable issue in real applications , however . In the previous section , we have seen that using up to two rules in a transformation can bring a very high accuracy . From Fig. 8 and Fig. 9 , we can conclude that the numbers of visited nodes are stable and thus the efficiency of our method keeps high with larger vocabulary size and number of rules . It indicates that our pruning strategy is very effective . From all the figures , we can see that our method is always efficient especially when the words are relatively short . 5.4 Experiments on Model Constraints In Section 3.1 , we introduce the non-positive constraints on the parameters , i.e. , bar < 0 , to enable the pruning technique for efficient top k retrieval . We experimentally verified the impact of the constraints to both the accuracy and efficiency . For ease of reference , we name the model with the non-positive constraints as bounded , and the origi nal model as unbounded . The experimental results are shown in Fig. 11 and Fig. 12 . All the experiments were conducted based on the typical setting of our experiments : 973,902 words in the vocabulary , 10,597 rules , and up to two rules in one transformation . In Fig . 11 , we can see that the difference between bounded and unbounded in terms of accuracy is negligible , and we can draw a conclusion that adding the constraints does not hurt the accuracy . From Fig. 12 , it is easy to note that bounded is much faster than unbounded because our pruning strategy can be applied to bounded . 6 Conclusion In this paper , we have proposed a new method for approximate string search , including spelling error correction , which is both accurate and efficient . Our method is novel and unique in its model , learning algorithm , and retrieval algorithm . Experimental results on a large data set show that our method improves upon existing methods in terms of accuracy , and particularly our method can perform better when the dictionary is large and when there are many rules . Experimental results have also verified the high efficiency of our method . As future work , we plan to add contextual features into the model and apply our method to other data sets in other tasks . "
            }
        },
        {
            "id": "P15-1098",
            "result": [
                {
                    "value": {
                        "start": 622,
                        "end": 641,
                        "text": "vector space models",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1098:E0"
                },
                {
                    "value": {
                        "start": 1063,
                        "end": 1067,
                        "text": "IMDB",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P15-1098:E1"
                },
                {
                    "value": {
                        "start": 1072,
                        "end": 1086,
                        "text": "Yelp datasets1",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P15-1098:E2"
                },
                {
                    "value": {
                        "start": 2899,
                        "end": 2935,
                        "text": "User Product Neural Network ( UPNN )",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1098:E3"
                },
                {
                    "value": {
                        "start": 2929,
                        "end": 2933,
                        "text": "UPNN",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1098:E4"
                },
                {
                    "value": {
                        "start": 3589,
                        "end": 3611,
                        "text": "Yelp Dataset Challenge",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P15-1098:E5"
                },
                {
                    "value": {
                        "start": 3668,
                        "end": 3693,
                        "text": "recursive neural networks",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1098:E6"
                },
                {
                    "value": {
                        "start": 3875,
                        "end": 3880,
                        "text": "JMARS",
                        "labels": [
                            "Tool"
                        ]
                    },
                    "id": "P15-1098:E7"
                },
                {
                    "value": {
                        "start": 4120,
                        "end": 4128,
                        "text": "accuracy",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P15-1098:E8"
                },
                {
                    "value": {
                        "start": 4771,
                        "end": 4798,
                        "text": "Yelp Dataset Challenge 2013",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P15-1098:E9"
                },
                {
                    "value": {
                        "start": 9266,
                        "end": 9302,
                        "text": "convolutional neural network ( CNN )",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1098:E12"
                },
                {
                    "value": {
                        "start": 3668,
                        "end": 3692,
                        "text": "recursive neural network",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1098:E13"
                },
                {
                    "value": {
                        "start": 9297,
                        "end": 9300,
                        "text": "CNN",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1098:E14"
                },
                {
                    "value": {
                        "start": 9687,
                        "end": 9694,
                        "text": "n-grams",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1098:E15"
                },
                {
                    "value": {
                        "start": 9860,
                        "end": 9868,
                        "text": "trigrams",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1098:E16"
                },
                {
                    "value": {
                        "start": 9981,
                        "end": 9989,
                        "text": "unigrams",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1098:E17"
                },
                {
                    "value": {
                        "start": 9992,
                        "end": 9999,
                        "text": "bigrams",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1098:E18"
                },
                {
                    "value": {
                        "start": 11265,
                        "end": 11289,
                        "text": "recurrent neural network",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1098:E19"
                },
                {
                    "value": {
                        "start": 14486,
                        "end": 14513,
                        "text": "stochastic gradient descent",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1098:E20"
                },
                {
                    "value": {
                        "start": 15011,
                        "end": 15038,
                        "text": "Stanford Sentiment Treebank",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P15-1098:E21"
                },
                {
                    "value": {
                        "start": 15368,
                        "end": 15391,
                        "text": "Yelp Dataset Challenge4",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P15-1098:E22"
                },
                {
                    "value": {
                        "start": 15431,
                        "end": 15443,
                        "text": "IMDB dataset",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P15-1098:E23"
                },
                {
                    "value": {
                        "start": 15476,
                        "end": 15485,
                        "text": "Yelp 2014",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P15-1098:E24"
                },
                {
                    "value": {
                        "start": 15490,
                        "end": 15508,
                        "text": "Yelp 2013 datasets",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P15-1098:E25"
                },
                {
                    "value": {
                        "start": 15973,
                        "end": 15989,
                        "text": "Stanford CoreNLP",
                        "labels": [
                            "Tool"
                        ]
                    },
                    "id": "P15-1098:E26"
                },
                {
                    "value": {
                        "start": 16174,
                        "end": 16177,
                        "text": "MAE",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P15-1098:E27"
                },
                {
                    "value": {
                        "start": 16182,
                        "end": 16186,
                        "text": "RM5E",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P15-1098:E28"
                },
                {
                    "value": {
                        "start": 16753,
                        "end": 16768,
                        "text": "SVM classifiers",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1098:E29"
                },
                {
                    "value": {
                        "start": 17152,
                        "end": 17156,
                        "text": "SSWE",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1098:E30"
                },
                {
                    "value": {
                        "start": 17371,
                        "end": 17385,
                        "text": "senti accuracy",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P15-1098:E31"
                },
                {
                    "value": {
                        "start": 17443,
                        "end": 17447,
                        "text": "RMSE",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P15-1098:E32"
                },
                {
                    "value": {
                        "start": 17488,
                        "end": 17501,
                        "text": "UPNN ( full )",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1098:E33"
                },
                {
                    "value": {
                        "start": 17575,
                        "end": 17589,
                        "text": "UPNN ( no UP )",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1098:E34"
                },
                {
                    "value": {
                        "start": 17649,
                        "end": 17666,
                        "text": "RNTN + R eccurent",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1098:E35"
                },
                {
                    "value": {
                        "start": 9860,
                        "end": 9867,
                        "text": "trigram",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1098:E36"
                },
                {
                    "value": {
                        "start": 21449,
                        "end": 21491,
                        "text": "graphbased semi-supervised learning method",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1098:E37"
                },
                {
                    "value": {
                        "start": 22998,
                        "end": 23027,
                        "text": "stacked denoising autoencoder",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1098:E38"
                },
                {
                    "value": {
                        "start": 23076,
                        "end": 23114,
                        "text": "recursive deep neural networks ( RNN )",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1098:E39"
                },
                {
                    "value": {
                        "start": 23109,
                        "end": 23112,
                        "text": "RNN",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1098:E40"
                },
                {
                    "value": {
                        "start": 23584,
                        "end": 23613,
                        "text": "convolutional neural networks",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1098:E41"
                },
                {
                    "value": {
                        "start": 23650,
                        "end": 23666,
                        "text": "Paragraph Vector",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1098:E42"
                }
            ],
            "data": {
                "text": "Learning Semantic Representations of Users and Products for Document Level Sentiment Classification Abstract Neural network methods have achieved promising results for sentiment classification of text . However , these models only use semantics of texts , while ignoring users who express the sentiment and products which are evaluated , both of which have great influences on interpreting the sentiment of text . In this paper , we address this issue by incorporating user - and product - level information into a neural network approach for document level sentiment classification . Users and products are modeled using vector space models , the representations of which capture important global clues such as individual preferences of users or overall qualities of products . Such global evidence in turn facilitates embedding learning procedure at document level , yielding better text representations . By combining evidence at user - , product - and documentlevel in a unified neural framework , the proposed model achieves state-of-the-art performances on IMDB and Yelp datasets1 . 1 Introduction Document-level sentiment classification is a fundamental problem in the field of sentiment analysis and opinion mining ( Pang and Lee , 2008 ; Liu , 2012 ) . The task is to infer the sentiment polarity or intensity ( e.g. 1-5 or 1-10 stars on review sites ) of a document . Dominating studies follow Pang et al. ( 2002 ; 2005 ) and regard this problem as a multi-class classification task . They usually use machine learning algorithms , and build sentiment classifier from documents with accompanying sentiment labels . Since the performance of a machine learner is heavily dependent on the choice of data representations ( Domingos , 2012 ) , many works focus on designing effective features ( Pang et al. , 2002 ; Qu et al. , 2010 ; Kiritchenko et al. , 2014 ) or learning discriminative features from data with neural networks ( Socher et al. , 2013 ; Kalchbrenner et al. , 2014 ; Le and Mikolov , 2014 ) . Despite the apparent success of neural network methods , they typically only use text information while ignoring the important influences of users and products . Let us take reviews with respect to 1-5 rating scales as an example . A critical user might write a review it works great and mark 4 stars , while a lenient user might give 5 stars even if he posts an ( almost ) identical review . In this case , user preference affects the sentiment rating of a review . Product quality also has an impact on review sentiment rating . Reviews towards high-quality products ( e.g. Macbook ) tend to receive higher ratings than those towards low-quality products . Therefore , it is feasible to leverage individual preferences of users and overall qualities of products to build a smarter sentiment classifier and achieve better performance2 . In this paper , we propose a new model dubbed User Product Neural Network ( UPNN ) to capture user - and product-level information for sentiment classification of documents ( e.g . reviews ) . UPNN takes as input a variable-sized document as well as the user who writes the review and the product which is evaluated . It outputs sentiment polarity label of a document . Users and products are encoded in continuous vector spaces , the representations of which capture important global clues such as user preferences and product qualities . These representations are further integrated with continuous text representation in a unified neural framework for sentiment classification . We apply UPNN to three datasets derived from IMDB and Yelp Dataset Challenge . We compare to several neural network models including recursive neural networks ( Socher et al . , 2013 ) , paragraph vector ( Le and Mikolov , 2014 ) , sentimentspecific word embedding ( Tang et al . , 2014b ) , and a state-of-the-art recommendation algorithm JMARS ( Diao et al . , 2014 ) . Experimental results show that : ( 1 ) UPNN outperforms baseline methods for sentiment classification of documents ; ( 2 ) incorporating representations of users and products significantly improves classification accuracy . The main contributions of this work are as follows : 2 Consistency Assumption Verification We detail the effects of users and products in terms of sentiment ( e.g. 1-5 rating stars ) and text , and verify them on review datasets . We argue that the influences of users and products include the following four aspects . Algorithm 1 Consistency Assumption Testing Input : data X , number of users/products m , number of iterations n We test four consistency assumptions mentioned above with the same testing criterion , which is formalized in Algorithm 1 . For each consistency assumption , we test it for n = 50 iterations on each of IMDB , Yelp Dataset Challenge 2013 and 2014 datasets . Taking user-sentiment consistency as an example , in each iteration , we randomly select two reviews xi , x + i written by the same user ui , and a review xi written by another randomly selected user . Afterwards , we calculate the measurements of ( xi , x + i ) and ( xi , xi ) , and aggregate these statistics for m users . In user-sentiment assumption test , we use absolute rating difference | | ratinga ratingb | | as the measurement between two reviews a and b . We illustrate the results in Figure 1 ( a ) 3 , where 2013same/2014same/amzsame ( red plots ) means that two reviews are written by a same user , and 2013diff/2014diff/amzdiff ( black plots ) means that two reviews are written by different users . We can find that : the absolute rating differences between two reviews written by a same user are lower than those written by different users ( t-test with p-value < 0.01 ) . In other words , sentiment ratings from the same user are more consistent than those from different users . This validates the user-sentiment consistency . For testing product-sentiment consistency , we use absolute rating difference as the measurement . The reviews xi , x + i are towards a same product pi , and xi is towards another randomly selected product . From Figure 1 ( b ) , we can see that sentiment ratings towards the same product are more consistent than those towards different products . In order to verify the assumptions of user-text and product-text consistencies , we use cosine similarity between bag-of-words of two reviews as the measurement . Results are given in Figure 1 ( c ) and ( d ) . We can see that the textual similarity between two reviews written by a same user ( or towards a same product ) are higher than those written by different users ( or towards different products ) . 3 User Product Neural Network (UPNN) for Sentiment Classification We present the details of User Product Neural Network ( UPNN ) for sentiment classification . An illustration of UPNN is given in Figure 2 . It takes as input a review , the user who posts the review , and the product which is evaluated . UPNN captures four kinds of consistencies which are verified in Section 2 . It outputs the sentiment category ( e.g. 1-5 stars ) of a review by considering not only the semantics of review text , but also the information of user and product . In following subsections , we first describe the use of neural network for modeling semantics of variable-sized documents . We then present the methods for incorporating user and product information , followed by the use of UPNN in a supervised learning framework for sentiment classification . 3.1 Modeling Semantics of Document We model the semantics of documents based on the principle of compositionality ( Frege , 1892 ) , which states that the meaning of a longer expression ( e.g. a sentence or a document ) comes from the meanings of its words and the rules used to combine them . Since a document consists of a list of sentences and each sentence is made up of a list of words , we model the semantic representation of a document in two stages . We first produce continuous vector of each sentence from word representations . Afterwards , we feed sentence vectors as inputs to compose document representation . For modeling the semantics of words , we represent each word as a low dimensional , continu i-th word of a review text . uk and pj are continuous vector representations of user k and product j for capturing user-sentiment and product-sentiment consistencies . Uk and Pj are continuous matrix representations of user k and product j for capturing user-text and product-text consistencies . ous and real-valued vector , also known as word embedding ( Bengio et al. , 2003 ) . All the word vectors are stacked in a word embedding matrix Lw E RdxV , where d is the dimension of word vector and | V | is the size of word vocabulary . These word vectors can be randomly initialized from a uniform distribution , regarded as a parameter and jointly trained with other parameters of neural networks . Alternatively , they can be pretrained from text corpus with embedding learning algorithms ( Mikolov et al . , 2013 ; Pennington et al . , 2014 ; Tang et al . , 2014b ) , and applied as initial values of word embedding matrix . We adopt the latter strategy which better exploits the semantic and grammatical associations of words . To model semantic representations of sentences , convolutional neural network ( CNN ) and recursive neural network ( Socher et al . , 2013 ) are two state-of-the-art methods . We use CNN ( Kim , 2014 ; Kalchbrenner et al . , 2014 ) in this work as it does not rely on external parse tree . Specifically , we use multiple convolutional filters with different widths to produce sentence representation . The reason is that they are capable of capturing local semantics of n-grams of various granularities , which are proven powerful for sentiment classification . The convolutional filter with a width of 3 essentially captures the semantics of trigrams in a sentence . Accordingly , multiple convolutional filters with widths of 1 , 2 and 3 encode the semantics of unigrams , bigrams and trigrams in a sentence . An illustration of CNN with three convolutional filters is given in Figure 3 . Let us denote a sentence consisting of n words as [ w1 , w2 , ... wi , ... wn ] . Each word wi is mapped to its embedding representation ei E Rd. A convolutional filter is a list of linear layers with shared parameters . Let lcf be the width of a convolutional filter , and let Wcf , bcf be the shared parameters of linear layers in the filter . The input of a linear layer is the concatenation of word embeddings in a fixed-length window size lcf , which is denoted as Icf = [ ei ; ei +1 ; ... ; ei + lgf1 ] E Rdlgf . The output of a linear layer is calculated as where Wcf E Rlenxdlgf , bcf E Rlen , len is the output length of linear layer . In order to capture the global semantics of a sentence , we feed the output of a convolutional filter to an average pooling layer , resulting in an output vector with fixedlength . We further add hyperbolic tangent functions ( tanh ) to incorporate element-wise nonlinearity , and fold ( average ) their outputs to generate sentence representation . We feed sentence vectors as the input of an average pooling layer to obtain the document representation . Alternative document modeling approaches include CNN or recurrent neural network . However , we prefer average pooling for its computational efficiency and good performance in our experiment . 3.2 Modeling Semantics of Users and Products We integrate semantic representations of users and products in UPNN to capture user-sentiment , product-sentiment , user-text and product-text consistencies . For modeling user-sentiment and productsentiment consistencies , we embed each user as a continuous vector uk E Rdu and embed each product as a continuous vector pj E Rdp , where du and dp are dimensions of user vector and product vector , respectively . The basic idea behind this is to map users with similar rating preferences ( e.g. prefer assigning 4 stars ) into close vectors in user embedding space . Similarly , the products which receive similar averaged ratings are mapped into neighboring vectors in product embedding space . In order to model user-text consistency , we represent each user as a continuous matrix Uk E RdU d , which acts as an operator to modify the semantic meaning of a word . This is on the basis of vector based semantic composition ( Mitchell and Lapata , 2010 ) . They regard compositional modifier as a matrix X1 to modify another component x2 , and use matrix-vector multiplication y = X1 x x2 as the composition function . Multiplicative semantic composition is suitable for our need of user modifying word meaning , and it has been successfully utilized to model adjectivenoun composition ( Clark et al. , 2008 ; Baroni and Zamparelli , 2010 ) and adverb-adjective composition ( Socher et al. , 2012 ) . Similarly , we model product-text consistency by encoding each product as a matrix Pj E RdP d , where d is the dimension of word vector , dP is the output length of product-word multiplicative composition . After conducting user-word multiplication and productword multiplication operations , we concatenate their outputs and feed them to CNN ( detailed in Section 3.1 ) for producing user and product enhanced document representation . 3.3 Sentiment Classification We apply UPNN to document level sentiment classification under a supervised learning framework ( Pang and Lee , 2005 ) . Instead of using handcrafted features , we use continuous representation of documents , users and products as discriminative features . The sentiment classifier is built from documents with gold standard sentiment labels . As is shown in Figure 2 , the feature representation for building rating predictor is the concatenation of three parts : continuous user representation uk , continuous product representation pj and continuous document representation vd , where vd encodes user-text consistency , product-text consistency and document level semantic composition . We use softmax to build the classifier because its outputs can be interpreted as conditional probabilities . Softmax is calculated as given in Equation 2 , where C is the category number ( e.g. 5 or 10 ) . We regard cross-entropy error between gold sentiment distribution and predicted sentiment distribution as the loss function of softmax . We take the derivative of loss function through back-propagation with respect to the whole set of parameters = and update parameters with stochastic gradient descent . We set the widths of three convolutional filters as 1 , 2 and 3 . We learn 200-dimensional sentiment-specific word embeddings ( Tang et al. , 2014b ) on each dataset separately , randomly initialize other parameters from a uniform distribution U ( 0.01,0.01 ) , and set learning rate as 0.03 . 4 Experiment We conduct experiments to evaluate UPNN by applying it to sentiment classification of documents . 4.1 Experimental Setting Existing benchmark datasets for sentiment classification such as Stanford Sentiment Treebank ( Socher et al . , 2013 ) typically only have text information , but do not contain users who express the sentiment or products which are evaluated . Therefore , we build the datasets by ourselves . In order to obtain large scale corpora without manual annotation , we derive three datasets from IMDB ( Diao et al . , 2014 ) and Yelp Dataset Challenge4 in 2013 and 2014 . The rating scale of IMDB dataset is 1 - 10 . The rating scale of Yelp 2014 and Yelp 2013 datasets is 1 - 5 . | V | is the vocabulary size of words in each dataset . #users is the number of users , #docs / user means the average number of documents per user posts in the corpus . et al. , 2014 ) and Yelp Dataset Challenge4 in 2013 and 2014 . Statistical information of the generated datasets are given in Table 1 . We split each corpus into training , development and testing sets with a 80 / 10 / 10 split , and conduct tokenization and sentence splitting with Stanford CoreNLP ( Manning et al . , 2014 ) . We use standard accuracy ( Manning and Schutze , 1999 ; Jurafsky and Martin , 2000 ) to measure the overall sentiment classification performance , and use MAE and RM5E to measure the divergences between predicted sentiment ratings ( pr ) and ground truth ratings ( gd ) . 4.2 Baseline Methods We compare UPNN with the following baseline methods for document-level sentiment classification . 4.3 Model Comparisons Experimental results are given in Table 2 . The results are separated into two groups : the methods above only use texts of review , and the methods below also use user and product information . From the first group , we can see that majority performs very poor because it does not capture any text or user information . SVM classifiers with trigrams and hand-crafted text features are powerful for document level sentiment classification and hard to beat . We compare the word embedding learnt from each corpus with off-theshell general word embeddings5 . Results show that tailored word embedding from each corpus performs slightly better than general word embeddings ( about 0.01 improvement in terms of accuracy ) . SSWE performs better than context-based word embedding by incorporating sentiment information of texts . Setting a large window size ( e.g. 15 ) is crucial for effectively training SSWE from documents with accompanying senti accuracy ( Acc , higher is better ) , MAE ( lower is better ) and RMSE ( lower is better ) . Our full model is UPNN ( full ) . Our model without using user and product information is abbreviated as UPNN ( no UP ) . The best method in each group is in bold . ment labels . RNTN + R eccurent is a strong performer by effectively modeling document representation with semantic composition . Our text based model ( UPNN no UP ) performs slightly better than RNTN + R eccurent , trigram and text features . From the second group , we can see that concatenating user product feature ( UPF ) with existing feature sets does not show significant improvements . This is because the dimension of existing feature sets is typically huge ( e.g . 1M trigram features in Yelp 2014 ) , so that concatenating a small number of UPF features does not have a great influence on the whole model . We do not evaluate JMARS in terms of accuracy because JMARS outputs real-valued ratings . Our full model UPNN yields the best performance on all three datasets . Incorporating semantic representations of user and product significantly ( t-test with p-value < 0.01 ) boosts our text based model ( UPNN no UP ) . This shows the effectiveness of UPNN over standard trigrams and hand-crafted features when incorporating user and product information . 4.4 Model Analysis: Effect of User and Product Representations We investigate the effects of vector based user and product representations ( uk , pj ) as well as matrix based user and product representations ( Uk , Pj ) for sentiment classification . We remove vector based representations ( uk , pj ) and matrix based representations ( Uk , Pj ) from UPNN separately , and conduct experiments on three datasets . From Table 3 , we can find that vector based representations ( uk , pj ) are more effective than matrix based representations ( Uk , Pj ) . This is because uk and pj encode user-sentiment and product-sentiment consistencies , which are more directly associated with sentiment labels than user-text ( Uk ) and product-text ( Pj ) consistencies . Another reason might be that the parameters of vector representations are less than the matrix representations , so that the vector representations are better estimated . We also see the contribution from each of user and product by removing ( Uk , uk ) and ( Pj , pj ) separately . Results are given in Table 3 . It is interesting to find that user representations are obviously more effective than product representations for review rating prediction . 4.5 Discussion: Out-Of-Vocabulary Users and Products Out-of-vocabulary ( OOV ) situation occurs if a user or a product in testing/decoding process is never seen in training data . We give two natural solutions ( avg UP and unk UP ) to deal with OOV users and products . One solution ( avg UP ) is to regard the averaged representations of users/products in training data as the representation of OOV user/product . Another way ( unk UP ) is to learn a shared unknown user/product representation for low-frequency users in training data , and apply it to OOV user/product . In order to evaluate the two strategies for OOV problem , we randomly select 10 percent users and products from each development set , and mask their user and product information . We run avg UP , unk UP together with UPNN ( no UP ) which only uses text information , and UPNN ( full ) which learns tailored representation for each user and product . We evaluate classification accuracy on the extracted OOV test set . Experimental results are given in Figure 5 . We can find that these two strategies perform slightly better than UPNN ( no UP ) , but still worse than the full model . 5 Related Work 5.1 Sentiment Classification Sentiment classification is a fundamental problem in sentiment analysis , which targets at inferring the sentiment label of a document . Pang and Lee ( 2002 ; 2005 ) cast this problem a classification task , and use machine learning method in a supervised learning framework . Goldberg and Zhu ( 2006 ) use unlabelled reviews in a graphbased semi-supervised learning method . Many studies design effective features , such as text topic ( Ganu et al. , 2009 ) , bag-of-opinion ( Qu et al. , 2010 ) and sentiment lexicon features ( Kiritchenko et al. , 2014 ) . User information is also used for sentiment classification . Gao et al. ( 2013 ) design user-specific features to capture user leniency . Li et al. ( 2014 ) incorporate textual topic and user-word factors with supervised topic modeling . Tan et al. ( 2011 ) and Hu et al. ( 2013 ) utilize usertext and user-user relations for Twitter sentiment analysis . Unlike most previous studies that use hand-crafted features , we learn discriminative features from data . We differ from Li et al. ( 2014 ) in that we encode four kinds of consistencies and use neural network approach . User representation is also leveraged for recommendation ( Weston et al. , 2013 ) , web search ( Song et al. , 2014 ) and social media analytics ( Perozzi et al. , 2014 ) . 5.2 Neural Network for Sentiment Classification Neural networks have achieved promising results for sentiment classification . Existing neural network methods can be divided into two groups : word embedding and semantic composition . For learning word embeddings , ( Mikolov et al. , 2013 ; Pennington et al. , 2014 ) use local and global contexts , ( Maas et al. , 2011 ; Labutov and Lipson , 2013 ; Tang et al. , 2014b ; Tang et al. , 2014a ; Zhou et al. , 2015 ) further incorporate sentiment of texts . For learning semantic composition , Glorot et al . ( 2011 ) use stacked denoising autoencoder , Socher et al . ( 2013 ) introduce a family of recursive deep neural networks ( RNN ) . RNN is extended with adaptive composition functions ( Dong et al . , 2014 ) , global feedbackward ( Paulus et al . , 2014 ) , feature weight tuning ( Li , 2014 ) , and also used for opinion relation detection ( Xu et al . , 2014 ) . Li et al . ( 2015 ) compare the effectiveness of recursive neural network and recurrent neural network on five NLP tasks including sentiment classification . ( Kalchbrenner et al . , 2014 ; Kim , 2014 ; Johnson and Zhang , 2014 ) use convolutional neural networks . Le and Mikolov ( 2014 ) introduce Paragraph Vector . Unlike existing neural network approaches that only use the semantics of texts , we take consideration of user and product representations and leverage their connections with text semantics for sentiment classification . This work is an extension of our previous work ( Tang et al. , 2015 ) , which only takes consideration of userword association . 6 Conclusion In this paper , we introduce User Product Neural Network ( UPNN ) for document level sentiment classification under a supervised learning framework . We validate user-sentiment , productsentiment , user-text and product-text consistencies on massive reviews , and effectively integrate them in UPNN . We apply the model to three datasets derived from IMDB and Yelp Dataset Challenge . Empirical results show that : ( 1 ) UPNN outperforms state-of-the-art methods for document level sentiment classification ; ( 2 ) incorporating continuous user and product representations significantly boosts sentiment classification accuracy . "
            }
        },
        {
            "id": "P15-1101",
            "result": [
                {
                    "value": {
                        "start": 444,
                        "end": 462,
                        "text": "factor graph model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1101:E0"
                },
                {
                    "value": {
                        "start": 14406,
                        "end": 14418,
                        "text": "Hamming loss",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P15-1101:E1"
                },
                {
                    "value": {
                        "start": 14479,
                        "end": 14487,
                        "text": "accuracy",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P15-1101:E2"
                },
                {
                    "value": {
                        "start": 14492,
                        "end": 14501,
                        "text": "F-measure",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P15-1101:E3"
                },
                {
                    "value": {
                        "start": 14716,
                        "end": 14749,
                        "text": "maximum entropy ( ME ) classifier",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1101:E4"
                },
                {
                    "value": {
                        "start": 15044,
                        "end": 15060,
                        "text": "Bayesian network",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1101:E5"
                },
                {
                    "value": {
                        "start": 3702,
                        "end": 3714,
                        "text": "DFG approach",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1101:E6"
                },
                {
                    "value": {
                        "start": 15514,
                        "end": 15519,
                        "text": "Hloss",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P15-1101:E7"
                },
                {
                    "value": {
                        "start": 15541,
                        "end": 15549,
                        "text": "Accuracy",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P15-1101:E8"
                },
                {
                    "value": {
                        "start": 15575,
                        "end": 15577,
                        "text": "F1",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P15-1101:E9"
                },
                {
                    "value": {
                        "start": 5635,
                        "end": 5670,
                        "text": "KNN-based classification algorithms",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1101:E10"
                },
                {
                    "value": {
                        "start": 5911,
                        "end": 5934,
                        "text": "coarse-to-fine strategy",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1101:E11"
                },
                {
                    "value": {
                        "start": 6271,
                        "end": 6287,
                        "text": "Binary Relevance",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1101:E12"
                },
                {
                    "value": {
                        "start": 6562,
                        "end": 6589,
                        "text": "factor graph-based approach",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1101:E13"
                },
                {
                    "value": {
                        "start": 10023,
                        "end": 10060,
                        "text": "dependence factor graph ( DFG ) model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1101:E14"
                },
                {
                    "value": {
                        "start": 12547,
                        "end": 12569,
                        "text": "gradient decent method",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1101:E15"
                },
                {
                    "value": {
                        "start": 12754,
                        "end": 12796,
                        "text": "Loopy Belief Propagation ( LBP ) algorithm",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1101:E16"
                },
                {
                    "value": {
                        "start": 12738,
                        "end": 12741,
                        "text": "LBP",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1101:E17"
                },
                {
                    "value": {
                        "start": 15733,
                        "end": 15748,
                        "text": "LabelD approach",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1101:E18"
                },
                {
                    "value": {
                        "start": 14828,
                        "end": 14834,
                        "text": "LabelD",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1101:E19"
                },
                {
                    "value": {
                        "start": 17948,
                        "end": 17965,
                        "text": "Transfer approach",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P15-1101:E20"
                }
            ],
            "data": {
                "text": "Sentence-level Emotion Classification with Label and Context Dependence Abstract Predicting emotion categories , such as anger , joy , and anxiety , expressed by a sentence is challenging due to its inherent multi-label classification difficulty and data sparseness . In this paper , we address above two challenges by incorporating the label dependence among the emotion labels and the context dependence among the contextual instances into a factor graph model . Specifically , we recast sentence-level emotion classification as a factor graph inferring problem in which the label and context dependence are modeled as various factor functions . Empirical evaluation demonstrates the great potential and effectiveness of our proposed approach to sentencelevel emotion classification . 1 Introduction Predicting emotion categories , such as anger , joy , and anxiety , expressed by a piece of text encompasses a variety of applications , such as online chatting ( Galik et al. , 2012 ) , news classification ( Liu et al. , 2013 ) and stock marketing ( Bollen et al. , 2011 ) . Over the past decade , there has been a substantial body of research on emotion classification , where a considerable amount of work has focused on document-level emotion classification . Recently , the research community has become increasingly aware of the need on sentence-level emotion classification due to its wide potential applications , e.g. the massively growing importance of analyzing short text in social media ( Kiritchenko et al. , 2014 ; Wen and Wan , 2014 ) . In general , sentence-level emotion classification exhibits two challenges . sentences therein with their emotion categories from the corpus collected by Quan and Ren ( 2009 ) On one hand , like document-level emotion classification , sentence-level emotion classification is naturally a multi-label classification problem . That is , each sentence might involve more than one emotion category . For example , as shown in Figure 1 , in one paragraph , two sentences , i.e. , S1 and S3 , have two and three emotion categories respectively . Automatically classifying instances with multiple possible categories is ... . sometimes much more difficult than classifying instances with a single label . On the other hand , unlike document-level emotion classification , sentence-level emotion classification is prone to the data sparseness problem because a sentence normally contains much less content . Given the short text of a sentence , it is often difficult to predict its emotion due to the limited information therein . For example , in S2 , only one phrase W ( that is all I want ) expresses the joy emotion . Once this phrase fails to appear in the training data , it will be hard for the classifier to give a correct prediction according to the limited content in this sentence . In this paper , we address above two challenges in sentence-level emotion classification by modeling both the label and context dependence . Here , the label dependence indicates that multiple emotion labels of an instance are highly correlated to each other . For instance , the two positive emotions , joy and love , are more likely to appear at the same time than the two counterpart emotions , joy and hate . The context dependence indicates that two neighboring sentences or two sentences in the same paragraph ( or document ) might share the same emotion categories . For instance , in Figure 1 , S1 , S2 , and S3 , from the same paragraph , all share the emotion category joy . Specifically , we propose a factor graph , namely Dependence Factor Graph ( DFG ) , to model the label and context dependence in sentence-level emotion classification . In our DFG approach , both the label and context dependence are modeled as various factor functions and the learning task aims to maximize the joint probability of all these factor functions . Empirical evaluation demonstrates the effectiveness of our DFG approach to capturing the inherent label and context dependence . To the best of our knowledge , this work is the first attempt to incorporate both the label and context dependence of sentence-level emotion classification into a unified framework . The remainder of this paper is organized as follows . Section 2 overviews related work on emotion analysis . Section 3 presents our observations on label and context dependence in the corpus . Section 4 proposes our DFG approach to sentence-level emotion classification . Section 5 evaluates the proposed approach . Finally , Section 6 gives the conclusion and future work . 2 Related Work Over the last decade , there has been an explosion of work exploring various aspects of emotion analysis , such as emotion resource creation ( Wiebe et al. , 2005 ; Quan and Ren , 2009 ; Xu et al. , 2010 ) , writers emotion vs. readers emotion analysis ( Lin et al. , 2008 ; Liu et al. , 2013 ) , emotion cause event analysis ( Chen et al. , 2010 ) , document-level emotion classification ( Alm et al. , 2005 ; Li et al. , 2014 ) and sentence-level or short text-level emotion classification ( Tokushisa et al. , 2008 ; Bhowmick et al. , 2009 ; Xu et al. , 2012 ) . This work focuses on sentence-level emotion classification . Among the studies on sentence-level emotion classification , Tokushisa et al. ( 2008 ) propose a data-oriented method for inferring the emotion of an utterance sentence in a dialog system . They leverage a huge collection of emotion-provoking event instances from the Web to deal with the data sparseness problem in sentence-level emotion classification . Bhowmick et al . ( 2009 ) and Bhowmick et al . ( 2010 ) apply KNN-based classification algorithms to classify news sentences into multiple reader emotion categories . Although the multi-label classification difficulty has been noticed in their study , the label dependence is not exploited . More recently , Xu et al. ( 2012 ) proposes a coarse-to-fine strategy for sentence-level emotion classification . They deal with the data sparseness problem by incorporating the transfer probabilities from the neighboring sentences to refine the emotion categories . To some extent , this can be seen a specific kind of context information . However , they ignore the label dependence by directly applying Binary Relevance to overcome the multi-label classification difficulty . Unlike all above studies , this paper emphasizes the importance of the label dependence and exploits it in sentence-level emotion classification via a factor graph model . Moreover , besides the label dependence , our factor graph-based approach incorporates the context dependence in a unified framework to further improve the performance of sentence-level emotion classification . 3 Observations To better illustrate our motivation of modeling the label and context dependence , we systematically investigate both dependence phenomena in our evaluation corpus . The corpus contains 100 documents , randomly selected from Quan and Ren ( 2009 ) . There are totally 2751 sentences and each of them is manually annotated with one or more emotion labels . Table 1 shows the sentence distribution of the eight emotion categories . Obviously , the distribution is a bit imbalanced . While about to one quarter of sentences express the emotion category love , only 6 % and 10 % express surprise and anger respectively , with the remaining 5 emotion categories distributed rather evenly from 20 % to 25 % . Table 2 shows the numbers of the sentences grouped by the emotion labels they contain . From this table , we can see that more than half sentences have two or more emotion labels . This indicates the popularity of the multi-label issue in sentence-level emotion classification . To investigate the phenomenon of label dependence , we first assume that denotes an input domain of instances and Y = 11 , 12 , ... ,1.1 be a finite domain of possible emotion labels . Each instance is associated with a subset of Y and this subset is described as an in-dimensional vector y = [ y ' , yz , ... , ym ] where y ` = 1 only if instance x has label l ; . and otherwise . Then , we can calculate the probability that an instance takes both emotion labels l ; and , denoted as p ( l ; , lj ) . Figure 2 shows the probability distribution of most and least frequently-occurred pairs of emotion categories , with left four most frequently-occurred and right four least frequentlyoccurred , among all 28 pairs . From this figure , we can see that some pairs , e.g. , joy and love , are much more likely to be taken by one sentence than some other pairs , e.g. joy and anger . Finally , we investigate the phenomenon of the context dependence by calculating the probabilities that two instances xk and x , have at least one identical emotion label , i.e. , p ( y , in different settings . f o represents a factor function for modeling textual features . represents a factor function for modeling the label dependence between two pseudo samples . represents a factor function for modeling the context dependence between two instances in the same context . Figure 3 : Probabilities that two instances have an identical emotion label in different settings Figure 3 shows the probabilities that two instances have at least one identical emotion label in different settings , where neighbor , paragraph , document and random mean two neighboring instances , two instances from the same paragraph , two instances from the same document , and two instances from a random selection , respectively . From this figure , we can see that two instances from the same context are much more likely to take an identical emotion label than two random instances . From above statistics , we come to two basic observations : hate and angry than some other pair of emotion labels , e.g. , hate and happy . 2 ) Context dependency : Two instances from the same context are more likely to share the same emotion label than those from a random selection . 4 Dependence Factor Graph Model In this section , we propose a dependence factor graph ( DFG ) model for learning emotion labels of sentences with both label and context dependence . 4.1 Preliminary Factor Graph A factor graph consists of two layers of nodes , i.e. , variable nodes and factor nodes , with links between them . The joint distribution over the whole set of variables can be factorized as a product of all factors . Figure 4 gives an example of our dependence factor graph ( DFG ) when two instances , i.e . , sentence - 1 and sentence - 2 are involved . Binary Relevance A popular solution to multi-label classification is called binary relevance which constructs a binary classifier for each label , resulting a set of inde pendent binary classification problems ( Tsoumakas and Katakis , 2007 ; Tsoumakas et al. , 2009 ) . In our approach , binary relevance is utilized as a preliminary step so that each original instance is transformed into K pseudo samples , where K is the number of categories . For example , in Figure 4 , , , and represent the three pseudo samples , generated from the same original instance sentence-1 . 4.2 Model Definition Formally , let G = ( V , E , X ) represent an instance network , where V denotes a set of sentence instances . is a set of relationships between sentences . Two kinds of relationship exist in our instance network : One represents the label dependence between each two pseudo instances generated from the same original instance , while the other represents the context dependence when the two instances are from the same context , e.g. , the same paragraph . is the textual feature vector associated with a sentence . We model the above network with a factor graph and our objective is to infer the emotion categories of instances by learning the following joint distribution : where three kinds of factor functions are used . functions associated with each text . The textual feature factor function is instantiated as follows : ( 3 ) Where/3 , is the weight of the function , representing the influence degree of the two instancesyk andy ; . 3 ) Context dependence factor function : h ( yk , H ( yk ) ) denotes the additional context dependence relationship among the instances , whereH ( yk ) is the set of the instances connected to H ( yk ) andyk are the labels of the pseudo instances from the same context but generated from different original instances . The context dependence factor function is inample , we can write the gradient of eachak with regard to the objective function : In this study , we employ the gradient decent method to optimize the objective function . For exWhere E C ( D ( x , , yk ) ] is the expectation of feature function ( D ( x , , yk ) given the data distribution . Note that LBP denotes the Loopy Belief Propagation ( LBP ) algorithm which is applied to approximately infer the marginal distribution in a factor graph ( Frey and MacKay , 1998 ) . A similar gradient can be derived for the other parameters . Ren , 2009 ) . In our experiments , we use 80 documents as the training data and the remaining 20 documents as the test data . 4.4 Model Prediction With the learned parameter configuration , the prediction task is to find a which optimizes the objective function , i.e. , Where are the labels of the instances in the testing data . Again , we utilize LBP to calculate the marginal probability of each instance and predict the label with the largest marginal probability . As all instances in the test data are concerned , above prediction is performed in an iteration process until the results converge . 5 Experimentation We have systematically evaluated our DFG approach to sentence-level emotion classification . 5.1 Experimental Setting Corpus The corpus contains 100 documents ( 2751 sentences ) from the Ren-CECps corpus ( Quan and Each instance is treated as a bag-of-words and transformed into a binary vector encoding the presence or absence of word unigrams . Evaluation Metrics In our study , we employ three evaluation metrics to measure the performances of different approaches to sentence-level emotion classification . These metrics have been popularly used in some multi-label classification problems ( Godbole and Sarawagi , 2004 ; Schapire and Singer , 2000 ) . where q is the number of all test instances and m is the number of all emotion labels . is the estimated label while is the true label . Note that smaller Hamming loss corresponds to better classification quality , while larger accuracy and F-measure corresponds to better classification quality . In this section , we compare following approaches which only consider the label dependence among pseudo instances : Baseline : As a baseline , this approach applies a maximum entropy ( ME ) classifier with only textual features , ignoring both the label and context dependence . LabelD : As the state-of-the-art approach to handling multi-label classification , this approach incorporates label dependence , as described in ( Wang et al. , 2014 ) . Specifically , this approach first utilizes a Bayesian network to infer the relationship among the labels and then employ them in the classifier . DFG-label : Our DFG approach with the label dependence . Figure 6 compares the performance of different approaches to sentence-level emotion classification with the label dependence . From this figure , we can see that our DFG approach improves the baseline approach with an impressive improvement in all three kinds of evaluation metrics , i.e . , 23.5 % reduction in Hloss , 25.6 % increase in Accuracy , and 11.8 % increase in F1 . This result verifies the effectiveness of incorporating the label dependence in sentence-level emotion classification . Compared to the state-of-the-art LabelD approach , our DFG approach is much superior . Significant test show that our DFG approach significantly outperforms both the baseline approach and LabelD ( p-value < 0.01 ) . One reason that LabelD performs worse than our approach is possibly due to their separating learning on textual features and label relationships . Also , different from ours , their approach could not capture the information between two conflict emotion labels , such as happy and sad ( they are not possibly appearing together ) . 5.3 Experimental Results with Context Dependence In this section , we compare following approaches which only consider the context dependence among pseudo instances : Baseline : same as the one in Section 5.2 , which applies a maximum entropy ( ME ) classifier with only textual features , ignoring both the label and context dependence . Transfer : As the state-of-the-art approach to incorporating contextual information in sentence-level emotion classification ( Xu et al. , 2012 ) , this approach utilizes the label transformation probability to refine the classification results . DFG-label ( Neighbor ) : Our DFG approach with the context dependence only . Specifically , the neighboring instances are considered as context . DFG-label ( Paragraph ) : Our DFG approach with the context dependence only . Specifically , the instances in the same paragraph are considered as context . DFG-label ( Document ) : Our DFG approach with the context dependence only . Specifically , the instances in the same document are considered as context . Figure 7 compares the performance of different approaches to sentence-level emotion classification with the context dependence only . From this figure , we can see that our DFG approach consistently improves the state-of-the-art in all three kinds of evaluation metrics , i.e . , 6.1 % reduction in Hloss , 6.5 % increase in Accuracy , and 3.1 % increase in F1 when the neighboring instances are considered as context . Among the three kinds of context , the neighboring setting performs best . We also find that using the whole document as the context is not helpful and it performs even worse than the baseline approach . Compared to the stateof-the-art Transfer approach , our DFG approach with the neighboring context dependence is much superior . Significant test show that our DFG approach with the neighboring context dependence significantly outperforms the baseline approach and the state-of-the-art LabelD approach ( pvalue < 0.01 ) . 5.4 Experimental Results with Both Label and Context Dependence Table 3 shows the performance of our DFG approach with both label and context dependence , denoted as DGF-both . From this table , we can see that using both label and context dependence further improves the performance . Figure 8 shows the performance of our DGF-both approach when different sizes of training data are used to train the model . From this figure , we can see that incorporating both the label and context dependence consistently improves the performance with a large margin , irrespective of the amount of training data available . proach when different sizes of training data are used 6 Conclusion In this paper , we propose a novel approach to sentence-level emotion classification by incorporating both the label dependence among the emotion labels and the context dependence among the contextual instances into a factor graph , where the label and context dependence is modeled as various factor functions . Empirical evaluation shows that our DFG approach performs significantly better than the state-of-the-art . In the future work , we would like to explore better ways of modeling the label and context dependence and apply our DFG approach in more applications , e.g . micro-blogging emotion classification . "
            }
        },
        {
            "id": "P16-1007",
            "result": [
                {
                    "value": {
                        "start": 25705,
                        "end": 25724,
                        "text": "phrase-based models",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E0"
                },
                {
                    "value": {
                        "start": 457,
                        "end": 477,
                        "text": "beam search strategy",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E1"
                },
                {
                    "value": {
                        "start": 483,
                        "end": 507,
                        "text": "n-best extraction method",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E2"
                },
                {
                    "value": {
                        "start": 575,
                        "end": 599,
                        "text": "hierarchical joint model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E3"
                },
                {
                    "value": {
                        "start": 697,
                        "end": 705,
                        "text": "accuracy",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P16-1007:E4"
                },
                {
                    "value": {
                        "start": 224,
                        "end": 243,
                        "text": "phrase-based system",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E5"
                },
                {
                    "value": {
                        "start": 788,
                        "end": 823,
                        "text": "recurrent neural translation system",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E6"
                },
                {
                    "value": {
                        "start": 82,
                        "end": 94,
                        "text": "phrase-based",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E7"
                },
                {
                    "value": {
                        "start": 3639,
                        "end": 3656,
                        "text": "neural approaches",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E8"
                },
                {
                    "value": {
                        "start": 4823,
                        "end": 4831,
                        "text": "Accuracy",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P16-1007:E9"
                },
                {
                    "value": {
                        "start": 4834,
                        "end": 4837,
                        "text": "WPA",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P16-1007:E10"
                },
                {
                    "value": {
                        "start": 5749,
                        "end": 5771,
                        "text": "Prefix-Bleu ( pxBleu )",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P16-1007:E11"
                },
                {
                    "value": {
                        "start": 5756,
                        "end": 5760,
                        "text": "Bleu",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P16-1007:E12"
                },
                {
                    "value": {
                        "start": 5854,
                        "end": 5860,
                        "text": "n-gram",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E13"
                },
                {
                    "value": {
                        "start": 5861,
                        "end": 5871,
                        "text": "precisions",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P16-1007:E14"
                },
                {
                    "value": {
                        "start": 5894,
                        "end": 5909,
                        "text": "brevity penalty",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P16-1007:E15"
                },
                {
                    "value": {
                        "start": 6064,
                        "end": 6068,
                        "text": "BLEU",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P16-1007:E16"
                },
                {
                    "value": {
                        "start": 5763,
                        "end": 5769,
                        "text": "pxBleu",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P16-1007:E17"
                },
                {
                    "value": {
                        "start": 5861,
                        "end": 5870,
                        "text": "precision",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P16-1007:E18"
                },
                {
                    "value": {
                        "start": 7118,
                        "end": 7125,
                        "text": "n-grams",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E19"
                },
                {
                    "value": {
                        "start": 8876,
                        "end": 8895,
                        "text": "log-linear approach",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E20"
                },
                {
                    "value": {
                        "start": 9335,
                        "end": 9356,
                        "text": "phrase-based decoding",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E21"
                },
                {
                    "value": {
                        "start": 457,
                        "end": 468,
                        "text": "beam search",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E22"
                },
                {
                    "value": {
                        "start": 1428,
                        "end": 1448,
                        "text": "constrained decoding",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E23"
                },
                {
                    "value": {
                        "start": 9843,
                        "end": 9861,
                        "text": "target beam search",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E24"
                },
                {
                    "value": {
                        "start": 11627,
                        "end": 11638,
                        "text": "IBM Model 2",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E25"
                },
                {
                    "value": {
                        "start": 12927,
                        "end": 12934,
                        "text": "AdaGrad",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E26"
                },
                {
                    "value": {
                        "start": 12965,
                        "end": 12990,
                        "text": "online subgradient method",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E27"
                },
                {
                    "value": {
                        "start": 8899,
                        "end": 8923,
                        "text": "phrase-based translation",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E28"
                },
                {
                    "value": {
                        "start": 13427,
                        "end": 13446,
                        "text": "unified joint model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E29"
                },
                {
                    "value": {
                        "start": 13462,
                        "end": 13492,
                        "text": "hierarchical adaptation method",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E30"
                },
                {
                    "value": {
                        "start": 13541,
                        "end": 13586,
                        "text": "frustratingly easy domain adaptation ( FEDA )",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E31"
                },
                {
                    "value": {
                        "start": 7907,
                        "end": 7910,
                        "text": "KSR",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P16-1007:E32"
                },
                {
                    "value": {
                        "start": 17268,
                        "end": 17292,
                        "text": "recurrent neural network",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E33"
                },
                {
                    "value": {
                        "start": 10251,
                        "end": 10258,
                        "text": "Phrasal",
                        "labels": [
                            "Tool"
                        ]
                    },
                    "id": "P16-1007:E34"
                },
                {
                    "value": {
                        "start": 18761,
                        "end": 18766,
                        "text": "mgiza",
                        "labels": [
                            "Tool"
                        ]
                    },
                    "id": "P16-1007:E35"
                },
                {
                    "value": {
                        "start": 18805,
                        "end": 18837,
                        "text": "5 - gram language models ( LMs )",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E36"
                },
                {
                    "value": {
                        "start": 18843,
                        "end": 18848,
                        "text": "KenLM",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E37"
                },
                {
                    "value": {
                        "start": 18963,
                        "end": 18975,
                        "text": "Common Crawl",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P16-1007:E38"
                },
                {
                    "value": {
                        "start": 18980,
                        "end": 18996,
                        "text": "Europarl corpora",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P16-1007:E39"
                },
                {
                    "value": {
                        "start": 19002,
                        "end": 19010,
                        "text": "WMT 2015",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P16-1007:E40"
                },
                {
                    "value": {
                        "start": 19223,
                        "end": 19238,
                        "text": "OPUS collection",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P16-1007:E41"
                },
                {
                    "value": {
                        "start": 19347,
                        "end": 19366,
                        "text": "Common Crawl corpus",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P16-1007:E42"
                },
                {
                    "value": {
                        "start": 19485,
                        "end": 19514,
                        "text": "Autodesk post editing corpus4",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P16-1007:E43"
                },
                {
                    "value": {
                        "start": 19565,
                        "end": 19577,
                        "text": "newstest2014",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P16-1007:E44"
                },
                {
                    "value": {
                        "start": 19597,
                        "end": 19609,
                        "text": "newstest2015",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P16-1007:E45"
                },
                {
                    "value": {
                        "start": 19632,
                        "end": 19641,
                        "text": "WMT 20165",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P16-1007:E46"
                },
                {
                    "value": {
                        "start": 19757,
                        "end": 19770,
                        "text": "Autodesk data",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P16-1007:E47"
                },
                {
                    "value": {
                        "start": 19778,
                        "end": 19799,
                        "text": "newstest2013 data set",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P16-1007:E48"
                },
                {
                    "value": {
                        "start": 20174,
                        "end": 20194,
                        "text": "attention mechanisms",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E49"
                },
                {
                    "value": {
                        "start": 17889,
                        "end": 17912,
                        "text": "constrained beam search",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E50"
                },
                {
                    "value": {
                        "start": 20502,
                        "end": 20528,
                        "text": "phrase-based model ( PBM )",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E51"
                },
                {
                    "value": {
                        "start": 21554,
                        "end": 21567,
                        "text": "prefix tuning",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E52"
                },
                {
                    "value": {
                        "start": 21684,
                        "end": 21690,
                        "text": "recall",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P16-1007:E53"
                },
                {
                    "value": {
                        "start": 483,
                        "end": 500,
                        "text": "n-best extraction",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E54"
                },
                {
                    "value": {
                        "start": 20502,
                        "end": 20520,
                        "text": "phrase-based model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E55"
                },
                {
                    "value": {
                        "start": 20135,
                        "end": 20143,
                        "text": "ensemble",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E56"
                },
                {
                    "value": {
                        "start": 19525,
                        "end": 19536,
                        "text": "news domain",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P16-1007:E57"
                },
                {
                    "value": {
                        "start": 23543,
                        "end": 23568,
                        "text": "single network NMT system",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E58"
                },
                {
                    "value": {
                        "start": 23666,
                        "end": 23669,
                        "text": "CPU",
                        "labels": [
                            "Tool"
                        ]
                    },
                    "id": "P16-1007:E59"
                },
                {
                    "value": {
                        "start": 23693,
                        "end": 23696,
                        "text": "GPU",
                        "labels": [
                            "Tool"
                        ]
                    },
                    "id": "P16-1007:E60"
                },
                {
                    "value": {
                        "start": 16832,
                        "end": 16835,
                        "text": "NMT",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E61"
                },
                {
                    "value": {
                        "start": 23942,
                        "end": 23963,
                        "text": "newstest2015 data set",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P16-1007:E62"
                },
                {
                    "value": {
                        "start": 14575,
                        "end": 14595,
                        "text": "phrase-based systems",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E63"
                },
                {
                    "value": {
                        "start": 25730,
                        "end": 25765,
                        "text": "stochastic finite state transducers",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E64"
                },
                {
                    "value": {
                        "start": 27198,
                        "end": 27221,
                        "text": "recurrent neural system",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E65"
                },
                {
                    "value": {
                        "start": 15182,
                        "end": 15194,
                        "text": "n-best lists",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P16-1007:E66"
                },
                {
                    "value": {
                        "start": 27397,
                        "end": 27408,
                        "text": "prefix-Bleu",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P16-1007:E67"
                },
                {
                    "value": {
                        "start": 27832,
                        "end": 27863,
                        "text": "phrase-based translation system",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1007:E68"
                }
            ],
            "data": {
                "text": "Models and Inference for Prefix-Constrained Machine Translation Abstract We apply phrase-based and neural models to a core task in interactive machine translation : suggesting how to complete a partial translation . For the phrase-based system, we demonstrate improvements in suggestion quality using novel objective functions, learning techniques, and inference algorithms tailored to this task. Our contributions include new tunable metrics , an improved beam search strategy , an n-best extraction method that increases suggestion diversity , and a tuning procedure for a hierarchical joint model of alignment and translation . The combination of these techniques improves next-word suggestion accuracy dramatically from 28.5% to 41.2% in a large-scale English-German experiment . Our recurrent neural translation system increases accuracy yet further to 53.0% , but inference is two orders of magnitude slower . Manual error analysis shows the strengths and weaknesses of both approaches. 1 Introduction A core prediction task in interactive machine translation ( MT ) is to complete a partial translation ( Ortiz-Martínez et al., 2009; Koehn et al., 2014 ). Sentence completion enables interfaces that are richer than basic post-editing of MT output. For example, the translator can receive updated suggestions after each word typed ( Langlais et al., 2000 ). However, we show that completing partial translations by naïve constrained decoding—the standard in prior work—yields poor suggestion quality. We describe new phrase-based objective functions, learning techniques, and inference algorithms for the sentence completion task.1 We then compare this improved phrase-based system to a state-of-the-art recurrent neural translation system in large-scale English-German experiments. A system for completing partial translations takes as input a source sentence and a prefix of the target sentence. It predicts a suffix: a sequence of tokens that extends the prefix to form a full sentence. In an interactive setting, the first words of the suffix are critical; these words are the focus of the user’s attention and can typically be appended to the translation with a single keystroke. We introduce a tuning metric that scores correctness of the whole suffix, but is particularly sensitive to these first words. Phrase-based inference for this task involves aligning the prefix to the source, then generating the suffix by translating the unaligned words. We describe a beam search strategy and a hierarchical joint model of alignment and translation that together improve suggestions dramatically . For English-German news , next-word accuracy increases from 28.5% to 41.2% . An interactiveMT system could also display multiple suggestions to the user. We describe an algorithm for efficiently finding the n-best next words directly following a prefix and their corresponding best suffixes. Our experiments show that this approach to n-best list extraction , combined with our other improvements , increased next-word suggestion accuracy of 10 - best lists from 33.4% to 55.5% . We also train a recurrent neural translation system to maximize the conditional likelihood of the next word following a translation prefix , which is both a standard training objective in neural translation and an ideal fit for our task . This neural system provides even more accurate predictions than our improved phrase-based system . However, inference is two orders of magnitude slower, which is problematic for an interactive setting. We conclude with a manual error analysis that reveals the strengths and weaknesses of both the phrase-based and neural approaches to suffix prediction . 2 Evaluating Suffix Prediction Let F and E denote the set of all source and target language strings, respectively. Given a source sentence f ∈ F and target prefix ep ∈ E , a predicted suffix es ∈ E can be evaluated by comparing the full sentence e = epes to a reference e∗. Let e∗s denote the suffix of the reference that follows ep. We define three metrics below that score translations by the characteristics that are most relevant in an interactive setting : the accuracy of the first words of the suffix and the overall quality of the suffix . Each metric takes example triples ( f, ep, e∗ ) produced during an interactiveMT session in which ep was generated in the process of constructing e∗. A simulated corpus of examples can be produced from a parallel corpus of ( f, e∗ ) pairs by selecting prefixes of each e∗. An exhaustive simulation selects all possible prefixes, while a sampled simulation selects only k prefixes uniformly at random for each e∗. Computing metrics for exhaustive simulations is expensive because it requires performing suffix prediction inference for every prefix: |e∗| times for each reference. Word Prediction Accuracy ( WPA ) or nextword accuracy ( Koehn et al . , 2014 ) is 1 if the first word of the predicted suffix es is also the first word of reference suffix e ∗ s , and 0 otherwise . Averaging over examples gives the frequency that the word following the prefix was predicted correctly. In a sampled simulation, all reference words that follow the first word of a sampled suffix are ignored by the metric, so most reference information is unused. Number of Predicted Words ( # prd ) is the maximum number of contiguous words at the start of the predicted suffix that match the reference . Like WPA , this metric is 0 if the first word of es is not also the first word of e ∗ s . In a sampled simulation, all reference words that follow the first mis-predicted word in the sampled suffix are ignored. While it is possible that the metric will require the full reference suffix, most reference information is unused in practice. Prefix-Bleu ( pxBleu ) : Bleu ( Papineni et al . , 2002 ) is computed from the geometric mean of clipped n-gram precisions precn ( · , · ) and a brevity penalty BP ( · , · ) . Given a sequence of references E∗ = e∗1, . . . , e∗t and corresponding predictions E = e1, . . . , et. Ortiz-Martínez et al . ( 2010 ) use BLEU directly for training an interactive system , but we propose a variant that only scores the predicted suffix and not the input prefix . The pxBleu metric computes Bleu ( Eˆ , Eˆ ∗ ) for the following constructed sequences Eˆ and Eˆ ∗ : For each ( f, ep, e∗ ) and suffix prediction es, Eˆ includes the full sentence e = epes. For each ( f, ep, e∗ ), Eˆ∗ is a masked copy of e∗ in which all prefix words that do not match any word in e are replaced by null tokens. This construction maintains the original computation of the brevity penalty , but does not include the prefix in the precision calculations . Unlike the two previous metrics , the pxBleu metric uses all available reference information . In order to account for boundary conditions, the reference e∗ is masked by the prefix ep as follows: we replace each of the first |ep − 3| words with a null token enull, unless the word also appears in the suffix e∗s. Masking retains the last three words of the prefix so that the first words after the prefix can contribute to the precision of all n-grams that overlap with the prefix , up to n = 4 . Words that also appear in the suffix are retained so that their correct prediction in the suffix can contribute to those precisions , which would otherwise be clipped . 2.1 Loss Functions for Learning All of these metrics can be used as the tuning objective of a phrase-based machine translation system . Tuning toward a sampled simulation that includes one or two prefixes per reference is much faster than using an exhaustive set of prefixes. A linear combination of these metrics can be used to trade off the relative importance of the full suffix and the words immediately following the prefix. With a combined metric, learning can focus on these words while using all available information in the references. 2.2 Keystroke Ratio ( KSR ) In addition to these metrics , suffix prediction can be evaluated by the widely used keystroke ratio ( KSR ) metric ( Och et al . , 2003 ) . This ratio assumes that any number of characters from the beginning of the suggested suffix can be appended to the user prefix using a single keystroke. It computes the ratio of key strokes required to enter the reference interactively to the character count of the reference. Our MT architecture does not permit tuning to KSR . Other methods of quantifying effort in an interactive MT system are more appropriate for user studies than for direct evaluation of MT predictions. For example , measuring pupil dilation , pause duration and frequency ( Schilperoord , 1996 ) , mouse-action ratio ( Sanchis-Trilles et al . , 2008 ) , or source difficulty ( Bernth and McCord , 2000 ) would certainly be relevant for evaluating a full interactive system , but are beyond the scope of this work . 3 Phrase-Based Inference In the log-linear approach to phrase-based translation ( Och and Ney, 2004 ), the distribution of translations e ∈ E given a source sentence f ∈ F is. Here, r is a phrasal derivation with source and target projections src( r ) and tgt( r ), w ∈ Rd is the vector of model parameters, φ( · ) ∈ Rd is a feature map, and Z( f ) is an appropriate normalizing constant. For the same model, the distribution over suffixes es ∈ E must also condition on a prefix ep ∈ E . In phrase-based decoding , the best scoring derivation r given a source sentence f and weights w is found efficiently by beam search , with one beam for every count of source words covered by a partial derivation ( known as the source coverage cardinality ) . To predict a suffix conditioned on a prefix by constrained decoding , Barrachina et al . ( 2008 ) and Ortiz-Martínez et al . ( 2009 ) modify the beam search by discarding hypotheses ( partial derivations ) that do not match the prefix ep . We propose target beam search , a two-step inference procedure . The first step is to produce a phrase-based alignment between the target prefix and a subset of the source words. The target is aligned left-to-right by appending aligned phrase pairs. However, each beam is associated with a target word count, rather than a source word count. Therefore, each beam contains hypotheses for a fixed prefix of target words. Phrasal translation candidates are bundled and sorted with respect to each target phrase rather than each source phrase. Crucially, the source distortion limit is not enforced during alignment, so that long-range reorderings can be analyzed correctly. The second step generates the suffix using standard beam search . Once the target prefix is completely aligned, each hypothesis from the final target beam is copied to an appropriate source beam. Search starts with the lowest-count source beam that contains at least one hypothesis. Here, we re-instate the distortion limit with the following modification to avoid search failures: The decoder can always translate any source position before the last source position that was covered in the alignment phase. 3.1 Synthetic Phrase Pairs The phrase pairs available during decoding may not be sufficient to align the target prefix to the source. Pre-compiled phrase tables ( Koehn et al., 2003 ) are typically pruned, and dynamic phrase tables ( Levenberg et al., 2010 ) require sampling for efficient lookup. To improve alignment coverage , we include additional synthetic phrases extracted from word-level alignments between the source sentence and target prefix inferred using unpruned lexical statistics . We first find the intersection of two directional word alignments. The directional alignments are obtained similar to IBM Model 2 ( Brown et al . , 1993 ) by aligning the most likely source word to each target word . Given a source sequence f = f1 . . . f|f | and a target sequence e = e1 . . . e|e|, we define the alignment a = a1 . . . a|e|, where ai = j means that ei is aligned to fj . The likelihood is modeled by a single-word lexicon probability that is provided by our translation model and an alignment probability modeled as a Poisson distribution Poisson( k, λ ) in the distance to the diagonal. Here, cnt( ei, fj ) is the count of all word alignments between ei and fj in the training bitext, and cnt( fj ) the monolingual occurrence count of fj . We perform standard phrase extraction ( Och et al., 1999; Koehn et al., 2003 ) to obtain our synthetic phrases, whose translation probabilities are again estimated based on the single-word probabilities p( ei|fj ) from our translation model. Given a synthetic phrase pair ( e, f ), the phrase translation probability is computed as. Additionally, we introduce three indicator features that count the number of synthetic phrase pairs, source words and target words, respectively. 4 Tuning In order to tune the model for suffix prediction, we optimize the weights w in Equation 2 to maximize the metrics introduced in Section 2. Model tuning is performed with AdaGrad ( Duchi et al . , 2011 ) , an online subgradient method . It features an adaptive learning rate and comes with good theoretical guarantees. See Green et al . ( 2013 ) for the details of applying AdaGrad to phrase-based translation . The same model scores both alignment of the prefix and translation of the suffix. However, different feature weights may be appropriate for scoring each step of the inference process. In order to learn different weights for alignment and translation within a unified joint model , we apply the hierarchical adaptation method of Wuebker et al . ( 2015 ) , which is based on frustratingly easy domain adaptation ( FEDA ) ( Daumé III , 2007 ) . We define three sub-segment domains: prefix, overlap and suffix. The prefix domain contains all phrases that are used for aligning the prefix with the source sentence. Phrases that span both prefix and suffix additionally belong to the overlap domain. Finally, once the prefix has been completely covered, the suffix domain applies to all phrases that are used to translate the remainder of the sentence. The root domain spans the entire phrasal derivation. Formally, given a set of domains D = {root, prefix, overlap, suffix}, each feature is replicated for each domain d ∈ D. These replicas can be interpreted as domain-specific “offsets” to the baseline weights. For an original feature vector φ with a set of domains D ⊆ D, the replicated feature vector contains |D| copies fd of each feature f ∈ φ, one for each d ∈ D. The weights of the replicated feature space are initialized with 0 except for the root domain, where we copy the baseline weights w. All our phrase-based systems are first tuned without prefixes or domains to maximize Bleu . When tuning for suffix prediction, we keep these baseline weights wroot fixed to maintain baseline translation quality and only update the weights corresponding to the prefix, overlap and suffix domains. 5 Diverse n-best Extraction Consider the interactive MT application setting in which the user is presented with an autocomplete list of alternative translations ( Langlais et al., 2000 ). The user query may be satisfied if the machine predicts the correct completion in its top-n output. However, it is well-known that n-best lists are poor approximations of MT structured output spaces ( Macherey et al., 2008; Gimpel et al., 2013 ). Even very large values of n can fail to produce alternatives that differ in the first words of the suffix , which limits n-best KSR and WPA improvements at test time . For tuning , WPA is often zero for every item on the n-best list , which prevents learning . Fortunately, the prefix can help efficiently enumerate diverse next-word alternatives. If we can find all edges in the decoding lattice that span the prefix ep and suffix es, then we can generate diverse alternatives in precisely the right location in the target. LetG = ( V,E ) be the search lattice created by decoding, where V are nodes and E are the edges produced by rule applications. For any w ∈ V , let parent( w ) return v s.t. v, w ∈ E, target( w ) return the target sequence e defined by following the next pointers from w, and length( w ) be the length of the target sequence up tow. During decoding, we set parent pointers and also assign monotonically increasing integer ids to each w. To extract a full sentence completion given an edge v, w ∈ E that spans the prefix/suffix boundary, we must find the best path to a goal node efficiently. To do this, we sort V in reverse topological order and set forward pointers from each node v to the child node on the best goal path. During this traversal, we also mark all child nodes of edges that span the prefix/suffix boundary. Finally, we use the parent and child pointers to extract an n-best list of translations. Algorithm 1 shows the full procedure. 6 Neural machine translation Neural machine translation ( NMT ) models the conditional probability p ( e | f ) of translating a source sentence f to a target sentence e . In the encoderdecoder NMT framework ( Sutskever et al., 2014; Cho et al., 2014 ), an encoder computes a representation s for each source sentence. From that source representation, the decoder generates a translation one word at a time by maximizing. The individual probabilities in Equation 10 are often parameterized by a recurrent neural network which repeatedly predicts the next word ei given all previous target words e<i. Since this model generates translations by repeatedly predicting next words, it is a natural choice for the sentence completion task. Even in unconstrained decoding, it predicts one word at a time conditioned on the most likely prefix. We modified the state-of-the-art English-German NMT system described in ( Luong et al . , 2015 ) to conduct a beam search that constrains the translation tomatch a fixed prefix . As we decode from left to right, the decoder transitions from a constrained prefix decodingmode to unconstrained beam search. In the constrained mode—the next word to predict ei is known—we set the beam size to 1, aggregate the score of predicting ei immediately without having to sort the softmax distribution over all words, and feed ei directly to the next time step. Once the prefix has been consumed , the decoder switches to standard beam search with a larger beam size ( 12 in our experiments ) . In this mode, the most probable word ei is passed to the next time step. 7 Experimental Results We evaluate our models and methods for English-French and English-German on two domains: software and news. The phrase-based systems are built with Phrasal ( Green et al . , 2014 ) , an open source toolkit . We use a dynamic phrase table ( Levenberg et al . , 2010 ) and tune parameters with AdaGrad . All systems have 42 dense baseline features. We align the bitexts with mgiza ( Gao and Vogel , 2008 ) and estimate 5 - gram language models ( LMs ) with KenLM ( Heafield et al . , 2013 ) . The English-French bilingual training data consists of 4.9M sentence pairs from the Common Crawl and Europarl corpora from WMT 2015 ( Bojar et al . , 2015 ) . The LM was estimated from the target side of the bitext . For English-German we run large-scale experiments. The bitext contains 19.9M parallel segments collected from WMT 2015 and the OPUS collection ( Skadin ¸ š et al . , 2014 ) . The LM was estimated from the target side of the bitext and the monolingual Common Crawl corpus ( Buck et al . , 2014 ) , altogether 37.2B running words . The software test set includes 10k sentence pairs from the Autodesk post editing corpus4 . For the news domain we chose the English-French newstest2014 and English-German newstest2015 sets provided for the WMT 20165 shared task . The translation systems were tuned towards the specific domain , using another 10k segments from the Autodesk data or the newstest2013 data set , respectively . On the English-French tune set we randomly select one target prefix from each sentence pair for rapid experimentation. On all other test and tune sets we select two target prefixes at random.6 The selected prefixes remain fixed throughout all experiments. For NMT, we report results both using a single network and an ensemble of eight models using various attention mechanisms ( Luong et al., 2015 ). 7.1 Phrase-based Results Tables 1 and 2 show the main phrase-based results. The baseline system corresponds to constrained beam search , which performed best in ( Ortiz-Martínez et al . , 2009 ) and ( Barrachina et al . , 2008 ) , where it was referred to as phrase-based ( PB ) and phrase-based model ( PBM ) , respectively . Our target beam search strategy improves all metrics on both test sets . For English-French , we observe absolute improvements of up to 3.2% pxBleu , 11.4% WPA and 10.6% KSR . We experimented with four different prefix-constrained tuning criteria: pxBleu, WPA, #prd, and the linear combination ( pxBleu+WPA )2 . We see that tuning towards prefix decoding increases all metrics. Across our two test sets, the combined metric yielded the most stable results. Here , we obtain gains of up to 3.0% pxBleu , 3.1% WPA and 2.1% KSR . We continue using the linear combination criterion for all subsequent experiments. For English-German—the large-scale setting — we observe similar total gains of up to 3.9% pxBleu , 11.2% WPA and 8.2% KSR . The target beam search procedure contributes the most gain among our various improvements. Table 3 illustrates the differences in the translation output on three example sentences taken from the newstest2015 test set . It is clearly visible that both target beam search and prefix tuning improve the prefix alignment , which results in better translation suffixes . 7.2 Diverse n-best Results To improve recall in interactive MT , the user can be presented with multiple alternative sentence completions ( Langlais et al . , 2000 ) , which correspond to an n-best list of translation hypotheses generated by the prefix-constrained inference procedure . The diverse extraction scheme introduced in section 5 is particularly designed for next-word prediction recall. Table 4 shows results for 10-best lists. We see that WPA is increased by up to 15.3% by including the 10 - best candidates , 11.3% being contributed by our novel diverse n-best extraction . Jointly, target beam search, prefix tuning and diverse n-best extraction lead to an absolute improvement of up to 23.5% over the baseline 10-best oracle. We believe that n = 10 suggestions are the maximum number of candidates that should be presented to a user, but we also ran experiments with n = 3 and n = 5, which would result in an interface with reduced cognitive load. These settings yield 5.5% and 10.0% WPA gains respectively on English-German news . 7.3 Comparison with NMT We compare this phrase-based system to the NMT system described in Section 6 for English-German . Table 5 shows the results. We observe a clear advantage of NMT over our best phrase-based system when comparing WPA . For pxBleu , the phrase-based model outperforms the single neural network system on the Autodesk set , but underperforms the ensemble . This stands in contrast to unconstrained full-sentence translation quality , where the phrase-based system is slightly better than the ensemble . The neural system substantially outperforms the phrase-based system for all metrics in the news domain . In an interactive setting, the system must make predictions in near real-time, so we report average decoding times. We observe a clear time vs . accuracy trade-off ; the phrase-based is 10.6 to 31.3 times faster than the single network NMT system and more than 100 times faster than the ensemble . Crucially , the phrase-based system runs on a CPU , while NMT requires a GPU for these speeds . Further , the 10 - best oracle WPA of the phrase-based system is higher than the NMT ensemble in both genres . Following the example of Neubig et al . ( 2015 ) , we performed a manual analysis of the first 100 segments on the newstest2015 data set in order to qualitatively compare the constrained translations produced by the phrase-based and single network NMT systems . We observe four main error categories in which the translations differ, for which we have given examples in Table 6. NMT is generally better with long-range verb reorderings, which often lead to the verb being dropped by the phrase-based system. E.g. the word erscheinen in Ex. 1 and veröffentlicht in Ex. 2 are missing in the phrase-based translation. Also, the NMT engine often produces better German grammar and morphological agreement, e.g. kein vs. keine in Ex. 3 or the verb conjugations in Ex. 4. Especially interesting is that the NMT system generated the negation nicht in the second half of Ex . 3 . This word does not have a direct correspondence in the English source, but makes the sentence feel more natural in German. On the other hand, NMT sometimes drops content words, as in Ex. 5, where middle-class jobs,Minnesota and Progressive Caucus co-chair remain entirely untranslated by NMT. Finally, incorrect prefix alignment sometimes leads to incorrect portions of the source sentence being translated after the prefix or even superfluous output by the phrase-based engine, like , die in Ex. 6. Table 7 summarizes how many times each of the systems produced a better output than the other, broken down by category. 8 Related Work Target-mediated interactive MT was first proposed by Foster et al. ( 1997 ) and then further developed within the TransType ( Langlais et al., 2000 ) and TransType2 ( Esteban et al., 2004; Barrachina et al., 2008 ) projects. In TransType2, several different approaches were evaluated. Barrachina et al . ( 2008 ) reports experimental results that show the superiority of phrase-based models over stochastic finite state transducers and alignment templates , which were extended for the interactive translation paradigm by Och et al . ( 2003 ) . Ortiz-Martínez et al. ( 2009 ) confirm this observation, and find that their own suggested method using partial statistical phrase-based alignments performs on a similar level on most tasks. The approach using phrase-based models is used as the baseline in this paper . In order to make the interaction sufficiently responsive, Barrachina et al. ( 2008 ) resort to search within a word graph, which is generated by the translation decoder without constraints at the beginning of the workflow. A given prefix is then matched to the paths within the word graph. This approach was recently refined with more permissive matching criteria by Koehn et al . ( 2014 ) , who report strong improvements in prediction accuracy . Instead of using a word graph, it is also possible to perform a new search for every interaction ( Bender et al., 2005; Ortiz-Martínez et al., 2009 ), which is the approach we have adopted. Ortiz-Martínez et al. ( 2009 ) perform the most similar study to our work in the literature. The authors also define prefix decoding as a two-stage process, but focus on investigating different smoothing techniques, while our work includes new metrics, models, and inference. 9 Conclusion We have shown that both phrase-based and neural translation approaches can be used to complete partial translations . The recurrent neural system provides higher word prediction accuracy , but requires lengthy inference on a GPU . The phrase-based system is fast , produces diverse n-best lists , and provides reasonable prefix-Bleu performance . The complementary strengths of both systems suggest future work in combining these techniques. We have also shown decisively that simply performing constrained decoding for a phrase-based model is not an effective approach to the task of completing translations . Instead, the learning objective, model, and inference procedure should all be tailored to the task. The combination of these changes can adapt a phrase-based translation system to perform prefix alignment and suffix prediction jointly with fewer search errors and greater accuracy for the critical first words of the suffix . In light of the dramatic improvements in prediction quality that result from the techniques we have described, we look forward to investigating the effect on user experience for interactive translation systems that employ these methods. "
            }
        },
        {
            "id": "P16-1037",
            "result": [
                {
                    "value": {
                        "start": 424,
                        "end": 464,
                        "text": "collaborative filtering-based approaches",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1037:E1"
                },
                {
                    "value": {
                        "start": 2201,
                        "end": 2238,
                        "text": "collaborative filtering-based methods",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1037:E2"
                },
                {
                    "value": {
                        "start": 380,
                        "end": 412,
                        "text": "content similarity-based methods",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1037:E3"
                },
                {
                    "value": {
                        "start": 4174,
                        "end": 4184,
                        "text": "Bing News1",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P16-1037:E4"
                },
                {
                    "value": {
                        "start": 6261,
                        "end": 6267,
                        "text": "recall",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P16-1037:E5"
                },
                {
                    "value": {
                        "start": 6934,
                        "end": 6942,
                        "text": "word2vec",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1037:E6"
                },
                {
                    "value": {
                        "start": 7081,
                        "end": 7096,
                        "text": "skip-gram model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1037:E7"
                },
                {
                    "value": {
                        "start": 7409,
                        "end": 7440,
                        "text": "Word Mover ’ s Distance ( WMD )",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1037:E8"
                },
                {
                    "value": {
                        "start": 8908,
                        "end": 8917,
                        "text": "Wikipedia",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P16-1037:E10"
                },
                {
                    "value": {
                        "start": 8922,
                        "end": 8930,
                        "text": "Freebase",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P16-1037:E11"
                },
                {
                    "value": {
                        "start": 9075,
                        "end": 9084,
                        "text": "precision",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P16-1037:E12"
                },
                {
                    "value": {
                        "start": 9315,
                        "end": 9336,
                        "text": "Freebase entity graph",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P16-1037:E13"
                },
                {
                    "value": {
                        "start": 9382,
                        "end": 9432,
                        "text": "Large-scale Information Network Embedding ( LINE )",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1037:E14"
                },
                {
                    "value": {
                        "start": 10851,
                        "end": 10866,
                        "text": "TF-IDF Distance",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1037:E15"
                },
                {
                    "value": {
                        "start": 10876,
                        "end": 10891,
                        "text": "TF-IDF distance",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1037:E16"
                },
                {
                    "value": {
                        "start": 12401,
                        "end": 12413,
                        "text": "linear model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1037:E17"
                },
                {
                    "value": {
                        "start": 4174,
                        "end": 4183,
                        "text": "Bing News",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P16-1037:E18"
                },
                {
                    "value": {
                        "start": 13691,
                        "end": 13715,
                        "text": "Stanford CoreNLP toolkit",
                        "labels": [
                            "Tool"
                        ]
                    },
                    "id": "P16-1037:E19"
                },
                {
                    "value": {
                        "start": 13868,
                        "end": 13879,
                        "text": "JERL system",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1037:E20"
                },
                {
                    "value": {
                        "start": 10851,
                        "end": 10857,
                        "text": "TF-IDF",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1037:E21"
                },
                {
                    "value": {
                        "start": 14948,
                        "end": 14977,
                        "text": "linear learning to rank model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1037:E22"
                },
                {
                    "value": {
                        "start": 15259,
                        "end": 15273,
                        "text": "skipgram model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1037:E23"
                },
                {
                    "value": {
                        "start": 15304,
                        "end": 15320,
                        "text": "Wikipedia corpus",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P16-1037:E24"
                },
                {
                    "value": {
                        "start": 15398,
                        "end": 15402,
                        "text": "PPDB",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P16-1037:E25"
                },
                {
                    "value": {
                        "start": 7081,
                        "end": 7090,
                        "text": "skip-gram",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1037:E26"
                },
                {
                    "value": {
                        "start": 15708,
                        "end": 15716,
                        "text": "RankLib3",
                        "labels": [
                            "Tool"
                        ]
                    },
                    "id": "P16-1037:E27"
                },
                {
                    "value": {
                        "start": 15881,
                        "end": 15894,
                        "text": "Precision @ 1",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P16-1037:E28"
                },
                {
                    "value": {
                        "start": 15897,
                        "end": 15910,
                        "text": "Precision @ 5",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P16-1037:E29"
                },
                {
                    "value": {
                        "start": 15913,
                        "end": 15921,
                        "text": "NDCG @ 5",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P16-1037:E30"
                },
                {
                    "value": {
                        "start": 15926,
                        "end": 15929,
                        "text": "MAP",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P16-1037:E31"
                },
                {
                    "value": {
                        "start": 7435,
                        "end": 7438,
                        "text": "WMD",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1037:E32"
                },
                {
                    "value": {
                        "start": 20068,
                        "end": 20117,
                        "text": "cooccurrence-based representation learning method",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1037:E33"
                },
                {
                    "value": {
                        "start": 21938,
                        "end": 21959,
                        "text": "content-based methods",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1037:E34"
                },
                {
                    "value": {
                        "start": 424,
                        "end": 447,
                        "text": "collaborative filtering",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1037:E35"
                },
                {
                    "value": {
                        "start": 22448,
                        "end": 22472,
                        "text": "context-based approaches",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1037:E36"
                },
                {
                    "value": {
                        "start": 22617,
                        "end": 22645,
                        "text": "Restricted Boltzmann Machine",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1037:E37"
                },
                {
                    "value": {
                        "start": 23700,
                        "end": 23725,
                        "text": "Convolutional neural nets",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1037:E38"
                },
                {
                    "value": {
                        "start": 25223,
                        "end": 25226,
                        "text": "CNN",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P16-1037:E39"
                }
            ],
            "data": {
                "text": "News Citation Recommendation with Implicit and Explicit Semantics Abstract In this work, we focus on the problem of news citation recommendation. The task aims to recommend news citations for both authors and readers to create and search news references. Due to the sparsity issue of news citations and the engineering difficulty in obtaining information on authors , we focus on content similarity-based methods instead of collaborative filtering-based approaches . In this paper, we explore word embedding ( i.e., implicit semantics ) and grounded entities ( i.e., explicit semantics ) to address the variety and ambiguity issues of language. We formulate the problem as a reranking task and integrate different similarity measures under the learning to rank framework. We evaluate our approach on a real-world dataset. The experimental results show the efficacy of our method. 1 Introduction When an author writes an online news article, s/he often cites previously published news reports to elaborate a mentioned event or support his/her point of view. For the convenience of the readers, the editor usually associates the words with hyperlinks. Through the links the readers can directly access the referenced articles to know more details about the events. If there is no reference for a mentioned event, the readers may search the related news reports for further reading. Hence, it is valuable to have automatic news citation recommendations for authors and readers to create or search news references. In this paper, we focus on the problem of news citation recommendation. As shown in Table 1, given a snippet of citing context ( left ), the task aims to retrieve a list of news articles ( right ) as references. This task differs from traditional recommendation tasks, e.g., citation recommendation for scientific papers, in that: ( a ) based on the statistics from our dataset, the number of references per news article is 4.56 on average, much less than the number of citations per academic paper ( typically dozens ); ( b ) the author-topic information is usually unavailable, since it is technically difficult to obtain author information from news articles. These differences make the collaborative filtering-based methods , which have been widely applied to paper citation recommendation , less available in our scenario . Therefore , in this paper we focus on content similarity-based methods to deal with the task of news citation recommendation . Previous studies use string-based overlap ( Xu et al., 2014 ), machine translation measures ( Madnani et al., 2012 ), and dependency syntax ( Wan et al., 2006; Wang et al., 2015 ) to model text similarity. More recent work focuses on neural network methods ( Yin and Schu¨tze, 2015; He et al., 2015; Hu et al., 2014; dos Santos et al., 2015; Lei et al., 2016 ). There are two major challenges rendering these approaches not suitable for this task: ( i ) the variety and ( ii ) the ambiguity of language. By variety, we mean that the same meaning may be expressed with different phrases. Taking the first row in Table 1 for example, Vlaar in the citing context refers to Ron Vlaar, a Dutch football player, who is referred to as Dutch star and Netherlands international in the cited article. By ambiguity, we mean that the same expression may have different meanings in different contexts. In the second example in Table 1, the mention tiger refers to tiger the mammal. By contrast, in “Detroit Tigers links: The Tigers are in trouble” for example, the word Tiger is the name of a team. In this paper, we explore both implicit and explicit semantics to address the above issues. Specifically, the implicit semantics can be obtained from the word embedding trained on large scale corpus, and the explicit semantics through linking entity mentions to the grounded entities in a knowledge base. In this paper, we explore using both word embedding and grounded knowledge to model the relatedness between citing context and articles. We formulate the problem as a re-ranking task. We use learning to rank to integrate different similarity measures and evaluate the models on a real world dataset constructed from Bing News1 . We further give quantitative analysis of the effects of word embedding and grounded entities in the task. In summary, the main contributions of this paper are three-fold: We propose the task of news citation recommendation and construct a real-world dataset for this task. We utilize both word embedding based similarity measures and knowledge-based methods to tackle the problem . We formulate the problem as a re-ranking task and leverage learning to rank algorithm to integrate different similarity measures. We conduct extensive experiments on a large dataset. The results show the effectiveness of word embedding and grounded entities. We further quantitatively analyze how the implicit semantics from word embedding and explicit semantics from grounded knowledge benefit the task of interest. 2 Problem Formulation In this section, we introduce the news citation recommendation problem and formulate it as a reranking task. We first introduce definitions that will be used through the rest of the paper: Citing Context. Citing context is a sentence which contains an anchor text associated with a hyperlink. As shown in Table 1, the underlined words are associated with a hyperlink pointing to another news article, and the sentence ( left ) which contains the anchor is the citing context. Cited Article. Given a piece of citing context, the article that the hyperlink links to is defined as its cited article. It is expected that a news article is well-structured, and its headline together with its lead paragraph gives a good brief description of the whole story ( Kianmehr et al., 2009 ). In this paper, a news article can either be represented by its title and lead paragraph or by the passage as a whole. We conduct experiments under both of the two different settings. Candidate Article Set. Considering efficiency, we follow the procedure adopted by many recommendation systems ( Lei et al., 2016; Tan et al., 2015 ) and formulate the problem as a re-ranking task. In another word , given a citing context , we first use efficient retrieval methods with high recall to generate a list of articles as the candidate article set , and then run the system to get a re-ranked list . News Citation Recommendation. Given a citing context, the task aims to construct an ordered list of news articles, top of which are most relevant to the context, and can serve as the cited articles. 3 Method In this section, we first explain the similarity measures based on word embedding ( implicit semantics ) and grounded knowledge ( explicit semantics ) to deal with variety and ambiguity problems. Then we briefly introduce the baselines and the learning to rank framework. 3.1 Implicit Semantics for Variety The distributed word representation by word2vec factors word distance and captures semantic similarities through vector arithmetic ( Mikolov et al . , 2013 ) . In this work , we train a skip-gram model to bridge the vocabulary gap between context-article pairs . Previous work represents the documents with averaged vectors of words ( Tang et al., 2014; Tan et al., 2015 ). However, this may lead to the loss of detailed information of the documents. In this paper, we adopt a different approach, explained below. Word Mover ’ s Distance ( WMD ) . Kusner et al . ( 2015 ) combine distributed word representations with the earth mover ’ s distance ( EMD ) ( Rubner et al . , 1998 ; Wan , 2007 ) to measure the distance between documents . They use the Euclidean distance between words ’ low dimensional representations as building blocks , and optimize a special case of the EMD to obtain the cumulative distance . More formally, let X = {( x1, wx1 ), ( x2, wx2 ), · · · , ( xm, wxm )} be the normalized bag-of-words representation for a citing context after removing stop-words, where word xi appearswxi times ( then normalized by the total count of words in X ), i = 1, 2, · · · ,m. Similarly, we have the representation for a candidate article, Y ={( y1, wy1 ), ( y2, wy2 ), · · · , ( yn, wyn )}. The WMD calculates the minimum cumulative cost by solving the linear programming problem below : where T ∈ Rm × n is the transportation flow matrix , and cij indicates the distance between xi and yj . Here cij = ‖vector( xi )− vector( yj )‖, where function vector( w ) returns the word vector of w. Then the distance is normalized by the total flow. 3.2 Explicit Semantics for Ambiguity News articles tend to be well written, and contain many named entity mentions. Making use of this property, we deal with the ambiguity problem by using grounded entities ( explicit semantics ). Given a context-article pair , we first recognize all named entity mentions on both sides and link them to knowledge bases ( e.g . , Wikipedia and Freebase ) , then use the following measures to model the similarity . Entity Overlap. Given a context–article pair , we consider two metrics , namely , precision and recall , to measure their entity overlap . The precision is defined as. Embedding Based Matching. We build two separate information networks for Wikipedia entities using ( a ) the anchor links on Wikipedia pages and ( b ) the Freebase entity graph ( Bollacker et al . , 2008 ) . Then we apply Large-scale Information Network Embedding ( LINE ) ( Tang et al . , 2015 ) system2 to the networks to embed the entities into low-dimensional spaces . We then measure the similarity by the minimized cosine distance between entities’ on the citing and the cited side: where X refers to the citing context, and xi ∈ X are the grounded entities in the citing part. Similar with Y and yj . Wikipedia Evidence . Given a context-article pair, we refer to world knowledge for supporting evidence. In particular, we first apply an entity linking system to detect the entity mentions on both sides and ground them into Wikipedia entries, each of which has its own description page. Second, we collect the descriptions for entities from the candidate article and extract as evidence those sentences containing entities from citing context. We refer to this evidence as cited evidence. For instance, the article in Table 8 contains grounded entity Scottish National Party. And in the description for it, there is a sentence containing the entity Scotland from the citing context: “The Scottish National Party ( SNP ) is a Scottish nationalist and social-democratic political party in Scotland.” Thus we extract this sentence as cited evidence supporting this pair. We count the overlapping nouns between the citing context and the cited evidence to calculate precision and recall . 3.3 Baselines We design several baseline features for the two groups of features mentioned above: TF-IDF Distance . We use TF-IDF distance as a basic measure . The similarity is calculated with cosine distance based on TF-IDF vector representations for the text . Ungrounded Mentions. Note that entity overlap features also adapt to ungrounded mentions. The embedding-based matching features for ungrounded mentions are similar to those for grounded entities. The only difference is that here each mention is represented by the averaged vectors of all the words it contains. Wikipedia evidence is not feasible for ungrounded mentions . Table 2 summarizes all the features we use. A cited article can either be represented by its headline+lead paragraph or as a whole. Therefore, we extract features under two different settings: ( a ) headlines and lead paragraphs only; ( b ) the full articles. Most of the features are extracted under both of the settings. However, feature 2 is much too computation-intensive and feature 7 needs POS-tagging as the preprocessing. Thus these two are only extracted under setting ( a ). 3.4 Learning to Rank Framework Many different learning to rank algorithms have been proposed to deal with the ranking problem , including pointwise , pairwise , and listwise approaches ( Xia et al . , 2008 ) . Listwise methods receive ranked lists as training samples , and directly optimize the metric of interest by minimizing the respective cost . And it has been reported that the listwise method usually achieves better performance compared to others ( Qin et al . , 2008 ; Cao et al . , 2007 ) . In this work, we use the linear model and apply coordinate ascent for parameter optimization. 4 Experiments 4.1 Data Collection We collect one month ’ s news articles from Bing News . The citing context set consists of all the sentences associated with anchor link( s ). For each piece of citing context, its cited article is extracted through its hyperlink. If there are multiple links associated with the context, only the first one is considered. We pair each citing context and its cited article as a ground truth sample. We further label as ground truths those articles sharing the same title as the cited article. This is rather reasonable since a single passage may have multiple reprints by difference sources. On average, there are 2.20 ground truth cited articles for each citing context in the dataset. In order to focus only on news events, we filter out those pairs whose hyperlinks are associated with three words or less ( usually names for persons or places, and lead to definition pages ). We also discard those samples whose citing contexts contain or are exactly the same as the titles of the cited articles. For example, “READ MORE: The stories you need to read, in one handy email” links to an article titled “The stories you need to read, in one handy email”. The dataset is preprocessed with Stanford CoreNLP toolkit ( Manning et al . , 2014 ) , including sentence splitting , tokenizing for whole passages , and POS-tagging for titles and lead paragraphs . We use the JERL system by Luo et al . ( 2015 ) for entity detection and grounding . It recognizes entity mentions and links them to Wikipedia entries . We use each mention ’ s text span as an ungrounded mention , and its corresponding Wikipedia ID as a grounded entity . For instance, in Table 8, the detected text span Westminster is an ungrounded mention, and it’s grounded to the entry Parliament of the United Kindom. 4.2 Selecting Candidates Given a citing context , we construct its candidate article set with the top 200 articles retrieved by TF-IDF distance . In the experiments, approximately 92.61% of the ground truth cited articles appear in the candidate sets. We discard those that do not. We further randomly split the remaining 33318 pairs into training/validation/test sets with the proportion of 3:1:1. For each training pair, we randomly sample 5 articles from its candidate article set ( excluding ground truth ) and pair them with the citing context as negative samples. According to Tan et al. ( 2015 ), the number of negative samples does not significantly affect the linear learning to rank model’s performance. During validation and testing, all of the 200 candidates are taken into account. 4.3 Experimental Setup In the experiments , we set the TF-IDF as the baseline , and incrementally add different groups of features to the system . The word embedding is pretrained with skipgram model ( Mikolov et al . , 2013 ) on Wikipedia corpus and then fine-tuned using the method proposed in Wieting et al . ( 2015 ) on PPDB ( Ganitkevitch et al . , 2013 ) . The embedding fine-tuned with paraphrase pairs can better capture the semantic relatedness of different phrase. In the experiments , we observe a 1% − 2% improvement by the finetuned word representations compared to vanilla skip-gram vectors . We use the linear model in RankLib3 for the learning to rank implementation . Coordinate ascent is used for parameter optimization . The model is trained to directly optimize the evaluation metrics , Precision @ 1 , Precision @ 5 , NDCG @ 5 and MAP , respectively . For NDCG @ 5 measure , we set a binary relevance score , i.e . , the scores equal to 1 for ground truths , 0 for negative samples . 4.4 Experimental Results Table 3 gives the performance of the baselines and the systems using different groups of features on test and validation sets. The results show that WMD brings a consistent improvement over its TF-IDF baseline , and so do grounded entities compared to ungrounded mentions . Individually added to the TF-IDF baseline , WMD has the largest performance boost , followed by grounded entity features . Besides , the additional information from grounded entity knowledge helps the model outperform the ungrounded mentions , with a consistent margin of 1.0% - 2.0% NDCG @ 5 . We further compare the performance of the models when using features from headlines+lead paragraphs only and those from full passages. As shown in Table 3, the former brings much better performance on each metric compared to the latter. It’s worth noting that there are ground truths mis-labeled as irrelevant in the dataset. A primary reason is that news sites sometimes individually publish different reports on a certain event. And the articles don’t necessarily share the same title. To see how this affects the model, we randomly build a subset S of the test set and manually label the selected samples, which gives S˜4. Table 4 compares the model’s performance on S and S˜ under Headline+Lead paragraph setting. There is a consistent improvement of NDCG @ 5 score on S ˜ compared to that on S . Besides that, on manually labeled data, the model’s performance across different feature settings is almost in accord with that on the full test set. These results show that there are indeed mis-labeled ground truths in the dataset, but they have little influence when comparing different groups of features. 5 Analysis In this section , we give detailed win-loss analysis for the models trained with NDCG @ 5 metric under headlines + lead paragraphs setting . Specifically, given two systems with different feature configurations, we compare their performance on each test sample. The results are shown as a heatmap in Figure 1. X and Y axises indicate the identifiers for each feature group, following those in Table 3. For example , the data point at ( 5 , 1 ) indicates that the inclusion of WMD brings better ranking scores to TF-IDF on 18.4% of the test samples ; and as a trade off , it lowers the scores on 11.4% of the samples . We also observe that grounded entities brings gain to 15.9% of the samples, and loss for 9.6% of them. On average, two different groups of features disagree on 26.4% of the test samples. We further give several mis-predictions by the model using certain groups of features, and illustrate how they are corrected by the inclusion of others ( or the other way round ). By misprediction, we mean that no ground truth cited article appears in the top 5 predictions of the returned list. 5.1 Dealing with Variety Table 5 shows a mis-prediction by TF-IDF , but corrected after including WMD . TF-IDF distance favors the high-score matching keywords approval and rating between the citing context and mis-predicted article . On the other hand, distributed word representations factor the distances between word pairs, which helps to capture their semantic closeness, e.g., ( Argentines, Argentina, Cosince distance: 0.210 ), ( poll, election, 0.020 ), and ( increasingly, growing, 0.286 ). WMD helps to bridge the vocabulary gap between the citing context and the cited article . On the other hand, though not often, the use of distributed representation can also create mistakes. Table 6 gives an example where the inclusion of the WMD feature changes a correct prediction by TF-IDF into a mistake . By analyzing the WMD ’ s transportation flow matrix T , we find that the used word embedding relates MP to minister , and publicly to government . More curiously, persons’ names are very similar in its semantic space: ( Davies, Stephen, 0.602 ), and ( Davies, Harper, 0.635 ). A possible reason could be that both of the two names are very common , and thus the cooccurrence-based representation learning method is not able to distinguish them . This also justifies our use of grounded entities as additional information : from the Wikipedia description for entity Stephen Harper , the system might be able to find out that he actually serves in Canadian government , not in the UK ’ s nor in the Welsh . 5.2 Dealing with Ambiguity Entity grounding helps by resolving the ambiguity e.g., alias, abbreviation, of the entity mentions. As shown in Table 7, tiger refers to the mammal in the ground truth pair. However, the same word refers to Detroit Tigers the team in the mispredicted article. This ambiguity is resolved when the mention is grounded to its Wikipedia entry . In another example shown in Table 8, ungrounded mention SNP, though detected, contributes little to supporting the ground truth pair. However, when it’s grounded to the entry Scottish National Party, the system leverages world knowledge and relates it to the mention of Scotland in the citing context. The inclusion of grounded entity information may also lead to mistakes, many of which are due to the limited performance of the entity recognition and disambiguation system. We’d like to discuss another kind of error here, shown in Table 9. In the citing context, The Daily Telegraph is a newspaper published in the UK. It has little to do with the involved event except for reporting it. However, the system favors a farmers’ story which actually happened in the UK. We find that this contributes a lot to the system’s errors when including grounded entities. We leave it to future work to figure out how to deal with this issue. 6 Related Work This section reviews three lines of related work: ( i ) document recommendation, ( ii ) pharaphrase identification, ( iii ) question retrieval. 6.1 Document Recommendation Existing literature mainly focuses on content-based methods and collaborative filtering ( Adomavicius and Tuzhilin , 2005 ) . There are studies trying to recommend documents based on citation contexts, either through identifying the motivations of the citations ( Aya et al., 2005 ), or through the topical similarity ( Ritchie, 2008; Ramachandran and Amir, 2007 ). On the other hand, Mcnee et al. ( 2002 ) leverage multiple information sources from authorship, paper-citation relations, and cocitations to recommend research papers. Combining the context-based approaches and collaborative filtering , Torres et al . ( 2004 ) and Strohman et al . ( 2007 ) report better performance . Tang and Zhang ( 2009 ) use the Restricted Boltzmann Machine to model citations for placeholders , and Tan et al . ( 2015 ) integrate multiple features to recommend quotes for writings . In the news domain , context-based approaches are presumably favorable due to the fact that the articles are relatively content-rich and citationsparse . Previous studies manage to utilize information retrieval techniques to recommend news articles given a seed article ( Yang et al., 2009; Bogers and van den Bosch, 2007 ). 6.2 Paraphrase Identification Several hand-crafted features have proven helpful in modeling sentence/phrase similarity, e.g., string-based overlap ( Xu et al., 2014 ), machine translation measures ( Madnani et al., 2012 ), and dependency syntax ( Wan et al., 2006; Wang et al., 2015 ). Using the combination and discriminative re-weighting of the mentioned features, Ji and Eisenstein ( 2013 ) manage to obtain more competitive results. More recent work has switched the focus onto neural methods. Socher et al. ( 2011 ) recursively encode the representations of sentences by the compositions of words. Convolutional neural nets ( LeCun et al . , 1998 ; Collobert and Weston , 2008 ) are also exploited in the tasks of paraphrase identification and sentence matching ( Yin and Schu ¨ tze , 2015 ; He et al . , 2015 ; Hu et al . , 2014 ) . Story link detection ( SLD ) is a similar task which aims to classify whether two news stories discuss the same event. Farahat et al. ( 2003 ) leverage part of speech tagging technique as well as task-specific similarity measures to boost the system’s performance. Shah et al. ( 2006 ) show that entity based document representation is a better choice compared to word-based representations in SLD. In our scenario, the query is typically a piece of context sentence instead of an entire article. Therefore, we find that document level methods yield sub-optimal performance when used to model the similarity of citing context and the articles. Besides, due to the fact that there might be multiple reports for a single event, we consider it reasonable to formulate our problem into a ranking task instead of classification. 6.3 Question Retrieval The key problem in question retrieval lies in modeling questions’ similarity. Machine translation techniques ( Jeon et al., 2005 ) and topic models ( Duan et al., 2008 ) have been utilized by previous works. An alternative is representation learning. Zhou et al. ( 2015 ) use category-based meta-data to learn word embeddings. dos Santos et al . ( 2015 ) and Lei et al . ( 2016 ) obtain superior performance over hand-crafted features with CNN . News articles are more well-written than most documents in QA communities, which results in the feasibility of high-quality entity detection and grounding. 7 Discussions In this paper, we propose a novel problem of news citation recommendation, which aims to recommend news citations for references based on a citing context. We develop a re-ranking system leveraging implicit and explicit semantics for content similarity. We construct a real-world dataset. The experimental results show the efficacy of our approach. "
            }
        },
        {
            "id": "P19-1035",
            "result": [
                {
                    "value": {
                        "start": 7558,
                        "end": 7607,
                        "text": "sequence-to-sequence neural network architectures",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P19-1035:E0"
                },
                {
                    "value": {
                        "start": 11404,
                        "end": 11444,
                        "text": "leave-one-out cross validation ( LOOCV )",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P19-1035:E2"
                },
                {
                    "value": {
                        "start": 11489,
                        "end": 11519,
                        "text": "rooted mean squared error RMSE",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P19-1035:E3"
                },
                {
                    "value": {
                        "start": 11530,
                        "end": 11558,
                        "text": "quadratic weighted kappa qwκ",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P19-1035:E4"
                },
                {
                    "value": {
                        "start": 11054,
                        "end": 11057,
                        "text": "SVM",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P19-1035:E5"
                },
                {
                    "value": {
                        "start": 12024,
                        "end": 12030,
                        "text": "Ubuntu",
                        "labels": [
                            "Tool"
                        ]
                    },
                    "id": "P19-1035:E6"
                },
                {
                    "value": {
                        "start": 12530,
                        "end": 12561,
                        "text": "multi-layer perceptrons ( MLP )",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P19-1035:E7"
                },
                {
                    "value": {
                        "start": 12566,
                        "end": 12627,
                        "text": "bi-directional long shortterm memory ( BiLSTM ) architectures",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P19-1035:E8"
                },
                {
                    "value": {
                        "start": 12556,
                        "end": 12559,
                        "text": "MLP",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P19-1035:E9"
                },
                {
                    "value": {
                        "start": 12605,
                        "end": 12611,
                        "text": "BiLSTM",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P19-1035:E10"
                },
                {
                    "value": {
                        "start": 11515,
                        "end": 11519,
                        "text": "RMSE",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P19-1035:E11"
                },
                {
                    "value": {
                        "start": 13638,
                        "end": 13652,
                        "text": "C-test dataset",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P19-1035:E12"
                },
                {
                    "value": {
                        "start": 141,
                        "end": 148,
                        "text": "C-tests",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P19-1035:E13"
                },
                {
                    "value": {
                        "start": 1202,
                        "end": 1208,
                        "text": "Ctests",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P19-1035:E14"
                },
                {
                    "value": {
                        "start": 14214,
                        "end": 14223,
                        "text": "SVM model",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P19-1035:E15"
                },
                {
                    "value": {
                        "start": 18243,
                        "end": 18247,
                        "text": "SVMs",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P19-1035:E16"
                },
                {
                    "value": {
                        "start": 18306,
                        "end": 18318,
                        "text": "Brown corpus",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P19-1035:E17"
                },
                {
                    "value": {
                        "start": 18919,
                        "end": 18932,
                        "text": "Pearson ’ s ρ",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P19-1035:E18"
                },
                {
                    "value": {
                        "start": 141,
                        "end": 147,
                        "text": "C-test",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P19-1035:E19"
                },
                {
                    "value": {
                        "start": 20352,
                        "end": 20361,
                        "text": "Gutenberg",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P19-1035:E20"
                },
                {
                    "value": {
                        "start": 20382,
                        "end": 20389,
                        "text": "Reuters",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P19-1035:E21"
                },
                {
                    "value": {
                        "start": 18306,
                        "end": 18311,
                        "text": "Brown",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P19-1035:E22"
                },
                {
                    "value": {
                        "start": 21881,
                        "end": 21898,
                        "text": "Gutenberg corpora",
                        "labels": [
                            "Dataset"
                        ]
                    },
                    "id": "P19-1035:E23"
                },
                {
                    "value": {
                        "start": 15811,
                        "end": 15814,
                        "text": "SEL",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P19-1035:E24"
                },
                {
                    "value": {
                        "start": 16985,
                        "end": 16989,
                        "text": "SIZE",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P19-1035:E25"
                },
                {
                    "value": {
                        "start": 26757,
                        "end": 26779,
                        "text": "Bonferroni-corrected α",
                        "labels": [
                            "Metric"
                        ]
                    },
                    "id": "P19-1035:E26"
                },
                {
                    "value": {
                        "start": 26832,
                        "end": 26839,
                        "text": "t-tests",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P19-1035:E27"
                },
                {
                    "value": {
                        "start": 26886,
                        "end": 26906,
                        "text": "Mann–Whitney U tests",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P19-1035:E28"
                },
                {
                    "value": {
                        "start": 28688,
                        "end": 28707,
                        "text": "Mann-Whitney-U test",
                        "labels": [
                            "Method"
                        ]
                    },
                    "id": "P19-1035:E29"
                }
            ],
            "data": {
                "text": "Manipulating the Difficulty of C-Tests Abstract We propose two novel manipulation strategies for increasing and decreasing the difficulty of C-tests automatically. This is a crucial step towards generating learner-adaptive exercises for self-directed language learning and preparing language assessment tests. To reach the desired difficulty level, we manipulate the size and the distribution of gaps based on absolute and relative gap difficulty predictions. We evaluate our approach in corpus-based experiments and in a user study with 60 participants. We find that both strategies are able to generate C-tests with the desired difficulty level. 1 Introduction Learning languages is of utmost importance in an international society and formulated as a major political goal by institutions such as the European Council, who called for action to “teaching at least two foreign languages” ( EC, 2002, p. 20 ). But also beyond Europe, there is a huge demand for language learning worldwide due to increasing globalization, digital communication, and migration. Among multiple different learning activities required for effective language learning, we study one particular type of exercise in this paper: Ctests are a special type of cloze test in which the second half of every second word in a given text is replaced by a gap ( Klein-Braley and Raatz, 1982 ). Figure 1 ( a ) shows an example. To provide context, the first and last sentences of the text do not contain any gaps. C-tests rely on the reduced redundancy principle ( Spolsky, 1969 ) arguing that a language typically employs more linguistic information than theoretically necessary to communicate unambiguously. Proficient speakers intuitively understand an utterance even if the level of redundancy is reduced ( e.g., when replacing a word’s suffix with a gap ), whereas learners typically rely on the redundant signal to extrapolate the meaning of an utterance. Besides general vocabulary knowledge, C-tests require orthographic, morphologic, syntactic, and semantic competencies ( Chapelle, 1994 ) to correctly fill in all gaps, which make them a frequently used tool for language assessment ( e.g., placement tests ). Given that C-tests can be easily generated automatically by introducing gaps into an arbitrary text and that there is usually only a single correct answer per gap given its context, C-tests are also relevant for self-directed language learning and massive open online courses ( MOOC ), where largescale personalized exercise generation is necessary. A crucial question for such tasks is predicting and manipulating the difficulty of a C-test. For language assessment, it is important to generate C-tests with a certain target difficulty to allow for comparison across multiple assessments. For selfdirected language learning and MOOCs, it is important to adapt the difficulty to the learner’s current skill level, as an exercise should be neither too easy nor too hard so as to maximize the learning effect and avoid boredom and frustration ( Vygotsky, 1978 ). Automatic difficulty prediction of C-tests is hard, even for humans, which is why there have been many attempts to theoretically explain C-test difficulty ( e.g., Sigott, 1995 ) and to model features used in machine learning systems for automatic difficulty prediction ( e.g., Beinborn et al., 2014 ). While state-of-the-art systems produce good prediction results compared to humans ( Beinborn, 2016 ), there is yet no work on automatically manipulating the difficulty of C-tests. Instead, C-tests are generated according to a fixed scheme and manually post-edited by teachers, who might use the predictions as guidance. But this procedure is extremely time-consuming for language assessment and no option for large-scale self-directed learning. In this paper, we propose and evaluate two strategies for automatically changing the gaps of a C-test in order to reach a given target difficulty. Our first strategy varies the distribution of the gaps in the underlying text and our second strategy learns to decide to increase or decrease a gap in order to make the test easier or more difficult. Our approach breaks away from the previously fixed C-test creation scheme and explores new ways of motivating learners by using texts they are interested in and generating tests from them at the appropriate level of difficulty. We evaluate our strategies both automatically and in a user study with 60 participants. 2 Related Work In language learning research, there is vast literature on cloze tests. For example, Taylor ( 1953 ) studies the relation of cloze tests and readability. In contrast to C-tests ( Klein-Braley and Raatz, 1982 ), cloze tests remove whole words to produce a gap leading to more ambiguous solutions. Chapelle and Abraham ( 1990 ) contrast four types of cloze tests, including fixed-ratio cloze tests replacing every ith word with a gap, rational cloze tests that allow selecting the words to replace according to the language trait that should be assessed, multiple-choice tests, and C-tests. Similar to our work, they conduct a user study and measure the difficulty posed by the four test types. They find that cloze tests replacing entire words with a gap are more difficult than C-tests or multiplechoice tests. In our work, we go beyond this by not only varying between gaps spanning the entire word ( cloze test ) or half of the word ( C-test ), but also changing the size of the C-test gaps. Laufer and Nation ( 1999 ) propose using C-tests to assess vocabulary knowledge. To this end, they manually construct C-tests with only a single gap, but use larger gaps than half of the word’s letters. Our work is different to these previous works, since we test varying positions and sizes for C-test gaps and, more importantly, we aim at manipulating the difficulty of a C-test automatically by learning to predict the difficulty of the gaps and how their manipulation affects the difficulty. Previous work on automatically controlling and manipulating test difficulty has largely focused on multiple-choice tests by generating appropriate distractors ( i.e., incorrect solutions ). Wojatzki et al. ( 2016 ) avoid ambiguity of their generated distractors, Hill and Simha ( 2016 ) fit them to the context, and Perez and Cuadros ( 2017 ) consider multiple languages. Further work by Zesch and Melamud ( 2014 ), Beinborn ( 2016 ), and Lee and Luo ( 2016 ) employ word difficulty, lexical substitution, and the learner’s answer history to control distractor difficulty. For C-tests, Kamimoto ( 1993 ) and Sigott ( 2006 ) study features of hand-crafted tests that influence the difficulty, and Beinborn et al. ( 2014 ) and Beinborn ( 2016 ) propose an automatic approach to estimate C-test difficulty, which we use as a starting point for our work. Another related field of research in computerassisted language learning is readability assessment and, subsequently, text simplification. There exists ample research on predicting the reading difficulty for various learner groups ( Hancke et al., 2012; Collins-Thompson, 2014; Pila´n et al., 2014 ). A specific line of research focuses on reducing the reading difficulty by text simplification ( Chandrasekar et al., 1996 ). By reducing complex texts or sentences to simpler ones, more texts are made accessible for less proficient learners. This is done either on a word level by substituting difficult words with easier ones ( e.g., Kilgarriff et al., 2014 ) or on a sentence level ( Vajjala and Meurers, 2014 ). More recent work also explores sequence-to-sequence neural network architectures for this task ( Nisioi et al . , 2017 ) . Although the reading difficulty of a text partly contributes to the overall exercise difficulty of C-tests, there are many other factors with a substantial influence ( Sigott, 1995 ). In particular, we can generate many different C-tests from the same text and thus reading difficulty and text simplification alone are not sufficient to determine and manipulate the difficulty of C-tests. 3 Task Overview We define a C-test T = ( u,w1, . . . , w2n, v,G ) as a tuple of left and right context u and v ( typically one sentence ) enframing 2n words wi where n= |G| is the number of gaps in the gap set G. In each gap g=( i,` )∈ G, the last ` characters of word wi are replaced by a blank for the learners to fill in. Klein-Braley and Raatz ( 1982 ) propose the default gap generation scheme DEF with G = {( 2j, d |w2j |2 e ) | 1 ≤ j ≤ n} in order to trim the ( larger ) second half of every second word. Single-letter words, numerals, and punctuation are not counted as words wi and thus never contain gaps. Figure 1 ( a ) shows an example C-test generated with the DEF scheme. A major limitation of DEF is that the difficulty of a C-test is solely determined by the input text. Most texts, however, yield a medium difficulty ( cf. section 6 ) and thus do not allow any adaptation to beginners or advanced learners unless they are manually postprocessed. In this paper, we therefore propose two strategies to manipulate the gap set G in order to achieve a given target difficulty τ ∈ [0, 1] ranging from small values for beginners to high values for advanced learners. To estimate the difficulty d( T  ) = 1|G| ∑ g∈G d( g ) of a C-test T , we aggregate the predicted difficulty scores d( g ) of each gap. In section 4, we reproduce the system by Beinborn ( 2016 ) modeling d( g ) ≈ e( g ) as the estimated mean error rates e( g ) per gap across multiple learners, and we conduct additional validation experiments on a newly acquired dataset. The core of our work is the manipulation of the gap set G in order to minimize the difference |d( T  ) − τ | between the predicted test difficulty d( T  ) and the requested target difficulty τ . To this end, we employ our difficulty prediction system for validation and propose a new regression setup that predicts the relative change of d( g ) when manipulating the size ` of a gap. Figure 2 shows our system architecture: Based on a text corpus, we generate C-tests for arbitrary texts ( e.g., according to the learner’s interests ). Then, we manipulate the difficulty of the generated text by employing the difficulty prediction system in order to reach the given target difficulty τ for a learner ( i.e., the estimated learner proficiency ) to provide neither too easy nor too hard tests. 4 C-Test Difficulty Prediction Beinborn et al. ( 2014 ) and Beinborn ( 2016 ) report state-of-the-art results for the C-test difficulty prediction task. However, there is yet no opensource implementation of their code and there is little knowledge about the performance of newer approaches. Therefore, we ( 1 ) conduct a reproduction study of Beinborn’s ( 2016 ) system, ( 2 ) evaluate newer neural network architectures, and ( 3 ) validate the results on a newly acquired dataset. Reproduction study. We obtain the original software and data from Beinborn ( 2016 ). This system predicts the difficulty d ( g ) for each gap within a Ctest using a support vector machine ( SVM ; Vapnik , 1998 ) with 59 hand-crafted features . The proposed features are motivated by four factors which are deemed important for assessing the gap difficulty: item dependency, candidate ambiguity, word difficulty, and text difficulty. We use the same data ( 819 filled C-tests ), metrics, and setup as Beinborn ( 2016 ). That is , we perform leave-one-out cross validation ( LOOCV ) and measure the Pearson correlation ρ , the rooted mean squared error RMSE , and the quadratic weighted kappa qwκ as reported in the original work . The left hand side of table 1 shows the results of our reproduced SVM compared to the original SVM results reported by Beinborn ( 2016 ) . Even though we reuse the same code as in their original work, we observe small differences between our reproduction and the previously reported scores. We were able to trace these differences back to libraries and resources which have been updated and thus changed over time. One example is Ubuntu ’ s system dictionary , the American English dictionary words ( wamerican ) , on which the original system relies . We experiment with different versions of the dictionary between Ubuntu 14.04 ( wamerican v .7.1.1 ) and 18.04 ( wamerican v .2018.04.16 - 1 ) and observe differences of one or two percentage points . As a best practice, we suggest to fix the versions of all resources and avoid any system dependencies. Neural architectures. We compare the system with deep learning methods based on multi-layer perceptrons ( MLP ) and bi-directional long shortterm memory ( BiLSTM ) architectures , which are able to capture non-linear feature dependencies . To cope for the non-deterministic behavior of the neural networks, we repeat all experiments ten times with different random weight initializations and report the averaged results ( Reimers and Gurevych, 2017 ). While the MLP is trained similar as our reproduced SVM , the BiLSTM receives all gaps of a C-test as sequential input . We hypothesize that this sequence regression setup is better suited to capture gaps interdependencies. As can be seen from the table , the results of the neural architectures are , however , consistently worse than the SVM results . We analyze the RMSE on the train and development sets and observe a low bias , but a high variance . Thus, we conclude that although neural architectures are able to perform well for this task, they lack a sufficient amount of data to generalize. Experiments on new data. To validate the results and assess the robustness of the difficulty prediction system , we have acquired a new C-test dataset from our university ’ s language center . 803 participants of placement tests for English courses solved five C-tests ( from a pool of 53 different Ctests ) with 20 gaps each. Similar to the data used by Beinborn ( 2016 ), we use the error rates e( g ) for each gap as the d( g ) the methods should predict. The right-hand side of table 1 shows the performance of our SVM and the two neural methods . The results indicate that the SVM setup is well-suited for the difficulty prediction task and that it successfully generalizes to new data . We train our final SVM model on all available data ( i.e . , the original and the new data ) and publish our source code and the trained model on GitHub .2 Similar to Beinborn ( 2016 ), we cannot openly publish our dataset due to copyright. 5 C-Test Difficulty Manipulation Given a C-test T = ( u,w1, . . . , w2n, v,G ) and a target difficulty τ , the goal of our manipulation strategies is to find a gap set G such that d( T  ) approximates τ . A naı¨ve way to achieve this goal would be to generate C-tests for all texts in a large corpus with the DEF scheme and use the one with minimal |d( T  )−τ |. However, most corpora tend to yield texts of a limited difficulty range that only suit a specific learner profile ( cf. section 6 ). Another drawback of the naı¨ve strategy is that it is difficult to control for the topic of the underlying text and in the worst case, the necessity to search through a whole corpus for selecting a fitting C-test. In contrast to the naı¨ve strategy, our proposed manipulation strategies are designed to be used in real time and manipulate any given C-test within 15 seconds at an acceptable quality.3 Both strategies operate on a given text ( e.g., on a topic a learner is interested in ) and manipulate its gap set G in order to come close to the learner’s current language skill. The first strategy varies the position of the gaps and the second strategy learns to increase or decrease the size of the gaps. 5.1 Gap Selection Strategy The default C-test generation scheme DEF creates a gap in every second word w2j , 1 ≤ j ≤ n. The core idea of our first manipulation strategy SEL is to distribute the n gaps differently among the all 2n words in order to create gaps for easier or harder words than in the default generation scheme. Therefore, we use the difficulty prediction system to predict d( g ) for any possible gap g ∈ GFULL = {( i, d |wi|2 e ) | 1 ≤ i ≤ 2n} ( i.e., assuming a gap in all words rather than in every second word ). Then, we alternate between adding gaps to the resultingGSEL that are easier and harder than the preferred target difficulty τ , starting with those having a minimal difference |d( g )− τ |. Algorithm 1 shows this procedure in pseudocode and figure 1 shows a C-test whose difficulty has been increased with this strategy. Note that it has selected gaps at corresponding rather than with, and soothsayers rather than the. Our proposed algorithm is optimized for runtime. An exhaustive search would require testing ( 2n n  ) combinations if the number of gaps is constant. For n= 20, this yields 137 billion combinations. While more advanced optimization methods might find better gap selections, we show in section 6 that our strategy achieves good results. 5.2 Gap Size Strategy Our second manipulation strategy SIZE changes the size of the gaps based on a pre-defined gap set. Increasing a gap g=( i, ` ) by one or more characters, yielding g′=( i, `+ k ) increases its difficulty ( i.e., d( g′ )≥ d( g ) ), while smaller gaps make the gap easier. We identify a major challenge in estimating the effect of increasing or decreasing the gap size on the gap difficulty. Although d( g′ ) could be estimated using the full difficulty prediction system, the search space is even larger than for the gap selection strategy, since each of the n gaps has |wi|−2 possible gap sizes to test. For n = 20 and an average word length of six, this amounts to one trillion possible combinations. We therefore propose a new approach to predict the relative difficulty change of a gap g = ( i, ` ) when increasing the gap size by one letter ∆inc( g ) ≈ d( g′ )− d( g ), g′ = ( i, `+ 1 ) and correspondingly when decreasing the gap size by one letter ∆dec( g ) ≈ d( g )−d( g′ ), g′ = ( i, `−1 ). The notion of relative difficulty change enables gap size manipulation in real time, since we do not have to invoke the full difficulty prediction system for all combinations. Instead, we can incrementally predict the effect of changing a single gap. To predict ∆ inc and ∆ dec , we train two SVMs on all gap size combinations of 120 random texts from the Brown corpus ( Francis , 1965 ) using the following features : predicted absolute gap difficulty , word length , new gap size , modified character , a binary indicator if the gap is at a th sound , and logarithmic difference of alternative solutions capturing the degree of ambiguity with varying gap size . With a final set of only six features , our new models are able to approximate the relative difficulty change very well deviating from the original system ’ s prediction only by 0.06 RMSE for ∆ inc and 0.13 RMSE for ∆ dec . The predictions of both models highly correlate with the predictions achieving a Pearson ’ s ρ of over 0.8 . Besides achieving a much faster average runtime of 0.056 seconds for the relative model vs . 11 seconds for the full prediction of a single change , we can invoke the relative model iteratively to estimate d ( T ) for multiple changes of the gap size more efficiently . The final manipulation strategy then requires just a single call of the full prediction system. If d( T  )<τ , we incrementally increase the gap sizes to make T more difficult and, vice-versa, decrease the gap sizes if d( T  ) > τ . In each iteration, we modify the gap with the highest relative difficulty change in order to approach the given target difficulty τ as quickly as possible. Algorithm 2 shows pseudocode for creating Gsize with increased difficulty ( i.e., d( T  ) < τ  ) based on the default gap scheme DEF. The procedure for d( T  )> τ works analogously, but using ∆dec and decreasing the gap size. Figure 1 ( c ) shows a much easier version of the example C-test, in which a learner often only has to complete the last one or two letters. 6 Evaluation of the Manipulation System To evaluate our C-test manipulation strategies , we first test their ability to cover a higher range of target difficulties than the default generation scheme and then measure how well they meet the desired target difficulty for texts from different domains . We conduct our experiments on 1,000 randomly chosen paragraphs for each of the Gutenberg ( Lahiri , 2014 ) , Reuters ( Lewis et al . , 2004 ) , and Brown ( Francis , 1965 ) corpora . We conduct our experiments on English, but our strategies can be adapted to many related languages. Difficulty range. The black - marked line of figure 3 shows the distribution of d ( T ) based on our difficulty prediction system when creating a C-test with the default generation scheme DEF for all our samples of the Brown corpus . The vast majority of C-tests range between 0.15 and 0.30 with a predominant peak at 0.22. To assess the maximal difficulty range our strategies can achieve, we generate C-tests with maximal ( τ = 1 ) and minimal target difficulty ( τ = 0 ) for both strategies S ∈ {SEL, SIZE}, which are also shown in figure 3 as ( S, τ ). Both strategies are able to clearly increase and decrease the test difficulty in the correct direction and they succeed in substantially increasing the total difficulty range beyond DEF. While SEL is able to reach lower difficulty ranges, it has bigger issues with generating very difficult tests. This is due to its limitation to the fixed gap sizes, whereas SIZE can in some cases create large gaps that are ambiguous or even unsolvable. Since SIZE is, however, limited to the 20 predefined gaps, it shows a higher variance. Especially short gaps such as is and it cannot be made more difficult. Combining the two strategies is thus a logical next step for future work, building upon our findings for both strategies. We make similar observations on the Reuters and Gutenberg corpora and provide the respective figures in the appendix . Manipulation quality. We finally evaluate how well each strategy S reaches a given target difficulty. That is , we sample a random corpus text and τ , create the C-test using strategy S , predict the test difficulty d ( T ) and measure its difference to τ using RMSE . Table 2 shows the results for our three corpora. Throughout all three corpora, both manipulation strategies perform well. SEL consistently outperforms SIZE, which matches our observations from the previous experiment. Mind that these results depend on the quality of the automatic difficulty predictions, which is why we conduct a user-based evaluation in the next section. 7 User-based Evaluation Hypothesis. To evaluate the effectiveness of our manipulation strategies in a real setting, we conduct a user study and analyze the difficulty of the manipulated and unmanipulated C-tests. We investigate the following hypothesis: When increasing a test’s difficulty using strategy S, the participants will make more errors and judge the test harder than a default C-test and, vice versa, when decreasing a test’s difficulty using S, the participants will make less errors and judge the test easier. Experimental design. We select four different English texts from the Brown corpus and shorten them to about 100 words with keeping their paragraph structure intact . None of the four texts is particularly easy to read with an average grade level above 12 and a Flesh reading ease score ranging between 25 ( very difficult ) to 56 ( fairly difficult ). In the supplementary material, we provide results of an automated readability analysis using standard metrics. From the four texts, we then generate the C-tests Ti, 1 ≤ i ≤ 4 using the default generation scheme DEF. All tests contain exactly n = 20 gaps and their predicted difficulties d( Ti ) are in a mid range between 0.24 and 0.28. T1 remains unchanged in all test conditions and is used to allow the participants to familiarize with the task. For the remaining three texts, we generate an easier variant TS,deci with target difficulty τ = 0.1 and a harder variant TS,inci with τ = 0.5 for both strategies S ∈ {SEL, SIZE}. From these tests, we create 12 sequences of four C-tests that we give to the participants. Each participant receives T1 first to familiarize with the task. Then, they receive one easy TS,deci , one default Ti, and one hard T S,inc i C-test for the same strategy S based on the texts i ∈ {2, 3, 4} in random order without duplicates ( e.g., the sequence T1 T SEL,dec 2 T3 T SEL,inc 4  ). Having finished a C-test, we ask them to judge the difficulty of this test on a five-point Likert scale ranging from too easy to too hard. After solving the last test, we additionally collect a ranking of all four tests by their difficulty. Data collection. We collect the data from our participants with a self-implemented web interface for solving C-tests. We create randomized credentials linked to a unique ID for each participant and obfuscate their order, such that we can distinguish them but cannot trace back their identity and thus avoid collecting any personal information. Additionally, we ask each participant for their consent on publishing the collected data. For experiments with a similar setup and task, we obtained the approval of the university’s ethics commission. After login, the participants receive instructions and provide a self-assessment of their English proficiency and their time spent on language learning. The participants then solve the four successive C-tests without knowing the test difficulty or the manipulation strategy applied. They are instructed to spend a maximum of five minutes per C-test to avoid time-based effects and to prevent them from consulting external resources, which would bias the results. Participants. A total of 60 participants completed the study. We uniformly distributed the 12 test sequences ( six per strategy ), such that we have 30 easy, 30 default, and 30 hard C-test results for each manipulation strategy. No participant is native in English, 17 are taking language courses, and 57 have higher education or are currently university students. The frequency of their use of English varies, as we found a similar number of participants using English daily, weekly, monthly, and ( almost ) never in practice. An analysis of the questionnaire is provided in the paper’s appendix. Hypothesis testing. We evaluate our hypothesis along three dimensions: ( 1 ) the actual error rate of the participants, ( 2 ) the perceived difficulty after each individual C-test ( Likert feedback ), and ( 3 ) the participants’ final difficulty ranking. While the latter forces the participants to provide an explicit ranking, the former allows them to rate C-tests equally difficult. We conduct significance testing at the Bonferroni-corrected α = 0.052 = 0.025 for each dimension using one-tailed t-tests for the continuous error rates and one-tailed Mann–Whitney U tests for the ordinal-scaled perceived difficulties and rankings . Figure 4 shows notched boxplots of our results. To test our hypothesis, we first formulate a null hypothesis that ( a ) the mean error rate, ( b ) the median perceived difficulty ( Likert feedback ), and ( c ) the median rank of the manipulated tests equal the default tests. While the participants have an average error rate of 0.3 on default C-tests , the TS , deci tests are significantly easier with an average error rate of 0.15 ( t = 7.49 , p < 10 − 5 ) and the TS , inci tests are significantly harder with an average error rate of 0.49 ( t = − 7.83 , p < 10 − 5 ) , so we can safely reject the null hypothesis for error rates . Table 3 shows the error rates per C-test and strategy . Both SEL and SIZE are overall able to significantly ( p < 0.025 ) increase and decrease the test’s difficulty over DEF, and with the exception of T SEL,dec4 , the effect is also statistically significant for all individual text and strategy pairs. Figure 5 shows the 30 participants per strategy on the x-axis and their error rates in their second to fourth C-test on the y-axis . C-tests, for which we increased the difficulty ( S, inc ), yield more errors than C-tests with decreased difficulty ( S,dec ) in all cases. The easier tests also yield less errors than the test with the default scheme DEF in most cases. While hard tests often have a much higher error rate than DEF , we find some exceptions , in which the participant ’ s error rate is close or even below the DEF error rate . Regarding the perceived difficulty , we find that the participants judge the manipulated C-tests with lower d ( T ) as easier on both the Likert scale ( z = 6.16 , p < 10 − 5 ) and in the rankings ( z = 6.59 , p < 10 − 5 ) based on the Mann-Whitney-U test . The same is true for C-tests that have been manipulated to a higher difficulty level, which the participant judge harder ( z = −4.57, p < 10−5 ) and rank higher ( z = −3.86, p < 6 · 10−5 ). We therefore reject the null hypotheses for the Likert feedback and the rankings and conclude that both strategies can effectively manipulate a C-test’s difficulty. Manipulation quality. We further investigate if the strategies yield different difficulty levels. Therefore , we use two-tailed significance testing between SEL and SIZE for all three dimensions . We find that SIZE yields significantly easier C-tests than SEL in terms of error rates ( p = 0.0014 ) and Likert feedback ( p = 6 · 10 − 5 ) , and observe p = 0.0394 for the rankings . For increasing the difficulty, we, however, do not find significant differences between the two strategies. Since both strategies successfully modify the difficulty individually, this motivates research on combined strategies in the future. We furthermore investigate how well our strategies perform in creating C-tests with the given target difficulty τ . Table 4 shows the RMSE for e ( T ) and d ( T ) as well as for e ( T ) and τ for both strategies . As expected, our difficulty prediction system works best for C-tests generated with DEF as they use the same scheme as C-tests in the training data. Though slightly worse than for DEF , we still find very low RMSE scores for manipulated C-tests . This is especially good when considering that the system ’ s performance on our newly acquired dataset yields and RMSE of 0.21 ( cf . section 6 ) . Computing the RMSE with respect to our chosen target difficulties τ yields equally good results for SEL and exceptionally good results for SIZE . Figure 6 displays d( T  ) in comparison to e( T  ) for each individual text and strategy. With the exception of T SEL,inc2 and T SEL,dec 4 , all predictions are close to the optimum ( i.e., the diagonal ) and also close to the desired target difficulty τ . In a more detailed analysis, we find two main sources of problems demanding further investigation: First, the difficulty prediction quality when deviating from DEF and second, the increasing ambiguity in harder C-tests. However , it underestimates the d ( T ) = 0.11 for T SEL , dec4 ( the same text used in figure 1 ) , for which we found an actual error rate of 0.28 . This is due to chains of four successive gaps, such as: As the prediction system has been trained only on DEF-generated C-tests, it underestimates d( g ) for cases with limited context. It will be interesting for future work to focus on modeling gap interdependencies in C-tests deviating from DEF. Another issue we observe is that the gap size strategy might increase the ambiguity of the C-test. In the standard scheme, there is in most cases only a single correct answer per gap. In T SIZE,inc2 , however, the SIZE strategy increased the gap of the word professional to its maximal length yielding p . One participant answered popularising for this gap, which also fits the given context. We carefully checked our datasetfor other ambiguity, but only found one additional case: In T4, instead of the word close, 13 participants out of 30 used clear as a modifier of correspondence, which both produce meaningful contexts. Given that this case is already ambiguous in the DEF scheme yielding the gap cl , we conclude that the issue is not severe, but that the difficulty prediction system should be improved to better capture ambiguous cases; for example, by introducing collocational features weighted by their distribution within a corpus into ∆inc and ∆dec. 8 Conclusion In this work, we proposed two novel strategies for automatically manipulating the difficulty of C-test exercises. Our first strategy selects which words should be turned into a gap, and the second strategy learns to increase or decrease the size of the gaps. Both strategies automatically predict the difficulty of a test to make informed decisions. To this end, we reproduced previous results, compared them to neural architectures, and tested them on a newly acquired dataset. We evaluate our difficulty manipulation pipeline in a corpus-based study and with real users. We show that both strategies can effectively manipulate the C-test difficulty, as both the participants’ error rates and their perceived difficulty yield statistically significant effects. Both strategies reach close to the desired difficulty level. Our error analysis points out important directions for future work on detecting ambiguous gaps and modeling gap interdependencies for C-tests deviating from the default generation scheme. An important observation is that manipulating the gaps’ size and position does not only influence the C-test difficulty, but also addresses different competencies ( e.g., requires more vocabulary knowledge or more grammatical knowledge ). Future manipulation strategies that take the competencies into account have the potential to train particular skills and to better control the competencies required for a placement test. Another strand of research will be combining both strategies and deploying the manipulation strategies in a large scale testing platform that allows the system to adapt to an individual learner over time. A core advantage of our manipulation strategies is that we can work with any given text and thus provide C-tests that do not only have the desired difficulty, but also integrate the learner’s interest or the current topic of a language course. "
            }
        }
    ]
}