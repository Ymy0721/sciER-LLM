[
  {
    "id": "abstract-2021--acl-long--96",
    "result": [
      {
        "value": {
          "start": 872,
          "end": 910,
          "text": "quality of the augmentation strategies",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--96:E0"
      },
      {
        "value": {
          "start": 445,
          "end": 456,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--96:E1"
      },
      {
        "value": {
          "start": 836,
          "end": 863,
          "text": "number of training examples",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--96:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--96:E0",
        "to_id": "abstract-2021--acl-long--96:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--96:E2",
        "to_id": "abstract-2021--acl-long--96:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "In this paper we implement and compare 7 different data augmentation strategies for the task of automatic scoring of children’s ability to understand others’ thoughts, feelings, and desires (or “mindreading”). We recruit in-domain experts to re-annotate augmented samples and determine to what extent each strategy preserves the original rating. We also carry out multiple experiments to measure how much each augmentation strategy improves the performance of automatic scoring systems. To determine the capabilities of automatic systems to generalize to unseen data, we create UK-MIND-20 - a new corpus of children’s performance on tests of mindreading, consisting of 10,320 question-answer pairs. We obtain a new state-of-the-art performance on the MIND-CA corpus, improving macro-F1-score by 6 points. Results indicate that both the number of training examples and the quality of the augmentation strategies affect the performance of the systems. The task-specific augmentations generally outperform task-agnostic augmentations. Automatic augmentations based on vectors (GloVe, FastText) perform the worst. We find that systems trained on MIND-CA generalize well to UK-MIND-20. We demonstrate that data augmentation strategies also improve the performance on unseen data."
    }
  },
  {
    "id": "abstract-2020--acl-main--268",
    "result": [
      {
        "value": {
          "start": 883,
          "end": 916,
          "text": "use the auxiliary dataset for MTL",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--268:E0"
      },
      {
        "value": {
          "start": 943,
          "end": 954,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--268:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--268:E0",
        "to_id": "abstract-2020--acl-main--268:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Multi-task learning (MTL) and transfer learning (TL) are techniques to overcome the issue of data scarcity when training state-of-the-art neural networks. However, finding beneficial auxiliary datasets for MTL or TL is a time- and resource-consuming trial-and-error approach. We propose new methods to automatically assess the similarity of sequence tagging datasets to identify beneficial auxiliary data for MTL or TL setups. Our methods can compute the similarity between any two sequence tagging datasets, they do not need to be annotated with the same tagset or multiple labels in parallel. Additionally, our methods take tokens and their labels into account, which is more robust than only using either of them as an information source, as conducted in prior work. We empirically show that our similarity measures correlate with the change in test score of neural networks that use the auxiliary dataset for MTL to increase the main task performance. We provide an efficient, open-source implementation."
    }
  },
  {
    "id": "abstract-2020--acl-main--195",
    "result": [
      {
        "value": {
          "start": 309,
          "end": 319,
          "text": "MobileBERT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--195:E0"
      },
      {
        "value": {
          "start": 1120,
          "end": 1130,
          "text": "GLUE score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--195:E1"
      },
      {
        "value": {
          "start": 205,
          "end": 212,
          "text": "latency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--195:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--195:E0",
        "to_id": "abstract-2020--acl-main--195:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--195:E0",
        "to_id": "abstract-2020--acl-main--195:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resource-limited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning. Basically, MobileBERT is a thin version of BERT_LARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an inverted-bottleneck incorporated BERT_LARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is 4.3x smaller and 5.5x faster than BERT_BASE while achieving competitive results on well-known benchmarks. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERT_BASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERT_BASE)."
    }
  },
  {
    "id": "abstract-2020--acl-main--14",
    "result": [
      {
        "value": {
          "start": 776,
          "end": 818,
          "text": "obtain the better argument representations",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--14:E0"
      },
      {
        "value": {
          "start": 841,
          "end": 852,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--14:E1"
      },
      {
        "value": {
          "start": 423,
          "end": 442,
          "text": "multi-level encoder",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--14:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--14:E0",
        "to_id": "abstract-2020--acl-main--14:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--14:E2",
        "to_id": "abstract-2020--acl-main--14:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Implicit discourse relation recognition is a challenging task due to the lack of connectives as strong linguistic clues. Previous methods primarily encode two arguments separately or extract the specific interaction patterns for the task, which have not fully exploited the annotated relation signal. Therefore, we propose a novel TransS-driven joint learning architecture to address the issues. Specifically, based on the multi-level encoder, we 1) translate discourse relations in low-dimensional embedding space (called TransS), which could mine the latent geometric structure information of argument-relation instances; 2) further exploit the semantic features of arguments to assist discourse understanding; 3) jointly learn 1) and 2) to mutually reinforce each other to obtain the better argument representations, so as to improve the performance of the task. Extensive experimental results on the Penn Discourse TreeBank (PDTB) show that our model achieves competitive results against several state-of-the-art systems."
    }
  },
  {
    "id": "abstract-2021--acl-long--267",
    "result": [
      {
        "value": {
          "start": 530,
          "end": 543,
          "text": "G-Transformer",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--267:E0"
      },
      {
        "value": {
          "start": 804,
          "end": 815,
          "text": "BLEU scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--267:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--267:E0",
        "to_id": "abstract-2021--acl-long--267:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Document-level MT models are still far from satisfactory. Existing work extend translation unit from single sentence to multiple sentences. However, study shows that when we further enlarge the translation unit to a whole document, supervised training of Transformer can fail. In this paper, we find such failure is not caused by overfitting, but by sticking around local minima during training. Our analysis shows that the increased complexity of target-to-source attention is a reason for the failure. As a solution, we propose G-Transformer, introducing locality assumption as an inductive bias into Transformer, reducing the hypothesis space of the attention from target to source. Experiments show that G-Transformer converges faster and more stably than Transformer, achieving new state-of-the-art BLEU scores for both nonpretraining and pre-training settings on three benchmark datasets."
    }
  },
  {
    "id": "abstract-2020--acl-main--139",
    "result": [
      {
        "value": {
          "start": 478,
          "end": 494,
          "text": "weak supervision",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--139:E0"
      },
      {
        "value": {
          "start": 1041,
          "end": 1050,
          "text": "F1 scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--139:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--139:E0",
        "to_id": "abstract-2020--acl-main--139:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Named Entity Recognition (NER) performance often degrades rapidly when applied to target domains that differ from the texts observed during training. When in-domain labelled data is available, transfer learning techniques can be used to adapt existing NER models to the target domain. But what should one do when there is no hand-labelled data for the target domain? This paper presents a simple but powerful approach to learn NER models in the absence of labelled data through weak supervision. The approach relies on a broad spectrum of labelling functions to automatically annotate texts from the target domain. These annotations are then merged together using a hidden Markov model which captures the varying accuracies and confusions of the labelling functions. A sequence labelling model can finally be trained on the basis of this unified annotation. We evaluate the approach on two English datasets (CoNLL 2003 and news articles from Reuters and Bloomberg) and demonstrate an improvement of about 7 percentage points in entity-level F1 scores compared to an out-of-domain neural NER model."
    }
  },
  {
    "id": "P10-1036",
    "result": [
      {
        "value": {
          "start": 19,
          "end": 39,
          "text": "self-training method",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1036:E0"
      },
      {
        "value": {
          "start": 166,
          "end": 174,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1036:E1"
      },
      {
        "value": {
          "start": 130,
          "end": 135,
          "text": "speed",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1036:E2"
      },
      {
        "from_id": "P10-1036:E0",
        "to_id": "P10-1036:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P10-1036:E0",
        "to_id": "P10-1036:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We propose a novel self-training method for a parser which uses a lexicalised grammar and supertagger, focusing on increasing the speed of the parser rather than its accuracy. The idea is to train the supertagger on large amounts of parser output, so that the supertagger can learn to supply the supertags that the parser will eventually choose as part of the highest-scoring derivation. Since the supertagger supplies fewer supertags overall, the parsing speed is increased. We demonstrate the effectiveness of the method using a CCG supertagger and parser, obtaining significant speed increases on newspaper text with no loss in accuracy. We also show that the method can be used to adapt the CCG parser to new domains, obtaining accuracy and speed improvements for Wikipedia and biomedical text."
    }
  },
  {
    "id": "abstract-2020--acl-main--656",
    "result": [
      {
        "value": {
          "start": 917,
          "end": 933,
          "text": "multi-task model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--656:E0"
      },
      {
        "value": {
          "start": 816,
          "end": 831,
          "text": "informativeness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--656:E1"
      },
      {
        "value": {
          "start": 833,
          "end": 841,
          "text": "coverage",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--656:E2"
      },
      {
        "value": {
          "start": 846,
          "end": 861,
          "text": "overall quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--656:E3"
      },
      {
        "value": {
          "start": 621,
          "end": 664,
          "text": "optimising both objectives at the same time",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--656:E4"
      },
      {
        "value": {
          "start": 717,
          "end": 754,
          "text": "performance of a fact checking system",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--656:E5"
      },
      {
        "from_id": "abstract-2020--acl-main--656:E0",
        "to_id": "abstract-2020--acl-main--656:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--656:E0",
        "to_id": "abstract-2020--acl-main--656:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--656:E0",
        "to_id": "abstract-2020--acl-main--656:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--656:E4",
        "to_id": "abstract-2020--acl-main--656:E5",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Most existing work on automated fact checking is concerned with predicting the veracity of claims based on metadata, social network spread, language used in claims, and, more recently, evidence supporting or denying claims. A crucial piece of the puzzle that is still missing is to understand how to automate the most elaborate part of the process – generating justifications for verdicts on claims. This paper provides the first study of how these explanations can be generated automatically based on available claim context, and how this task can be modelled jointly with veracity prediction. Our results indicate that optimising both objectives at the same time, rather than training them separately, improves the performance of a fact checking system. The results of a manual evaluation further suggest that the informativeness, coverage and overall quality of the generated explanations are also improved in the multi-task model."
    }
  },
  {
    "id": "abstract-2020--acl-main--22",
    "result": [
      {
        "value": {
          "start": 769,
          "end": 827,
          "text": "rearrangement to produce a sequence of position embeddings",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--22:E0"
      },
      {
        "value": {
          "start": 1105,
          "end": 1114,
          "text": "diversity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--22:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--22:E0",
        "to_id": "abstract-2020--acl-main--22:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Paraphrasing natural language sentences is a multifaceted process: it might involve replacing individual words or short phrases, local rearrangement of content, or high-level restructuring like topicalization or passivization. Past approaches struggle to cover this space of paraphrase possibilities in an interpretable manner. Our work, inspired by pre-ordering literature in machine translation, uses syntactic transformations to softly “reorder” the source sentence and guide our neural paraphrasing model. First, given an input sentence, we derive a set of feasible syntactic rearrangements using an encoder-decoder model. This model operates over a partially lexical, partially syntactic view of the sentence and can reorder big chunks. Next, we use each proposed rearrangement to produce a sequence of position embeddings, which encourages our final encoder-decoder paraphrase model to attend to the source words in a particular order. Our evaluation, both automatic and human, shows that the proposed system retains the quality of the baseline approaches while giving a substantial increase in the diversity of the generated paraphrases."
    }
  },
  {
    "id": "abstract-2020--acl-main--239",
    "result": [
      {
        "value": {
          "start": 317,
          "end": 368,
          "text": "adding a decoding layer for sentence reconstruction",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--239:E0"
      },
      {
        "value": {
          "start": 389,
          "end": 400,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--239:E1"
      },
      {
        "value": {
          "start": 586,
          "end": 594,
          "text": "F1 score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--239:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--239:E0",
        "to_id": "abstract-2020--acl-main--239:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--239:E0",
        "to_id": "abstract-2020--acl-main--239:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "This work revisits the task of training sequence tagging models with limited resources using transfer learning. We investigate several proposed approaches introduced in recent works and suggest a new loss that relies on sentence reconstruction from normalized embeddings. Specifically, our method demonstrates how by adding a decoding layer for sentence reconstruction, we can improve the performance of various baselines. We show improved results on the CoNLL02 NER and UD 1.2 POS datasets and demonstrate the power of the method for transfer learning with low-resources achieving 0.6 F1 score in Dutch using only one sample from it."
    }
  },
  {
    "id": "abstract-2021--acl-long--253",
    "result": [
      {
        "value": {
          "start": 888,
          "end": 894,
          "text": "Refuel",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--253:E0"
      },
      {
        "value": {
          "start": 928,
          "end": 939,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--253:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--253:E0",
        "to_id": "abstract-2021--acl-long--253:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In open-domain question answering, questions are highly likely to be ambiguous because users may not know the scope of relevant topics when formulating them. Therefore, a system needs to find possible interpretations of the question, and predict one or multiple plausible answers. When multiple plausible answers are found, the system should rewrite the question for each answer to resolve the ambiguity. In this paper, we present a model that aggregates and combines evidence from multiple passages to adaptively predict a single answer or a set of question-answer pairs for ambiguous questions. In addition, we propose a novel round-trip prediction approach to iteratively generate additional interpretations that our model fails to find in the first pass, and then verify and filter out the incorrect question-answer pairs to arrive at the final disambiguated output. Our model, named Refuel, achieves a new state-of-the-art performance on the AmbigQA dataset, and shows competitive performance on NQ-Open and TriviaQA. The proposed round-trip prediction is a model-agnostic general approach for answering ambiguous open-domain questions, which improves our Refuel as well as several baseline models. We release source code for our models and experiments at https://github.com/amzn/refuel-open-domain-qa."
    }
  },
  {
    "id": "abstract-2020--acl-main--202",
    "result": [
      {
        "value": {
          "start": 472,
          "end": 546,
          "text": "stage-wise optimization scheme leveraging teacher internal representations",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--202:E0"
      },
      {
        "value": {
          "start": 933,
          "end": 943,
          "text": "parameters",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--202:E1"
      },
      {
        "value": {
          "start": 802,
          "end": 809,
          "text": "latency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--202:E2"
      },
      {
        "value": {
          "start": 1019,
          "end": 1027,
          "text": "F1-score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--202:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--202:E0",
        "to_id": "abstract-2020--acl-main--202:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--202:E0",
        "to_id": "abstract-2020--acl-main--202:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--202:E0",
        "to_id": "abstract-2020--acl-main--202:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Deep and large pre-trained language models are the state-of-the-art for various natural language processing tasks. However, the huge size of these models could be a deterrent to using them in practice. Some recent works use knowledge distillation to compress these huge models into shallow ones. In this work we study knowledge distillation with a focus on multilingual Named Entity Recognition (NER). In particular, we study several distillation strategies and propose a stage-wise optimization scheme leveraging teacher internal representations, that is agnostic of teacher architecture, and show that it outperforms strategies employed in prior works. Additionally, we investigate the role of several factors like the amount of unlabeled data, annotation resources, model architecture and inference latency to name a few. We show that our approach leads to massive compression of teacher models like mBERT by upto 35x in terms of parameters and 51x in terms of latency for batch inference while retaining 95% of its F1-score for NER over 41 languages."
    }
  },
  {
    "id": "abstract-2021--acl-long--311",
    "result": [
      {
        "value": {
          "start": 684,
          "end": 734,
          "text": "constraints provided as part of the input sequence",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--311:E0"
      },
      {
        "value": {
          "start": 901,
          "end": 920,
          "text": "errors in agreement",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--311:E1"
      },
      {
        "value": {
          "start": 1015,
          "end": 1030,
          "text": "overall quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--311:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--311:E0",
        "to_id": "abstract-2021--acl-long--311:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--311:E0",
        "to_id": "abstract-2021--acl-long--311:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Lexically constrained machine translation allows the user to manipulate the output sentence by enforcing the presence or absence of certain words and phrases. Although current approaches can enforce terms to appear in the translation, they often struggle to make the constraint word form agree with the rest of the generated output. Our manual analysis shows that 46% of the errors in the output of a baseline constrained model for English to Czech translation are related to agreement. We investigate mechanisms to allow neural machine translation to infer the correct word inflection given lemmatized constraints. In particular, we focus on methods based on training the model with constraints provided as part of the input sequence. Our experiments on English-Czech language pair show that this approach improves translation of constrained terms in both automatic and manual evaluation by reducing errors in agreement. Our approach thus eliminates inflection errors, without introducing new errors or decreasing overall quality of the translation."
    }
  },
  {
    "id": "abstract-2020--acl-main--12",
    "result": [
      {
        "value": {
          "start": 581,
          "end": 626,
          "text": "synthesized in-domain data on the SUMBT model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--12:E0"
      },
      {
        "value": {
          "start": 438,
          "end": 446,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--12:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--12:E0",
        "to_id": "abstract-2020--acl-main--12:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Zero-shot transfer learning for multi-domain dialogue state tracking can allow us to handle new domains without incurring the high cost of data acquisition. This paper proposes new zero-short transfer learning technique for dialogue state tracking where the in-domain training data are all synthesized from an abstract dialogue model and the ontology of the domain. We show that data augmentation through synthesized data can improve the accuracy of zero-shot learning for both the TRADE model and the BERT-based SUMBT model on the MultiWOZ 2.1 dataset. We show training with only synthesized in-domain data on the SUMBT model can reach about 2/3 of the accuracy obtained with the full training dataset. We improve the zero-shot learning state of the art on average across domains by 21%."
    }
  },
  {
    "id": "abstract-2020--acl-main--757",
    "result": [
      {
        "value": {
          "start": 376,
          "end": 464,
          "text": "introduce a gate mechanism to control the source and target contributions in Transformer",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--757:E0"
      },
      {
        "value": {
          "start": 809,
          "end": 819,
          "text": "BLEU score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--757:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--757:E0",
        "to_id": "abstract-2020--acl-main--757:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Context gates are effective to control the contributions from the source and target contexts in the recurrent neural network (RNN) based neural machine translation (NMT). However, it is challenging to extend them into the advanced Transformer architecture, which is more complicated than RNN. This paper first provides a method to identify source and target contexts and then introduce a gate mechanism to control the source and target contributions in Transformer. In addition, to further reduce the bias problem in the gate mechanism, this paper proposes a regularization method to guide the learning of the gates with supervision automatically generated using pointwise mutual information. Extensive experiments on 4 translation datasets demonstrate that the proposed model obtains an averaged gain of 1.0 BLEU score over a strong Transformer baseline."
    }
  },
  {
    "id": "P10-1033",
    "result": [
      {
        "value": {
          "start": 428,
          "end": 462,
          "text": "discriminative ITG alignment model",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1033:E0"
      },
      {
        "value": {
          "start": 516,
          "end": 523,
          "text": "F-score",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1033:E1"
      },
      {
        "value": {
          "start": 528,
          "end": 538,
          "text": "Bleu score",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1033:E2"
      },
      {
        "from_id": "P10-1033:E0",
        "to_id": "P10-1033:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P10-1033:E0",
        "to_id": "P10-1033:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "While Inversion Transduction Grammar (ITG) has regained more and more attention in recent years, it still suffers from the major obstacle of speed. We propose a discriminative ITG pruning framework using Minimum Error Rate Training and various features from previous work on ITG alignment. Experiment results show that it is superior to all existing heuristics in ITG pruning. On top of the pruning framework, we also propose a discriminative ITG alignment model using hierarchical phrase pairs, which improves both F-score and Bleu score over the baseline alignment system of GIZA++."
    }
  },
  {
    "id": "abstract-2021--acl-long--333",
    "result": [
      {
        "value": {
          "start": 50,
          "end": 62,
          "text": "convolutions",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--333:E0"
      },
      {
        "value": {
          "start": 590,
          "end": 601,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--333:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--333:E0",
        "to_id": "abstract-2021--acl-long--333:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In this paper, we detail the relationship between convolutions and self-attention in natural language tasks. We show that relative position embeddings in self-attention layers are equivalent to recently-proposed dynamic lightweight convolutions, and we consider multiple new ways of integrating convolutions into Transformer self-attention. Specifically, we propose composite attention, which unites previous relative position encoding methods under a convolutional framework. We conduct experiments by training BERT with composite attention, finding that convolutions consistently improve performance on multiple downstream tasks, replacing absolute position embeddings. To inform future work, we present results comparing lightweight convolutions, dynamic convolutions, and depthwise-separable convolutions in language model pre-training, considering multiple injection points for convolutions in self-attention layers."
    }
  },
  {
    "id": "abstract-2020--acl-main--752",
    "result": [
      {
        "value": {
          "start": 1001,
          "end": 1028,
          "text": "trigger-annotated sentences",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--752:E0"
      },
      {
        "value": {
          "start": 1053,
          "end": 1064,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--752:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--752:E0",
        "to_id": "abstract-2020--acl-main--752:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Training neural models for named entity recognition (NER) in a new domain often requires additional human annotations (e.g., tens of thousands of labeled instances) that are usually expensive and time-consuming to collect. Thus, a crucial research question is how to obtain supervision in a cost-effective way. In this paper, we introduce “entity triggers,” an effective proxy of human explanations for facilitating label-efficient learning of NER models. An entity trigger is defined as a group of words in a sentence that helps to explain why humans would recognize an entity in the sentence. We crowd-sourced 14k entity triggers for two well-studied NER datasets. Our proposed model, Trigger Matching Network, jointly learns trigger representations and soft matching module with self-attention such that can generalize to unseen sentences easily for tagging. Our framework is significantly more cost-effective than the traditional neural NER frameworks. Experiments show that using only 20% of the trigger-annotated sentences results in a comparable performance as using 70% of conventional annotated sentences."
    }
  },
  {
    "id": "abstract-2021--acl-long--383",
    "result": [
      {
        "value": {
          "start": 992,
          "end": 1016,
          "text": "small unpretrained model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--383:E0"
      },
      {
        "value": {
          "start": 1086,
          "end": 1099,
          "text": "effectiveness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--383:E1"
      },
      {
        "value": {
          "start": 1104,
          "end": 1114,
          "text": "efficiency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--383:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--383:E0",
        "to_id": "abstract-2021--acl-long--383:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--383:E0",
        "to_id": "abstract-2021--acl-long--383:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Personalization of natural language generation plays a vital role in a large spectrum of tasks, such as explainable recommendation, review summarization and dialog systems. In these tasks, user and item IDs are important identifiers for personalization. Transformer, which is demonstrated with strong language modeling capability, however, is not personalized and fails to make use of the user and item IDs since the ID tokens are not even in the same semantic space as the words. To address this problem, we present a PErsonalized Transformer for Explainable Recommendation (PETER), on which we design a simple and effective learning objective that utilizes the IDs to predict the words in the target explanation, so as to endow the IDs with linguistic meanings and to achieve personalized Transformer. Besides generating explanations, PETER can also make recommendations, which makes it a unified model for the whole recommendation-explanation pipeline. Extensive experiments show that our small unpretrained model outperforms fine-tuned BERT on the generation task, in terms of both effectiveness and efficiency, which highlights the importance and the nice utility of our design."
    }
  },
  {
    "id": "abstract-2021--acl-long--437",
    "result": [
      {
        "value": {
          "start": 548,
          "end": 556,
          "text": "SepaCVAE",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--437:E0"
      },
      {
        "value": {
          "start": 910,
          "end": 946,
          "text": "relative distance between data pairs",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--437:E1"
      },
      {
        "value": {
          "start": 481,
          "end": 557,
          "text": "Self-separated Conditional Variational AutoEncoder (abbreviated as SepaCVAE)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--437:E2"
      },
      {
        "value": {
          "start": 69,
          "end": 78,
          "text": "diversity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--437:E3"
      },
      {
        "value": {
          "start": 83,
          "end": 98,
          "text": "informativeness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--437:E4"
      },
      {
        "from_id": "abstract-2021--acl-long--437:E0",
        "to_id": "abstract-2021--acl-long--437:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--437:E2",
        "to_id": "abstract-2021--acl-long--437:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--437:E2",
        "to_id": "abstract-2021--acl-long--437:E4",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Conditional Variational AutoEncoder (CVAE) effectively increases the diversity and informativeness of responses in open-ended dialogue generation tasks through enriching the context vector with sampled latent variables. However, due to the inherent one-to-many and many-to-one phenomena in human dialogues, the sampled latent variables may not correctly reflect the contexts’ semantics, leading to irrelevant and incoherent generated responses. To resolve this problem, we propose Self-separated Conditional Variational AutoEncoder (abbreviated as SepaCVAE) that introduces group information to regularize the latent variables, which enhances CVAE by improving the responses’ relevance and coherence while maintaining their diversity and informativeness. SepaCVAE actively divides the input data into groups, and then widens the absolute difference between data pairs from distinct groups, while narrowing the relative distance between data pairs in the same group. Empirical results from automatic evaluation and detailed analysis demonstrate that SepaCVAE can significantly boost responses in well-established open-domain dialogue datasets."
    }
  },
  {
    "id": "abstract-2020--acl-main--369",
    "result": [
      {
        "value": {
          "start": 333,
          "end": 340,
          "text": "CluBERT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--369:E0"
      },
      {
        "value": {
          "start": 777,
          "end": 788,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--369:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--369:E0",
        "to_id": "abstract-2020--acl-main--369:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Knowing the Most Frequent Sense (MFS) of a word has been proved to help Word Sense Disambiguation (WSD) models significantly. However, the scarcity of sense-annotated data makes it difficult to induce a reliable and high-coverage distribution of the meanings in a language vocabulary. To address this issue, in this paper we present CluBERT, an automatic and multilingual approach for inducing the distributions of word senses from a corpus of raw sentences. Our experiments show that CluBERT learns distributions over English senses that are of higher quality than those extracted by alternative approaches. When used to induce the MFS of a lemma, CluBERT attains state-of-the-art results on the English Word Sense Disambiguation tasks and helps to improve the disambiguation performance of two off-the-shelf WSD models. Moreover, our distributions also prove to be effective in other languages, beating all their alternatives for computing the MFS on the multilingual WSD tasks. We release our sense distributions in five different languages at https://github.com/SapienzaNLP/clubert."
    }
  },
  {
    "id": "abstract-2020--acl-main--271",
    "result": [
      {
        "value": {
          "start": 193,
          "end": 255,
          "text": "replicates the effects of a model ensemble with a single model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--271:E0"
      },
      {
        "value": {
          "start": 565,
          "end": 575,
          "text": "parameters",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--271:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--271:E0",
        "to_id": "abstract-2020--acl-main--271:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Model ensemble techniques often increase task performance in neural networks; however, they require increased time, memory, and management effort. In this study, we propose a novel method that replicates the effects of a model ensemble with a single model. Our approach creates K-virtual models within a single parameter space using K-distinct pseudo-tags and K-distinct vectors. Experiments on text classification and sequence labeling tasks on several datasets demonstrate that our method emulates or outperforms a traditional model ensemble with 1/K-times fewer parameters."
    }
  },
  {
    "id": "abstract-2021--acl-long--360",
    "result": [
      {
        "value": {
          "start": 379,
          "end": 383,
          "text": "FEAE",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--360:E0"
      },
      {
        "value": {
          "start": 966,
          "end": 977,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--360:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--360:E0",
        "to_id": "abstract-2021--acl-long--360:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Implicit Event Argument Extraction seeks to identify arguments that play direct or implicit roles in a given event. However, most prior works focus on capturing direct relations between arguments and the event trigger. The lack of reasoning ability brings many challenges to the extraction of implicit arguments. In this work, we present a Frame-aware Event Argument Extraction (FEAE) learning framework to tackle this issue through reasoning in event frame-level scope. The proposed method leverages related arguments of the expected one as clues to guide the reasoning process. To bridge the gap between oracle knowledge used in the training phase and the imperfect related arguments in the test stage, we further introduce a curriculum knowledge distillation strategy to drive a final model that could operate without extra inputs through mimicking the behavior of a well-informed teacher model. Experimental results demonstrate FEAE obtains new state-of-the-art performance on the RAMS dataset."
    }
  },
  {
    "id": "abstract-2021--acl-long--308",
    "result": [
      {
        "value": {
          "start": 415,
          "end": 473,
          "text": "plug a cross-attention module into the Transformer encoder",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--308:E0"
      },
      {
        "value": {
          "start": 1355,
          "end": 1359,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--308:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--308:E0",
        "to_id": "abstract-2021--acl-long--308:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Existing work in multilingual pretraining has demonstrated the potential of cross-lingual transferability by training a unified Transformer encoder for multiple languages. However, much of this work only relies on the shared vocabulary and bilingual contexts to encourage the correlation across languages, which is loose and implicit for aligning the contextual representations between languages. In this paper, we plug a cross-attention module into the Transformer encoder to explicitly build the interdependence between languages. It can effectively avoid the degeneration of predicting masked words only conditioned on the context in its own language. More importantly, when fine-tuning on downstream tasks, the cross-attention module can be plugged in or out on-demand, thus naturally benefiting a wider range of cross-lingual tasks, from language understanding to generation. As a result, the proposed cross-lingual model delivers new state-of-the-art results on various cross-lingual understanding tasks of the XTREME benchmark, covering text classification, sequence labeling, question answering, and sentence retrieval. For cross-lingual generation tasks, it also outperforms all existing cross-lingual models and state-of-the-art Transformer variants on WMT14 English-to-German and English-to-French translation datasets, with gains of up to 1 2 BLEU."
    }
  },
  {
    "id": "P10-1051",
    "result": [
      {
        "value": {
          "start": 93,
          "end": 170,
          "text": "grammar-informed tag transitions and models minimized via integer programming",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1051:E0"
      },
      {
        "value": {
          "start": 386,
          "end": 391,
          "text": "error",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1051:E1"
      },
      {
        "from_id": "P10-1051:E0",
        "to_id": "P10-1051:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "We combine two complementary ideas for learning supertaggers from highly ambiguous lexicons: grammar-informed tag transitions and models minimized via integer programming. Each strategy on its own greatly improves performance over basic expectation-maximization training with a bitag Hidden Markov Model, which we show on the CCGbank and CCG-TUT corpora. The strategies provide further error reductions when combined. We describe a new two-stage integer programming strategy that efficiently deals with the high degree of ambiguity on these datasets while obtaining the full effect of model minimization."
    }
  },
  {
    "id": "abstract-2021--acl-long--200",
    "result": [
      {
        "value": {
          "start": 273,
          "end": 289,
          "text": "context-aware ST",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--200:E0"
      },
      {
        "value": {
          "start": 725,
          "end": 744,
          "text": "translation quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--200:E1"
      },
      {
        "value": {
          "start": 757,
          "end": 761,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--200:E2"
      },
      {
        "value": {
          "start": 821,
          "end": 831,
          "text": "robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--200:E3"
      },
      {
        "value": {
          "start": 867,
          "end": 873,
          "text": "errors",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--200:E4"
      },
      {
        "value": {
          "start": 887,
          "end": 894,
          "text": "latency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--200:E5"
      },
      {
        "from_id": "abstract-2021--acl-long--200:E0",
        "to_id": "abstract-2021--acl-long--200:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--200:E0",
        "to_id": "abstract-2021--acl-long--200:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--200:E0",
        "to_id": "abstract-2021--acl-long--200:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--200:E0",
        "to_id": "abstract-2021--acl-long--200:E4",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--200:E0",
        "to_id": "abstract-2021--acl-long--200:E5",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Document-level contextual information has shown benefits to text-based machine translation, but whether and how context helps end-to-end (E2E) speech translation (ST) is still under-studied. We fill this gap through extensive experiments using a simple concatenation-based context-aware ST model, paired with adaptive feature selection on speech encodings for computational efficiency. We investigate several decoding approaches, and introduce in-model ensemble decoding which jointly performs document- and sentence-level translation using the same model. Our results on the MuST-C benchmark with Transformer demonstrate the effectiveness of context to E2E ST. Compared to sentence-level ST, context-aware ST obtains better translation quality (+0.18-2.61 BLEU), improves pronoun and homophone translation, shows better robustness to (artificial) audio segmentation errors, and reduces latency and flicker to deliver higher quality for simultaneous translation."
    }
  },
  {
    "id": "abstract-2021--acl-long--326",
    "result": [
      {
        "value": {
          "start": 416,
          "end": 423,
          "text": "TrigNet",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--326:E0"
      },
      {
        "value": {
          "start": 1252,
          "end": 1262,
          "text": "average F1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--326:E1"
      },
      {
        "value": {
          "start": 1278,
          "end": 1286,
          "text": "flow GAT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--326:E2"
      },
      {
        "value": {
          "start": 1299,
          "end": 1304,
          "text": "FLOPS",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--326:E3"
      },
      {
        "value": {
          "start": 1309,
          "end": 1315,
          "text": "Memory",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--326:E4"
      },
      {
        "from_id": "abstract-2021--acl-long--326:E0",
        "to_id": "abstract-2021--acl-long--326:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--326:E2",
        "to_id": "abstract-2021--acl-long--326:E3",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--326:E2",
        "to_id": "abstract-2021--acl-long--326:E4",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Most of the recent work on personality detection from online posts adopts multifarious deep neural networks to represent the posts and builds predictive models in a data-driven manner, without the exploitation of psycholinguistic knowledge that may unveil the connections between one’s language use and his psychological traits. In this paper, we propose a psycholinguistic knowledge-based tripartite graph network, TrigNet, which consists of a tripartite graph network and a BERT-based graph initializer. The graph network injects structural psycholinguistic knowledge in LIWC, a computerized instrument for psycholinguistic analysis, by constructing a heterogeneous tripartite graph. The initializer is employed to provide initial embeddings for the graph nodes. To reduce the computational cost in graph learning, we further propose a novel flow graph attention network (GAT) that only transmits messages between neighboring parties in the tripartite graph. Benefiting from the tripartite graph, TrigNet can aggregate post information from a psychological perspective, which is a novel way of exploiting domain knowledge. Extensive experiments on two datasets show that TrigNet outperforms the existing state-of-art model by 3.47 and 2.10 points in average F1. Moreover, the flow GAT reduces the FLOPS and Memory measures by 38% and 32%, respectively, in comparison to the original GAT in our setting."
    }
  },
  {
    "id": "abstract-2020--acl-main--103",
    "result": [
      {
        "value": {
          "start": 939,
          "end": 977,
          "text": "soft and the hard exclusion mechanisms",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--103:E0"
      },
      {
        "value": {
          "start": 1060,
          "end": 1069,
          "text": "diversity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--103:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--103:E0",
        "to_id": "abstract-2020--acl-main--103:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Keyphrase generation (KG) aims to summarize the main ideas of a document into a set of keyphrases. A new setting is recently introduced into this problem, in which, given a document, the model needs to predict a set of keyphrases and simultaneously determine the appropriate number of keyphrases to produce. Previous work in this setting employs a sequential decoding process to generate keyphrases. However, such a decoding method ignores the intrinsic hierarchical compositionality existing in the keyphrase set of a document. Moreover, previous work tends to generate duplicated keyphrases, which wastes time and computing resources. To overcome these limitations, we propose an exclusive hierarchical decoding framework that includes a hierarchical decoding process and either a soft or a hard exclusion mechanism. The hierarchical decoding process is to explicitly model the hierarchical compositionality of a keyphrase set. Both the soft and the hard exclusion mechanisms keep track of previously-predicted keyphrases within a window size to enhance the diversity of the generated keyphrases. Extensive experiments on multiple KG benchmark datasets demonstrate the effectiveness of our method to generate less duplicated and more accurate keyphrases."
    }
  },
  {
    "id": "abstract-2020--acl-main--769",
    "result": [
      {
        "value": {
          "start": 1032,
          "end": 1049,
          "text": "debiasing methods",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--769:E0"
      },
      {
        "value": {
          "start": 946,
          "end": 956,
          "text": "robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--769:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--769:E0",
        "to_id": "abstract-2020--acl-main--769:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Several recent studies have shown that strong natural language understanding (NLU) models are prone to relying on unwanted dataset biases without learning the underlying task, resulting in models that fail to generalize to out-of-domain datasets and are likely to perform poorly in real-world scenarios. We propose two learning strategies to train neural models, which are more robust to such biases and transfer better to out-of-domain datasets. The biases are specified in terms of one or more bias-only models, which learn to leverage the dataset biases. During training, the bias-only models’ predictions are used to adjust the loss of the base model to reduce its reliance on biases by down-weighting the biased examples and focusing the training on the hard examples. We experiment on large-scale natural language inference and fact verification benchmarks, evaluating on out-of-domain datasets that are specifically designed to assess the robustness of models against known biases in the training data. Results show that our debiasing methods greatly improve robustness in all settings and better transfer to other textual entailment datasets. Our code and data are publicly available in  https://github.com/rabeehk/robust-nli ."
    }
  },
  {
    "id": "abstract-2020--acl-main--190",
    "result": [
      {
        "value": {
          "start": 483,
          "end": 490,
          "text": "ExpBERT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--190:E0"
      },
      {
        "value": {
          "start": 536,
          "end": 548,
          "text": "labeled data",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--190:E1"
      },
      {
        "value": {
          "start": 586,
          "end": 595,
          "text": "F1 points",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--190:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--190:E0",
        "to_id": "abstract-2020--acl-main--190:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--190:E0",
        "to_id": "abstract-2020--acl-main--190:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Suppose we want to specify the inductive bias that married couples typically go on honeymoons for the task of extracting pairs of spouses from text. In this paper, we allow model developers to specify these types of inductive biases as natural language explanations. We use BERT fine-tuned on MultiNLI to “interpret” these explanations with respect to the input sentence, producing explanation-guided representations of the input. Across three relation extraction tasks, our method, ExpBERT, matches a BERT baseline but with 3–20x less labeled data and improves on the baseline by 3–10 F1 points with the same amount of labeled data."
    }
  },
  {
    "id": "abstract-2020--acl-main--283",
    "result": [
      {
        "value": {
          "start": 459,
          "end": 468,
          "text": "HyperCaps",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--283:E0"
      },
      {
        "value": {
          "start": 239,
          "end": 250,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--283:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--283:E0",
        "to_id": "abstract-2020--acl-main--283:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Although deep neural networks are effective at extracting high-level features, classification methods usually encode an input into a vector representation via simple feature aggregation operations (e.g. pooling). Such operations limit the performance. For instance, a multi-label document may contain several concepts. In this case, one vector can not sufficiently capture its salient and discriminative content. Thus, we propose Hyperbolic Capsule Networks (HyperCaps) for Multi-Label Classification (MLC), which have two merits. First, hyperbolic capsules are designed to capture fine-grained document information for each label, which has the ability to characterize complicated structures among labels and documents. Second, Hyperbolic Dynamic Routing (HDR) is introduced to aggregate hyperbolic capsules in a label-aware manner, so that the label-level discriminative information can be preserved along the depth of neural networks. To efficiently handle large-scale MLC datasets, we additionally present a new routing method to adaptively adjust the capsule number during routing. Extensive experiments are conducted on four benchmark datasets. Compared with the state-of-the-art methods, HyperCaps significantly improves the performance of MLC especially on tail labels."
    }
  },
  {
    "id": "abstract-2021--acl-long--256",
    "result": [
      {
        "value": {
          "start": 703,
          "end": 727,
          "text": "using retrieved evidence",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--256:E0"
      },
      {
        "value": {
          "start": 935,
          "end": 945,
          "text": "SARI score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--256:E1"
      },
      {
        "value": {
          "start": 671,
          "end": 698,
          "text": "based on the T5 transformer",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--256:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--256:E0",
        "to_id": "abstract-2021--acl-long--256:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--256:E2",
        "to_id": "abstract-2021--acl-long--256:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This paper introduces the task of factual error correction: performing edits to a claim so that the generated rewrite is better supported by evidence. This extends the well-studied task of fact verification by providing a mechanism to correct written texts that are refuted or only partially supported by evidence. We demonstrate that it is feasible to train factual error correction systems from existing fact checking datasets which only contain labeled claims accompanied by evidence, but not the correction. We achieve this by employing a two-stage distant supervision approach that incorporates evidence into masked claims when generating corrections. Our approach, based on the T5 transformer and using retrieved evidence, achieved better results than existing work which used a pointer copy network and gold evidence, producing accurate factual error corrections for 5x more instances in human evaluation and a .125 increase in SARI score. The evaluation is conducted on a dataset of 65,000 instances based on a recent fact verification shared task and we release it to enable further work on the task."
    }
  },
  {
    "id": "abstract-2021--acl-long--432",
    "result": [
      {
        "value": {
          "start": 853,
          "end": 903,
          "text": "unsupervised and supervised crowdsourcing learning",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--432:E0"
      },
      {
        "value": {
          "start": 1113,
          "end": 1124,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--432:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--432:E0",
        "to_id": "abstract-2021--acl-long--432:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Crowdsourcing is regarded as one prospective solution for effective supervised learning, aiming to build large-scale annotated training data by crowd workers. Previous studies focus on reducing the influences from the noises of the crowdsourced annotations for supervised models. We take a different point in this work, regarding all crowdsourced annotations as gold-standard with respect to the individual annotators. In this way, we find that crowdsourcing could be highly similar to domain adaptation, and then the recent advances of cross-domain methods can be almost directly applied to crowdsourcing. Here we take named entity recognition (NER) as a study case, suggesting an annotator-aware representation learning model that inspired by the domain adaptation methods which attempt to capture effective domain-aware features. We investigate both unsupervised and supervised crowdsourcing learning, assuming that no or only small-scale expert annotations are available. Experimental results on a benchmark crowdsourced NER dataset show that our method is highly effective, leading to a new state-of-the-art performance. In addition, under the supervised setting, we can achieve impressive performance gains with only a very small scale of expert annotations."
    }
  },
  {
    "id": "P10-1008",
    "result": [
      {
        "value": {
          "start": 433,
          "end": 470,
          "text": "reinforcement learning (RL) framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1008:E0"
      },
      {
        "value": {
          "start": 1072,
          "end": 1085,
          "text": "dialogue time",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1008:E1"
      },
      {
        "value": {
          "start": 1011,
          "end": 1030,
          "text": "adaptation accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1008:E2"
      },
      {
        "from_id": "P10-1008:E0",
        "to_id": "P10-1008:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "P10-1008:E0",
        "to_id": "P10-1008:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We present a data-driven approach to learn user-adaptive referring expression generation (REG) policies for spoken dialogue systems. Referring expressions can be difficult to understand in technical domains where users may not know the technical 'jargon' names of the domain entities. In such cases, dialogue systems must be able to model the user's (lexical) domain knowledge and use appropriate referring expressions. We present a reinforcement learning (RL) framework in which the system learns REG policies which can adapt to unknown users online. Furthermore, unlike supervised learning methods which require a large corpus of expert adaptive behaviour to train on, we show that effective adaptive policies can be learned from a small dialogue corpus of non-adaptive human-machine interaction, by using a RL framework and a statistical user simulation. We show that in comparison to adaptive hand-coded baseline policies, the learned policy performs significantly better, with an 18.6% average increase in adaptation accuracy. The best learned policy also takes less dialogue time (average 1.07 min less) than the best hand-coded policy. This is because the learned policies can adapt online to changing evidence about the user's domain expertise."
    }
  },
  {
    "id": "abstract-2021--acl-long--157",
    "result": [
      {
        "value": {
          "start": 959,
          "end": 972,
          "text": "LoopCAG model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--157:E0"
      },
      {
        "value": {
          "start": 1078,
          "end": 1089,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--157:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--157:E0",
        "to_id": "abstract-2021--acl-long--157:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Generating image captions with user intention is an emerging need. The recently published Localized Narratives dataset takes mouse traces as another input to the image captioning task, which is an intuitive and efficient way for a user to control what to describe in the image. However, how to effectively employ traces to improve generation quality and controllability is still under exploration. This paper aims to solve this problem by proposing a novel model called LoopCAG, which connects Contrastive constraints and Attention Guidance in a Loop manner, engaged explicit spatial and temporal constraints to the generating process. Precisely, each generated sentence is temporally aligned to the corresponding trace sequence through a contrastive learning strategy. Besides, each generated text token is supervised to attend to the correct visual objects under heuristic spatial attention guidance. Comprehensive experimental results demonstrate that our LoopCAG model learns better correspondence among the three modalities (vision, language, and traces) and achieves SOTA performance on trace-controlled image captioning task. Moreover, the controllability and explainability of LoopCAG are validated by analyzing spatial and temporal sensitivity during the generation process."
    }
  },
  {
    "id": "abstract-2020--acl-main--402",
    "result": [
      {
        "value": {
          "start": 1140,
          "end": 1153,
          "text": "multi-tasking",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--402:E0"
      },
      {
        "value": {
          "start": 1169,
          "end": 1180,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--402:E1"
      },
      {
        "value": {
          "start": 626,
          "end": 640,
          "text": "multi-modality",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--402:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--402:E0",
        "to_id": "abstract-2020--acl-main--402:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--402:E2",
        "to_id": "abstract-2020--acl-main--402:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The task of Dialogue Act Classification (DAC) that purports to capture communicative intent has been studied extensively. But these studies limit themselves to text. Non-verbal features (change of tone, facial expressions etc.) can provide cues to identify DAs, thus stressing the benefit of incorporating multi-modal inputs in the task. Also, the emotional state of the speaker has a substantial effect on the choice of the dialogue act, since conversations are often influenced by emotions. Hence, the effect of emotion too on automatic identification of DAs needs to be studied. In this work, we address the role of  both  multi-modality and emotion recognition (ER) in DAC. DAC and ER help each other by way of multi-task learning. One of the major contributions of this work is a new dataset- multimodal Emotion aware Dialogue Act dataset called EMOTyDA, collected from open-sourced dialogue datasets. To demonstrate the utility of EMOTyDA, we build an attention based (self, inter-modal, inter-task) multi-modal, multi-task Deep Neural Network (DNN) for joint learning of DAs and emotions. We show empirically that multi-modality and multi-tasking achieve better performance of DAC compared to uni-modal and single task DAC variants."
    }
  },
  {
    "id": "abstract-2020--acl-main--630",
    "result": [
      {
        "value": {
          "start": 360,
          "end": 398,
          "text": "topic-informed BERT-based architecture",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--630:E0"
      },
      {
        "value": {
          "start": 475,
          "end": 486,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--630:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--630:E0",
        "to_id": "abstract-2020--acl-main--630:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Semantic similarity detection is a fundamental task in natural language understanding. Adding topic information has been useful for previous feature-engineered semantic similarity models as well as neural models for other tasks. There is currently no standard way of combining topics with pretrained contextual representations such as BERT. We propose a novel topic-informed BERT-based architecture for pairwise semantic similarity detection and show that our model improves performance over strong neural baselines across a variety of English language datasets. We find that the addition of topics to BERT helps particularly with resolving domain-specific cases."
    }
  },
  {
    "id": "P10-1060",
    "result": [
      {
        "value": {
          "start": 457,
          "end": 475,
          "text": "generated lexicons",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1060:E0"
      },
      {
        "value": {
          "start": 710,
          "end": 721,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1060:E1"
      },
      {
        "from_id": "P10-1060:E0",
        "to_id": "P10-1060:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We present a method for automatically generating focused and accurate topic-specific subjectivity lexicons from a general purpose polarity lexicon that allow users to pin-point subjective on-topic information in a set of relevant documents. We motivate the need for such lexicons in the field of media analysis, describe a bootstrapping method for generating a topic-specific lexicon from a general purpose polarity lexicon, and evaluate the quality of the generated lexicons both manually and using a TREC Blog track test set for opinionated blog post retrieval. Although the generated lexicons can be an order of magnitude more selective than the general purpose lexicon, they maintain, or even improve, the performance of an opinion retrieval system."
    }
  },
  {
    "id": "abstract-2020--acl-main--767",
    "result": [
      {
        "value": {
          "start": 452,
          "end": 538,
          "text": "leveraging recent advances in neural text generation and sequence-to-sequence learning",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--767:E0"
      },
      {
        "value": {
          "start": 550,
          "end": 554,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--767:E1"
      },
      {
        "value": {
          "start": 559,
          "end": 571,
          "text": "ROUGE scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--767:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--767:E0",
        "to_id": "abstract-2020--acl-main--767:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--767:E0",
        "to_id": "abstract-2020--acl-main--767:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Intelligent features in email service applications aim to increase productivity by helping people organize their folders, compose their emails and respond to pending tasks. In this work, we explore a new application, Smart-To-Do, that helps users with task management over emails. We introduce a new task and dataset for automatically generating To-Do items from emails where the sender has promised to perform an action. We design a two-stage process leveraging recent advances in neural text generation and sequence-to-sequence learning, obtaining BLEU and ROUGE scores of 0.23 and 0.63 for this task. To the best of our knowledge, this is the first work to address the problem of composing To-Do items from emails."
    }
  },
  {
    "id": "abstract-2020--acl-main--66",
    "result": [
      {
        "value": {
          "start": 693,
          "end": 708,
          "text": "loss truncation",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--66:E0"
      },
      {
        "value": {
          "start": 466,
          "end": 484,
          "text": "distinguishability",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--66:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--66:E0",
        "to_id": "abstract-2020--acl-main--66:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Neural language models are usually trained to match the distributional properties of large-scale corpora by minimizing the log loss. While straightforward to optimize, this approach forces the model to reproduce all variations in the dataset, including noisy and invalid references (e.g., misannotations and hallucinated facts). Even a small fraction of noisy data can degrade the performance of log loss. As an alternative, prior work has shown that minimizing the distinguishability of generated samples is a principled and robust loss that can handle invalid references. However, distinguishability has not been used in practice due to challenges in optimization and estimation. We propose loss truncation: a simple and scalable procedure which adaptively removes high log loss examples as a way to optimize for distinguishability. Empirically, we demonstrate that loss truncation outperforms existing baselines on distinguishability on a summarization task. Furthermore, we show that samples generated by the loss truncation model have factual accuracy ratings that exceed those of baselines and match human references."
    }
  },
  {
    "id": "abstract-2021--acl-long--436",
    "result": [
      {
        "value": {
          "start": 301,
          "end": 308,
          "text": "HMCEval",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--436:E0"
      },
      {
        "value": {
          "start": 1078,
          "end": 1086,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--436:E1"
      },
      {
        "value": {
          "start": 1104,
          "end": 1123,
          "text": "human effort spared",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--436:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--436:E0",
        "to_id": "abstract-2021--acl-long--436:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--436:E0",
        "to_id": "abstract-2021--acl-long--436:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Conversational dialogue systems (CDSs) are hard to evaluate due to the complexity of natural language. Automatic evaluation of dialogues often shows insufficient correlation with human judgements. Human evaluation is reliable but labor-intensive. We introduce a human-machine collaborative framework, HMCEval, that can guarantee reliability of the evaluation outcomes with reduced human effort. HMCEval casts dialogue evaluation as a sample assignment problem, where we need to decide to assign a sample to a human or a machine for evaluation. HMCEval includes a model confidence estimation module to estimate the confidence of the predicted sample assignment, and a human effort estimation module to estimate the human effort should the sample be assigned to human evaluation, as well as a sample assignment execution module that finds the optimum assignment solution based on the estimated confidence and effort. We assess the performance of HMCEval on the task of evaluating malevolence in dialogues. The experimental results show that HMCEval achieves around 99% evaluation accuracy with half of the human effort spared, showing that HMCEval provides reliable evaluation outcomes while reducing human effort by a large amount."
    }
  },
  {
    "id": "P11-1085",
    "result": [
      {
        "value": {
          "start": 197,
          "end": 273,
          "text": "transform the source tree phrasal structure into a set of simpler structures",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1085:E0"
      },
      {
        "value": {
          "start": 833,
          "end": 837,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1085:E1"
      },
      {
        "value": {
          "start": 665,
          "end": 736,
          "text": "The syntax-based translation system integrating the proposed techniques",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1085:E2"
      },
      {
        "from_id": "P11-1085:E0",
        "to_id": "P11-1085:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P11-1085:E2",
        "to_id": "P11-1085:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We propose a novel technique of learning how to transform the source parse trees to improve the translation qualities of syntax-based translation models using synchronous context-free grammars. We transform the source tree phrasal structure into a set of simpler structures, expose such decisions to the decoding process, and find the least expensive transformation operation to better model word reordering. In particular, we integrate synchronous binarizations, verb regrouping, removal of redundant parse nodes, and incorporate a few important features such as translation boundaries. We learn the structural preferences from the data in a generative framework. The syntax-based translation system integrating the proposed techniques outperforms the best Arabic-English unconstrained system in NIST-08 evaluations by 1.3 absolute BLEU, which is statistically significant."
    }
  },
  {
    "id": "abstract-2021--acl-long--271",
    "result": [
      {
        "value": {
          "start": 631,
          "end": 688,
          "text": "generative triple-wise model with hierarchical variations",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--271:E0"
      },
      {
        "value": {
          "start": 1031,
          "end": 1038,
          "text": "fluency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--271:E1"
      },
      {
        "value": {
          "start": 464,
          "end": 473,
          "text": "coherence",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--271:E2"
      },
      {
        "value": {
          "start": 1054,
          "end": 1063,
          "text": "diversity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--271:E3"
      },
      {
        "from_id": "abstract-2021--acl-long--271:E0",
        "to_id": "abstract-2021--acl-long--271:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--271:E0",
        "to_id": "abstract-2021--acl-long--271:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--271:E0",
        "to_id": "abstract-2021--acl-long--271:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Generating some appealing questions in open-domain conversations is an effective way to improve human-machine interactions and lead the topic to a broader or deeper direction. To avoid dull or deviated questions, some researchers tried to utilize answer, the “future” information, to guide question generation. However, they separate a post-question-answer (PQA) triple into two parts: post-question (PQ) and question-answer (QA) pairs, which may hurt the overall coherence. Besides, the QA relationship is modeled as a one-to-one mapping that is not reasonable in open-domain conversations. To tackle these problems, we propose a generative triple-wise model with hierarchical variations for open-domain conversational question generation (CQG). Latent variables in three hierarchies are used to represent the shared background of a triple and one-to-many semantic mappings in both PQ and QA pairs. Experimental results on a large-scale CQG dataset show that our method significantly improves the quality of questions in terms of fluency, coherence and diversity over competitive baselines."
    }
  },
  {
    "id": "abstract-2020--acl-main--164",
    "result": [
      {
        "value": {
          "start": 784,
          "end": 805,
          "text": "longer excerpt length",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--164:E0"
      },
      {
        "value": {
          "start": 759,
          "end": 770,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--164:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--164:E0",
        "to_id": "abstract-2020--acl-main--164:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recent advancements in neural language modelling make it possible to rapidly generate vast amounts of human-sounding text. The capabilities of humans and automatic discriminators to detect machine-generated text have been a large source of research interest, but humans and machines rely on different cues to make their decisions. Here, we perform careful benchmarking and analysis of three popular sampling-based decoding strategies—top-_k_, nucleus sampling, and untruncated random sampling—and show that improvements in decoding methods have primarily optimized for fooling humans. This comes at the expense of introducing statistical abnormalities that make detection easy for automatic systems. We also show that though both human and automatic detector performance improve with longer excerpt length, even multi-sentence excerpts can fool expert human raters over 30% of the time. Our findings reveal the importance of using both human and automatic detectors to assess the humanness of text generation systems."
    }
  },
  {
    "id": "P10-1110",
    "result": [
      {
        "value": {
          "start": 373,
          "end": 424,
          "text": "merging \"equivalent\" stacks based on feature values",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1110:E0"
      },
      {
        "value": {
          "start": 557,
          "end": 565,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1110:E1"
      },
      {
        "from_id": "P10-1110:E0",
        "to_id": "P10-1110:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Incremental parsing techniques such as shift-reduce have gained popularity thanks to their efficiency, but there remains a major problem: the search is greedy and only explores a tiny fraction of the whole space (even with beam search) as opposed to dynamic programming. We show that, surprisingly, dynamic programming is in fact possible for many shift-reduce parsers, by merging \"equivalent\" stacks based on feature values. Empirically, our algorithm yields up to a five-fold speedup over a state-of-the-art shift-reduce dependency parser with no loss in accuracy. Better search also leads to better learning, and our final parser outperforms all previously reported dependency parsers for English and Chinese, yet is much faster."
    }
  },
  {
    "id": "abstract-2020--acl-main--537",
    "result": [
      {
        "value": {
          "start": 330,
          "end": 338,
          "text": "FastBERT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--537:E0"
      },
      {
        "value": {
          "start": 316,
          "end": 321,
          "text": "speed",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--537:E1"
      },
      {
        "value": {
          "start": 512,
          "end": 555,
          "text": "adopts a unique self-distillation mechanism",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--537:E2"
      },
      {
        "value": {
          "start": 599,
          "end": 621,
          "text": "computational efficacy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--537:E3"
      },
      {
        "value": {
          "start": 284,
          "end": 295,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--537:E4"
      },
      {
        "from_id": "abstract-2020--acl-main--537:E0",
        "to_id": "abstract-2020--acl-main--537:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--537:E2",
        "to_id": "abstract-2020--acl-main--537:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--537:E2",
        "to_id": "abstract-2020--acl-main--537:E4",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Pre-trained language models like BERT have proven to be highly performant. However, they are often computationally expensive in many practical scenarios, for such heavy models can hardly be readily implemented with limited resources. To improve their efficiency with an assured model performance, we propose a novel speed-tunable FastBERT with adaptive inference time. The speed at inference can be flexibly adjusted under varying demands, while redundant calculation of samples is avoided. Moreover, this model adopts a unique self-distillation mechanism at fine-tuning, further enabling a greater computational efficacy with minimal loss in performance. Our model achieves promising results in twelve English and Chinese datasets. It is able to speed up by a wide range from 1 to 12 times than BERT if given different speedup thresholds to make a speed-performance tradeoff."
    }
  },
  {
    "id": "abstract-2021--acl-long--45",
    "result": [
      {
        "value": {
          "start": 661,
          "end": 665,
          "text": "CODA",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--45:E0"
      },
      {
        "value": {
          "start": 885,
          "end": 895,
          "text": "perplexity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--45:E1"
      },
      {
        "value": {
          "start": 945,
          "end": 949,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--45:E2"
      },
      {
        "value": {
          "start": 502,
          "end": 522,
          "text": "parameter efficiency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--45:E3"
      },
      {
        "from_id": "abstract-2021--acl-long--45:E0",
        "to_id": "abstract-2021--acl-long--45:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--45:E0",
        "to_id": "abstract-2021--acl-long--45:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--45:E0",
        "to_id": "abstract-2021--acl-long--45:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Transformers have advanced the field of natural language processing (NLP) on a variety of important tasks. At the cornerstone of the Transformer architecture is the multi-head attention (MHA) mechanism which models pairwise interactions between the elements of the sequence. Despite its massive success, the current framework ignores interactions among different heads, leading to the problem that many of the heads are redundant in practice, which greatly wastes the capacity of the model. To improve parameter efficiency, we re-formulate the MHA as a latent variable model from a probabilistic perspective. We present c ascaded head-c o lli d ing a ttention (CODA) which explicitly models the interactions between attention heads through a hierarchical variational distribution. We conduct extensive experiments and demonstrate that CODA outperforms the transformer baseline, by 0.6 perplexity on Wikitext-103 in language modeling, and by 0.6 BLEU on WMT14 EN-DE in machine translation, due to its improvements on the parameter efficiency."
    }
  },
  {
    "id": "abstract-2021--acl-long--415",
    "result": [
      {
        "value": {
          "start": 522,
          "end": 525,
          "text": "L2E",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--415:E0"
      },
      {
        "value": {
          "start": 1002,
          "end": 1014,
          "text": "faithfulness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--415:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--415:E0",
        "to_id": "abstract-2021--acl-long--415:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "The importance of explaining the outcome of a machine learning model, especially a black-box model, is widely acknowledged. Recent approaches explain an outcome by identifying the contributions of input features to this outcome. In environments involving large black-box models or complex inputs, this leads to computationally demanding algorithms. Further, these algorithms often suffer from low stability, with explanations varying significantly across similar examples. In this paper, we propose a Learning to Explain (L2E) approach that learns the behaviour of an underlying explanation algorithm simultaneously from all training examples. Once the explanation algorithm is distilled into an explainer network, it can be used to explain new instances. Our experiments on three classification tasks, which compare our approach to six explanation algorithms, show that L2E is between 5 and 7.5×10ˆ4 times faster than these algorithms, while generating more stable explanations, and having comparable faithfulness to the black-box model."
    }
  },
  {
    "id": "abstract-2020--acl-main--97",
    "result": [
      {
        "value": {
          "start": 499,
          "end": 503,
          "text": "DTCA",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--97:E0"
      },
      {
        "value": {
          "start": 1205,
          "end": 1216,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--97:E1"
      },
      {
        "value": {
          "start": 1231,
          "end": 1239,
          "text": "F1-score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--97:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--97:E0",
        "to_id": "abstract-2020--acl-main--97:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--97:E0",
        "to_id": "abstract-2020--acl-main--97:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recently, many methods discover effective evidence from reliable sources by appropriate neural networks for explainable claim verification, which has been widely recognized. However, in these methods, the discovery process of evidence is nontransparent and unexplained. Simultaneously, the discovered evidence is aimed at the interpretability of the whole sequence of claims but insufficient to focus on the false parts of claims. In this paper, we propose a Decision Tree-based Co-Attention model (DTCA) to discover evidence for explainable claim verification. Specifically, we first construct Decision Tree-based Evidence model (DTE) to select comments with high credibility as evidence in a transparent and interpretable way. Then we design Co-attention Self-attention networks (CaSa) to make the selected evidence interact with claims, which is for 1) training DTE to determine the optimal decision thresholds and obtain more powerful evidence; and 2) utilizing the evidence to find the false parts in the claim. Experiments on two public datasets, RumourEval and PHEME, demonstrate that DTCA not only provides explanations for the results of claim verification but also achieves the state-of-the-art performance, boosting the F1-score by more than 3.11%, 2.41%, respectively."
    }
  },
  {
    "id": "abstract-2021--acl-long--218",
    "result": [
      {
        "value": {
          "start": 831,
          "end": 863,
          "text": "move from unimodal to multimodal",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--218:E0"
      },
      {
        "value": {
          "start": 794,
          "end": 817,
          "text": "Progressive performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--218:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--218:E0",
        "to_id": "abstract-2021--acl-long--218:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In this paper, we aim to explore an uncharted territory, which is Chinese multimodal named entity recognition (NER) with both textual and acoustic contents. To achieve this, we construct a large-scale human-annotated Chinese multimodal NER dataset, named CNERTA. Our corpus totally contains 42,987 annotated sentences accompanying by 71 hours of speech data. Based on this dataset, we propose a family of strong and representative baseline models, which can leverage textual features or multimodal features. Upon these baselines, to capture the natural monotonic alignment between the textual modality and the acoustic modality, we further propose a simple multimodal multitask model by introducing a speech-to-text alignment auxiliary task. Through extensive experiments, we observe that: (1) Progressive performance boosts as we move from unimodal to multimodal, verifying the necessity of integrating speech clues into Chinese NER. (2) Our proposed model yields state-of-the-art (SoTA) results on CNERTA, demonstrating its effectiveness. For further research, the annotated dataset is publicly available at http://github.com/DianboWork/CNERTA ."
    }
  },
  {
    "id": "abstract-2020--acl-main--483",
    "result": [
      {
        "value": {
          "start": 337,
          "end": 399,
          "text": "extract post-hoc explanations from fine-tuned BERT classifiers",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--483:E0"
      },
      {
        "value": {
          "start": 768,
          "end": 779,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--483:E1"
      },
      {
        "value": {
          "start": 464,
          "end": 488,
          "text": "regularization technique",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--483:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--483:E0",
        "to_id": "abstract-2020--acl-main--483:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--483:E2",
        "to_id": "abstract-2020--acl-main--483:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Hate speech classifiers trained on imbalanced datasets struggle to determine if group identifiers like “gay” or “black” are used in offensive or prejudiced ways. Such biases manifest in false positives when these identifiers are present, due to models’ inability to learn the contexts which constitute a hateful usage of identifiers. We extract post-hoc explanations from fine-tuned BERT classifiers to detect bias towards identity terms. Then, we propose a novel regularization technique based on these explanations that encourages models to learn from the context of group identifiers in addition to the identifiers themselves. Our approach improved over baselines in limiting false positives on out-of-domain data while maintaining and in cases improving in-domain performance."
    }
  },
  {
    "id": "abstract-2020--acl-main--517",
    "result": [
      {
        "value": {
          "start": 922,
          "end": 932,
          "text": "our method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--517:E0"
      },
      {
        "value": {
          "start": 975,
          "end": 991,
          "text": "task consistency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--517:E1"
      },
      {
        "value": {
          "start": 993,
          "end": 1009,
          "text": "response quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--517:E2"
      },
      {
        "value": {
          "start": 1015,
          "end": 1024,
          "text": "diversity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--517:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--517:E0",
        "to_id": "abstract-2020--acl-main--517:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--517:E0",
        "to_id": "abstract-2020--acl-main--517:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--517:E0",
        "to_id": "abstract-2020--acl-main--517:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Training the generative models with minimal corpus is one of the critical challenges for building open-domain dialogue systems. Existing methods tend to use the meta-learning framework which pre-trains the parameters on all non-target tasks then fine-tunes on the target task. However, fine-tuning distinguishes tasks from the parameter perspective but ignores the model-structure perspective, resulting in similar dialogue models for different tasks. In this paper, we propose an algorithm that can customize a unique dialogue model for each task in the few-shot setting. In our approach, each dialogue model consists of a shared module, a gating module, and a private module. The first two modules are shared among all the tasks, while the third one will differentiate into different network structures to better capture the characteristics of the corresponding task. The extensive experiments on two datasets show that our method outperforms all the baselines in terms of task consistency, response quality, and diversity."
    }
  },
  {
    "id": "abstract-2020--acl-main--392",
    "result": [
      {
        "value": {
          "start": 892,
          "end": 922,
          "text": "neighborhood routing algorithm",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--392:E0"
      },
      {
        "value": {
          "start": 942,
          "end": 956,
          "text": "expressiveness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--392:E1"
      },
      {
        "value": {
          "start": 961,
          "end": 977,
          "text": "interpretability",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--392:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--392:E0",
        "to_id": "abstract-2020--acl-main--392:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--392:E0",
        "to_id": "abstract-2020--acl-main--392:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "With the explosion of news information, personalized news recommendation has become very important for users to quickly find their interested contents. Most existing methods usually learn the representations of users and news from news contents for recommendation. However, they seldom consider high-order connectivity underlying the user-news interactions. Moreover, existing methods failed to disentangle a user’s latent preference factors which cause her clicks on different news. In this paper, we model the user-news interactions as a bipartite graph and propose a novel Graph Neural News Recommendation model with Unsupervised Preference Disentanglement, named GNUD. Our model can encode high-order relationships into user and news representations by information propagation along the graph. Furthermore, the learned representations are disentangled with latent preference factors by a neighborhood routing algorithm, which can enhance expressiveness and interpretability. A preference regularizer is also designed to force each disentangled subspace to independently reflect an isolated preference, improving the quality of the disentangled representations. Experimental results on real-world news datasets demonstrate that our proposed model can effectively improve the performance of news recommendation and outperform state-of-the-art news recommendation methods."
    }
  },
  {
    "id": "abstract-2021--acl-long--5",
    "result": [
      {
        "value": {
          "start": 796,
          "end": 800,
          "text": "CICD",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--5:E0"
      },
      {
        "value": {
          "start": 680,
          "end": 691,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--5:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--5:E0",
        "to_id": "abstract-2021--acl-long--5:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recent studies constructing direct interactions between the claim and each single user response (a comment or a relevant article) to capture evidence have shown remarkable success in interpretable claim verification. Owing to different single responses convey different cognition of individual users (i.e., audiences), the captured evidence belongs to the perspective of individual cognition. However, individuals’ cognition of social things is not always able to truly reflect the objective. There may be one-sided or biased semantics in their opinions on a claim. The captured evidence correspondingly contains some unobjective and biased evidence fragments, deteriorating task performance. In this paper, we propose a Dual-view model based on the views of Collective and Individual Cognition (CICD) for interpretable claim verification. From the view of the collective cognition, we not only capture the word-level semantics based on individual users, but also focus on sentence-level semantics (i.e., the overall responses) among all users and adjust the proportion between them to generate global evidence. From the view of individual cognition, we select the top- k articles with high degree of difference and interact with the claim to explore the local key evidence fragments. To weaken the bias of individual cognition-view evidence, we devise inconsistent loss to suppress the divergence between global and local evidence for strengthening the consistent shared evidence between the both. Experiments on three benchmark datasets confirm that CICD achieves state-of-the-art performance."
    }
  },
  {
    "id": "abstract-2020--acl-main--575",
    "result": [
      {
        "value": {
          "start": 223,
          "end": 258,
          "text": "a method of instance-based learning",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--575:E0"
      },
      {
        "value": {
          "start": 623,
          "end": 639,
          "text": "interpretability",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--575:E1"
      },
      {
        "value": {
          "start": 660,
          "end": 671,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--575:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--575:E0",
        "to_id": "abstract-2020--acl-main--575:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--575:E0",
        "to_id": "abstract-2020--acl-main--575:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Interpretable rationales for model predictions play a critical role in practical applications. In this study, we develop models possessing interpretable inference process for structured prediction. Specifically, we present a method of instance-based learning that learns similarities between spans. At inference time, each span is assigned a class label based on its similar spans in the training set, where it is easy to understand how much each training instance contributes to the predictions. Through empirical analysis on named entity recognition, we demonstrate that our method enables to build models that have high interpretability without sacrificing performance."
    }
  },
  {
    "id": "abstract-2021--acl-long--171",
    "result": [
      {
        "value": {
          "start": 679,
          "end": 688,
          "text": "EarlyBERT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--171:E0"
      },
      {
        "value": {
          "start": 1213,
          "end": 1224,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--171:E1"
      },
      {
        "value": {
          "start": 217,
          "end": 230,
          "text": "training time",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--171:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--171:E0",
        "to_id": "abstract-2021--acl-long--171:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--171:E0",
        "to_id": "abstract-2021--acl-long--171:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Heavily overparameterized language models such as BERT, XLNet and T5 have achieved impressive success in many NLP tasks. However, their high model complexity requires enormous computation resources and extremely long training time for both pre-training and fine-tuning. Many works have studied model compression on large NLP models, but only focusing on reducing inference time while still requiring an expensive training process. Other works use extremely large batch sizes to shorten the pre-training time, at the expense of higher computational resource demands. In this paper, inspired by the Early-Bird Lottery Tickets recently studied for computer vision tasks, we propose EarlyBERT, a general computationally-efficient training algorithm applicable to both pre-training and fine-tuning of large-scale language models. By slimming the self-attention and fully-connected sub-layers inside a transformer, we are the first to identify structured winning tickets in the early stage of BERT training. We apply those tickets towards efficient BERT training, and conduct comprehensive pre-training and fine-tuning experiments on GLUE and SQuAD downstream tasks. Our results show that EarlyBERT achieves comparable performance to standard BERT, with 35 45% less training time. Code is available at https://github.com/VITA-Group/EarlyBERT."
    }
  },
  {
    "id": "P11-1099",
    "result": [
      {
        "value": {
          "start": 214,
          "end": 252,
          "text": "using features specific to each scheme",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1099:E0"
      },
      {
        "value": {
          "start": 265,
          "end": 275,
          "text": "accuracies",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1099:E1"
      },
      {
        "from_id": "P11-1099:E0",
        "to_id": "P11-1099:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Argumentation schemes are structures or templates for various kinds of arguments. Given the text of an argument with premises and conclusion identified, we classify it as an instance of one of five common schemes, using features specific to each scheme. We achieve accuracies of 63--91% in one-against-others classification and 80--94% in pairwise classification (baseline = 50% in both cases)."
    }
  },
  {
    "id": "abstract-2021--acl-long--180",
    "result": [
      {
        "value": {
          "start": 50,
          "end": 54,
          "text": "XDTS",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--180:E0"
      },
      {
        "value": {
          "start": 796,
          "end": 816,
          "text": "exact match accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--180:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--180:E0",
        "to_id": "abstract-2021--acl-long--180:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "The cross-database context-dependent Text-to-SQL (XDTS) problem has attracted considerable attention in recent years due to its wide range of potential applications. However, we identify two biases in existing datasets for XDTS: (1) a high proportion of context-independent questions and (2) a high proportion of easy SQL queries. These biases conceal the major challenges in XDTS to some extent. In this work, we present Chase, a large-scale and pragmatic Chinese dataset for XDTS. It consists of 5,459 coherent question sequences (17,940 questions with their SQL queries annotated) over 280 databases, in which only 35% of questions are context-independent, and 28% of SQL queries are easy. We experiment on Chase with three state-of-the-art XDTS approaches. The best approach only achieves an exact match accuracy of 40% over all questions and 16% over all question sequences, indicating that Chase highlights the challenging problems of XDTS. We believe that XDTS can provide fertile soil for addressing the problems."
    }
  },
  {
    "id": "abstract-2021--acl-long--339",
    "result": [
      {
        "value": {
          "start": 344,
          "end": 404,
          "text": "diluting sentence-level information in style representations",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--339:E0"
      },
      {
        "value": {
          "start": 557,
          "end": 589,
          "text": "BERT-based style intensity score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--339:E1"
      },
      {
        "value": {
          "start": 605,
          "end": 616,
          "text": "BLEU scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--339:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--339:E0",
        "to_id": "abstract-2021--acl-long--339:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--339:E0",
        "to_id": "abstract-2021--acl-long--339:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Generating open-domain conversational responses in the desired style usually suffers from the lack of parallel data in the style. Meanwhile, using monolingual stylistic data to increase style intensity often leads to the expense of decreasing content relevance. In this paper, we propose to disentangle the content and style in latent space by diluting sentence-level information in style representations. Combining the desired style representation and a response content representation will then obtain a stylistic response. Our approach achieves a higher BERT-based style intensity score and comparable BLEU scores, compared with baselines. Human evaluation results show that our approach significantly improves style intensity and maintains content relevance."
    }
  },
  {
    "id": "abstract-2021--acl-long--259",
    "result": [
      {
        "value": {
          "start": 887,
          "end": 892,
          "text": "T-DNA",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--259:E0"
      },
      {
        "value": {
          "start": 1266,
          "end": 1285,
          "text": "computational costs",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--259:E1"
      },
      {
        "value": {
          "start": 605,
          "end": 695,
          "text": "explicitly incorporating multi-granularity information of unseen and domain-specific words",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--259:E2"
      },
      {
        "value": {
          "start": 308,
          "end": 319,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--259:E3"
      },
      {
        "from_id": "abstract-2021--acl-long--259:E0",
        "to_id": "abstract-2021--acl-long--259:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--259:E2",
        "to_id": "abstract-2021--acl-long--259:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Large pre-trained models such as BERT are known to improve different downstream NLP tasks, even when such a model is trained on a generic domain. Moreover, recent studies have shown that when large domain-specific corpora are available, continued pre-training on domain-specific data can further improve the performance of in-domain tasks. However, this practice requires significant domain-specific data and computational resources which may not always be available. In this paper, we aim to adapt a generic pretrained model with a relatively small amount of domain-specific data. We demonstrate that by explicitly incorporating multi-granularity information of unseen and domain-specific words via the adaptation of (word based) n-grams, the performance of a generic pretrained model can be greatly improved. Specifically, we introduce a Transformer-based Domain-aware N-gram Adaptor, T-DNA, to effectively learn and incorporate the semantic representation of different combinations of words in the new domain. Experimental results illustrate the effectiveness of T-DNA on eight low-resource downstream tasks from four domains. We show that T-DNA is able to achieve significant improvements compared to existing methods on most tasks using limited data with lower computational costs. Moreover, further analyses demonstrate the importance and effectiveness of both unseen words and the information of different granularities. Our code is available at https://github.com/shizhediao/T-DNA."
    }
  },
  {
    "id": "P11-1070",
    "result": [
      {
        "value": {
          "start": 367,
          "end": 405,
          "text": "surface evidence of lexical affinities",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1070:E0"
      },
      {
        "value": {
          "start": 554,
          "end": 568,
          "text": "relative error",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1070:E1"
      },
      {
        "value": {
          "start": 417,
          "end": 438,
          "text": "paraphrase-based cues",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1070:E2"
      },
      {
        "from_id": "P11-1070:E0",
        "to_id": "P11-1070:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "P11-1070:E2",
        "to_id": "P11-1070:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Counts from large corpora (like the web) can be powerful syntactic cues. Past work has used web counts to help resolve isolated ambiguities, such as binary noun-verb PP attachments and noun compound bracketings. In this work, we first present a method for generating web count features that address the full range of syntactic attachments. These features encode both surface evidence of lexical affinities as well as paraphrase-based cues to syntactic structure. We then integrate our features into full-scale dependency and constituent parsers. We show relative error reductions of 7.0% over the second-order dependency parser of McDonald and Pereira (2006), 9.2% over the constituent parser of Petrov et al. (2006), and 3.4% over a non-local constituent reranker."
    }
  },
  {
    "id": "abstract-2020--acl-main--63",
    "result": [
      {
        "value": {
          "start": 636,
          "end": 675,
          "text": "introduces a general learning framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--63:E0"
      },
      {
        "value": {
          "start": 484,
          "end": 495,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--63:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--63:E0",
        "to_id": "abstract-2020--acl-main--63:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In modular dialogue systems, natural language understanding (NLU) and natural language generation (NLG) are two critical components, where NLU extracts the semantics from the given texts and NLG is to construct corresponding natural language sentences based on the input semantic representations. However, the dual property between understanding and generation has been rarely explored. The prior work is the first attempt that utilized the duality between NLU and NLG to improve the performance via a dual supervised learning framework. However, the prior work still learned both components in a supervised manner; instead, this paper introduces a general learning framework to effectively exploit such duality, providing flexibility of incorporating both supervised and unsupervised learning algorithms to train language understanding and generation models in a joint fashion. The benchmark experiments demonstrate that the proposed approach is capable of boosting the performance of both NLU and NLG. The source code is available at: https://github.com/MiuLab/DuaLUG."
    }
  },
  {
    "id": "P11-1125",
    "result": [
      {
        "value": {
          "start": 191,
          "end": 225,
          "text": "novel system combination framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1125:E0"
      },
      {
        "value": {
          "start": 822,
          "end": 833,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1125:E1"
      },
      {
        "from_id": "P11-1125:E0",
        "to_id": "P11-1125:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "The state-of-the-art system combination method for machine translation (MT) is based on confusion networks constructed by aligning hypotheses with regard to word similarities. We introduce a novel system combination framework in which hypotheses are encoded as a confusion forest, a packed forest representing alternative trees. The forest is generated using syntactic consensus among parsed hypotheses: First, MT outputs are parsed. Second, a context free grammar is learned by extracting a set of rules that constitute the parse trees. Third, a packed forest is generated starting from the root symbol of the extracted grammar through non-terminal rewriting. The new hypothesis is produced by searching the best derivation in the forest. Experimental results on the WMT10 system combination shared task yield comparable performance to the conventional confusion network based method with smaller space."
    }
  },
  {
    "id": "P11-1134",
    "result": [
      {
        "value": {
          "start": 24,
          "end": 90,
          "text": "use of bilingual parallel corpora as a source of lexical knowledge",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1134:E0"
      },
      {
        "value": {
          "start": 686,
          "end": 694,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1134:E1"
      },
      {
        "from_id": "P11-1134:E0",
        "to_id": "P11-1134:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "This paper explores the use of bilingual parallel corpora as a source of lexical knowledge for cross-lingual textual entailment. We claim that, in spite of the inherent difficulties of the task, phrase tables extracted from parallel data allow to capture both lexical relations between single words, and contextual information useful for inference. We experiment with a phrasal matching method in order to: i) build a system portable across languages, and ii) evaluate the contribution of lexical knowledge in isolation, without interaction with other inference mechanisms. Results achieved on an English-Spanish corpus obtained from the RTE3 dataset support our claim, with an overall accuracy above average scores reported by RTE participants on monolingual data. Finally, we show that using parallel corpora to extract paraphrase tables reveals their potential also in the monolingual setting, improving the results achieved with other sources of lexical knowledge."
    }
  },
  {
    "id": "abstract-2020--acl-main--42",
    "result": [
      {
        "value": {
          "start": 341,
          "end": 373,
          "text": "opportunistic decoding technique",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--42:E0"
      },
      {
        "value": {
          "start": 238,
          "end": 245,
          "text": "latency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--42:E1"
      },
      {
        "value": {
          "start": 814,
          "end": 818,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--42:E2"
      },
      {
        "value": {
          "start": 825,
          "end": 838,
          "text": "revision rate",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--42:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--42:E0",
        "to_id": "abstract-2020--acl-main--42:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--42:E0",
        "to_id": "abstract-2020--acl-main--42:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--42:E0",
        "to_id": "abstract-2020--acl-main--42:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Simultaneous translation has many important application scenarios and attracts much attention from both academia and industry recently. Most existing frameworks, however, have difficulties in balancing between the translation quality and latency, i.e., the decoding policy is usually either too aggressive or too conservative. We propose an opportunistic decoding technique with timely correction ability, which always (over-)generates a certain mount of extra words at each step to keep the audience on track with the latest information. At the same time, it also corrects, in a timely fashion, the mistakes in the former overgenerated words when observing more source context to ensure high translation quality. Experiments show our technique achieves substantial reduction in latency and up to +3.1 increase in BLEU, with revision rate under 8% in Chinese-to-English and English-to-Chinese translation."
    }
  },
  {
    "id": "abstract-2021--acl-long--133",
    "result": [
      {
        "value": {
          "start": 576,
          "end": 598,
          "text": "Our detection approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--133:E0"
      },
      {
        "value": {
          "start": 645,
          "end": 653,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--133:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--133:E0",
        "to_id": "abstract-2021--acl-long--133:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "To defend against machine-generated fake news, an effective mechanism is urgently needed. We contribute a novel benchmark for fake news detection at the knowledge element level, as well as a solution for this task which incorporates cross-media consistency checking to detect the fine-grained knowledge elements making news articles misinformative. Due to training data scarcity, we also formulate a novel data synthesis method by manipulating knowledge elements within the knowledge graph to generate noisy training data with specific, hard to detect, known inconsistencies. Our detection approach outperforms the state-of-the-art (up to 16.8% accuracy gain), and more critically, yields fine-grained explanations."
    }
  },
  {
    "id": "abstract-2021--acl-long--287",
    "result": [
      {
        "value": {
          "start": 11,
          "end": 20,
          "text": "NeuralWOZ",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--287:E0"
      },
      {
        "value": {
          "start": 879,
          "end": 898,
          "text": "joint goal accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--287:E1"
      },
      {
        "value": {
          "start": 970,
          "end": 978,
          "text": "coverage",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--287:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--287:E0",
        "to_id": "abstract-2021--acl-long--287:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--287:E0",
        "to_id": "abstract-2021--acl-long--287:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We propose NeuralWOZ, a novel dialogue collection framework that uses model-based dialogue simulation. NeuralWOZ has two pipelined models, Collector and Labeler. Collector generates dialogues from (1) user’s goal instructions, which are the user context and task constraints in natural language, and (2) system’s API call results, which is a list of possible query responses for user requests from the given knowledge base. Labeler annotates the generated dialogue by formulating the annotation as a multiple-choice problem, in which the candidate labels are extracted from goal instructions and API call results. We demonstrate the effectiveness of the proposed method in the zero-shot domain transfer learning for dialogue state tracking. In the evaluation, the synthetic dialogue corpus generated from NeuralWOZ achieves a new state-of-the-art with improvements of 4.4% point joint goal accuracy on average across domains, and improvements of 5.7% point of zero-shot coverage against the MultiWOZ 2.1 dataset."
    }
  },
  {
    "id": "abstract-2021--acl-long--428",
    "result": [
      {
        "value": {
          "start": 491,
          "end": 509,
          "text": "Adopting BanditMTL",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--428:E0"
      },
      {
        "value": {
          "start": 593,
          "end": 604,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--428:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--428:E0",
        "to_id": "abstract-2021--acl-long--428:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Task variance regularization, which can be used to improve the generalization of Multi-task Learning (MTL) models, remains unexplored in multi-task text classification. Accordingly, to fill this gap, this paper investigates how the task might be effectively regularized, and consequently proposes a multi-task learning method based on adversarial multi-armed bandit. The proposed method, named BanditMTL, regularizes the task variance by means of a mirror gradient ascent-descent algorithm. Adopting BanditMTL in the multi-task text classification context is found to achieve state-of-the-art performance. The results of extensive experiments back up our theoretical analysis and validate the superiority of our proposals."
    }
  },
  {
    "id": "abstract-2020--acl-main--552",
    "result": [
      {
        "value": {
          "start": 682,
          "end": 748,
          "text": "instantiating the framework with a simple form of a matching model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--552:E0"
      },
      {
        "value": {
          "start": 846,
          "end": 853,
          "text": "ROUGE-1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--552:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--552:E0",
        "to_id": "abstract-2020--acl-main--552:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This paper creates a paradigm shift with regard to the way we build neural extractive summarization systems. Instead of following the commonly used framework of extracting sentences individually and modeling the relationship between sentences, we formulate the extractive summarization task as a semantic text matching problem, in which a source document and candidate summaries will be (extracted from the original text) matched in a semantic space. Notably, this paradigm shift to semantic matching framework is well-grounded in our comprehensive analysis of the inherent gap between sentence-level and summary-level extractors based on the property of the dataset. Besides, even instantiating the framework with a simple form of a matching model, we have driven the state-of-the-art extractive result on CNN/DailyMail to a new level (44.41 in ROUGE-1). Experiments on the other five datasets also show the effectiveness of the matching framework. We believe the power of this matching-based summarization framework has not been fully exploited. To encourage more instantiations in the future, we have released our codes, processed dataset, as well as generated summaries in  https://github.com/maszhongming/MatchSum ."
    }
  },
  {
    "id": "abstract-2020--acl-main--577",
    "result": [
      {
        "value": {
          "start": 451,
          "end": 465,
          "text": "biaffine model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--577:E0"
      },
      {
        "value": {
          "start": 776,
          "end": 787,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--577:E1"
      },
      {
        "value": {
          "start": 809,
          "end": 817,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--577:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--577:E0",
        "to_id": "abstract-2020--acl-main--577:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--577:E0",
        "to_id": "abstract-2020--acl-main--577:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Named Entity Recognition (NER) is a fundamental task in Natural Language Processing, concerned with identifying spans of text expressing references to entities. NER research is often focused on flat entities only (flat NER), ignoring the fact that entity references can be nested, as in [Bank of [China]] (Finkel and Manning, 2009). In this paper, we use ideas from graph-based dependency parsing to provide our model a global view on the input via a biaffine model (Dozat and Manning, 2017). The biaffine model scores pairs of start and end tokens in a sentence which we use to explore all spans, so that the model is able to predict named entities accurately. We show that the model works well for both nested and flat NER through evaluation on 8 corpora and achieving SoTA performance on all of them, with accuracy gains of up to 2.2 percentage points."
    }
  },
  {
    "id": "abstract-2020--acl-main--530",
    "result": [
      {
        "value": {
          "start": 595,
          "end": 616,
          "text": "use of future context",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--530:E0"
      },
      {
        "value": {
          "start": 384,
          "end": 395,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--530:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--530:E0",
        "to_id": "abstract-2020--acl-main--530:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The advent of context-aware NMT has resulted in promising improvements in the overall translation quality and specifically in the translation of discourse phenomena such as pronouns. Previous works have mainly focused on the use of past sentences as context with a focus on anaphora translation. In this work, we investigate the effect of future sentences as context by comparing the performance of a contextual NMT model trained with the future context to the one trained with the past context. Our experiments and evaluation, using generic and pronoun-focused automatic metrics, show that the use of future context not only achieves significant improvements over the context-agnostic Transformer, but also demonstrates comparable and in some cases improved performance over its counterpart trained on past context. We also perform an evaluation on a targeted cataphora test suite and report significant gains over the context-agnostic Transformer in terms of BLEU."
    }
  },
  {
    "id": "P11-1054",
    "result": [
      {
        "value": {
          "start": 875,
          "end": 908,
          "text": "using domain-specific constraints",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1054:E0"
      },
      {
        "value": {
          "start": 929,
          "end": 940,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1054:E1"
      },
      {
        "from_id": "P11-1054:E0",
        "to_id": "P11-1054:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We present a novel approach to discovering relations and their instantiations from a collection of documents in a single domain. Our approach learns relation types by exploiting meta-constraints that characterize the general qualities of a good relation in any domain. These constraints state that instances of a single relation should exhibit regularities at multiple levels of linguistic structure, including lexicography, syntax, and document-level context. We capture these regularities via the structure of our probabilistic model as well as a set of declaratively-specified constraints enforced during posterior inference. Across two domains our approach successfully recovers hidden relation structure, comparable to or outperforming previous state-of-the-art approaches. Furthermore, we find that a small set of constraints is applicable across the domains, and that using domain-specific constraints can further improve performance."
    }
  },
  {
    "id": "abstract-2021--acl-long--568",
    "result": [
      {
        "value": {
          "start": 966,
          "end": 986,
          "text": "tune a RoBERTa model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--568:E0"
      },
      {
        "value": {
          "start": 1024,
          "end": 1035,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--568:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--568:E0",
        "to_id": "abstract-2021--acl-long--568:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Although pretrained language models can be fine-tuned to produce state-of-the-art results for a very wide range of language understanding tasks, the dynamics of this process are not well understood, especially in the low data regime. Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples? In this paper, we argue that analyzing fine-tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon. We empirically show that common pre-trained models have a very low intrinsic dimension; in other words, there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space. For example, by optimizing only 200 trainable parameters randomly projected back into the full space, we can tune a RoBERTa model to achieve 90% of the full parameter performance levels on MRPC. Furthermore, we empirically show that pre-training implicitly minimizes intrinsic dimension and, perhaps surprisingly, larger models tend to have lower intrinsic dimension after a fixed number of pre-training updates, at least in part explaining their extreme effectiveness. Lastly, we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count."
    }
  },
  {
    "id": "abstract-2021--acl-long--420",
    "result": [
      {
        "value": {
          "start": 1162,
          "end": 1201,
          "text": "global syntactic distances among tokens",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--420:E0"
      },
      {
        "value": {
          "start": 977,
          "end": 988,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--420:E1"
      },
      {
        "value": {
          "start": 585,
          "end": 596,
          "text": "Transformer",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--420:E2"
      },
      {
        "value": {
          "start": 977,
          "end": 988,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--420:E3"
      },
      {
        "from_id": "abstract-2021--acl-long--420:E0",
        "to_id": "abstract-2021--acl-long--420:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--420:E2",
        "to_id": "abstract-2021--acl-long--420:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We study the problem of leveraging the syntactic structure of text to enhance pre-trained models such as BERT and RoBERTa. Existing methods utilize syntax of text either in the pre-training stage or in the fine-tuning stage, so that they suffer from discrepancy between the two stages. Such a problem would lead to the necessity of having human-annotated syntactic information, which limits the application of existing methods to broader scenarios. To address this, we present a model that utilizes the syntax of text in both pre-training and fine-tuning stages. Our model is based on Transformer with a syntax-aware attention layer that considers the dependency tree of the text. We further introduce a new pre-training task of predicting the syntactic distance among tokens in the dependency tree. We evaluate the model on three downstream tasks, including relation classification, entity typing, and question answering. Results show that our model achieves state-of-the-art performance on six public benchmark datasets. We have two major findings. First, we demonstrate that infusing automatically produced syntax of text improves pre-trained models. Second, global syntactic distances among tokens bring larger performance gains compared to local head relations between contiguous tokens."
    }
  },
  {
    "id": "abstract-2020--acl-main--664",
    "result": [
      {
        "value": {
          "start": 221,
          "end": 260,
          "text": "explore semantics available in captions",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--664:E0"
      },
      {
        "value": {
          "start": 942,
          "end": 953,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--664:E1"
      },
      {
        "value": {
          "start": 181,
          "end": 210,
          "text": "image captioning architecture",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--664:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--664:E0",
        "to_id": "abstract-2020--acl-main--664:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--664:E2",
        "to_id": "abstract-2020--acl-main--664:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Image captioning is a multimodal problem that has drawn extensive attention in both the natural language processing and computer vision community. In this paper, we present a novel image captioning architecture to better explore semantics available in captions and leverage that to enhance both image representation and caption generation. Our models first construct caption-guided visual relationship graphs that introduce beneficial inductive bias using weakly supervised multi-instance learning. The representation is then enhanced with neighbouring and contextual nodes with their textual and visual features. During generation, the model further incorporates visual relationships using multi-task learning for jointly predicting word and object/predicate tag sequences. We perform extensive experiments on the MSCOCO dataset, showing that the proposed framework significantly outperforms the baselines, resulting in the state-of-the-art performance under a wide range of evaluation metrics. The code of our paper has been made publicly available."
    }
  },
  {
    "id": "abstract-2020--acl-main--230",
    "result": [
      {
        "value": {
          "start": 291,
          "end": 333,
          "text": "Weakly Aligned Structured Embedding (WASE)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--230:E0"
      },
      {
        "value": {
          "start": 735,
          "end": 742,
          "text": "F-score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--230:E1"
      },
      {
        "value": {
          "start": 735,
          "end": 742,
          "text": "F-score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--230:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--230:E0",
        "to_id": "abstract-2020--acl-main--230:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--230:E0",
        "to_id": "abstract-2020--acl-main--230:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We introduce a new task, MultiMedia Event Extraction, which aims to extract events and their arguments from multimedia documents. We develop the first benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events and arguments. We propose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space. The structures are aligned across modalities by employing a weakly supervised training strategy, which enables exploiting available resources without explicit cross-media annotation. Compared to uni-modal state-of-the-art methods, our approach achieves 4.0% and 9.8% absolute F-score gains on text event argument role labeling and visual event extraction. Compared to state-of-the-art multimedia unstructured representations, we achieve 8.3% and 5.0% absolute F-score gains on multimedia event extraction and argument role labeling, respectively. By utilizing images, we extract 21.4% more event mentions than traditional text-only methods."
    }
  },
  {
    "id": "abstract-2021--acl-long--296",
    "result": [
      {
        "value": {
          "start": 475,
          "end": 480,
          "text": "DARCY",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--296:E0"
      },
      {
        "value": {
          "start": 786,
          "end": 789,
          "text": "TPR",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--296:E1"
      },
      {
        "value": {
          "start": 807,
          "end": 810,
          "text": "FPR",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--296:E2"
      },
      {
        "value": {
          "start": 239,
          "end": 247,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--296:E3"
      },
      {
        "from_id": "abstract-2021--acl-long--296:E0",
        "to_id": "abstract-2021--acl-long--296:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--296:E0",
        "to_id": "abstract-2021--acl-long--296:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--296:E0",
        "to_id": "abstract-2021--acl-long--296:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "The Universal Trigger (UniTrigger) is a recently-proposed powerful adversarial textual attack method. Utilizing a learning-based mechanism, UniTrigger generates a fixed phrase that, when added to any benign inputs, can drop the prediction accuracy of a textual neural network (NN) model to near zero on a target class. To defend against this attack that can cause significant harm, in this paper, we borrow the “honeypot” concept from the cybersecurity community and propose DARCY, a honeypot-based defense framework against UniTrigger. DARCY greedily searches and injects multiple trapdoors into an NN model to “bait and catch” potential attacks. Through comprehensive experiments across four public datasets, we show that DARCY detects UniTrigger’s adversarial attacks with up to 99% TPR and less than 2% FPR in most cases, while maintaining the prediction accuracy (in F1) for clean inputs within a 1% margin. We also demonstrate that DARCY with multiple trapdoors is also robust to a diverse set of attack scenarios with attackers’ varying levels of knowledge and skills. We release the source code of DARCY at: https://github.com/lethaiq/ACL2021-DARCY-HoneypotDefenseNLP."
    }
  },
  {
    "id": "abstract-2020--acl-main--737",
    "result": [
      {
        "value": {
          "start": 691,
          "end": 769,
          "text": "adding inductive bias through phonetic and visual priors on character mappings",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--737:E0"
      },
      {
        "value": {
          "start": 805,
          "end": 816,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--737:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--737:E0",
        "to_id": "abstract-2020--acl-main--737:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Informal romanization is an idiosyncratic process used by humans in informal digital communication to encode non-Latin script languages into Latin character sets found on common keyboards. Character substitution choices differ between users but have been shown to be governed by the same main principles observed across a variety of languages—namely, character pairs are often associated through phonetic or visual similarity. We propose a noisy-channel WFST cascade model for deciphering the original non-Latin script from observed romanized text in an unsupervised fashion. We train our model directly on romanized data from two languages: Egyptian Arabic and Russian. We demonstrate that adding inductive bias through phonetic and visual priors on character mappings substantially improves the model’s performance on both languages, yielding results much closer to the supervised skyline. Finally, we introduce a new dataset of romanized Russian, collected from a Russian social network website and partially annotated for our experiments."
    }
  },
  {
    "id": "abstract-2020--acl-main--540",
    "result": [
      {
        "value": {
          "start": 1188,
          "end": 1208,
          "text": "adversarial training",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--540:E0"
      },
      {
        "value": {
          "start": 1145,
          "end": 1155,
          "text": "robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--540:E1"
      },
      {
        "value": {
          "start": 603,
          "end": 712,
          "text": "incorporates the sememe-based word substitution method and particle swarm optimization-based search algorithm",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--540:E2"
      },
      {
        "value": {
          "start": 954,
          "end": 974,
          "text": "attack success rates",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--540:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--540:E0",
        "to_id": "abstract-2020--acl-main--540:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--540:E2",
        "to_id": "abstract-2020--acl-main--540:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Adversarial attacks are carried out to reveal the vulnerability of deep neural networks. Textual adversarial attacking is challenging because text is discrete and a small perturbation can bring significant change to the original input. Word-level attacking, which can be regarded as a combinatorial optimization problem, is a well-studied class of textual attack methods. However, existing word-level attack models are far from perfect, largely because unsuitable search space reduction methods and inefficient optimization algorithms are employed. In this paper, we propose a novel attack model, which incorporates the sememe-based word substitution method and particle swarm optimization-based search algorithm to solve the two problems separately. We conduct exhaustive experiments to evaluate our attack model by attacking BiLSTM and BERT on three benchmark datasets. Experimental results demonstrate that our model consistently achieves much higher attack success rates and crafts more high-quality adversarial examples as compared to baseline methods. Also, further experiments show our model has higher transferability and can bring more robustness enhancement to victim models by adversarial training. All the code and data of this paper can be obtained on https://github.com/thunlp/SememePSO-Attack."
    }
  },
  {
    "id": "abstract-2021--acl-long--444",
    "result": [
      {
        "value": {
          "start": 566,
          "end": 605,
          "text": "design three latent variational modules",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--444:E0"
      },
      {
        "value": {
          "start": 170,
          "end": 181,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--444:E1"
      },
      {
        "value": {
          "start": 1253,
          "end": 1257,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--444:E2"
      },
      {
        "value": {
          "start": 1262,
          "end": 1265,
          "text": "TER",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--444:E3"
      },
      {
        "from_id": "abstract-2021--acl-long--444:E0",
        "to_id": "abstract-2021--acl-long--444:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--444:E0",
        "to_id": "abstract-2021--acl-long--444:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--444:E0",
        "to_id": "abstract-2021--acl-long--444:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Neural chat translation aims to translate bilingual conversational text, which has a broad application in international exchanges and cooperation. Despite the impressive performance of sentence-level and context-aware Neural Machine Translation (NMT), there still remain challenges to translate bilingual conversational text due to its inherent characteristics such as role preference, dialogue coherence, and translation consistency. In this paper, we aim to promote the translation quality of conversational text by modeling the above properties. Specifically, we design three latent variational modules to learn the distributions of bilingual conversational characteristics. Through sampling from these learned distributions, the latent variables, tailored for role preference, dialogue coherence, and translation consistency, are incorporated into the NMT model for better translation. We evaluate our approach on the benchmark dataset BConTrasT (English<->German) and a self-collected bilingual dialogue corpus, named BMELD (English<->Chinese). Extensive experiments show that our approach notably boosts the performance over strong baselines by a large margin and significantly surpasses some state-of-the-art context-aware NMT models in terms of BLEU and TER. Additionally, we make the BMELD dataset publicly available for the research community."
    }
  },
  {
    "id": "P11-1064",
    "result": [
      {
        "value": {
          "start": 394,
          "end": 424,
          "text": "completely probabilistic model",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1064:E0"
      },
      {
        "value": {
          "start": 489,
          "end": 497,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1064:E1"
      },
      {
        "value": {
          "start": 489,
          "end": 497,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1064:E2"
      },
      {
        "value": {
          "start": 450,
          "end": 462,
          "text": "phrase table",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1064:E3"
      },
      {
        "from_id": "P11-1064:E0",
        "to_id": "P11-1064:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "P11-1064:E0",
        "to_id": "P11-1064:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "P11-1064:E0",
        "to_id": "P11-1064:E3",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "We present an unsupervised model for joint phrase alignment and extraction using non-parametric Bayesian methods and inversion transduction grammars (ITGs). The key contribution is that phrases of many granularities are included directly in the model through the use of a novel formulation that memorizes phrases generated not only by terminal, but also non-terminal symbols. This allows for a completely probabilistic model that is able to create a phrase table that achieves competitive accuracy on phrase-based machine translation tasks directly from unaligned sentence pairs. Experiments on several language pairs demonstrate that the proposed model matches the accuracy of traditional two-step word alignment/phrase extraction approach while reducing the phrase table to a fraction of the original size."
    }
  },
  {
    "id": "abstract-2020--acl-main--320",
    "result": [
      {
        "value": {
          "start": 440,
          "end": 476,
          "text": "retrieval and rewriting based method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--320:E0"
      },
      {
        "value": {
          "start": 211,
          "end": 222,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--320:E1"
      },
      {
        "value": {
          "start": 1073,
          "end": 1084,
          "text": "BLEU scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--320:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--320:E0",
        "to_id": "abstract-2020--acl-main--320:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--320:E0",
        "to_id": "abstract-2020--acl-main--320:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The commonly used framework for unsupervised machine translation builds initial translation models of both translation directions, and then performs iterative back-translation to jointly boost their translation performance. The initialization stage is very important since bad initialization may wrongly squeeze the search space, and too much noise introduced in this stage may hurt the final performance. In this paper, we propose a novel retrieval and rewriting based method to better initialize unsupervised translation models. We first retrieve semantically comparable sentences from monolingual corpora of two languages and then rewrite the target side to minimize the semantic gap between the source and retrieved targets with a designed rewriting model. The rewritten sentence pairs are used to initialize SMT models which are used to generate pseudo data for two NMT models, followed by the iterative back-translation. Experiments show that our method can build better initial unsupervised translation models and improve the final translation performance by over 4 BLEU scores. Our code is released at https://github.com/Imagist-Shuo/RRforUNMT.git."
    }
  },
  {
    "id": "abstract-2020--acl-main--50",
    "result": [
      {
        "value": {
          "start": 287,
          "end": 302,
          "text": "cascaded method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--50:E0"
      },
      {
        "value": {
          "start": 792,
          "end": 800,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--50:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--50:E0",
        "to_id": "abstract-2020--acl-main--50:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Discovering the stances of media outlets and influential people on current, debatable topics is important for social statisticians and policy makers. Many supervised solutions exist for determining viewpoints, but manually annotating training data is costly. In this paper, we propose a cascaded method that uses unsupervised learning to ascertain the stance of Twitter users with respect to a polarizing topic by leveraging their retweet behavior; then, it uses supervised learning based on user labels to characterize both the general political leaning of online media and of popular Twitter users, as well as their stance with respect to the target polarizing topic. We evaluate the model by comparing its predictions to gold labels from the Media Bias/Fact Check website, achieving 82.6% accuracy."
    }
  },
  {
    "id": "P11-1163",
    "result": [
      {
        "value": {
          "start": 405,
          "end": 448,
          "text": "taking the tree of event-argument relations",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1163:E0"
      },
      {
        "value": {
          "start": 872,
          "end": 880,
          "text": "F1 score",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1163:E1"
      },
      {
        "value": {
          "start": 453,
          "end": 525,
          "text": "using it directly as the representation in a reranking dependency parser",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1163:E2"
      },
      {
        "from_id": "P11-1163:E0",
        "to_id": "P11-1163:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "P11-1163:E2",
        "to_id": "P11-1163:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Nested event structures are a common occurrence in both open domain and domain specific extraction tasks, e.g., a \"crime\" event can cause a \"investigation\" event, which can lead to an \"arrest\" event. However, most current approaches address event extraction with highly local models that extract each event and argument independently. We propose a simple approach for the extraction of such structures by taking the tree of event-argument relations and using it directly as the representation in a reranking dependency parser. This provides a simple framework that captures global properties of both nested and flat event structures. We explore a rich feature space that models both the events to be parsed and context from the original supporting text. Our approach obtains competitive results in the extraction of biomedical events from the BioNLP'09 shared task with a F1 score of 53.5% in development and 48.6% in testing."
    }
  },
  {
    "id": "abstract-2020--acl-main--403",
    "result": [
      {
        "value": {
          "start": 389,
          "end": 423,
          "text": "supervised machine learning models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--403:E0"
      },
      {
        "value": {
          "start": 670,
          "end": 678,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--403:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--403:E0",
        "to_id": "abstract-2020--acl-main--403:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries. Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances."
    }
  },
  {
    "id": "abstract-2020--acl-main--637",
    "result": [
      {
        "value": {
          "start": 254,
          "end": 282,
          "text": "bidirectional language model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--637:E0"
      },
      {
        "value": {
          "start": 717,
          "end": 736,
          "text": "joint goal accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--637:E1"
      },
      {
        "value": {
          "start": 166,
          "end": 191,
          "text": "multi-task learning model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--637:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--637:E0",
        "to_id": "abstract-2020--acl-main--637:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--637:E2",
        "to_id": "abstract-2020--acl-main--637:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Based on the recently proposed transferable dialogue state generator (TRADE) that predicts dialogue states from utterance-concatenated dialogue context, we propose a multi-task learning model with a simple yet effective utterance tagging technique and a bidirectional language model as an auxiliary task for task-oriented dialogue state generation. By enabling the model to learn a better representation of the long dialogue context, our approaches attempt to solve the problem that the performance of the baseline significantly drops when the input dialogue context sequence is long. In our experiments, our proposed model achieves a 7.03% relative improvement over the baseline, establishing a new state-of-the-art joint goal accuracy of 52.04% on the MultiWOZ 2.0 dataset."
    }
  },
  {
    "id": "abstract-2020--acl-main--545",
    "result": [
      {
        "value": {
          "start": 373,
          "end": 438,
          "text": "two-hand hybrid model leveraging a self-built paraphrase resource",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--545:E0"
      },
      {
        "value": {
          "start": 878,
          "end": 889,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--545:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--545:E0",
        "to_id": "abstract-2020--acl-main--545:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Given a sentence and its relevant answer, how to ask good questions is a challenging task, which has many real applications. Inspired by human’s paraphrasing capability to ask questions of the same meaning but with diverse expressions, we propose to incorporate paraphrase knowledge into question generation(QG) to generate human-like questions. Specifically, we present a two-hand hybrid model leveraging a self-built paraphrase resource, which is automatically conducted by a simple back-translation method. On the one hand, we conduct multi-task learning with sentence-level paraphrase generation (PG) as an auxiliary task to supplement paraphrase knowledge to the task-share encoder. On the other hand, we adopt a new loss function for diversity training to introduce more question patterns to QG. Extensive experimental results show that our proposed model obtains obvious performance gain over several strong baselines, and further human evaluation validates that our model can ask questions of high quality by leveraging paraphrase knowledge."
    }
  },
  {
    "id": "abstract-2020--acl-main--208",
    "result": [
      {
        "value": {
          "start": 607,
          "end": 648,
          "text": "using semantic scaffolds during inference",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--208:E0"
      },
      {
        "value": {
          "start": 691,
          "end": 707,
          "text": "top-100 accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--208:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--208:E0",
        "to_id": "abstract-2020--acl-main--208:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We propose a method for program generation based on semantic scaffolds, lightweight structures representing the high-level semantic and syntactic composition of a program. By first searching over plausible scaffolds then using these as constraints for a beam search over programs, we achieve better coverage of the search space when compared with existing techniques. We apply our hierarchical search method to the SPoC dataset for pseudocode-to-code generation, in which we are given line-level natural language pseudocode annotations and aim to produce a program satisfying execution-based test cases. By using semantic scaffolds during inference, we achieve a 10% absolute improvement in top-100 accuracy over the previous state-of-the-art. Additionally, we require only 11 candidates to reach the top-3000 performance of the previous best approach when tested against unseen problems, demonstrating a substantial improvement in efficiency."
    }
  },
  {
    "id": "abstract-2021--acl-long--531",
    "result": [
      {
        "value": {
          "start": 285,
          "end": 323,
          "text": "neural semi-Markov CRF alignment model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--531:E0"
      },
      {
        "value": {
          "start": 805,
          "end": 821,
          "text": "generalizability",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--531:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--531:E0",
        "to_id": "abstract-2021--acl-long--531:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Monolingual word alignment is important for studying fine-grained editing operations (i.e., deletion, addition, and substitution) in text-to-text generation tasks, such as paraphrase generation, text simplification, neutralizing biased language, etc. In this paper, we present a novel neural semi-Markov CRF alignment model, which unifies word and phrase alignments through variable-length spans. We also create a new benchmark with human annotations that cover four different text genres to evaluate monolingual word alignment models in more realistic settings. Experimental results show that our proposed model outperforms all previous approaches for monolingual word alignment as well as a competitive QA-based baseline, which was previously only applied to bilingual data. Our model demonstrates good generalizability to three out-of-domain datasets and shows great utility in two downstream applications: automatic text simplification and sentence pair classification tasks."
    }
  },
  {
    "id": "P11-1117",
    "result": [
      {
        "value": {
          "start": 22,
          "end": 67,
          "text": "supervised pronoun anaphora resolution system",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1117:E0"
      },
      {
        "value": {
          "start": 525,
          "end": 536,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1117:E1"
      },
      {
        "value": {
          "start": 77,
          "end": 115,
          "text": "factorial hidden Markov models (FHMMs)",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1117:E2"
      },
      {
        "from_id": "P11-1117:E0",
        "to_id": "P11-1117:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "P11-1117:E2",
        "to_id": "P11-1117:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "This paper presents a supervised pronoun anaphora resolution system based on factorial hidden Markov models (FHMMs). The basic idea is that the hidden states of FHMMs are an explicit short-term memory with an antecedent buffer containing recently described referents. Thus an observed pronoun can find its antecedent from the hidden buffer, or in terms of a generative model, the entries in the hidden buffer generate the corresponding pronouns. A system implementing this model is evaluated on the ACE corpus with promising performance."
    }
  },
  {
    "id": "abstract-2020--acl-main--38",
    "result": [
      {
        "value": {
          "start": 1010,
          "end": 1044,
          "text": "Lipschitz parameter initialization",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--38:E0"
      },
      {
        "value": {
          "start": 1137,
          "end": 1141,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--38:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--38:E0",
        "to_id": "abstract-2020--acl-main--38:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The Transformer translation model employs residual connection and layer normalization to ease the optimization difficulties caused by its multi-layer encoder/decoder structure. Previous research shows that even with residual connection and layer normalization, deep Transformers still have difficulty in training, and particularly Transformer models with more than 12 encoder/decoder layers fail to converge. In this paper, we first empirically demonstrate that a simple modification made in the official implementation, which changes the computation order of residual connection and layer normalization, can significantly ease the optimization of deep Transformers. We then compare the subtle differences in computation order in considerable detail, and present a parameter initialization method that leverages the Lipschitz constraint on the initialization of Transformer parameters that effectively ensures training convergence. In contrast to findings in previous research we further demonstrate that with Lipschitz parameter initialization, deep Transformers with the original computation order can converge, and obtain significant BLEU improvements with up to 24 layers. In contrast to previous research which focuses on deep encoders, our approach additionally enables Transformers to also benefit from deep decoders."
    }
  },
  {
    "id": "abstract-2020--acl-main--719",
    "result": [
      {
        "value": {
          "start": 772,
          "end": 785,
          "text": "our framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--719:E0"
      },
      {
        "value": {
          "start": 819,
          "end": 841,
          "text": "predictive performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--719:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--719:E0",
        "to_id": "abstract-2020--acl-main--719:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Nowadays, the interpretability of machine learning models is becoming increasingly important, especially in the medical domain. Aiming to shed some light on how to rationalize medical relation prediction, we present a new interpretable framework inspired by existing theories on how human memory works, e.g., theories of recall and recognition. Given the corpus-level statistics, i.e., a global co-occurrence graph of a clinical text corpus, to predict the relations between two entities, we first recall rich contexts associated with the target entities, and then recognize relational interactions between these contexts to form model rationales, which will contribute to the final prediction. We conduct experiments on a real-world public clinical dataset and show that our framework can not only achieve competitive predictive performance against a comprehensive list of neural baseline models, but also present rationales to justify its prediction. We further collaborate with medical experts deeply to verify the usefulness of our model rationales for clinical decision making."
    }
  },
  {
    "id": "abstract-2020--acl-main--212",
    "result": [
      {
        "value": {
          "start": 693,
          "end": 717,
          "text": "subject/object inversion",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--212:E0"
      },
      {
        "value": {
          "start": 728,
          "end": 743,
          "text": "BERT’s accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--212:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--212:E0",
        "to_id": "abstract-2020--acl-main--212:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Pretrained neural models such as BERT, when fine-tuned to perform natural language inference (NLI), often show high accuracy on standard datasets, but display a surprising lack of sensitivity to word order on controlled challenge sets. We hypothesize that this issue is not primarily caused by the pretrained model’s limitations, but rather by the paucity of crowdsourced NLI examples that might convey the importance of syntactic structure at the fine-tuning stage. We explore several methods to augment standard training sets with syntactically informative examples, generated by applying syntactic transformations to sentences from the MNLI corpus. The best-performing augmentation method, subject/object inversion, improved BERT’s accuracy on controlled examples that diagnose sensitivity to word order from 0.28 to 0.73, without affecting performance on the MNLI test set. This improvement generalized beyond the particular construction used for data augmentation, suggesting that augmentation causes BERT to recruit abstract syntactic representations."
    }
  },
  {
    "id": "abstract-2020--acl-main--582",
    "result": [
      {
        "value": {
          "start": 536,
          "end": 540,
          "text": "SDRN",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--582:E0"
      },
      {
        "value": {
          "start": 1186,
          "end": 1198,
          "text": "performances",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--582:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--582:E0",
        "to_id": "abstract-2020--acl-main--582:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Opinion entity extraction is a fundamental task in fine-grained opinion mining. Related studies generally extract aspects and/or opinion expressions without recognizing the relations between them. However, the relations are crucial for downstream tasks, including sentiment classification, opinion summarization, etc. In this paper, we explore Aspect-Opinion Pair Extraction (AOPE) task, which aims at extracting aspects and opinion expressions in pairs. To deal with this task, we propose Synchronous Double-channel Recurrent Network (SDRN) mainly consisting of an opinion entity extraction unit, a relation detection unit, and a synchronization unit. The opinion entity extraction unit and the relation detection unit are developed as two channels to extract opinion entities and relations simultaneously. Furthermore, within the synchronization unit, we design Entity Synchronization Mechanism (ESM) and Relation Synchronization Mechanism (RSM) to enhance the mutual benefit on the above two channels. To verify the performance of SDRN, we manually build three datasets based on SemEval 2014 and 2015 benchmarks. Extensive experiments demonstrate that SDRN achieves state-of-the-art performances."
    }
  },
  {
    "id": "abstract-2021--acl-long--278",
    "result": [
      {
        "value": {
          "start": 986,
          "end": 1018,
          "text": "dangling entity detection module",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--278:E0"
      },
      {
        "value": {
          "start": 1074,
          "end": 1085,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--278:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--278:E0",
        "to_id": "abstract-2021--acl-long--278:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This paper studies a new problem setting of entity alignment for knowledge graphs (KGs). Since KGs possess different sets of entities, there could be entities that cannot find alignment across them, leading to the problem of dangling entities. As the first attempt to this problem, we construct a new dataset and design a multi-task learning framework for both entity alignment and dangling entity detection. The framework can opt to abstain from predicting alignment for the detected dangling entities. We propose three techniques for dangling entity detection that are based on the distribution of nearest-neighbor distances, i.e., nearest neighbor classification, marginal ranking and background ranking. After detecting and removing dangling entities, an incorporated entity alignment model in our framework can provide more robust alignment for remaining entities. Comprehensive experiments and analyses demonstrate the effectiveness of our framework. We further discover that the dangling entity detection module can, in turn, improve alignment learning and the final performance. The contributed resource is publicly available to foster further research."
    }
  },
  {
    "id": "abstract-2020--acl-main--93",
    "result": [
      {
        "value": {
          "start": 430,
          "end": 457,
          "text": "one-to-one metric BERTScore",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--93:E0"
      },
      {
        "value": {
          "start": 475,
          "end": 492,
          "text": "human correlation",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--93:E1"
      },
      {
        "value": {
          "start": 557,
          "end": 568,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--93:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--93:E0",
        "to_id": "abstract-2020--acl-main--93:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--93:E0",
        "to_id": "abstract-2020--acl-main--93:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Evaluating image captions is very challenging partially due to the fact that there are multiple correct captions for every single image. Most of the existing one-to-one metrics operate by penalizing mismatches between reference and generative caption without considering the intrinsic variance between ground truth captions. It usually leads to over-penalization and thus a bad correlation to human judgment. Recently, the latest one-to-one metric BERTScore can achieve high human correlation in system-level tasks while some issues can be fixed for better performance. In this paper, we propose a novel metric based on BERTScore that could handle such a challenge and extend BERTScore with a few new features appropriately for image captioning evaluation. The experimental results show that our metric achieves state-of-the-art human judgment correlation."
    }
  },
  {
    "id": "abstract-2021--acl-long--316",
    "result": [
      {
        "value": {
          "start": 43,
          "end": 46,
          "text": "GAR",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--316:E0"
      },
      {
        "value": {
          "start": 387,
          "end": 398,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--316:E1"
      },
      {
        "value": {
          "start": 43,
          "end": 46,
          "text": "GAR",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--316:E2"
      },
      {
        "value": {
          "start": 387,
          "end": 398,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--316:E3"
      },
      {
        "from_id": "abstract-2021--acl-long--316:E0",
        "to_id": "abstract-2021--acl-long--316:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--316:E2",
        "to_id": "abstract-2021--acl-long--316:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We propose Generation-Augmented Retrieval (GAR) for answering open-domain questions, which augments a query through text generation of heuristically discovered relevant contexts without external resources as supervision. We demonstrate that the generated contexts substantially enrich the semantics of the queries and GAR with sparse representations (BM25) achieves comparable or better performance than state-of-the-art dense retrieval methods such as DPR. We show that generating diverse contexts for a query is beneficial as fusing their results consistently yields better retrieval accuracy. Moreover, as sparse and dense representations are often complementary, GAR can be easily combined with DPR to achieve even better performance. GAR achieves state-of-the-art performance on Natural Questions and TriviaQA datasets under the extractive QA setup when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used."
    }
  },
  {
    "id": "abstract-2021--acl-long--489",
    "result": [
      {
        "value": {
          "start": 609,
          "end": 612,
          "text": "AMR",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--489:E0"
      },
      {
        "value": {
          "start": 1139,
          "end": 1146,
          "text": "F-score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--489:E1"
      },
      {
        "value": {
          "start": 773,
          "end": 791,
          "text": "external knowledge",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--489:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--489:E0",
        "to_id": "abstract-2021--acl-long--489:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--489:E2",
        "to_id": "abstract-2021--acl-long--489:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Biomedical Information Extraction from scientific literature presents two unique and non-trivial challenges. First, compared with general natural language texts, sentences from scientific papers usually possess wider contexts between knowledge elements. Moreover, comprehending the fine-grained scientific entities and events urgently requires domain-specific background knowledge. In this paper, we propose a novel biomedical Information Extraction (IE) model to tackle these two challenges and extract scientific entities and events from English research papers. We perform Abstract Meaning Representation (AMR) to compress the wide context to uncover a clear semantic structure for each complex sentence. Besides, we construct the sentence-level knowledge graph from an external knowledge base and use it to enrich the AMR graph to improve the model’s understanding of complex scientific concepts. We use an edge-conditioned graph attention network to encode the knowledge-enriched AMR graph for biomedical IE tasks. Experiments on the GENIA 2011 dataset show that the AMR and external knowledge have contributed 1.8% and 3.0% absolute F-score gains respectively. In order to evaluate the impact of our approach on real-world problems that involve topic-specific fine-grained knowledge elements, we have also created a new ontology and annotated corpus for entity and event extraction for the COVID-19 scientific literature, which can serve as a new benchmark for the biomedical IE community."
    }
  },
  {
    "id": "abstract-2020--acl-main--84",
    "result": [
      {
        "value": {
          "start": 140,
          "end": 180,
          "text": "introduce an intermediate representation",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--84:E0"
      },
      {
        "value": {
          "start": 856,
          "end": 872,
          "text": "annotation speed",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--84:E1"
      },
      {
        "value": {
          "start": 895,
          "end": 920,
          "text": "complexity of the queries",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--84:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--84:E0",
        "to_id": "abstract-2020--acl-main--84:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--84:E0",
        "to_id": "abstract-2020--acl-main--84:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "In this paper, we introduce a novel methodology to efficiently construct a corpus for question answering over structured data. For this, we introduce an intermediate representation that is based on the logical query plan in a database, called Operation Trees (OT). This representation allows us to invert the annotation process without loosing flexibility in the types of queries that we generate. Furthermore, it allows for fine-grained alignment of the tokens to the operations. Thus, we randomly generate OTs from a context free grammar and annotators just have to write the appropriate question and assign the tokens. We compare our corpus OTTA (Operation Trees and Token Assignment), a large semantic parsing corpus for evaluating natural language interfaces to databases, to Spider and LC-QuaD 2.0 and show that our methodology more than triples the annotation speed while maintaining the complexity of the queries. Finally, we train a state-of-the-art semantic parsing model on our data and show that our dataset is a challenging dataset and that the token alignment can be leveraged to significantly increase the performance."
    }
  },
  {
    "id": "abstract-2020--acl-main--136",
    "result": [
      {
        "value": {
          "start": 931,
          "end": 967,
          "text": "employing a pre-trained BERT encoder",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--136:E0"
      },
      {
        "value": {
          "start": 1040,
          "end": 1048,
          "text": "F1-score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--136:E1"
      },
      {
        "value": {
          "start": 382,
          "end": 423,
          "text": "cascade binary tagging framework (CasRel)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--136:E2"
      },
      {
        "value": {
          "start": 908,
          "end": 919,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--136:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--136:E0",
        "to_id": "abstract-2020--acl-main--136:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--136:E2",
        "to_id": "abstract-2020--acl-main--136:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--136:E2",
        "to_id": "abstract-2020--acl-main--136:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Extracting relational triples from unstructured text is crucial for large-scale knowledge graph construction. However, few existing works excel in solving the overlapping triple problem where multiple relational triples in the same sentence share the same entities. In this work, we introduce a fresh perspective to revisit the relational triple extraction task and propose a novel cascade binary tagging framework (CasRel) derived from a principled problem formulation. Instead of treating relations as discrete labels as in previous works, our new framework models relations as functions that map subjects to objects in a sentence, which naturally handles the overlapping problem. Experiments show that the CasRel framework already outperforms state-of-the-art methods even when its encoder module uses a randomly initialized BERT encoder, showing the power of the new tagging framework. It enjoys further performance boost when employing a pre-trained BERT encoder, outperforming the strongest baseline by 17.5 and 30.2 absolute gain in F1-score on two public datasets NYT and WebNLG, respectively. In-depth analysis on different scenarios of overlapping triples shows that the method delivers consistent performance gain across all these scenarios. The source code and data are released online."
    }
  },
  {
    "id": "abstract-2020--acl-main--747",
    "result": [
      {
        "value": {
          "start": 316,
          "end": 321,
          "text": "XLM-R",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--747:E0"
      },
      {
        "value": {
          "start": 434,
          "end": 450,
          "text": "average accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--747:E1"
      },
      {
        "value": {
          "start": 465,
          "end": 481,
          "text": "average F1 score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--747:E2"
      },
      {
        "value": {
          "start": 473,
          "end": 481,
          "text": "F1 score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--747:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--747:E0",
        "to_id": "abstract-2020--acl-main--747:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--747:E0",
        "to_id": "abstract-2020--acl-main--747:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--747:E0",
        "to_id": "abstract-2020--acl-main--747:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available."
    }
  },
  {
    "id": "abstract-2020--acl-main--775",
    "result": [
      {
        "value": {
          "start": 1143,
          "end": 1191,
          "text": "joint decoder reconciles the two different views",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--775:E0"
      },
      {
        "value": {
          "start": 1236,
          "end": 1246,
          "text": "accuracies",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--775:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--775:E0",
        "to_id": "abstract-2020--acl-main--775:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "An interesting and frequent type of multi-word expression (MWE) is the headless MWE, for which there are no true internal syntactic dominance relations; examples include many named entities (“Wells Fargo”) and dates (“July 5, 2020”) as well as certain productive constructions (“blow for blow”, “day after day”). Despite their special status and prevalence, current dependency-annotation schemes require treating such flat structures as if they had internal syntactic heads, and most current parsers handle them in the same fashion as headed constructions. Meanwhile, outside the context of parsing, taggers are typically used for identifying MWEs, but taggers might benefit from structural information. We empirically compare these two common strategies—parsing and tagging—for predicting flat MWEs. Additionally, we propose an efficient joint decoding algorithm that combines scores from both strategies. Experimental results on the MWE-Aware English Dependency Corpus and on six non-English dependency treebanks with frequent flat structures show that: (1) tagging is more accurate than parsing for identifying flat-structure MWEs, (2) our joint decoder reconciles the two different views and, for non-BERT features, leads to higher accuracies, and (3) most of the gains result from feature sharing between the parsers and taggers."
    }
  },
  {
    "id": "abstract-2020--acl-main--216",
    "result": [
      {
        "value": {
          "start": 615,
          "end": 639,
          "text": "multiresolution training",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--216:E0"
      },
      {
        "value": {
          "start": 653,
          "end": 664,
          "text": "convergence",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--216:E1"
      },
      {
        "value": {
          "start": 703,
          "end": 724,
          "text": "word error rate (WER)",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--216:E2"
      },
      {
        "value": {
          "start": 790,
          "end": 822,
          "text": "incorporating visual information",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--216:E3"
      },
      {
        "value": {
          "start": 725,
          "end": 736,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--216:E4"
      },
      {
        "from_id": "abstract-2020--acl-main--216:E0",
        "to_id": "abstract-2020--acl-main--216:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--216:E0",
        "to_id": "abstract-2020--acl-main--216:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--216:E3",
        "to_id": "abstract-2020--acl-main--216:E4",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This paper presents an audio visual automatic speech recognition (AV-ASR) system using a Transformer-based architecture. We particularly focus on the scene context provided by the visual information, to ground the ASR. We extract representations for audio features in the encoder layers of the transformer and fuse video features using an additional crossmodal multihead attention layer. Additionally, we incorporate a multitask training criterion for multiresolution ASR, where we train the model to generate both character and subword level transcriptions. Experimental results on the How2 dataset, indicate that multiresolution training can speed up convergence by around 50% and relatively improves word error rate (WER) performance by upto 18% over subword prediction models. Further, incorporating visual information improves performance with relative gains upto 3.76% over audio only models. Our results are comparable to state-of-the-art Listen, Attend and Spell-based architectures."
    }
  },
  {
    "id": "abstract-2020--acl-main--148",
    "result": [
      {
        "value": {
          "start": 620,
          "end": 649,
          "text": "random online backtranslation",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--148:E0"
      },
      {
        "value": {
          "start": 585,
          "end": 606,
          "text": "zero-shot performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--148:E1"
      },
      {
        "value": {
          "start": 962,
          "end": 966,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--148:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--148:E0",
        "to_id": "abstract-2020--acl-main--148:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--148:E0",
        "to_id": "abstract-2020--acl-main--148:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Massively multilingual models for neural machine translation (NMT) are theoretically attractive, but often underperform bilingual models and deliver poor zero-shot translations. In this paper, we explore ways to improve them. We argue that multilingual NMT requires stronger modeling capacity to support language pairs with varying typological characteristics, and overcome this bottleneck via language-specific components and deepening NMT architectures. We identify the off-target translation issue (i.e. translating into a wrong target language) as the major source of the inferior zero-shot performance, and propose random online backtranslation to enforce the translation of unseen training language pairs. Experiments on OPUS-100 (a novel multilingual dataset with 100 languages) show that our approach substantially narrows the performance gap with bilingual models in both one-to-many and many-to-many settings, and improves zero-shot performance by ~10 BLEU, approaching conventional pivot-based methods."
    }
  },
  {
    "id": "abstract-2020--acl-main--213",
    "result": [
      {
        "value": {
          "start": 680,
          "end": 699,
          "text": "auxiliary objective",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--213:E0"
      },
      {
        "value": {
          "start": 743,
          "end": 757,
          "text": "generalization",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--213:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--213:E0",
        "to_id": "abstract-2020--acl-main--213:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Training objectives based on predictive coding have recently been shown to be very effective at learning meaningful representations from unlabeled speech. One example is Autoregressive Predictive Coding (Chung et al., 2019), which trains an autoregressive RNN to generate an unseen future frame given a context such as recent past frames. The basic hypothesis of these approaches is that hidden states that can accurately predict future frames are a useful representation for many downstream tasks. In this paper we extend this hypothesis and aim to enrich the information encoded in the hidden states by training the model to make more accurate future predictions. We propose an auxiliary objective that serves as a regularization to improve generalization of the future frame prediction task. Experimental results on phonetic classification, speech recognition, and speech translation not only support the hypothesis, but also demonstrate the effectiveness of our approach in learning representations that contain richer phonetic content."
    }
  },
  {
    "id": "abstract-2021--acl-long--161",
    "result": [
      {
        "value": {
          "start": 228,
          "end": 239,
          "text": "ChineseBERT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--161:E0"
      },
      {
        "value": {
          "start": 826,
          "end": 837,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--161:E1"
      },
      {
        "value": {
          "start": 876,
          "end": 890,
          "text": "training steps",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--161:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--161:E0",
        "to_id": "abstract-2021--acl-long--161:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--161:E0",
        "to_id": "abstract-2021--acl-long--161:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Recent pretraining models in Chinese neglect two important aspects specific to the Chinese language: glyph and pinyin, which carry significant syntax and semantic information for language understanding. In this work, we propose ChineseBERT, which incorporates both the glyph and pinyin information of Chinese characters into language model pretraining. The glyph embedding is obtained based on different fonts of a Chinese character, being able to capture character semantics from the visual features, and the pinyin embedding characterizes the pronunciation of Chinese characters, which handles the highly prevalent heteronym phenomenon in Chinese (the same character has different pronunciations with different meanings). Pretrained on large-scale unlabeled Chinese corpus, the proposed ChineseBERT model yields significant performance boost over baseline models with fewer training steps. The proposed model achieves new SOTA performances on a wide range of Chinese NLP tasks, including machine reading comprehension, natural language inference, text classification, sentence pair matching, and competitive performances in named entity recognition and word segmentation."
    }
  },
  {
    "id": "P11-1072",
    "result": [
      {
        "value": {
          "start": 706,
          "end": 754,
          "text": "A hybrid model with our learned sub-word lexicon",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1072:E0"
      },
      {
        "value": {
          "start": 763,
          "end": 768,
          "text": "error",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1072:E1"
      },
      {
        "value": {
          "start": 805,
          "end": 821,
          "text": "false alarm rate",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1072:E2"
      },
      {
        "from_id": "P11-1072:E0",
        "to_id": "P11-1072:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "P11-1072:E0",
        "to_id": "P11-1072:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Large vocabulary speech recognition systems fail to recognize words beyond their vocabulary, many of which are information rich terms, like named entities or foreign words. Hybrid word/sub-word systems solve this problem by adding sub-word units to large vocabulary word based systems; new words can then be represented by combinations of sub-word units. Previous work heuristically created the sub-word lexicon from phonetic representations of text using simple statistics to select common phone sequences. We propose a probabilistic model to learn the subword lexicon optimized for a given task. We consider the task of out of vocabulary (OOV) word detection, which relies on output from a hybrid model. A hybrid model with our learned sub-word lexicon reduces error by 6.3% and 7.6% (absolute) at a 5% false alarm rate on an English Broadcast News and MIT Lectures task respectively."
    }
  },
  {
    "id": "abstract-2021--acl-long--15",
    "result": [
      {
        "value": {
          "start": 429,
          "end": 478,
          "text": "Global-Locally Graph Interaction Network (GL-GIN)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--15:E0"
      },
      {
        "value": {
          "start": 850,
          "end": 861,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--15:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--15:E0",
        "to_id": "abstract-2021--acl-long--15:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Multi-intent SLU can handle multiple intents in an utterance, which has attracted increasing attention. However, the state-of-the-art joint models heavily rely on autoregressive approaches, resulting in two issues: slow inference speed and information leakage. In this paper, we explore a non-autoregressive model for joint multiple intent detection and slot filling, achieving more fast and accurate. Specifically, we propose a Global-Locally Graph Interaction Network (GL-GIN) where a local slot-aware graph interaction layer is proposed to model slot dependency for alleviating uncoordinated slots problem while a global intent-slot graph interaction layer is introduced to model the interaction between multiple intents and all slots in the utterance. Experimental results on two public datasets show that our framework achieves state-of-the-art performance while being 11.5 times faster."
    }
  },
  {
    "id": "abstract-2020--acl-main--142",
    "result": [
      {
        "value": {
          "start": 703,
          "end": 721,
          "text": "relabeled test set",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--142:E0"
      },
      {
        "value": {
          "start": 726,
          "end": 742,
          "text": "average F1 score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--142:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--142:E0",
        "to_id": "abstract-2020--acl-main--142:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "TACRED is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pre-training and knowledge enhanced neural RE, models still show a high error rate. In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement? And how do crowd annotations, dataset, and models contribute to this error rate? To answer these questions, we first validate the most challenging 5K examples in the development and test sets using trained annotators. We find that label errors account for 8% absolute F1 test error, and that more than 50% of the examples need to be relabeled. On the relabeled test set the average F1 score of a large baseline model set improves from 62.1 to 70.1. After validation, we analyze misclassifications on the challenging instances, categorize them into linguistically motivated error groups, and verify the resulting error hypotheses on three state-of-the-art RE models. We show that two groups of ambiguous relations are responsible for most of the remaining errors and that models may adopt shallow heuristics on the dataset when entities are not masked."
    }
  },
  {
    "id": "abstract-2020--acl-main--692",
    "result": [
      {
        "value": {
          "start": 539,
          "end": 561,
          "text": "data selection methods",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--692:E0"
      },
      {
        "value": {
          "start": 811,
          "end": 815,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--692:E1"
      },
      {
        "value": {
          "start": 820,
          "end": 829,
          "text": "precision",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--692:E2"
      },
      {
        "value": {
          "start": 834,
          "end": 840,
          "text": "recall",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--692:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--692:E0",
        "to_id": "abstract-2020--acl-main--692:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--692:E0",
        "to_id": "abstract-2020--acl-main--692:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--692:E0",
        "to_id": "abstract-2020--acl-main--692:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The notion of “in-domain data” in NLP is often over-simplistic and vague, as textual data varies in many nuanced linguistic aspects such as topic, style or level of formality. In addition, domain labels are many times unavailable, making it challenging to build domain-specific systems. We show that massive pre-trained language models implicitly learn sentence representations that cluster by domains without supervision – suggesting a simple data-driven definition of domains in textual data. We harness this property and propose domain data selection methods based on such models, which require only a small set of in-domain monolingual data. We evaluate our data selection methods for neural machine translation across five diverse domains, where they outperform an established approach as measured by both BLEU and precision and recall with respect to an oracle selection."
    }
  },
  {
    "id": "abstract-2020--acl-main--609",
    "result": [
      {
        "value": {
          "start": 501,
          "end": 561,
          "text": "structural-aware model at both the encoder and decoder phase",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--609:E0"
      },
      {
        "value": {
          "start": 145,
          "end": 156,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--609:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--609:E0",
        "to_id": "abstract-2020--acl-main--609:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Discourse representation tree structure (DRTS) parsing is a novel semantic parsing task which has been concerned most recently. State-of-the-art performance can be achieved by a neural sequence-to-sequence model, treating the tree construction as an incremental sequence generation problem. Structural information such as input syntax and the intermediate skeleton of the partial output has been ignored in the model, which could be potentially useful for the DRTS parsing. In this work, we propose a structural-aware model at both the encoder and decoder phase to integrate the structural information, where graph attention network (GAT) is exploited for effectively modeling. Experimental results on a benchmark dataset show that our proposed model is effective and can obtain the best performance in the literature."
    }
  },
  {
    "id": "abstract-2020--acl-main--538",
    "result": [
      {
        "value": {
          "start": 510,
          "end": 595,
          "text": "combining the two sources with data augmentation and retrieval-based data re-sampling",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--538:E0"
      },
      {
        "value": {
          "start": 657,
          "end": 667,
          "text": "BLEU score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--538:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--538:E0",
        "to_id": "abstract-2020--acl-main--538:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Open-domain code generation aims to generate code in a general-purpose programming language (such as Python) from natural language (NL) intents. Motivated by the intuition that developers usually retrieve resources on the web when writing code, we explore the effectiveness of incorporating two varieties of external knowledge into NL-to-code generation: automatically mined NL-code pairs from the online programming QA forum StackOverflow and programming language API documentation. Our evaluations show that combining the two sources with data augmentation and retrieval-based data re-sampling improves the current state-of-the-art by up to 2.2% absolute BLEU score on the code generation testbed CoNaLa. The code and resources are available at https://github.com/neulab/external-knowledge-codegen."
    }
  },
  {
    "id": "abstract-2020--acl-main--138",
    "result": [
      {
        "value": {
          "start": 339,
          "end": 380,
          "text": "two Noise-Aware Training (NAT) objectives",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--138:E0"
      },
      {
        "value": {
          "start": 394,
          "end": 404,
          "text": "robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--138:E1"
      },
      {
        "value": {
          "start": 1006,
          "end": 1014,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--138:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--138:E0",
        "to_id": "abstract-2020--acl-main--138:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--138:E0",
        "to_id": "abstract-2020--acl-main--138:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Sequence labeling systems should perform reliably not only under ideal conditions but also with corrupted inputs—as these systems often process user-generated text or follow an error-prone upstream component. To this end, we formulate the noisy sequence labeling problem, where the input may undergo an unknown noising process and propose two Noise-Aware Training (NAT) objectives that improve robustness of sequence labeling performed on perturbed input: Our data augmentation method trains a neural model using a mixture of clean and noisy samples, whereas our stability training algorithm encourages the model to create a noise-invariant latent representation. We employ a vanilla noise model at training time. For evaluation, we use both the original data and its variants perturbed with real OCR errors and misspellings. Extensive experiments on English and German named entity recognition benchmarks confirmed that NAT consistently improved robustness of popular sequence labeling models, preserving accuracy on the original input. We make our code and data publicly available for the research community."
    }
  },
  {
    "id": "abstract-2020--acl-main--29",
    "result": [
      {
        "value": {
          "start": 351,
          "end": 357,
          "text": "S-LSTM",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--29:E0"
      },
      {
        "value": {
          "start": 522,
          "end": 527,
          "text": "error",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--29:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--29:E0",
        "to_id": "abstract-2020--acl-main--29:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce Segment Pooling LSTM (S-LSTM), which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling."
    }
  },
  {
    "id": "abstract-2020--acl-main--523",
    "result": [
      {
        "value": {
          "start": 312,
          "end": 364,
          "text": "joint model that supports multi-class classification",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--523:E0"
      },
      {
        "value": {
          "start": 517,
          "end": 519,
          "text": "F1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--523:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--523:E0",
        "to_id": "abstract-2020--acl-main--523:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Exploiting sentence-level labels, which are easy to obtain, is one of the plausible methods to improve low-resource named entity recognition (NER), where token-level labels are costly to annotate. Current models for jointly learning sentence and token labeling are limited to binary classification. We present a joint model that supports multi-class classification and introduce a simple variant of self-attention that allows the model to learn scaling factors. Our model produces 3.78%, 4.20%, 2.08% improvements in F1 over the BiLSTM-CRF baseline on e-commerce product titles in three different low-resource languages: Vietnamese, Thai, and Indonesian, respectively."
    }
  },
  {
    "id": "abstract-2020--acl-main--622",
    "result": [
      {
        "value": {
          "start": 26,
          "end": 33,
          "text": "CorefQA",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--622:E0"
      },
      {
        "value": {
          "start": 973,
          "end": 984,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--622:E1"
      },
      {
        "value": {
          "start": 1030,
          "end": 1038,
          "text": "F1 score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--622:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--622:E0",
        "to_id": "abstract-2020--acl-main--622:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--622:E0",
        "to_id": "abstract-2020--acl-main--622:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In this paper, we present CorefQA, an accurate and extensible approach for the coreference resolution task. We formulate the problem as a span prediction task, like in question answering: A query is generated for each candidate mention using its surrounding context, and a span prediction module is employed to extract the text spans of the coreferences within the document using the generated query. This formulation comes with the following key advantages: (1) The span prediction strategy provides the flexibility of retrieving mentions left out at the mention proposal stage; (2) In the question answering framework, encoding the mention and its context explicitly in a query makes it possible to have a deep and thorough examination of cues embedded in the context of coreferent mentions; and (3) A plethora of existing question answering datasets can be used for data augmentation to improve the model’s generalization capability. Experiments demonstrate significant performance boost over previous models, with 83.1 (+3.5) F1 score on the CoNLL-2012 benchmark and 87.5 (+2.5) F1 score on the GAP benchmark."
    }
  },
  {
    "id": "abstract-2021--acl-long--234",
    "result": [
      {
        "value": {
          "start": 502,
          "end": 506,
          "text": "CMCL",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--234:E0"
      },
      {
        "value": {
          "start": 913,
          "end": 924,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--234:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--234:E0",
        "to_id": "abstract-2021--acl-long--234:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Medical report generation task, which targets to produce long and coherent descriptions of medical images, has attracted growing research interests recently. Different from the general image captioning tasks, medical report generation is more challenging for data-driven neural models. This is mainly due to 1) the serious data bias and 2) the limited medical data. To alleviate the data bias and make best use of available data, we propose a Competence-based Multimodal Curriculum Learning framework (CMCL). Specifically, CMCL simulates the learning process of radiologists and optimizes the model in a step by step manner. Firstly, CMCL estimates the difficulty of each training instance and evaluates the competence of current model; Secondly, CMCL selects the most suitable batch of training instances considering current model competence. By iterating above two steps, CMCL can gradually improve the model’s performance. The experiments on the public IU-Xray and MIMIC-CXR datasets show that CMCL can be incorporated into existing models to improve their performance."
    }
  },
  {
    "id": "abstract-2020--acl-main--227",
    "result": [
      {
        "value": {
          "start": 439,
          "end": 453,
          "text": "guider network",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--227:E0"
      },
      {
        "value": {
          "start": 688,
          "end": 699,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--227:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--227:E0",
        "to_id": "abstract-2020--acl-main--227:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Auto-regressive text generation models usually focus on local fluency, and may cause inconsistent semantic meaning in long text generation. Further, automatically generating words with similar semantics is challenging, and hand-crafted linguistic rules are difficult to apply. We consider a text planning scheme and present a model-based imitation-learning approach to alleviate the aforementioned issues. Specifically, we propose a novel guider network to focus on the generative process over a longer horizon, which can assist next-word prediction and provide intermediate rewards for generator optimization. Extensive experiments demonstrate that the proposed method leads to improved performance."
    }
  },
  {
    "id": "abstract-2020--acl-main--123",
    "result": [
      {
        "value": {
          "start": 1159,
          "end": 1221,
          "text": "headline generation model trained on filtered supervision data",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--123:E0"
      },
      {
        "value": {
          "start": 49,
          "end": 61,
          "text": "ROUGE scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--123:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--123:E0",
        "to_id": "abstract-2020--acl-main--123:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Most studies on abstractive summarization report ROUGE scores between system and reference summaries. However, we have a concern about the truthfulness of generated summaries: whether all facts of a generated summary are mentioned in the source text. This paper explores improving the truthfulness in headline generation on two popular datasets. Analyzing headlines generated by the state-of-the-art encoder-decoder model, we show that the model sometimes generates untruthful headlines. We conjecture that one of the reasons lies in untruthful supervision data used for training the model. In order to quantify the truthfulness of article-headline pairs, we consider the textual entailment of whether an article entails its headline. After confirming quite a few untruthful instances in the datasets, this study hypothesizes that removing untruthful instances from the supervision data may remedy the problem of the untruthful behaviors of the model. Building a binary classifier that predicts an entailment relation between an article and its headline, we filter out untruthful instances from the supervision data. Experimental results demonstrate that the headline generation model trained on filtered supervision data shows no clear difference in ROUGE scores but remarkable improvements in automatic and manual evaluations of the generated headlines."
    }
  },
  {
    "id": "abstract-2021--acl-long--563",
    "result": [
      {
        "value": {
          "start": 721,
          "end": 754,
          "text": "discriminative reranking approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--563:E0"
      },
      {
        "value": {
          "start": 489,
          "end": 493,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--563:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--563:E0",
        "to_id": "abstract-2021--acl-long--563:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Reranking models enable the integration of rich features to select a better output hypothesis within an n-best list or lattice. These models have a long history in NLP, and we revisit discriminative reranking for modern neural machine translation models by training a large transformer architecture. This takes as input both the source sentence as well as a list of hypotheses to output a ranked list. The reranker is trained to predict the observed distribution of a desired metric, e.g. BLEU, over the n-best list. Since such a discriminator contains hundreds of millions of parameters, we improve its generalization using pre-training and data augmentation techniques. Experiments on four WMT directions show that our discriminative reranking approach is effective and complementary to existing generative reranking approaches, yielding improvements of up to 4 BLEU over the beam search output."
    }
  },
  {
    "id": "abstract-2020--acl-main--677",
    "result": [
      {
        "value": {
          "start": 439,
          "end": 478,
          "text": "relation-aware self-attention mechanism",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--677:E0"
      },
      {
        "value": {
          "start": 801,
          "end": 812,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--677:E1"
      },
      {
        "value": {
          "start": 407,
          "end": 424,
          "text": "unified framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--677:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--677:E0",
        "to_id": "abstract-2020--acl-main--677:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--677:E2",
        "to_id": "abstract-2020--acl-main--677:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "When translating natural language questions into SQL queries to answer questions from a database, contemporary semantic parsing models struggle to generalize to unseen database schemas. The generalization challenge lies in (a) encoding the database relations in an accessible way for the semantic parser, and (b) modeling alignment between database columns and their mentions in a given query. We present a unified framework, based on the relation-aware self-attention mechanism, to address schema encoding, schema linking, and feature representation within a text-to-SQL encoder. On the challenging Spider dataset this framework boosts the exact match accuracy to 57.2%, surpassing its best counterparts by 8.7% absolute improvement. Further augmented with BERT, it achieves the new state-of-the-art performance of 65.6% on the Spider leaderboard. In addition, we observe qualitative improvements in the model’s understanding of schema linking and alignment. Our implementation will be open-sourced at https://github.com/Microsoft/rat-sql."
    }
  },
  {
    "id": "abstract-2020--acl-main--10",
    "result": [
      {
        "value": {
          "start": 477,
          "end": 514,
          "text": "Iterative Rectification Network (IRN)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--10:E0"
      },
      {
        "value": {
          "start": 909,
          "end": 930,
          "text": "slot error rate (ERR)",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--10:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--10:E0",
        "to_id": "abstract-2020--acl-main--10:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Data-driven approaches using neural networks have achieved promising performances in natural language generation (NLG). However, neural generators are prone to make mistakes, e.g., neglecting an input slot value and generating a redundant slot value. Prior works refer this to hallucination phenomenon. In this paper, we study slot consistency for building reliable NLG systems with all slot values of input dialogue act (DA) properly generated in output sentences. We propose Iterative Rectification Network (IRN) for improving general NLG systems to produce both correct and fluent responses. It applies a bootstrapping algorithm to sample training candidates and uses reinforcement learning to incorporate discrete reward related to slot inconsistency into training. Comprehensive studies have been conducted on multiple benchmark datasets, showing that the proposed methods have significantly reduced the slot error rate (ERR) for all strong baselines. Human evaluations also have confirmed its effectiveness."
    }
  },
  {
    "id": "abstract-2020--acl-main--17",
    "result": [
      {
        "value": {
          "start": 743,
          "end": 753,
          "text": "FactEditor",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--17:E0"
      },
      {
        "value": {
          "start": 1062,
          "end": 1070,
          "text": "fidelity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--17:E1"
      },
      {
        "value": {
          "start": 1075,
          "end": 1082,
          "text": "fluency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--17:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--17:E0",
        "to_id": "abstract-2020--acl-main--17:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--17:E0",
        "to_id": "abstract-2020--acl-main--17:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We propose a novel text editing task, referred to as  fact-based text editing , in which the goal is to revise a given document to better describe the facts in a knowledge base (e.g., several triples). The task is important in practice because reflecting the truth is a common requirement in text editing. First, we propose a method for automatically generating a dataset for research on fact-based text editing, where each instance consists of a draft text, a revised text, and several facts represented in triples. We apply the method into two public table-to-text datasets, obtaining two new datasets consisting of 233k and 37k instances, respectively. Next, we propose a new neural network architecture for fact-based text editing, called FactEditor, which edits a draft text by referring to given facts using a buffer, a stream, and a memory. A straightforward approach to address the problem would be to employ an encoder-decoder model. Our experimental results on the two datasets show that FactEditor outperforms the encoder-decoder approach in terms of fidelity and fluency. The results also show that FactEditor conducts inference faster than the encoder-decoder approach."
    }
  },
  {
    "id": "abstract-2020--acl-main--770",
    "result": [
      {
        "value": {
          "start": 767,
          "end": 803,
          "text": "introducing a novel debiasing method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--770:E0"
      },
      {
        "value": {
          "start": 296,
          "end": 307,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--770:E1"
      },
      {
        "value": {
          "start": 812,
          "end": 837,
          "text": "confidence regularization",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--770:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--770:E0",
        "to_id": "abstract-2020--acl-main--770:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--770:E2",
        "to_id": "abstract-2020--acl-main--770:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Models for natural language understanding (NLU) tasks often rely on the idiosyncratic biases of the dataset, which make them brittle against test cases outside the training distribution. Recently, several proposed debiasing methods are shown to be very effective in improving out-of-distribution performance. However, their improvements come at the expense of performance drop when models are evaluated on the in-distribution data, which contain examples with higher diversity. This seemingly inevitable trade-off may not tell us much about the changes in the reasoning and understanding capabilities of the resulting models on broader types of examples beyond the small subset represented in the out-of-distribution data. In this paper, we address this trade-off by introducing a novel debiasing method, called confidence regularization, which discourage models from exploiting biases while enabling them to receive enough incentive to learn from all the training examples. We evaluate our method on three NLU tasks and show that, in contrast to its predecessors, it improves the performance on out-of-distribution datasets (e.g., 7pp gain on HANS dataset) while maintaining the original in-distribution accuracy."
    }
  },
  {
    "id": "abstract-2020--acl-main--224",
    "result": [
      {
        "value": {
          "start": 542,
          "end": 555,
          "text": "dual encoding",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--224:E0"
      },
      {
        "value": {
          "start": 796,
          "end": 825,
          "text": "quality of the generated text",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--224:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--224:E0",
        "to_id": "abstract-2020--acl-main--224:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Generating sequential natural language descriptions from graph-structured data (e.g., knowledge graph) is challenging, partly because of the structural differences between the input graph and the output text. Hence, popular sequence-to-sequence models, which require serialized input, are not a natural fit for this task. Graph neural networks, on the other hand, can better encode the input graph but broaden the structural gap between the encoder and decoder, making faithful generation difficult. To narrow this gap, we propose DualEnc, a dual encoding model that can not only incorporate the graph structure, but can also cater to the linear structure of the output text. Empirical comparisons with strong single-encoder baselines demonstrate that dual encoding can significantly improve the quality of the generated text."
    }
  },
  {
    "id": "P10-1038",
    "result": [
      {
        "value": {
          "start": 352,
          "end": 377,
          "text": "conditional random fields",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1038:E0"
      },
      {
        "value": {
          "start": 483,
          "end": 494,
          "text": "error rates",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1038:E1"
      },
      {
        "from_id": "P10-1038:E0",
        "to_id": "P10-1038:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Finding allowable places in words to insert hyphens is an important practical problem. The algorithm that is used most often nowadays has remained essentially unchanged for 25 years. This method is the TEX hyphenation algorithm of Knuth and Liang. We present here a hyphenation method that is clearly more accurate. The new method is an application of conditional random fields. We create new training sets for English and Dutch from the CELEX European lexical resource, and achieve error rates for English of less than 0.1% for correctly allowed hyphens, and less than 0.01% for Dutch. Experiments show that both the Knuth/Liang method and a leading current commercial alternative have error rates several times higher for both languages."
    }
  },
  {
    "id": "abstract-2020--acl-main--735",
    "result": [
      {
        "value": {
          "start": 1245,
          "end": 1293,
          "text": "two-way attentions for joint CWS and POS tagging",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--735:E0"
      },
      {
        "value": {
          "start": 1318,
          "end": 1329,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--735:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--735:E0",
        "to_id": "abstract-2020--acl-main--735:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Chinese word segmentation (CWS) and part-of-speech (POS) tagging are important fundamental tasks for Chinese language processing, where joint learning of them is an effective one-step solution for both tasks. Previous studies for joint CWS and POS tagging mainly follow the character-based tagging paradigm with introducing contextual information such as n-gram features or sentential representations from recurrent neural models. However, for many cases, the joint tagging needs not only modeling from context features but also knowledge attached to them (e.g., syntactic relations among words); limited efforts have been made by existing research to meet such needs. In this paper, we propose a neural model named TwASP for joint CWS and POS tagging following the character-based sequence labeling paradigm, where a two-way attention mechanism is used to incorporate both context feature and their corresponding syntactic knowledge for each input character. Particularly, we use existing language processing toolkits to obtain the auto-analyzed syntactic knowledge for the context, and the proposed attention module can learn and benefit from them although their quality may not be perfect. Our experiments illustrate the effectiveness of the two-way attentions for joint CWS and POS tagging, where state-of-the-art performance is achieved on five benchmark datasets."
    }
  },
  {
    "id": "abstract-2020--acl-main--68",
    "result": [
      {
        "value": {
          "start": 1146,
          "end": 1184,
          "text": "pre-training and fine-tuning framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--68:E0"
      },
      {
        "value": {
          "start": 1220,
          "end": 1238,
          "text": "generation quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--68:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--68:E0",
        "to_id": "abstract-2020--acl-main--68:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Neural text generation has made tremendous progress in various tasks. One common characteristic of most of the tasks is that the texts are not restricted to some rigid formats when generating. However, we may confront some special text paradigms such as Lyrics (assume the music score is given), Sonnet, SongCi (classical Chinese poetry of the Song dynasty), etc. The typical characteristics of these texts are in three folds: (1) They must comply fully with the rigid predefined formats. (2) They must obey some rhyming schemes. (3) Although they are restricted to some formats, the sentence integrity must be guaranteed. To the best of our knowledge, text generation based on the predefined rigid formats has not been well investigated. Therefore, we propose a simple and elegant framework named SongNet to tackle this problem. The backbone of the framework is a Transformer-based auto-regressive language model. Sets of symbols are tailor-designed to improve the modeling performance especially on format, rhyme, and sentence integrity. We improve the attention mechanism to impel the model to capture some future information on the format. A pre-training and fine-tuning framework is designed to further improve the generation quality. Extensive experiments conducted on two collected corpora demonstrate that our proposed framework generates significantly better results in terms of both automatic metrics and the human evaluation."
    }
  },
  {
    "id": "abstract-2021--acl-long--203",
    "result": [
      {
        "value": {
          "start": 691,
          "end": 709,
          "text": "unified MMIN model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--203:E0"
      },
      {
        "value": {
          "start": 65,
          "end": 76,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--203:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--203:E0",
        "to_id": "abstract-2021--acl-long--203:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Multimodal fusion has been proved to improve emotion recognition performance in previous works. However, in real-world applications, we often encounter the problem of missing modality, and which modalities will be missing is uncertain. It makes the fixed multimodal fusion fail in such cases. In this work, we propose a unified model, Missing Modality Imagination Network (MMIN), to deal with the uncertain missing modality problem. MMIN learns robust joint multimodal representations, which can predict the representation of any missing modality given available modalities under different missing modality conditions.Comprehensive experiments on two benchmark datasets demonstrate that the unified MMIN model significantly improves emotion recognition performance under both uncertain missing-modality testing conditions and full-modality ideal testing condition. The code will be available at https://github.com/AIM3-RUC/MMIN."
    }
  },
  {
    "id": "abstract-2020--acl-main--80",
    "result": [
      {
        "value": {
          "start": 365,
          "end": 369,
          "text": "BALM",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--80:E0"
      },
      {
        "value": {
          "start": 715,
          "end": 725,
          "text": "perplexity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--80:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--80:E0",
        "to_id": "abstract-2020--acl-main--80:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Language modeling is the technique to estimate the probability of a sequence of words. A bilingual language model is expected to model the sequential dependency for words across languages, which is difficult due to the inherent lack of suitable training data as well as diverse syntactic structure across languages. We propose a bilingual attention language model (BALM) that simultaneously performs language modeling objective with a quasi-translation objective to model both the monolingual as well as the cross-lingual sequential dependency. The attention mechanism learns the bilingual context from a parallel corpus. BALM achieves state-of-the-art performance on the SEAME code-switch database by reducing the perplexity of 20.5% over the best-reported result. We also apply BALM in bilingual lexicon induction, and language normalization tasks to validate the idea."
    }
  },
  {
    "id": "abstract-2020--acl-main--174",
    "result": [
      {
        "value": {
          "start": 464,
          "end": 587,
          "text": "incorporate the underlying structure of narratives into general unsupervised and supervised extractive summarization models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--174:E0"
      },
      {
        "value": {
          "start": 998,
          "end": 1009,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--174:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--174:E0",
        "to_id": "abstract-2020--acl-main--174:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Most general-purpose extractive summarization models are trained on news articles, which are short and present all important information upfront. As a result, such models are biased on position and often perform a smart selection of sentences from the beginning of the document. When summarizing long narratives, which have complex structure and present information piecemeal, simple position heuristics are not sufficient. In this paper, we propose to explicitly incorporate the underlying structure of narratives into general unsupervised and supervised extractive summarization models. We formalize narrative structure in terms of key narrative events (turning points) and treat it as latent in order to summarize screenplays (i.e., extract an optimal sequence of scenes). Experimental results on the CSI corpus of TV screenplays, which we augment with scene-level summarization labels, show that latent turning points correlate with important aspects of a CSI episode and improve summarization performance over general extractive algorithms leading to more complete and diverse summaries."
    }
  },
  {
    "id": "abstract-2020--acl-main--729",
    "result": [
      {
        "value": {
          "start": 496,
          "end": 507,
          "text": "Transformer",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--729:E0"
      },
      {
        "value": {
          "start": 805,
          "end": 813,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--729:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--729:E0",
        "to_id": "abstract-2020--acl-main--729:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We present a new problem: grounding natural language instructions to mobile user interface actions, and create three new datasets for it. For full task evaluation, we create PixelHelp, a corpus that pairs English instructions with actions performed by people on a mobile UI emulator. To scale training, we decouple the language and action data by (a) annotating action phrase spans in How-To instructions and (b) synthesizing grounded descriptions of actions for mobile user interfaces. We use a Transformer to extract action phrase tuples from long-range natural language instructions. A grounding Transformer then contextually represents UI objects using both their content and screen position and connects them to object descriptions. Given a starting screen and instruction, our model achieves 70.59% accuracy on predicting complete ground-truth action sequences in PixelHelp."
    }
  },
  {
    "id": "abstract-2021--acl-long--371",
    "result": [
      {
        "value": {
          "start": 388,
          "end": 417,
          "text": "Structural Causal Model (SCM)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--371:E0"
      },
      {
        "value": {
          "start": 975,
          "end": 986,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--371:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--371:E0",
        "to_id": "abstract-2021--acl-long--371:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Distant supervision tackles the data bottleneck in NER by automatically generating training instances via dictionary matching. Unfortunately, the learning of DS-NER is severely dictionary-biased, which suffers from spurious correlations and therefore undermines the effectiveness and the robustness of the learned models. In this paper, we fundamentally explain the dictionary bias via a Structural Causal Model (SCM), categorize the bias into intra-dictionary and inter-dictionary biases, and identify their causes. Based on the SCM, we learn de-biased DS-NER via causal interventions. For intra-dictionary bias, we conduct backdoor adjustment to remove the spurious correlations introduced by the dictionary confounder. For inter-dictionary bias, we propose a causal invariance regularizer which will make DS-NER models more robust to the perturbation of dictionaries. Experiments on four datasets and three DS-NER models show that our method can significantly improve the performance of DS-NER."
    }
  },
  {
    "id": "P11-1003",
    "result": [
      {
        "value": {
          "start": 37,
          "end": 112,
          "text": "effective usage of function words to generate generalized translation rules",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1003:E0"
      },
      {
        "value": {
          "start": 866,
          "end": 876,
          "text": "BLEU score",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1003:E1"
      },
      {
        "from_id": "P11-1003:E0",
        "to_id": "P11-1003:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In the present paper, we propose the effective usage of function words to generate generalized translation rules for forest-based translation. Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. In order to constrain the exhaustive attachments of function words, we limit to bind them to the nearby syntactic chunks yielded by a target dependency parser. Therefore, the proposed approach can not only capture source-tree-to-target-chunk correspondences but can also use forest structures that compactly encode an exponential number of parse trees to properly generate target function words during decoding. Extensive experiments involving large-scale English-to-Japanese translation revealed a significant improvement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system."
    }
  },
  {
    "id": "P11-1055",
    "result": [
      {
        "value": {
          "start": 648,
          "end": 719,
          "text": "a novel approach for multi-instance learning with overlapping relations",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1055:E0"
      },
      {
        "value": {
          "start": 1019,
          "end": 1027,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1055:E1"
      },
      {
        "from_id": "P11-1055:E0",
        "to_id": "P11-1055:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Information extraction (IE) holds the promise of generating a large-scale knowledge base from the Web's natural language text. Knowledge-based weak supervision, using structured data to heuristically label a training corpus, works towards this goal by enabling the automated learning of a potentially unbounded number of relation extractors. Recently, researchers have developed multi-instance learning algorithms to combat the noisy training data that can come from heuristic labeling, but their models assume relations are disjoint --- for example they cannot extract the pair Founded(Jobs, Apple) and CEO-of(Jobs, Apple). \n \nThis paper presents a novel approach for multi-instance learning with overlapping relations that combines a sentence-level extraction model with a simple, corpus-level component for aggregating the individual facts. We apply our model to learn extractors for NY Times text using weak supervision from Free-base. Experiments show that the approach runs quickly and yields surprising gains in accuracy, at both the aggregate and sentence level."
    }
  },
  {
    "id": "abstract-2021--acl-long--73",
    "result": [
      {
        "value": {
          "start": 273,
          "end": 308,
          "text": "cross-lingual pre-training approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--73:E0"
      },
      {
        "value": {
          "start": 1201,
          "end": 1210,
          "text": "Smatch F1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--73:E1"
      },
      {
        "value": {
          "start": 1369,
          "end": 1373,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--73:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--73:E0",
        "to_id": "abstract-2021--acl-long--73:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--73:E0",
        "to_id": "abstract-2021--acl-long--73:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Due to the scarcity of annotated data, Abstract Meaning Representation (AMR) research is relatively limited and challenging for languages other than English. Upon the availability of English AMR dataset and English-to- X parallel datasets, in this paper we propose a novel cross-lingual pre-training approach via multi-task learning (MTL) for both zeroshot AMR parsing and AMR-to-text generation. Specifically, we consider three types of relevant tasks, including AMR parsing, AMR-to-text generation, and machine translation. We hope that knowledge gained while learning for English AMR parsing and text generation can be transferred to the counterparts of other languages. With properly pretrained models, we explore four different finetuning methods, i.e., vanilla fine-tuning with a single task, one-for-all MTL fine-tuning, targeted MTL fine-tuning, and teacher-studentbased MTL fine-tuning. Experimental results on AMR parsing and text generation of multiple non-English languages demonstrate that our approach significantly outperforms a strong baseline of pre-training approach, and greatly advances the state of the art. In detail, on LDC2020T07 we have achieved 70.45%, 71.76%, and 70.80% in Smatch F1 for AMR parsing of German, Spanish, and Italian, respectively, while for AMR-to-text generation of the languages, we have obtained 25.69, 31.36, and 28.42 in BLEU respectively. We make our code available on github https://github.com/xdqkid/XLPT-AMR."
    }
  },
  {
    "id": "abstract-2021--acl-long--140",
    "result": [
      {
        "value": {
          "start": 904,
          "end": 939,
          "text": "multi-stage computational framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--140:E0"
      },
      {
        "value": {
          "start": 1313,
          "end": 1322,
          "text": "F1-scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--140:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--140:E0",
        "to_id": "abstract-2021--acl-long--140:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Weak supervision has shown promising results in many natural language processing tasks, such as Named Entity Recognition (NER). Existing work mainly focuses on learning deep NER models only with weak supervision, i.e., without any human annotation, and shows that by merely using weakly labeled data, one can achieve good performance, though still underperforms fully supervised NER with manually/strongly labeled data. In this paper, we consider a more practical scenario, where we have both a small amount of strongly labeled data and a large amount of weakly labeled data. Unfortunately, we observe that weakly labeled data does not necessarily improve, or even deteriorate the model performance (due to the extensive noise in the weak labels) when we train deep NER models over a simple or weighted combination of the strongly labeled and weakly labeled data. To address this issue, we propose a new multi-stage computational framework – NEEDLE with three essential ingredients: (1) weak label completion, (2) noise-aware loss function, and (3) final fine-tuning over the strongly labeled data. Through experiments on E-commerce query NER and Biomedical NER, we demonstrate that NEEDLE can effectively suppress the noise of the weak labels and outperforms existing methods. In particular, we achieve new SOTA F1-scores on 3 Biomedical NER datasets: BC5CDR-chem 93.74, BC5CDR-disease 90.69, NCBI-disease 92.28."
    }
  },
  {
    "id": "abstract-2020--acl-main--549",
    "result": [
      {
        "value": {
          "start": 1133,
          "end": 1155,
          "text": "graph-based mechanisms",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--549:E0"
      },
      {
        "value": {
          "start": 1168,
          "end": 1176,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--549:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--549:E0",
        "to_id": "abstract-2020--acl-main--549:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Fact checking is a challenging task because verifying the truthfulness of a claim requires reasoning about multiple retrievable evidence. In this work, we present a method suitable for reasoning about the semantic-level structure of evidence. Unlike most previous works, which typically represent evidence sentences with either string concatenation or fusing the features of isolated evidence sentences, our approach operates on rich semantic structures of evidence obtained by semantic role labeling. We propose two mechanisms to exploit the structure of evidence while leveraging the advances of pre-trained models like BERT, GPT or XLNet. Specifically, using XLNet as the backbone, we first utilize the graph structure to re-define the relative distances of words, with the intuition that semantically related words should have short distances. Then, we adopt graph convolutional network and graph attention network to propagate and aggregate information from neighboring nodes on the graph. We evaluate our system on FEVER, a benchmark dataset for fact checking, and find that rich structural information is helpful and both our graph-based mechanisms improve the accuracy. Our model is the state-of-the-art system in terms of both official evaluation metrics, namely claim verification accuracy and FEVER score."
    }
  },
  {
    "id": "P10-1157",
    "result": [
      {
        "value": {
          "start": 733,
          "end": 770,
          "text": "using certainty-based active learning",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1157:E0"
      },
      {
        "value": {
          "start": 673,
          "end": 684,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1157:E1"
      },
      {
        "from_id": "P10-1157:E0",
        "to_id": "P10-1157:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Most previous work on trainable language generation has focused on two paradigms: (a) using a statistical model to rank a set of generated utterances, or (b) using statistics to inform the generation decision process. Both approaches rely on the existence of a handcrafted generator, which limits their scalability to new domains. This paper presents Bagel, a statistical language generator which uses dynamic Bayesian networks to learn from semantically-aligned data produced by 42 untrained annotators. A human evaluation shows that Bagel can generate natural and informative utterances from unseen inputs in the information presentation domain. Additionally, generation performance on sparse datasets is improved significantly by using certainty-based active learning, yielding ratings close to the human gold standard with a fraction of the data."
    }
  },
  {
    "id": "abstract-2021--acl-long--121",
    "result": [
      {
        "value": {
          "start": 499,
          "end": 554,
          "text": "Multi-metadata Embedding based Cross-Transformer (MECT)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--121:E0"
      },
      {
        "value": {
          "start": 570,
          "end": 581,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--121:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--121:E0",
        "to_id": "abstract-2021--acl-long--121:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recently, word enhancement has become very popular for Chinese Named Entity Recognition (NER), reducing segmentation errors and increasing the semantic and boundary information of Chinese words. However, these methods tend to ignore the information of the Chinese character structure after integrating the lexical information. Chinese characters have evolved from pictographs since ancient times, and their structure often reflects more information about the characters. This paper presents a novel Multi-metadata Embedding based Cross-Transformer (MECT) to improve the performance of Chinese NER by fusing the structural information of Chinese characters. Specifically, we use multi-metadata embedding in a two-stream Transformer to integrate Chinese character features with the radical-level embedding. With the structural characteristics of Chinese characters, MECT can better capture the semantic information of Chinese characters for NER. The experimental results obtained on several well-known benchmarking datasets demonstrate the merits and superiority of the proposed MECT method."
    }
  },
  {
    "id": "P10-1025",
    "result": [
      {
        "value": {
          "start": 366,
          "end": 428,
          "text": "encapsulation of a distributional model of semantic similarity",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1025:E0"
      },
      {
        "value": {
          "start": 628,
          "end": 636,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1025:E1"
      },
      {
        "from_id": "P10-1025:E0",
        "to_id": "P10-1025:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Current Semantic Role Labeling technologies are based on inductive algorithms trained over large scale repositories of annotated examples. Frame-based systems currently make use of the FrameNet database but fail to show suitable generalization capabilities in out-of-domain scenarios. In this paper, a state-of-art system for frame-based SRL is extended through the encapsulation of a distributional model of semantic similarity. The resulting argument classification model promotes a simpler feature space that limits the potential overfitting effects. The large scale empirical study here discussed confirms that state-of-art accuracy can be obtained for out-of-domain evaluations."
    }
  },
  {
    "id": "abstract-2020--acl-main--651",
    "result": [
      {
        "value": {
          "start": 316,
          "end": 367,
          "text": "bootstrapping framework (based on self-supervision)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--651:E0"
      },
      {
        "value": {
          "start": 678,
          "end": 687,
          "text": "precision",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--651:E1"
      },
      {
        "value": {
          "start": 738,
          "end": 744,
          "text": "recall",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--651:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--651:E0",
        "to_id": "abstract-2020--acl-main--651:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--651:E0",
        "to_id": "abstract-2020--acl-main--651:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Question answering and conversational systems are often baffled and need help clarifying certain ambiguities. However, limitations of existing datasets hinder the development of large-scale models capable of generating and utilising clarification questions. In order to overcome these limitations, we devise a novel bootstrapping framework (based on self-supervision) that assists in the creation of a diverse, large-scale dataset of clarification questions based on post-comment tuples extracted from stackexchange. The framework utilises a neural network based architecture for classifying clarification questions. It is a two-step method where the first aims to increase the precision of the classifier and second aims to increase its recall. We quantitatively demonstrate the utility of the newly created dataset by applying it to the downstream task of question-answering. The final dataset, ClarQ, consists of ~2M examples distributed across 173 domains of stackexchange. We release this dataset in order to foster research into the field of clarification question generation with the larger goal of enhancing dialog and question answering systems."
    }
  },
  {
    "id": "abstract-2020--acl-main--766",
    "result": [
      {
        "value": {
          "start": 380,
          "end": 392,
          "text": "hub language",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--766:E0"
      },
      {
        "value": {
          "start": 469,
          "end": 480,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--766:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--766:E0",
        "to_id": "abstract-2020--acl-main--766:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Most of recent work in cross-lingual word embeddings is severely Anglocentric. The vast majority of lexicon induction evaluation dictionaries are between English and another language, and the English embedding space is selected by default as the hub when learning in a multilingual setting. With this work, however, we challenge these practices. First, we show that the choice of hub language can significantly impact downstream lexicon induction zero-shot POS tagging performance. Second, we both expand a standard English-centered evaluation dictionary collection to include all language pairs using triangulation, and create new dictionaries for under-represented languages. Evaluating established methods over all these language pairs sheds light into their suitability for aligning embeddings from distant languages and presents new challenges for the field. Finally, in our analysis we identify general guidelines for strong cross-lingual embedding baselines, that extend to language pairs that do not include English."
    }
  },
  {
    "id": "abstract-2020--acl-main--201",
    "result": [
      {
        "value": {
          "start": 512,
          "end": 539,
          "text": "simple post-processing step",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--201:E0"
      },
      {
        "value": {
          "start": 555,
          "end": 563,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--201:E1"
      },
      {
        "value": {
          "start": 606,
          "end": 623,
          "text": "BLI test accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--201:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--201:E0",
        "to_id": "abstract-2020--acl-main--201:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--201:E0",
        "to_id": "abstract-2020--acl-main--201:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Cross-lingual word embeddings (CLWE) are often evaluated on bilingual lexicon induction (BLI). Recent CLWE methods use linear projections, which underfit the training dictionary, to generalize on BLI. However, underfitting can hinder generalization to other downstream tasks that rely on words from the training dictionary. We address this limitation by retrofitting CLWE to the training dictionary, which pulls training translation pairs closer in the embedding space and overfits the training dictionary. This simple post-processing step often improves accuracy on two downstream tasks, despite lowering BLI test accuracy. We also retrofit to both the training dictionary and a synthetic dictionary induced from CLWE, which sometimes generalizes even better on downstream tasks. Our results confirm the importance of fully exploiting training dictionary in downstream tasks and explains why BLI is a flawed CLWE evaluation."
    }
  },
  {
    "id": "abstract-2020--acl-main--244",
    "result": [
      {
        "value": {
          "start": 784,
          "end": 813,
          "text": "more diverse pretraining data",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--244:E0"
      },
      {
        "value": {
          "start": 250,
          "end": 260,
          "text": "robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--244:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--244:E0",
        "to_id": "abstract-2020--acl-main--244:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Although pretrained Transformers such as BERT achieve high accuracy on in-distribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers’ performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness."
    }
  },
  {
    "id": "abstract-2021--acl-long--246",
    "result": [
      {
        "value": {
          "start": 389,
          "end": 434,
          "text": "TM-based NMT within the Transformer framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--246:E0"
      },
      {
        "value": {
          "start": 812,
          "end": 816,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--246:E1"
      },
      {
        "value": {
          "start": 821,
          "end": 833,
          "text": "running time",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--246:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--246:E0",
        "to_id": "abstract-2021--acl-long--246:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--246:E0",
        "to_id": "abstract-2021--acl-long--246:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "It is generally believed that a translation memory (TM) should be beneficial for machine translation tasks. Unfortunately, existing wisdom demonstrates the superiority of TM-based neural machine translation (NMT) only on the TM-specialized translation tasks rather than general tasks, with a non-negligible computational overhead. In this paper, we propose a fast and accurate approach to TM-based NMT within the Transformer framework: the model architecture is simple and employs a single bilingual sentence as its TM, leading to efficient training and inference; and its parameters are effectively optimized through a novel training criterion. Extensive experiments on six TM-specialized tasks show that the proposed approach substantially surpasses several strong baselines that use multiple TMs, in terms of BLEU and running time. In particular, the proposed approach also advances the strong baselines on two general tasks (WMT news Zh->En and En->De)."
    }
  },
  {
    "id": "abstract-2021--acl-long--103",
    "result": [
      {
        "value": {
          "start": 463,
          "end": 517,
          "text": "attention weights assigned to the indispensable tokens",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--103:E0"
      },
      {
        "value": {
          "start": 553,
          "end": 564,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--103:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--103:E0",
        "to_id": "abstract-2021--acl-long--103:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Attention mechanisms have achieved substantial improvements in neural machine translation by dynamically selecting relevant inputs for different predictions. However, recent studies have questioned the attention mechanisms’ capability for discovering decisive inputs. In this paper, we propose to calibrate the attention weights by introducing a mask perturbation model that automatically evaluates each input’s contribution to the model outputs. We increase the attention weights assigned to the indispensable tokens, whose removal leads to a dramatic performance decrease. The extensive experiments on the Transformer-based translation have demonstrated the effectiveness of our model. We further find that the calibrated attention weights are more uniform at lower layers to collect multiple information while more concentrated on the specific inputs at higher layers. Detailed analyses also show a great need for calibration in the attention weights with high entropy where the model is unconfident about its decision."
    }
  },
  {
    "id": "abstract-2021--acl-long--228",
    "result": [
      {
        "value": {
          "start": 1235,
          "end": 1246,
          "text": "KD paradigm",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--228:E0"
      },
      {
        "value": {
          "start": 1443,
          "end": 1450,
          "text": "speedup",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--228:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--228:E0",
        "to_id": "abstract-2021--acl-long--228:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recently, knowledge distillation (KD) has shown great success in BERT compression. Instead of only learning from the teacher’s soft label as in conventional KD, researchers find that the rich information contained in the hidden layers of BERT is conducive to the student’s performance. To better exploit the hidden knowledge, a common practice is to force the student to deeply mimic the teacher’s hidden states of all the tokens in a layer-wise manner. In this paper, however, we observe that although distilling the teacher’s hidden state knowledge (HSK) is helpful, the performance gain (marginal utility) diminishes quickly as more HSK is distilled. To understand this effect, we conduct a series of analysis. Specifically, we divide the HSK of BERT into three dimensions, namely depth, length and width. We first investigate a variety of strategies to extract crucial knowledge for each single dimension and then jointly compress the three dimensions. In this way, we show that 1) the student’s performance can be improved by extracting and distilling the crucial HSK, and 2) using a tiny fraction of HSK can achieve the same performance as extensive HSK distillation. Based on the second finding, we further propose an efficient KD paradigm to compress BERT, which does not require loading the teacher during the training of student. For two kinds of student models and computing devices, the proposed KD paradigm gives rise to training speedup of 2.7x 3.4x."
    }
  },
  {
    "id": "abstract-2020--acl-main--298",
    "result": [
      {
        "value": {
          "start": 409,
          "end": 412,
          "text": "TSP",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--298:E0"
      },
      {
        "value": {
          "start": 650,
          "end": 661,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--298:E1"
      },
      {
        "value": {
          "start": 515,
          "end": 519,
          "text": "PLBA",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--298:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--298:E0",
        "to_id": "abstract-2020--acl-main--298:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--298:E2",
        "to_id": "abstract-2020--acl-main--298:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "State-of-the-art argument mining studies have advanced the techniques for predicting argument structures. However, the technology for capturing non-tree-structured arguments is still in its infancy. In this paper, we focus on non-tree argument mining with a neural model. We jointly predict proposition types and edges between propositions. Our proposed model incorporates (i) task-specific parameterization (TSP) that effectively encodes a sequence of propositions and (ii) a proposition-level biaffine attention (PLBA) that can predict a non-tree argument consisting of edges. Experimental results show that both TSP and PLBA boost edge prediction performance compared to baselines."
    }
  },
  {
    "id": "abstract-2020--acl-main--359",
    "result": [
      {
        "value": {
          "start": 1110,
          "end": 1136,
          "text": "availing of data selection",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--359:E0"
      },
      {
        "value": {
          "start": 1156,
          "end": 1167,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--359:E1"
      },
      {
        "value": {
          "start": 1025,
          "end": 1081,
          "text": "incorporating backtranslated data from different sources",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--359:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--359:E0",
        "to_id": "abstract-2020--acl-main--359:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--359:E2",
        "to_id": "abstract-2020--acl-main--359:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Machine translation (MT) has benefited from using synthetic training data originating from translating monolingual corpora, a technique known as backtranslation. Combining backtranslated data from different sources has led to better results than when using such data in isolation. In this work we analyse the impact that data translated with rule-based, phrase-based statistical and neural MT systems has on new MT systems. We use a real-world low-resource use-case (Basque-to-Spanish in the clinical domain) as well as a high-resource language pair (German-to-English) to test different scenarios with backtranslation and employ data selection to optimise the synthetic corpora. We exploit different data selection strategies in order to reduce the amount of data used, while at the same time maintaining high-quality MT systems. We further tune the data selection method by taking into account the quality of the MT systems used for backtranslation and lexical diversity of the resulting corpora. Our experiments show that incorporating backtranslated data from different sources can be beneficial, and that availing of data selection can yield improved performance."
    }
  },
  {
    "id": "abstract-2020--acl-main--185",
    "result": [
      {
        "value": {
          "start": 513,
          "end": 530,
          "text": "negative training",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--185:E0"
      },
      {
        "value": {
          "start": 639,
          "end": 670,
          "text": "hit rate of malicious responses",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--185:E1"
      },
      {
        "value": {
          "start": 717,
          "end": 735,
          "text": "response diversity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--185:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--185:E0",
        "to_id": "abstract-2020--acl-main--185:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--185:E0",
        "to_id": "abstract-2020--acl-main--185:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Although deep learning models have brought tremendous advancements to the field of open-domain dialogue response generation, recent research results have revealed that the trained models have undesirable generation behaviors, such as malicious responses and generic (boring) responses. In this work, we propose a framework named “Negative Training” to minimize such behaviors. Given a trained model, the framework will first find generated samples that exhibit the undesirable behavior, and then use them to feed negative training signals for fine-tuning the model. Our experiments show that negative training can significantly reduce the hit rate of malicious responses, or discourage frequent responses and improve response diversity."
    }
  },
  {
    "id": "abstract-2020--acl-main--684",
    "result": [
      {
        "value": {
          "start": 653,
          "end": 664,
          "text": "Our methods",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--684:E0"
      },
      {
        "value": {
          "start": 677,
          "end": 698,
          "text": "task completion rates",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--684:E1"
      },
      {
        "value": {
          "start": 294,
          "end": 406,
          "text": "leveraging the idea of inverse semantics from program synthesis to reason backwards from observed demonstrations",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--684:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--684:E0",
        "to_id": "abstract-2020--acl-main--684:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--684:E2",
        "to_id": "abstract-2020--acl-main--684:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We explore learning web-based tasks from a human teacher through natural language explanations and a single demonstration. Our approach investigates a new direction for semantic parsing that models explaining a demonstration in a context, rather than mapping explanations to demonstrations. By leveraging the idea of inverse semantics from program synthesis to reason backwards from observed demonstrations, we ensure that all considered interpretations are consistent with executable actions in any context, thus simplifying the problem of search over logical forms. We present a dataset of explanations paired with demonstrations for web-based tasks. Our methods show better task completion rates than a supervised semantic parsing baseline (40% relative improvement on average), and are competitive with simple exploration-and-demonstration based methods, while requiring no exploration of the environment. In learning to align explanations with demonstrations, basic properties of natural language syntax emerge as learned behavior. This is an interesting example of pragmatic language acquisition without any linguistic annotation."
    }
  },
  {
    "id": "abstract-2021--acl-long--305",
    "result": [
      {
        "value": {
          "start": 936,
          "end": 990,
          "text": "an adversarial bot between gold and fake tree diagrams",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--305:E0"
      },
      {
        "value": {
          "start": 247,
          "end": 258,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--305:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--305:E0",
        "to_id": "abstract-2021--acl-long--305:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Text-level discourse rhetorical structure (DRS) parsing is known to be challenging due to the notorious lack of training data. Although recent top-down DRS parsers can better leverage global document context and have achieved certain success, the performance is still far from perfect. To our knowledge, all previous DRS parsers make local decisions for either bottom-up node composition or top-down split point ranking at each time step, and largely ignore DRS parsing from the global view point. Obviously, it is not sufficient to build an entire DRS tree only through these local decisions. In this work, we present our insight on evaluating the pros and cons of the entire DRS tree for global optimization. Specifically, based on recent well-performing top-down frameworks, we introduce a novel method to transform both gold standard and predicted constituency trees into tree diagrams with two color channels. After that, we learn an adversarial bot between gold and fake tree diagrams to estimate the generated DRS trees from a global perspective. We perform experiments on both RST-DT and CDTB corpora and use the original Parseval for performance evaluation. The experimental results show that our parser can substantially improve the performance when compared with previous state-of-the-art parsers."
    }
  },
  {
    "id": "abstract-2020--acl-main--242",
    "result": [
      {
        "value": {
          "start": 638,
          "end": 644,
          "text": "PosCal",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--242:E0"
      },
      {
        "value": {
          "start": 831,
          "end": 848,
          "text": "calibration error",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--242:E1"
      },
      {
        "value": {
          "start": 886,
          "end": 936,
          "text": "penalizing drops in performance of both objectives",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--242:E2"
      },
      {
        "value": {
          "start": 866,
          "end": 882,
          "text": "task performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--242:E3"
      },
      {
        "value": {
          "start": 638,
          "end": 644,
          "text": "PosCal",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--242:E4"
      },
      {
        "value": {
          "start": 871,
          "end": 882,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--242:E5"
      },
      {
        "value": {
          "start": 831,
          "end": 848,
          "text": "calibration error",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--242:E6"
      },
      {
        "from_id": "abstract-2020--acl-main--242:E0",
        "to_id": "abstract-2020--acl-main--242:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--242:E2",
        "to_id": "abstract-2020--acl-main--242:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--242:E4",
        "to_id": "abstract-2020--acl-main--242:E5",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--242:E4",
        "to_id": "abstract-2020--acl-main--242:E6",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Most classification models work by first predicting a posterior probability distribution over all classes and then selecting that class with the largest estimated probability. In many settings however, the quality of posterior probability itself (e.g., 65% chance having diabetes), gives more reliable information than the final predicted class alone. When these methods are shown to be poorly calibrated, most fixes to date have relied on posterior calibration, which rescales the predicted probabilities but often has little impact on final classifications. Here we propose an end-to-end training procedure called posterior calibrated (PosCal) training that directly optimizes the objective while minimizing the difference between the predicted and empirical posterior probabilities.We show that PosCal not only helps reduce the calibration error but also improve task performance by penalizing drops in performance of both objectives. Our PosCal achieves about 2.5% of task performance gain and 16.1% of calibration error reduction on GLUE (Wang et al., 2018) compared to the baseline. We achieved the comparable task performance with 13.2% calibration error reduction on xSLUE (Kang and Hovy, 2019), but not outperforming the two-stage calibration baseline. PosCal training can be easily extendable to any types of classification tasks as a form of regularization term. Also, PosCal has the advantage that it incrementally tracks needed statistics for the calibration objective during the training process, making efficient use of large training sets."
    }
  },
  {
    "id": "abstract-2021--acl-long--48",
    "result": [
      {
        "value": {
          "start": 678,
          "end": 682,
          "text": "COSY",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--48:E0"
      },
      {
        "value": {
          "start": 137,
          "end": 148,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--48:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--48:E0",
        "to_id": "abstract-2021--acl-long--48:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Pre-trained multilingual language models, e.g., multilingual-BERT, are widely used in cross-lingual tasks, yielding the state-of-the-art performance. However, such models suffer from a large performance gap between source and target languages, especially in the zero-shot setting, where the models are fine-tuned only on English but tested on other languages for the same task. We tackle this issue by incorporating language-agnostic information, specifically, universal syntax such as dependency relations and POS tags, into language models, based on the observation that universal syntax is transferable across different languages. Our approach, called COunterfactual SYntax (COSY), includes the design of SYntax-aware networks as well as a COunterfactual training method to implicitly force the networks to learn not only the semantics but also the syntax. To evaluate COSY, we conduct cross-lingual experiments on natural language inference and question answering using mBERT and XLM-R as network backbones. Our results show that COSY achieves the state-of-the-art performance for both tasks, without using auxiliary training data."
    }
  },
  {
    "id": "abstract-2020--acl-main--217",
    "result": [
      {
        "value": {
          "start": 542,
          "end": 583,
          "text": "incorporate phone features into ST models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--217:E0"
      },
      {
        "value": {
          "start": 751,
          "end": 755,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--217:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--217:E0",
        "to_id": "abstract-2020--acl-main--217:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "End-to-end models for speech translation (ST) more tightly couple speech recognition (ASR) and machine translation (MT) than a traditional cascade of separate ASR and MT models, with simpler model architectures and the potential for reduced error propagation. Their performance is often assumed to be superior, though in many conditions this is not yet the case. We compare cascaded and end-to-end models across high, medium, and low-resource conditions, and show that cascades remain stronger baselines. Further, we introduce two methods to incorporate phone features into ST models. We show that these features improve both architectures, closing the gap between end-to-end models and cascades, and outperforming previous academic work – by up to 9 BLEU on our low-resource setting."
    }
  },
  {
    "id": "abstract-2020--acl-main--323",
    "result": [
      {
        "value": {
          "start": 940,
          "end": 957,
          "text": "sampling approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--323:E0"
      },
      {
        "value": {
          "start": 1272,
          "end": 1276,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--323:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--323:E0",
        "to_id": "abstract-2020--acl-main--323:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The choice of hyper-parameters affects the performance of neural models. While much previous research (Sutskever et al., 2013; Duchi et al., 2011; Kingma and Ba, 2015) focuses on accelerating convergence and reducing the effects of the learning rate, comparatively few papers concentrate on the effect of batch size. In this paper, we analyze how increasing batch size affects gradient direction, and propose to evaluate the stability of gradients with their angle change. Based on our observations, the angle change of gradient direction first tends to stabilize (i.e. gradually decrease) while accumulating mini-batches, and then starts to fluctuate. We propose to automatically and dynamically determine batch sizes by accumulating gradients of mini-batches and performing an optimization step at just the time when the direction of gradients starts to fluctuate. To improve the efficiency of our approach for large models, we propose a sampling approach to select gradients of parameters sensitive to the batch size. Our approach dynamically determines proper and efficient batch sizes during training. In our experiments on the WMT 14 English to German and English to French tasks, our approach improves the Transformer with a fixed 25k batch size by +0.73 and +0.82 BLEU respectively."
    }
  },
  {
    "id": "abstract-2021--acl-long--525",
    "result": [
      {
        "value": {
          "start": 891,
          "end": 929,
          "text": "incrementally learns latent structures",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--525:E0"
      },
      {
        "value": {
          "start": 1194,
          "end": 1202,
          "text": "F1 score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--525:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--525:E0",
        "to_id": "abstract-2021--acl-long--525:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Wet laboratory protocols (WLPs) are critical for conveying reproducible procedures in biological research. They are composed of instructions written in natural language describing the step-wise processing of materials by specific actions. This process flow description for reagents and materials synthesis in WLPs can be captured by material state transfer graphs (MSTGs), which encode global temporal and causal relationships between actions. Here, we propose methods to automatically generate a MSTG for a given protocol by extracting all action relationships across multiple sentences. We also note that previous corpora and methods focused primarily on local intra-sentence relationships between actions and entities and did not address two critical issues: (i) resolution of implicit arguments and (ii) establishing long-range dependencies across sentences. We propose a new model that incrementally learns latent structures and is better suited to resolving inter-sentence relations and implicit arguments. This model draws upon a new corpus WLP-MSTG which was created by extending annotations in the WLP corpora for inter-sentence relations and implicit arguments. Our model achieves an F1 score of 54.53% for temporal and causal relations in protocols from our corpus, which is a significant improvement over previous models - DyGIE++:28.17%; spERT:27.81%. We make our annotated WLP-MSTG corpus available to the research community."
    }
  },
  {
    "id": "P11-1105",
    "result": [
      {
        "value": {
          "start": 51,
          "end": 104,
          "text": "models translation by a linear sequence of operations",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1105:E0"
      },
      {
        "value": {
          "start": 615,
          "end": 619,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1105:E1"
      },
      {
        "from_id": "P11-1105:E0",
        "to_id": "P11-1105:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We present a novel machine translation model which models translation by a linear sequence of operations. In contrast to the \"N-gram\" model, this sequence includes not only translation but also reordering operations. Key ideas of our model are (i) a new reordering approach which better restricts the position to which a word or phrase can be moved, and is able to handle short and long distance re-orderings in a unified way, and (ii) a joint sequence model for the translation and reordering probabilities which is more flexible than standard phrase-based MT. We observe statistically significant improvements in BLEU over Moses for German-to-English and Spanish-to-English tasks, and comparable results for a French-to-English task."
    }
  },
  {
    "id": "abstract-2021--acl-long--546",
    "result": [
      {
        "value": {
          "start": 618,
          "end": 696,
          "text": "expressing the graph connections through restrictions on the attention weights",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--546:E0"
      },
      {
        "value": {
          "start": 766,
          "end": 784,
          "text": "space requirements",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--546:E1"
      },
      {
        "value": {
          "start": 817,
          "end": 826,
          "text": "precision",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--546:E2"
      },
      {
        "value": {
          "start": 860,
          "end": 881,
          "text": "perceived consistency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--546:E3"
      },
      {
        "from_id": "abstract-2021--acl-long--546:E0",
        "to_id": "abstract-2021--acl-long--546:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--546:E0",
        "to_id": "abstract-2021--acl-long--546:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--546:E0",
        "to_id": "abstract-2021--acl-long--546:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "To improve the coherence and knowledge retrieval capabilities of non-task-oriented dialogue systems, recent Transformer-based models aim to integrate fixed background context. This often comes in the form of knowledge graphs, and the integration is done by creating pseudo utterances through paraphrasing knowledge triples, added into the accumulated dialogue context. However, the context length is fixed in these architectures, which restricts how much background or dialogue context can be kept. In this work, we propose a more concise encoding for background context structured in the form of knowledge graphs, by expressing the graph connections through restrictions on the attention weights. The results of our human evaluation show that this encoding reduces space requirements without negative effects on the precision of reproduction of knowledge and perceived consistency. Further, models trained with our proposed context encoding generate dialogues that are judged to be more comprehensive and interesting."
    }
  },
  {
    "id": "P10-1124",
    "result": [
      {
        "value": {
          "start": 346,
          "end": 385,
          "text": "motivate this graph with an application",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1124:E0"
      },
      {
        "value": {
          "start": 525,
          "end": 536,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1124:E1"
      },
      {
        "from_id": "P10-1124:E0",
        "to_id": "P10-1124:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We propose a global algorithm for learning entailment relations between predicates. We define a graph structure over predicates that represents entailment relations as directed edges, and use a global transitivity constraint on the graph to learn the optimal set of edges, by formulating the optimization problem as an Integer Linear Program. We motivate this graph with an application that provides a hierarchical summary for a set of propositions that focus on a target concept, and show that our global algorithm improves performance by more than 10% over baseline algorithms."
    }
  },
  {
    "id": "abstract-2020--acl-main--257",
    "result": [
      {
        "value": {
          "start": 671,
          "end": 742,
          "text": "combinations of representations learned with our task-independent model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--257:E0"
      },
      {
        "value": {
          "start": 818,
          "end": 838,
          "text": "number of parameters",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--257:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--257:E0",
        "to_id": "abstract-2020--acl-main--257:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "We present a neural framework for learning associations between interrelated groups of words such as the ones found in Subject-Verb-Object (SVO) structures. Our model induces a joint function-specific word vector space, where vectors of e.g. plausible SVO compositions lie close together. The model retains information about word group membership even in the joint space, and can thereby effectively be applied to a number of tasks reasoning over the SVO structure. We show the robustness and versatility of the proposed framework by reporting state-of-the-art results on the tasks of estimating selectional preference and event similarity. The results indicate that the combinations of representations learned with our task-independent model outperform task-specific architectures from prior work, while reducing the number of parameters by up to 95%."
    }
  },
  {
    "id": "abstract-2021--acl-long--209",
    "result": [
      {
        "value": {
          "start": 380,
          "end": 418,
          "text": "directly models bilexical dependencies",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--209:E0"
      },
      {
        "value": {
          "start": 474,
          "end": 486,
          "text": "complexities",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--209:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--209:E0",
        "to_id": "abstract-2021--acl-long--209:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Neural lexicalized PCFGs (L-PCFGs) have been shown effective in grammar induction. However, to reduce computational complexity, they make a strong independence assumption on the generation of the child word and thus bilexical dependencies are ignored. In this paper, we propose an approach to parameterize L-PCFGs without making implausible independence assumptions. Our approach directly models bilexical dependencies and meanwhile reduces both learning and representation complexities of L-PCFGs. Experimental results on the English WSJ dataset confirm the effectiveness of our approach in improving both running speed and unsupervised parsing performance."
    }
  },
  {
    "id": "abstract-2020--acl-main--710",
    "result": [
      {
        "value": {
          "start": 270,
          "end": 296,
          "text": "recurrent generative model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--710:E0"
      },
      {
        "value": {
          "start": 578,
          "end": 595,
          "text": "number of outputs",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--710:E1"
      },
      {
        "value": {
          "start": 436,
          "end": 470,
          "text": "manipulating decoder hidden states",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--710:E2"
      },
      {
        "value": {
          "start": 377,
          "end": 386,
          "text": "diversity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--710:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--710:E0",
        "to_id": "abstract-2020--acl-main--710:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--710:E2",
        "to_id": "abstract-2020--acl-main--710:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Different texts shall by nature correspond to different number of keyphrases. This desideratum is largely missing from existing neural keyphrase generation models. In this study, we address this problem from both modeling and evaluation perspectives. We first propose a recurrent generative model that generates multiple keyphrases as delimiter-separated sequences. Generation diversity is further enhanced with two novel techniques by manipulating decoder hidden states. In contrast to previous approaches, our model is capable of generating diverse keyphrases and controlling number of outputs. We further propose two evaluation metrics tailored towards the variable-number generation. We also introduce a new dataset StackEx that expands beyond the only existing genre (i.e., academic writing) in keyphrase generation tasks. With both previous and new evaluation metrics, our model outperforms strong baselines on all datasets."
    }
  },
  {
    "id": "abstract-2020--acl-main--288",
    "result": [
      {
        "value": {
          "start": 743,
          "end": 773,
          "text": "ECPE-Two-Dimensional (ECPE-2D)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--288:E0"
      },
      {
        "value": {
          "start": 1255,
          "end": 1263,
          "text": "F1 score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--288:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--288:E0",
        "to_id": "abstract-2020--acl-main--288:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In recent years, a new interesting task, called emotion-cause pair extraction (ECPE), has emerged in the area of text emotion analysis. It aims at extracting the potential pairs of emotions and their corresponding causes in a document. To solve this task, the existing research employed a two-step framework, which first extracts individual emotion set and cause set, and then pair the corresponding emotions and causes. However, such a pipeline of two steps contains some inherent flaws: 1) the modeling does not aim at extracting the final emotion-cause pair directly; 2) the errors from the first step will affect the performance of the second step. To address these shortcomings, in this paper we propose a new end-to-end approach, called ECPE-Two-Dimensional (ECPE-2D), to represent the emotion-cause pairs by a 2D representation scheme. A 2D transformer module and two variants, window-constrained and cross-road 2D transformers, are further proposed to model the interactions of different emotion-cause pairs. The 2D representation, interaction, and prediction are integrated into a joint framework. In addition to the advantages of joint modeling, the experimental results on the benchmark emotion cause corpus show that our approach improves the F1 score of the state-of-the-art from 61.28% to 68.89%."
    }
  },
  {
    "id": "P11-1009",
    "result": [
      {
        "value": {
          "start": 433,
          "end": 453,
          "text": "smoothing techniques",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1009:E0"
      },
      {
        "value": {
          "start": 868,
          "end": 876,
          "text": "coverage",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1009:E1"
      },
      {
        "value": {
          "start": 891,
          "end": 900,
          "text": "precision",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1009:E2"
      },
      {
        "from_id": "P11-1009:E0",
        "to_id": "P11-1009:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P11-1009:E0",
        "to_id": "P11-1009:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We propose methods for estimating the probability that an entity from an entity database is associated with a web search query. Association is modeled using a query entity click graph, blending general query click logs with vertical query click logs. Smoothing techniques are proposed to address the inherent data sparsity in such graphs, including interpolation using a query synonymy model. A large-scale empirical analysis of the smoothing techniques, over a 2-year click graph collected from a commercial search engine, shows significant reductions in modeling error. The association models are then applied to the task of recommending products to web queries, by annotating queries with products from a large catalog and then mining query-product associations through web search session analysis. Experimental analysis shows that our smoothing techniques improve coverage while keeping precision stable, and overall, that our top-performing model affects 9% of general web queries with 94% precision."
    }
  },
  {
    "id": "P10-1086",
    "result": [
      {
        "value": {
          "start": 372,
          "end": 389,
          "text": "Similarity scores",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1086:E0"
      },
      {
        "value": {
          "start": 470,
          "end": 481,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1086:E1"
      },
      {
        "from_id": "P10-1086:E0",
        "to_id": "P10-1086:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This paper proposes new algorithms to compute the sense similarity between two units (words, phrases, rules, etc.) from parallel corpora. The sense similarity scores are computed by using the vector space model. We then apply the algorithms to statistical machine translation by computing the sense similarity between the source and target side of translation rule pairs. Similarity scores are used as additional features of the translation model to improve translation performance. Significant improvements are obtained over a state-of-the-art hierarchical phrase-based machine translation system."
    }
  },
  {
    "id": "abstract-2021--acl-long--19",
    "result": [
      {
        "value": {
          "start": 363,
          "end": 372,
          "text": "our model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--19:E0"
      },
      {
        "value": {
          "start": 867,
          "end": 875,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--19:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--19:E0",
        "to_id": "abstract-2021--acl-long--19:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Many joint entity relation extraction models setup two separated label spaces for the two sub-tasks (i.e., entity detection and relation classification). We argue that this setting may hinder the information interaction between entities and relations. In this work, we propose to eliminate the different treatment on the two sub-tasks’ label spaces. The input of our model is a table containing all word pairs from a sentence. Entities and relations are represented by squares and rectangles in the table. We apply a unified classifier to predict each cell’s label, which unifies the learning of two sub-tasks. For testing, an effective (yet fast) approximate decoder is proposed for finding squares and rectangles from tables. Experiments on three benchmarks (ACE04, ACE05, SciERC) show that, using only half the number of parameters, our model achieves competitive accuracy with the best extractor, and is faster."
    }
  },
  {
    "id": "abstract-2020--acl-main--153",
    "result": [
      {
        "value": {
          "start": 1088,
          "end": 1113,
          "text": "cross-lingual information",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--153:E0"
      },
      {
        "value": {
          "start": 1056,
          "end": 1067,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--153:E1"
      },
      {
        "value": {
          "start": 651,
          "end": 725,
          "text": "utilize bracketing transduction grammar (BTG)-based reordering information",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--153:E2"
      },
      {
        "value": {
          "start": 977,
          "end": 996,
          "text": "translation quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--153:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--153:E0",
        "to_id": "abstract-2020--acl-main--153:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--153:E2",
        "to_id": "abstract-2020--acl-main--153:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Position encoding (PE), an essential part of self-attention networks (SANs), is used to preserve the word order information for natural language processing tasks, generating fixed position indices for input sequences. However, in cross-lingual scenarios, machine translation, the PEs of source and target sentences are modeled independently. Due to word order divergences in different languages, modeling the cross-lingual positional relationships might help SANs tackle this problem. In this paper, we augment SANs with  cross-lingual position representations  to model the bilingually aware latent structure for the input sentence. Specifically, we utilize bracketing transduction grammar (BTG)-based reordering information to encourage SANs to learn bilingual diagonal alignments. Experimental results on WMT’14 English ⇒ German, WAT’17 Japanese ⇒ English, and WMT’17 Chinese ⇔ English translation tasks demonstrate that our approach significantly and consistently improves translation quality over strong baselines. Extensive analyses confirm that the performance gains come from the cross-lingual information."
    }
  },
  {
    "id": "abstract-2020--acl-main--495",
    "result": [
      {
        "value": {
          "start": 908,
          "end": 958,
          "text": "propose particular choices for module architecture",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--495:E0"
      },
      {
        "value": {
          "start": 102,
          "end": 110,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--495:E1"
      },
      {
        "value": {
          "start": 861,
          "end": 903,
          "text": "train the model with auxiliary supervision",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--495:E2"
      },
      {
        "value": {
          "start": 982,
          "end": 994,
          "text": "faithfulness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--495:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--495:E0",
        "to_id": "abstract-2020--acl-main--495:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--495:E2",
        "to_id": "abstract-2020--acl-main--495:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--495:E0",
        "to_id": "abstract-2020--acl-main--495:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--495:E2",
        "to_id": "abstract-2020--acl-main--495:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture. However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model’s reasoning; that is, that all modules perform their intended behaviour. In this work, we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP, two datasets which require composing multiple reasoning steps. We find that the intermediate outputs differ from the expected output, illustrating that the network structure does not provide a faithful explanation of model behaviour. To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy."
    }
  },
  {
    "id": "abstract-2021--acl-long--63",
    "result": [
      {
        "value": {
          "start": 380,
          "end": 423,
          "text": "construct a segment graph for each sentence",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--63:E0"
      },
      {
        "value": {
          "start": 1050,
          "end": 1052,
          "text": "F1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--63:E1"
      },
      {
        "value": {
          "start": 1070,
          "end": 1077,
          "text": "speedup",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--63:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--63:E0",
        "to_id": "abstract-2021--acl-long--63:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--63:E0",
        "to_id": "abstract-2021--acl-long--63:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Named entity recognition (NER) remains challenging when entity mentions can be discontinuous. Existing methods break the recognition process into several sequential steps. In training, they predict conditioned on the golden intermediate results, while at inference relying on the model output of the previous steps, which introduces exposure bias. To solve this problem, we first construct a segment graph for each sentence, in which each node denotes a segment (a continuous entity on its own, or a part of discontinuous entities), and an edge links two nodes that belong to the same entity. The nodes and edges can be generated respectively in one stage with a grid tagging scheme and learned jointly using a novel architecture named Mac. Then discontinuous NER can be reformulated as a non-parametric process of discovering maximal cliques in the graph and concatenating the spans in each clique. Experiments on three benchmarks show that our method outperforms the state-of-the-art (SOTA) results, with up to 3.5 percentage points improvement on F1, and achieves 5x speedup over the SOTA model."
    }
  },
  {
    "id": "abstract-2020--acl-main--586",
    "result": [
      {
        "value": {
          "start": 989,
          "end": 1008,
          "text": "multi-task learning",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--586:E0"
      },
      {
        "value": {
          "start": 1026,
          "end": 1036,
          "text": "robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--586:E1"
      },
      {
        "value": {
          "start": 945,
          "end": 965,
          "text": "contrastive learning",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--586:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--586:E0",
        "to_id": "abstract-2020--acl-main--586:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--586:E2",
        "to_id": "abstract-2020--acl-main--586:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Visual referring expression recognition is a challenging task that requires natural language understanding in the context of an image. We critically examine RefCOCOg, a standard benchmark for this task, using a human study and show that 83.7% of test instances do not require reasoning on linguistic structure, i.e., words are enough to identify the target object, the word order doesn’t matter. To measure the true progress of existing models, we split the test set into two sets, one which requires reasoning on linguistic structure and the other which doesn’t. Additionally, we create an out-of-distribution dataset Ref-Adv by asking crowdworkers to perturb in-domain examples such that the target object changes. Using these datasets, we empirically show that existing methods fail to exploit linguistic structure and are 12% to 23% lower in performance than the established progress for this task. We also propose two methods, one based on contrastive learning and the other based on multi-task learning, to increase the robustness of ViLBERT, the current state-of-the-art model for this task. Our datasets are publicly available at https://github.com/aws/aws-refcocog-adv."
    }
  },
  {
    "id": "abstract-2020--acl-main--131",
    "result": [
      {
        "value": {
          "start": 546,
          "end": 592,
          "text": "Pˆ2 Bot incorporates mutual persona perception",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--131:E0"
      },
      {
        "value": {
          "start": 608,
          "end": 651,
          "text": "quality of personalized dialogue generation",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--131:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--131:E0",
        "to_id": "abstract-2020--acl-main--131:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Despite the continuing efforts to improve the engagingness and consistency of chit-chat dialogue systems, the majority of current work simply focus on mimicking human-like responses, leaving understudied the aspects of modeling understanding between interlocutors. The research in cognitive science, instead, suggests that understanding is an essential signal for a high-quality chit-chat conversation. Motivated by this, we propose Pˆ2 Bot, a transmitter-receiver based framework with the aim of explicitly modeling understanding. Specifically, Pˆ2 Bot incorporates mutual persona perception to enhance the quality of personalized dialogue generation. Experiments on a large public dataset, Persona-Chat, demonstrate the effectiveness of our approach, with a considerable boost over the state-of-the-art baselines across both automatic metrics and human evaluations."
    }
  },
  {
    "id": "abstract-2020--acl-main--674",
    "result": [
      {
        "value": {
          "start": 584,
          "end": 623,
          "text": "append image captions to the scene-text",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--674:E0"
      },
      {
        "value": {
          "start": 482,
          "end": 490,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--674:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--674:E0",
        "to_id": "abstract-2020--acl-main--674:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We consider a task based on CVPR 2018 challenge dataset on advertisement (Ad) understanding. The task involves detecting the viewer’s interpretation of an Ad image captured as text. Recent results have shown that the embedded scene-text in the image holds a vital cue for this task. Motivated by this, we fine-tune the base BERT model for a sentence-pair classification task. Despite utilizing the scene-text as the only source of visual information, we could achieve a hit-or-miss accuracy of 84.95% on the challenge test data. To enable BERT to process other visual information, we append image captions to the scene-text. This achieves an accuracy of 89.69%, which is an improvement of 4.7%. This is the best reported result for this task."
    }
  },
  {
    "id": "P11-1075",
    "result": [
      {
        "value": {
          "start": 315,
          "end": 363,
          "text": "Lexically-Triggered Hidden Markov Model (LT-HMM)",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1075:E0"
      },
      {
        "value": {
          "start": 1019,
          "end": 1028,
          "text": "F-measure",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1075:E1"
      },
      {
        "from_id": "P11-1075:E0",
        "to_id": "P11-1075:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The automatic coding of clinical documents is an important task for today's healthcare providers. Though it can be viewed as multi-label document classification, the coding problem has the interesting property that most code assignments can be supported by a single phrase found in the input document. We propose a Lexically-Triggered Hidden Markov Model (LT-HMM) that leverages these phrases to improve coding accuracy. The LT-HMM works in two stages: first, a lexical match is performed against a term dictionary to collect a set of candidate codes for a document. Next, a discriminative HMM selects the best subset of codes to assign to the document by tagging candidates as present or absent. By confirming codes proposed by a dictionary, the LT-HMM can share features across codes, enabling strong performance even on rare codes. In fact, we are able to recover codes that do not occur in the training set at all. Our approach achieves the best ever performance on the 2007 Medical NLP Challenge test set, with an F-measure of 89.84."
    }
  },
  {
    "id": "abstract-2020--acl-main--639",
    "result": [
      {
        "value": {
          "start": 351,
          "end": 399,
          "text": "attentional sequence-to-sequence (Seq2seq) model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--639:E0"
      },
      {
        "value": {
          "start": 1403,
          "end": 1414,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--639:E1"
      },
      {
        "value": {
          "start": 1432,
          "end": 1449,
          "text": "transfer accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--639:E2"
      },
      {
        "value": {
          "start": 1273,
          "end": 1293,
          "text": "content preservation",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--639:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--639:E0",
        "to_id": "abstract-2020--acl-main--639:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--639:E0",
        "to_id": "abstract-2020--acl-main--639:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--639:E0",
        "to_id": "abstract-2020--acl-main--639:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Unsupervised style transfer aims to change the style of an input sentence while preserving its original content without using parallel training data. In current dominant approaches, owing to the lack of fine-grained control on the influence from the target style, they are unable to yield desirable output sentences. In this paper, we propose a novel attentional sequence-to-sequence (Seq2seq) model that dynamically exploits the relevance of each output word to the target style for unsupervised style transfer. Specifically, we first pretrain a style classifier, where the relevance of each input word to the original style can be quantified via layer-wise relevance propagation. In a denoising auto-encoding manner, we train an attentional Seq2seq model to reconstruct input sentences and repredict word-level previously-quantified style relevance simultaneously. In this way, this model is endowed with the ability to automatically predict the style relevance of each output word. Then, we equip the decoder of this model with a neural style component to exploit the predicted wordlevel style relevance for better style transfer. Particularly, we fine-tune this model using a carefully-designed objective function involving style transfer, style relevance consistency, content preservation and fluency modeling loss terms. Experimental results show that our proposed model achieves state-of-the-art performance in terms of both transfer accuracy and content preservation."
    }
  },
  {
    "id": "abstract-2020--acl-main--626",
    "result": [
      {
        "value": {
          "start": 480,
          "end": 511,
          "text": "improved crowdsourcing protocol",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--626:E0"
      },
      {
        "value": {
          "start": 379,
          "end": 387,
          "text": "coverage",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--626:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--626:E0",
        "to_id": "abstract-2020--acl-main--626:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Question-answer driven Semantic Role Labeling (QA-SRL) was proposed as an attractive open and natural flavour of SRL, potentially attainable from laymen. Recently, a large-scale crowdsourced QA-SRL corpus and a trained parser were released. Trying to replicate the QA-SRL annotation for new texts, we found that the resulting annotations were lacking in quality, particularly in coverage, making them insufficient for further research and evaluation. In this paper, we present an improved crowdsourcing protocol for complex semantic annotation, involving worker selection and training, and a data consolidation phase. Applying this protocol to QA-SRL yielded high-quality annotation with drastically higher coverage, producing a new gold evaluation dataset. We believe that our annotation protocol and gold standard will facilitate future replicable research of natural semantic annotations."
    }
  },
  {
    "id": "P11-1016",
    "result": [
      {
        "value": {
          "start": 985,
          "end": 1024,
          "text": "incorporating target-dependent features",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1016:E0"
      },
      {
        "value": {
          "start": 1148,
          "end": 1159,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1016:E1"
      },
      {
        "value": {
          "start": 1033,
          "end": 1073,
          "text": "taking related tweets into consideration",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1016:E2"
      },
      {
        "from_id": "P11-1016:E0",
        "to_id": "P11-1016:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P11-1016:E2",
        "to_id": "P11-1016:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Sentiment analysis on Twitter data has attracted much attention recently. In this paper, we focus on target-dependent Twitter sentiment classification; namely, given a query, we classify the sentiments of the tweets as positive, negative or neutral according to whether they contain positive, negative or neutral sentiments about that query. Here the query serves as the target of the sentiments. The state-of-the-art approaches for solving this problem always adopt the target-independent strategy, which may assign irrelevant sentiments to the given target. Moreover, the state-of-the-art approaches only take the tweet to be classified into consideration when classifying the sentiment; they ignore its context (i.e., related tweets). However, because tweets are usually short and more ambiguous, sometimes it is not enough to consider only the current tweet for sentiment classification. In this paper, we propose to improve target-dependent Twitter sentiment classification by 1) incorporating target-dependent features; and 2) taking related tweets into consideration. According to the experimental results, our approach greatly improves the performance of target-dependent sentiment classification."
    }
  },
  {
    "id": "abstract-2020--acl-main--189",
    "result": [
      {
        "value": {
          "start": 576,
          "end": 581,
          "text": "LEAQI",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--189:E0"
      },
      {
        "value": {
          "start": 290,
          "end": 297,
          "text": "queries",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--189:E1"
      },
      {
        "value": {
          "start": 863,
          "end": 873,
          "text": "accuracies",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--189:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--189:E0",
        "to_id": "abstract-2020--acl-main--189:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--189:E0",
        "to_id": "abstract-2020--acl-main--189:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Imitation learning algorithms provide state-of-the-art results on many structured prediction tasks by learning near-optimal search policies. Such algorithms assume training-time access to an expert that can provide the optimal action at any queried state; unfortunately, the number of such queries is often prohibitive, frequently rendering these approaches impractical. To combat this query complexity, we consider an active learning setting in which the learning algorithm has additional access to a much cheaper noisy heuristic that provides noisy guidance. Our algorithm, LEAQI, learns a difference classifier that predicts when the expert is likely to disagree with the heuristic, and queries the expert only when necessary. We apply LEAQI to three sequence labelling tasks, demonstrating significantly fewer queries to the expert and comparable (or better) accuracies over a passive approach."
    }
  },
  {
    "id": "abstract-2020--acl-main--516",
    "result": [
      {
        "value": {
          "start": 532,
          "end": 553,
          "text": "three-stage framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--516:E0"
      },
      {
        "value": {
          "start": 863,
          "end": 874,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--516:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--516:E0",
        "to_id": "abstract-2020--acl-main--516:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Maintaining a consistent personality in conversations is quite natural for human beings, but is still a non-trivial task for machines. The persona-based dialogue generation task is thus introduced to tackle the personality-inconsistent problem by incorporating explicit persona text into dialogue generation models. Despite the success of existing persona-based models on generating human-like responses, their one-stage decoding framework can hardly avoid the generation of inconsistent persona words. In this work, we introduce a three-stage framework that employs a generate-delete-rewrite mechanism to delete inconsistent words from a generated response prototype and further rewrite it to a personality-consistent one. We carry out evaluations by both human and automatic metrics. Experiments on the Persona-Chat dataset show that our approach achieves good performance."
    }
  },
  {
    "id": "P11-1077",
    "result": [
      {
        "value": {
          "start": 515,
          "end": 547,
          "text": "internet writing characteristics",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1077:E0"
      },
      {
        "value": {
          "start": 713,
          "end": 721,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1077:E1"
      },
      {
        "from_id": "P11-1077:E0",
        "to_id": "P11-1077:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We investigate whether wording, stylistic choices, and online behavior can be used to predict the age category of blog authors. Our hypothesis is that significant changes in writing style distinguish pre-social media bloggers from post-social media bloggers. Through experimentation with a range of years, we found that the birth dates of students in college at the time when social media such as AIM, SMS text messaging, MySpace and Facebook first became popular, enable accurate age prediction. We also show that internet writing characteristics are important features for age prediction, but that lexical content is also needed to produce significantly more accurate results. Our best results allow for 81.57% accuracy."
    }
  },
  {
    "id": "abstract-2020--acl-main--376",
    "result": [
      {
        "value": {
          "start": 371,
          "end": 415,
          "text": "enriched in-order shift-reduce linearization",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--376:E0"
      },
      {
        "value": {
          "start": 208,
          "end": 216,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--376:E1"
      },
      {
        "value": {
          "start": 621,
          "end": 655,
          "text": "deterministic attention mechanisms",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--376:E2"
      },
      {
        "value": {
          "start": 208,
          "end": 216,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--376:E3"
      },
      {
        "value": {
          "start": 669,
          "end": 674,
          "text": "speed",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--376:E4"
      },
      {
        "from_id": "abstract-2020--acl-main--376:E0",
        "to_id": "abstract-2020--acl-main--376:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--376:E2",
        "to_id": "abstract-2020--acl-main--376:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--376:E2",
        "to_id": "abstract-2020--acl-main--376:E4",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Sequence-to-sequence constituent parsing requires a linearization to represent trees as sequences. Top-down tree linearizations, which can be based on brackets or shift-reduce actions, have achieved the best accuracy to date. In this paper, we show that these results can be improved by using an in-order linearization instead. Based on this observation, we implement an enriched in-order shift-reduce linearization inspired by Vinyals et al. (2015)’s approach, achieving the best accuracy to date on the English PTB dataset among fully-supervised single-model sequence-to-sequence constituent parsers. Finally, we apply deterministic attention mechanisms to match the speed of state-of-the-art transition-based parsers, thus showing that sequence-to-sequence models can match them, not only in accuracy, but also in speed."
    }
  },
  {
    "id": "abstract-2021--acl-long--167",
    "result": [
      {
        "value": {
          "start": 188,
          "end": 193,
          "text": "IrEne",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--167:E0"
      },
      {
        "value": {
          "start": 925,
          "end": 930,
          "text": "error",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--167:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--167:E0",
        "to_id": "abstract-2021--acl-long--167:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Existing software-based energy measurements of NLP models are not accurate because they do not consider the complex interactions between energy consumption and model execution. We present IrEne, an interpretable and extensible energy prediction system that accurately predicts the inference energy consumption of a wide range of Transformer-based NLP models. IrEne constructs a model tree graph that breaks down the NLP model into modules that are further broken down into low-level machine learning (ML) primitives. IrEne predicts the inference energy consumption of the ML primitives as a function of generalizable features and fine-grained runtime resource usage. IrEne then aggregates these low-level predictions recursively to predict the energy of each module and finally of the entire model. Experiments across multiple Transformer models show IrEne predicts inference energy consumption of transformer models with an error of under 7% compared to the ground truth. In contrast, existing energy models see an error of over 50%. We also show how IrEne can be used to conduct energy bottleneck analysis and to easily evaluate the energy impact of different architectural choices. We release the code and data at https://github.com/StonyBrookNLP/irene."
    }
  },
  {
    "id": "abstract-2020--acl-main--484",
    "result": [
      {
        "value": {
          "start": 565,
          "end": 583,
          "text": "Double Hard Debias",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--484:E0"
      },
      {
        "value": {
          "start": 68,
          "end": 79,
          "text": "gender bias",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--484:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--484:E0",
        "to_id": "abstract-2020--acl-main--484:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Word embeddings derived from human-generated corpora inherit strong gender bias which can be further amplified by downstream models. Some commonly adopted debiasing approaches, including the seminal Hard Debias algorithm, apply post-processing procedures that project pre-trained word embeddings into a subspace orthogonal to an inferred gender subspace. We discover that semantic-agnostic corpus regularities such as word frequency captured by the word embeddings negatively impact the performance of these algorithms. We propose a simple but effective technique, Double Hard Debias, which purifies the word embeddings against such corpus regularities prior to inferring and removing the gender subspace. Experiments on three bias mitigation benchmarks show that our approach preserves the distributional semantics of the pre-trained word embeddings while reducing gender bias to a significantly larger degree than prior approaches."
    }
  },
  {
    "id": "abstract-2020--acl-main--690",
    "result": [
      {
        "value": {
          "start": 1077,
          "end": 1101,
          "text": "lattice-rescoring scheme",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--690:E0"
      },
      {
        "value": {
          "start": 1217,
          "end": 1221,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--690:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--690:E0",
        "to_id": "abstract-2020--acl-main--690:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Training data for NLP tasks often exhibits gender bias in that fewer sentences refer to women than to men. In Neural Machine Translation (NMT) gender bias has been shown to reduce translation quality, particularly when the target language has grammatical gender. The recent WinoMT challenge set allows us to measure this effect directly (Stanovsky et al, 2019) Ideally we would reduce system bias by simply debiasing all data prior to training, but achieving this effectively is itself a challenge. Rather than attempt to create a ‘balanced’ dataset, we use transfer learning on a small set of trusted, gender-balanced examples. This approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch. A known pitfall of transfer learning on new domains is ‘catastrophic forgetting’, which we address at adaptation and inference time. During adaptation we show that Elastic Weight Consolidation allows a performance trade-off between general translation quality and bias reduction. At inference time we propose a lattice-rescoring scheme which outperforms all systems evaluated in Stanovsky et al, 2019 on WinoMT with no degradation of general test set BLEU. We demonstrate our approach translating from English into three languages with varied linguistic properties and data availability."
    }
  },
  {
    "id": "abstract-2021--acl-long--276",
    "result": [
      {
        "value": {
          "start": 392,
          "end": 488,
          "text": "iteratively generating new examples and classifying event causality in a dual learning framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--276:E0"
      },
      {
        "value": {
          "start": 1076,
          "end": 1078,
          "text": "F1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--276:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--276:E0",
        "to_id": "abstract-2021--acl-long--276:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Modern models for event causality identification (ECI) are mainly based on supervised learning, which are prone to the data lacking problem. Unfortunately, the existing NLP-related augmentation methods cannot directly produce available data required for this task. To solve the data lacking problem, we introduce a new approach to augment training data for event causality identification, by iteratively generating new examples and classifying event causality in a dual learning framework. On the one hand, our approach is knowledge guided, which can leverage existing knowledge bases to generate well-formed new sentences. On the other hand, our approach employs a dual mechanism, which is a learnable augmentation framework, and can interactively adjust the generation process to generate task-related sentences. Experimental results on two benchmarks EventStoryLine and Causal-TimeBank show that 1) our method can augment suitable task-related training data for ECI; 2) our method outperforms previous methods on EventStoryLine and Causal-TimeBank (+2.5 and +2.1 points on F1 value respectively)."
    }
  },
  {
    "id": "abstract-2021--acl-long--449",
    "result": [
      {
        "value": {
          "start": 992,
          "end": 1003,
          "text": "adaptations",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--449:E0"
      },
      {
        "value": {
          "start": 1046,
          "end": 1054,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--449:E1"
      },
      {
        "value": {
          "start": 1067,
          "end": 1071,
          "text": "time",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--449:E2"
      },
      {
        "value": {
          "start": 1076,
          "end": 1092,
          "text": "space efficiency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--449:E3"
      },
      {
        "from_id": "abstract-2021--acl-long--449:E0",
        "to_id": "abstract-2021--acl-long--449:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--449:E0",
        "to_id": "abstract-2021--acl-long--449:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--449:E0",
        "to_id": "abstract-2021--acl-long--449:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "One of the main bottlenecks in developing discourse dependency parsers is the lack of annotated training data. A potential solution is to utilize abundant unlabeled data by using unsupervised techniques, but there is so far little research in unsupervised discourse dependency parsing. Fortunately, unsupervised syntactic dependency parsing has been studied by decades, which could potentially be adapted for discourse parsing. In this paper, we propose a simple yet effective method to adapt unsupervised syntactic dependency parsing methodology for unsupervised discourse dependency parsing. We apply the method to adapt two state-of-the-art unsupervised syntactic dependency parsing methods. Experimental results demonstrate that our adaptation is effective. Moreover, we extend the adapted methods to the semi-supervised and supervised setting and surprisingly, we find that they outperform previous methods specially designed for supervised discourse parsing. Further analysis shows our adaptations result in superiority not only in parsing accuracy but also in time and space efficiency."
    }
  },
  {
    "id": "abstract-2020--acl-main--301",
    "result": [
      {
        "value": {
          "start": 667,
          "end": 689,
          "text": "Using pre-trained BERT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--301:E0"
      },
      {
        "value": {
          "start": 554,
          "end": 556,
          "text": "F1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--301:E1"
      },
      {
        "value": {
          "start": 51,
          "end": 108,
          "text": "casts the parsing problem into a series of pointing tasks",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--301:E2"
      },
      {
        "value": {
          "start": 554,
          "end": 556,
          "text": "F1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--301:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--301:E0",
        "to_id": "abstract-2020--acl-main--301:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--301:E2",
        "to_id": "abstract-2020--acl-main--301:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We propose a novel constituency parsing model that casts the parsing problem into a series of pointing tasks. Specifically, our model estimates the likelihood of a span being a legitimate tree constituent via the pointing score corresponding to the boundary words of the span. Our parsing model supports efficient top-down decoding and our learning objective is able to enforce structural consistency without resorting to the expensive CKY inference. The experiments on the standard English Penn Treebank parsing task show that our method achieves 92.78 F1 without using pre-trained models, which is higher than all the existing methods with similar time complexity. Using pre-trained BERT, our model achieves 95.48 F1, which is competitive with the state-of-the-art while being faster. Our approach also establishes new state-of-the-art in Basque and Swedish in the SPMRL shared tasks on multilingual constituency parsing."
    }
  },
  {
    "id": "abstract-2020--acl-main--733",
    "result": [
      {
        "value": {
          "start": 159,
          "end": 194,
          "text": "frugal paradigm completion approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--733:E0"
      },
      {
        "value": {
          "start": 537,
          "end": 549,
          "text": "manual labor",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--733:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--733:E0",
        "to_id": "abstract-2020--acl-main--733:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Lexica distinguishing all morphologically related forms of each lexeme are crucial to many language technologies, yet building them is expensive. We propose a frugal paradigm completion approach that predicts all related forms in a morphological paradigm from as few manually provided forms as possible. It induces typological information during training which it uses to determine the best sources at test time. We evaluate our language-agnostic approach on 7 diverse languages. Compared to popular alternative approaches, ours reduces manual labor by 16-63% and is the most robust to typological variation."
    }
  },
  {
    "id": "abstract-2020--acl-main--150",
    "result": [
      {
        "value": {
          "start": 405,
          "end": 483,
          "text": "incorporate a language-aware interlingua into the Encoder-Decoder architecture",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--150:E0"
      },
      {
        "value": {
          "start": 312,
          "end": 323,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--150:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--150:E0",
        "to_id": "abstract-2020--acl-main--150:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Multilingual neural machine translation (NMT) has led to impressive accuracy improvements in low-resource scenarios by sharing common linguistic information across languages. However, the traditional multilingual model fails to capture the diversity and specificity of different languages, resulting in inferior performance compared with individual models that are sufficiently trained. In this paper, we incorporate a language-aware interlingua into the Encoder-Decoder architecture. The interlingual network enables the model to learn a language-independent representation from the semantic spaces of different languages, while still allowing for language-specific specialization of a particular language-pair. Experiments show that our proposed method achieves remarkable improvements over state-of-the-art multilingual NMT baselines and produces comparable performance with strong individual models."
    }
  },
  {
    "id": "P10-1047",
    "result": [
      {
        "value": {
          "start": 565,
          "end": 642,
          "text": "capturing various syntactic substructures as complex tags on the English side",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1047:E0"
      },
      {
        "value": {
          "start": 689,
          "end": 700,
          "text": "BLEU scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1047:E1"
      },
      {
        "from_id": "P10-1047:E0",
        "to_id": "P10-1047:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We present a novel scheme to apply factored phrase-based SMT to a language pair with very disparate morphological structures. Our approach relies on syntactic analysis on the source side (English) and then encodes a wide variety of local and non-local syntactic structures as complex structural tags which appear as additional factors in the training data. On the target side (Turkish), we only perform morphological analysis and disambiguation but treat the complete complex morphological tag as a factor, instead of separating morphemes. We incrementally explore capturing various syntactic substructures as complex tags on the English side, and evaluate how our translations improve in BLEU scores. Our maximal set of source and target side transformations, coupled with some additional techniques, provide an 39% relative improvement from a baseline 17.08 to 23.78 BLEU, all averaged over 10 training and test sets. Now that the syntactic analysis on the English side is available, we also experiment with more long distance constituent reordering to bring the English constituent order close to Turkish, but find that these transformations do not provide any additional consistent tangible gains when averaged over the 10 sets."
    }
  },
  {
    "id": "abstract-2021--acl-long--23",
    "result": [
      {
        "value": {
          "start": 1439,
          "end": 1454,
          "text": "MHPLSTM decoder",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--23:E0"
      },
      {
        "value": {
          "start": 1476,
          "end": 1480,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--23:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--23:E0",
        "to_id": "abstract-2021--acl-long--23:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "One of the reasons Transformer translation models are popular is that self-attention networks for context modelling can be easily parallelized at sequence level. However, the computational complexity of a self-attention network is O(n 2 ) , increasing quadratically with sequence length. By contrast, the complexity of LSTM-based approaches is only O(n). In practice, however, LSTMs are much slower to train than self-attention networks as they cannot be parallelized at sequence level: to model context, the current LSTM state relies on the full LSTM computation of the preceding state. This has to be computed n times for a sequence of length n. The linear transformations involved in the LSTM gate and state computations are the major cost factors in this. To enable sequence-level parallelization of LSTMs, we approximate full LSTM context modelling by computing hidden states and gates with the current input and a simple bag-of-words representation of the preceding tokens context. This allows us to compute each input step efficiently in parallel, avoiding the formerly costly sequential linear transformations. We then connect the outputs of each parallel step with computationally cheap element-wise computations. We call this the Highly Parallelized LSTM. To further constrain the number of LSTM parameters, we compute several small HPLSTMs in parallel like multi-head attention in the Transformer. The experiments show that our MHPLSTM decoder achieves significant BLEU improvements, while being even slightly faster than the self-attention network in training, and much faster than the standard LSTM."
    }
  },
  {
    "id": "abstract-2020--acl-main--777",
    "result": [
      {
        "value": {
          "start": 694,
          "end": 700,
          "text": "SeqVAT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--777:E0"
      },
      {
        "value": {
          "start": 866,
          "end": 877,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--777:E1"
      },
      {
        "value": {
          "start": 0,
          "end": 34,
          "text": "Virtual adversarial training (VAT)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--777:E2"
      },
      {
        "value": {
          "start": 70,
          "end": 86,
          "text": "model robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--777:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--777:E0",
        "to_id": "abstract-2020--acl-main--777:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--777:E2",
        "to_id": "abstract-2020--acl-main--777:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Virtual adversarial training (VAT) is a powerful technique to improve model robustness in both supervised and semi-supervised settings. It is effective and can be easily adopted on lots of image classification and text classification tasks. However, its benefits to sequence labeling tasks such as named entity recognition (NER) have not been shown as significant, mostly, because the previous approach can not combine VAT with the conditional random field (CRF). CRF can significantly boost accuracy for sequence models by putting constraints on label transitions, which makes it an essential component in most state-of-the-art sequence labeling model architectures. In this paper, we propose SeqVAT, a method which naturally applies VAT to sequence labeling models with CRF. Empirical studies show that SeqVAT not only significantly improves the sequence labeling performance over baselines under supervised settings, but also outperforms state-of-the-art approaches under semi-supervised settings."
    }
  },
  {
    "id": "P11-1049",
    "result": [
      {
        "value": {
          "start": 9,
          "end": 61,
          "text": "a joint model of sentence extraction and compression",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1049:E0"
      },
      {
        "value": {
          "start": 809,
          "end": 814,
          "text": "ROUGE",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1049:E1"
      },
      {
        "value": {
          "start": 819,
          "end": 826,
          "text": "Pyramid",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1049:E2"
      },
      {
        "value": {
          "start": 809,
          "end": 814,
          "text": "ROUGE",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1049:E3"
      },
      {
        "from_id": "P11-1049:E0",
        "to_id": "P11-1049:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P11-1049:E0",
        "to_id": "P11-1049:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P11-1049:E0",
        "to_id": "P11-1049:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We learn a joint model of sentence extraction and compression for multi-document summarization. Our model scores candidate summaries according to a combined linear model whose features factor over (1) the n-gram types in the summary and (2) the compressions used. We train the model using a margin-based objective whose loss captures end summary quality. Because of the exponentially large set of candidate summaries, we use a cutting-plane algorithm to incrementally detect and add active constraints efficiently. Inference in our model can be cast as an ILP and thereby solved in reasonable time; we also present a fast approximation scheme which achieves similar performance. Our jointly extracted and compressed summaries outperform both unlearned baselines and our learned extraction-only system on both ROUGE and Pyramid, without a drop in judged linguistic quality. We achieve the highest published ROUGE results to date on the TAC 2008 data set."
    }
  },
  {
    "id": "abstract-2020--acl-main--532",
    "result": [
      {
        "value": {
          "start": 349,
          "end": 389,
          "text": "adding a simple tag to back-translations",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--532:E0"
      },
      {
        "value": {
          "start": 296,
          "end": 303,
          "text": "quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--532:E1"
      },
      {
        "value": {
          "start": 284,
          "end": 303,
          "text": "translation quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--532:E2"
      },
      {
        "value": {
          "start": 483,
          "end": 585,
          "text": "helping the NMT system to distinguish back-translated data from original parallel data during training",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--532:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--532:E0",
        "to_id": "abstract-2020--acl-main--532:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--532:E0",
        "to_id": "abstract-2020--acl-main--532:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--532:E3",
        "to_id": "abstract-2020--acl-main--532:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In this paper, we show that neural machine translation (NMT) systems trained on large back-translated data overfit some of the characteristics of machine-translated texts. Such NMT systems better translate human-produced translations, i.e., translationese, but may largely worsen the translation quality of original texts. Our analysis reveals that adding a simple tag to back-translations prevents this quality degradation and improves on average the overall translation quality by helping the NMT system to distinguish back-translated data from original parallel data during training. We also show that, in contrast to high-resource configurations, NMT systems trained in low-resource settings are much less vulnerable to overfit back-translations. We conclude that the back-translations in the training data should always be tagged especially when the origin of the text to be translated is unknown."
    }
  },
  {
    "id": "P10-1040",
    "result": [
      {
        "value": {
          "start": 369,
          "end": 407,
          "text": "each of the three word representations",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1040:E0"
      },
      {
        "value": {
          "start": 82,
          "end": 90,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1040:E1"
      },
      {
        "from_id": "P10-1040:E0",
        "to_id": "P10-1040:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize.com/projects/wordreprs/"
    }
  },
  {
    "id": "P11-1050",
    "result": [
      {
        "value": {
          "start": 146,
          "end": 181,
          "text": "unsupervised probabilistic approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1050:E0"
      },
      {
        "value": {
          "start": 609,
          "end": 614,
          "text": "ROUGE",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1050:E1"
      },
      {
        "from_id": "P11-1050:E0",
        "to_id": "P11-1050:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Extractive methods for multi-document summarization are mainly governed by information overlap, coherence, and content constraints. We present an unsupervised probabilistic approach to model the hidden abstract concepts across documents as well as the correlation between these concepts, to generate topically coherent and non-redundant summaries. Based on human evaluations our models generate summaries with higher linguistic quality in terms of coherence, readability, and redundancy compared to benchmark systems. Although our system is unsupervised and optimized for topical coherence, we achieve a 44.1 ROUGE on the DUC-07 test set, roughly in the range of state-of-the-art supervised models."
    }
  },
  {
    "id": "abstract-2020--acl-main--727",
    "result": [
      {
        "value": {
          "start": 743,
          "end": 764,
          "text": "regularization scheme",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--727:E0"
      },
      {
        "value": {
          "start": 391,
          "end": 402,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--727:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--727:E0",
        "to_id": "abstract-2020--acl-main--727:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Existing Visual Question Answering (VQA) methods tend to exploit dataset biases and spurious statistical correlations, instead of producing right answers for the right reasons. To address this issue, recent bias mitigation methods for VQA propose to incorporate visual cues (e.g., human attention maps) to better ground the VQA models, showcasing impressive gains. However, we show that the performance improvements are not a result of improved visual grounding, but a regularization effect which prevents over-fitting to linguistic priors. For instance, we find that it is not actually necessary to provide proper, human-based cues; random, insensible cues also result in similar improvements. Based on this observation, we propose a simpler regularization scheme that does not require any external annotations and yet achieves near state-of-the-art performance on VQA-CPv2."
    }
  },
  {
    "id": "abstract-2021--acl-long--404",
    "result": [
      {
        "value": {
          "start": 612,
          "end": 636,
          "text": "using UID regularization",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--404:E0"
      },
      {
        "value": {
          "start": 659,
          "end": 669,
          "text": "perplexity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--404:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--404:E0",
        "to_id": "abstract-2021--acl-long--404:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The uniform information density (UID) hypothesis, which posits that speakers behaving optimally tend to distribute information uniformly across a linguistic signal, has gained traction in psycholinguistics as an explanation for certain syntactic, morphological, and prosodic choices. In this work, we explore whether the UID hypothesis can be operationalized as an inductive bias for statistical language modeling. Specifically, we augment the canonical MLE objective for training language models with a regularizer that encodes UID. In experiments on ten languages spanning five language families, we find that using UID regularization consistently improves perplexity in language models, having a larger effect when training data is limited. Moreover, via an analysis of generated sequences, we find that UID-regularized language models have other desirable properties, e.g., they generate text that is more lexically diverse. Our results not only suggest that UID is a reasonable inductive bias for language modeling, but also provide an alternative validation of the UID hypothesis using modern-day NLP tools."
    }
  },
  {
    "id": "abstract-2020--acl-main--8",
    "result": [
      {
        "value": {
          "start": 1035,
          "end": 1057,
          "text": "Increasing model scale",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--8:E0"
      },
      {
        "value": {
          "start": 1121,
          "end": 1131,
          "text": "preference",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--8:E1"
      },
      {
        "value": {
          "start": 1365,
          "end": 1406,
          "text": "conditionally modeling past conversations",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--8:E2"
      },
      {
        "value": {
          "start": 985,
          "end": 995,
          "text": "perplexity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--8:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--8:E0",
        "to_id": "abstract-2020--acl-main--8:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--8:E2",
        "to_id": "abstract-2020--acl-main--8:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Non-goal oriented dialog agents (i.e. chatbots) aim to produce varying and engaging conversations with a user; however, they typically exhibit either inconsistent personality across conversations or the average personality of all users. This paper addresses these issues by controlling an agent’s persona upon generation via conditioning on prior conversations of a target actor. In doing so, we are able to utilize more abstract patterns within a person’s speech and better emulate them in generated responses. This work introduces the Generative Conversation Control model, an augmented and fine-tuned GPT-2 language model that conditions on past reference conversations to probabilistically model multi-turn conversations in the actor’s persona. We introduce an accompanying data collection procedure to obtain 10.3M conversations from 6 months worth of Reddit comments. We demonstrate that scaling model sizes from 117M to 8.3B parameters yields an improvement from 23.14 to 13.14 perplexity on 1.7M held out Reddit conversations. Increasing model scale yielded similar improvements in human evaluations that measure preference of model samples to the held out target distribution in terms of realism (31% increased to 37% preference), style matching (37% to 42%), grammar and content quality (29% to 42%), and conversation coherency (32% to 40%). We find that conditionally modeling past conversations improves perplexity by 0.47 in automatic evaluations. Through human trials we identify positive trends between conditional modeling and style matching and outline steps to further improve persona control."
    }
  },
  {
    "id": "abstract-2020--acl-main--279",
    "result": [
      {
        "value": {
          "start": 513,
          "end": 525,
          "text": "SIGNAL model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--279:E0"
      },
      {
        "value": {
          "start": 599,
          "end": 610,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--279:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--279:E0",
        "to_id": "abstract-2020--acl-main--279:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We propose a Semi-supervIsed GeNerative Active Learning (SIGNAL) model to address the imbalance, efficiency, and text camouflage problems of Chinese text spam detection task. A “self-diversity” criterion is proposed for measuring the “worthiness” of a candidate for annotation. A semi-supervised variational autoencoder with masked attention learning approach and a character variation graph-enhanced augmentation procedure are proposed for data augmentation. The preliminary experiment demonstrates the proposed SIGNAL model is not only sensitive to spam sample selection, but also can improve the performance of a series of conventional active learning models for Chinese spam detection task. To the best of our knowledge, this is the first work to integrate active learning and semi-supervised generative learning for text spam detection."
    }
  },
  {
    "id": "abstract-2020--acl-main--722",
    "result": [
      {
        "value": {
          "start": 525,
          "end": 601,
          "text": "incorporates ubiquitously available information from English knowledge bases",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--722:E0"
      },
      {
        "value": {
          "start": 794,
          "end": 802,
          "text": "F1 score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--722:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--722:E0",
        "to_id": "abstract-2020--acl-main--722:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Traditional named entity recognition models use gazetteers (lists of entities) as features to improve performance. Although modern neural network models do not require such hand-crafted features for strong performance, recent work has demonstrated their utility for named entity recognition on English data. However, designing such features for low-resource languages is challenging, because exhaustive entity gazetteers do not exist in these languages. To address this problem, we propose a method of “soft gazetteers” that incorporates ubiquitously available information from English knowledge bases, such as Wikipedia, into neural named entity recognition models through cross-lingual entity linking. Our experiments on four low-resource languages show an average improvement of 4 points in F1 score."
    }
  },
  {
    "id": "P11-1063",
    "result": [
      {
        "value": {
          "start": 389,
          "end": 426,
          "text": "Incremental syntactic language models",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1063:E0"
      },
      {
        "value": {
          "start": 872,
          "end": 882,
          "text": "BLEU score",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1063:E1"
      },
      {
        "value": {
          "start": 919,
          "end": 929,
          "text": "perplexity",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1063:E2"
      },
      {
        "from_id": "P11-1063:E0",
        "to_id": "P11-1063:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P11-1063:E0",
        "to_id": "P11-1063:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "This paper describes a novel technique for incorporating syntactic knowledge into phrase-based machine translation through incremental syntactic parsing. Bottom-up and top-down parsers typically require a completed string as input. This requirement makes it difficult to incorporate them into phrase-based translation, which generates partial hypothesized translations from left-to-right. Incremental syntactic language models score sentences in a similar left-to-right fashion, and are therefore a good mechanism for incorporating syntax into phrase-based translation. We give a formal definition of one such lineartime syntactic language model, detail its relation to phrase-based decoding, and integrate the model with the Moses phrase-based translation system. We present empirical results on a constrained Urdu-English translation task that demonstrate a significant BLEU score improvement and a large decrease in perplexity."
    }
  },
  {
    "id": "abstract-2020--acl-main--113",
    "result": [
      {
        "value": {
          "start": 972,
          "end": 997,
          "text": "our variability estimates",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--113:E0"
      },
      {
        "value": {
          "start": 1034,
          "end": 1078,
          "text": "correlation with human judgements of quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--113:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--113:E0",
        "to_id": "abstract-2020--acl-main--113:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Reliably evaluating Machine Translation (MT) through automated metrics is a long-standing problem. One of the main challenges is the fact that multiple outputs can be equally valid. Attempts to minimise this issue include metrics that relax the matching of MT output and reference strings, and the use of multiple references. The latter has been shown to significantly improve the performance of evaluation metrics. However, collecting multiple references is expensive and in practice a single reference is generally used. In this paper, we propose an alternative approach: instead of modelling linguistic variation in human reference we exploit the MT model uncertainty to generate multiple diverse translations and use these: (i) as surrogates to reference translations; (ii) to obtain a quantification of translation variability to either complement existing metric scores or (iii) replace references altogether. We show that for a number of popular evaluation metrics our variability estimates lead to substantial improvements in correlation with human judgements of quality by up 15%."
    }
  },
  {
    "id": "P11-1027",
    "result": [
      {
        "value": {
          "start": 647,
          "end": 679,
          "text": "language model caching technique",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1027:E0"
      },
      {
        "value": {
          "start": 589,
          "end": 600,
          "text": "query speed",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1027:E1"
      },
      {
        "from_id": "P11-1027:E0",
        "to_id": "P11-1027:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "N-gram language models are a major resource bottleneck in machine translation. In this paper, we present several language model implementations that are both highly compact and fast to query. Our fastest implementation is as fast as the widely used SRILM while requiring only 25% of the storage. Our most compact representation can store all 4 billion n-grams and associated counts for the Google n-gram corpus in 23 bits per n-gram, the most compact lossless representation to date, and even more compact than recent lossy compression techniques. We also discuss techniques for improving query speed during decoding, including a simple but novel language model caching technique that improves the query speed of our language models (and SRILM) by up to 300%."
    }
  },
  {
    "id": "abstract-2020--acl-main--184",
    "result": [
      {
        "value": {
          "start": 147,
          "end": 158,
          "text": "ConceptFlow",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--184:E0"
      },
      {
        "value": {
          "start": 774,
          "end": 784,
          "text": "parameters",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--184:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--184:E0",
        "to_id": "abstract-2020--acl-main--184:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Human conversations naturally evolve around related concepts and hop to distant concepts. This paper presents a new conversation generation model, ConceptFlow, which leverages commonsense knowledge graphs to explicitly model conversation flows. By grounding conversations to the concept space, ConceptFlow represents the potential conversation flow as traverses in the concept space along commonsense relations. The traverse is guided by graph attentions in the concept graph, moving towards more meaningful directions in the concept space, in order to generate more semantic and informative responses. Experiments on Reddit conversations demonstrate ConceptFlow’s effectiveness over previous knowledge-aware conversation models and GPT-2 based models while using 70% fewer parameters, confirming the advantage of explicit modeling conversation structures. All source codes of this work are available at https://github.com/thunlp/ConceptFlow."
    }
  },
  {
    "id": "abstract-2020--acl-main--676",
    "result": [
      {
        "value": {
          "start": 20,
          "end": 46,
          "text": "data augmentation protocol",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--676:E0"
      },
      {
        "value": {
          "start": 488,
          "end": 498,
          "text": "error rate",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--676:E1"
      },
      {
        "value": {
          "start": 641,
          "end": 651,
          "text": "perplexity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--676:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--676:E0",
        "to_id": "abstract-2020--acl-main--676:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--676:E0",
        "to_id": "abstract-2020--acl-main--676:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "We propose a simple data augmentation protocol aimed at providing a compositional inductive bias in conditional and unconditional sequence models. Under this protocol, synthetic training examples are constructed by taking real training examples and replacing (possibly discontinuous) fragments with other fragments that appear in at least one similar environment. The protocol is model-agnostic and useful for a variety of tasks. Applied to neural sequence-to-sequence models, it reduces error rate by as much as 87% on diagnostic tasks from the SCAN dataset and 16% on a semantic parsing task. Applied to n-gram language models, it reduces perplexity by roughly 1% on small corpora in several languages."
    }
  },
  {
    "id": "P10-1048",
    "result": [
      {
        "value": {
          "start": 464,
          "end": 530,
          "text": "use transliteration as a tool for disambiguation of Hindi homonyms",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1048:E0"
      },
      {
        "value": {
          "start": 653,
          "end": 664,
          "text": "BLEU scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1048:E1"
      },
      {
        "from_id": "P10-1048:E0",
        "to_id": "P10-1048:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We present a novel approach to integrate transliteration into Hindi-to-Urdu statistical machine translation. We propose two probabilistic models, based on conditional and joint probability formulations, that are novel solutions to the problem. Our models consider both transliteration and translation when translating a particular Hindi word given the context whereas in previous work transliteration is only used for translating OOV (out-of-vocabulary) words. We use transliteration as a tool for disambiguation of Hindi homonyms which can be both translated or transliterated or transliterated differently based on different contexts. We obtain final BLEU scores of 19.35 (conditional probability model) and 19.00 (joint probability model) as compared to 14.30 for a baseline phrase-based system and 16.25 for a system which transliterates OOV words in the baseline system. This indicates that transliteration is useful for more than only translating OOV words for language pairs like Hindi-Urdu."
    }
  },
  {
    "id": "abstract-2021--acl-long--351",
    "result": [
      {
        "value": {
          "start": 0,
          "end": 37,
          "text": "Pretrained multilingual models (PMMs)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--351:E0"
      },
      {
        "value": {
          "start": 696,
          "end": 704,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--351:E1"
      },
      {
        "value": {
          "start": 741,
          "end": 743,
          "text": "F1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--351:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--351:E0",
        "to_id": "abstract-2021--acl-long--351:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--351:E0",
        "to_id": "abstract-2021--acl-long--351:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Pretrained multilingual models (PMMs) enable zero-shot learning via cross-lingual transfer, performing best for languages seen during pretraining. While methods exist to improve performance for unseen languages, they have almost exclusively been evaluated using amounts of raw text only available for a small fraction of the world’s languages. In this paper, we evaluate the performance of existing methods to adapt PMMs to new languages using a resource available for close to 1600 languages: the New Testament. This is challenging for two reasons: (1) the small corpus size, and (2) the narrow domain. While performance drops for all approaches, we surprisingly still see gains of up to 17.69% accuracy for part-of-speech tagging and 6.29 F1 for NER on average over all languages as compared to XLM-R. Another unexpected finding is that continued pretraining, the simplest approach, performs best. Finally, we perform a case study to disentangle the effects of domain and size and to shed light on the influence of the finetuning source language."
    }
  },
  {
    "id": "abstract-2021--acl-long--26",
    "result": [
      {
        "value": {
          "start": 744,
          "end": 829,
          "text": "describing and evaluating an approach to automatically generating counterfactual data",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--26:E0"
      },
      {
        "value": {
          "start": 68,
          "end": 79,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--26:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--26:E0",
        "to_id": "abstract-2021--acl-long--26:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "While state-of-the-art NLP models have been achieving the excellent performance of a wide range of tasks in recent years, important questions are being raised about their robustness and their underlying sensitivity to systematic biases that may exist in their training and test data. Such issues come to be manifest in performance problems when faced with out-of-distribution data in the field. One recent solution has been to use counterfactually augmented datasets in order to reduce any reliance on spurious patterns that may exist in the original data. Producing high-quality augmented data can be costly and time-consuming as it usually needs to involve human feedback and crowdsourcing efforts. In this work, we propose an alternative by describing and evaluating an approach to automatically generating counterfactual data for the purpose of data augmentation and explanation. A comprehensive evaluation on several different datasets and using a variety of state-of-the-art benchmarks demonstrate how our approach can achieve significant improvements in model performance when compared to models training on the original data and even when compared to models trained with the benefit of human-generated augmented data."
    }
  },
  {
    "id": "abstract-2021--acl-long--418",
    "result": [
      {
        "value": {
          "start": 96,
          "end": 134,
          "text": "matrix product operator (short as MPO)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--418:E0"
      },
      {
        "value": {
          "start": 871,
          "end": 893,
          "text": "fine-tuning parameters",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--418:E1"
      },
      {
        "value": {
          "start": 28,
          "end": 82,
          "text": "pre-trained language models (PLM) compression approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--418:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--418:E0",
        "to_id": "abstract-2021--acl-long--418:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--418:E2",
        "to_id": "abstract-2021--acl-long--418:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "This paper presents a novel pre-trained language models (PLM) compression approach based on the matrix product operator (short as MPO) from quantum many-body physics. It can decompose an original matrix into central tensors (containing the core information) and auxiliary tensors (with only a small proportion of parameters). With the decomposed MPO structure, we propose a novel fine-tuning strategy by only updating the parameters from the auxiliary tensors, and design an optimization algorithm for MPO-based approximation over stacked network architectures. Our approach can be applied to the original or the compressed PLMs in a general way, which derives a lighter network and significantly reduces the parameters to be fine-tuned. Extensive experiments have demonstrated the effectiveness of the proposed approach in model compression, especially the reduction in fine-tuning parameters (91% reduction on average). The code to reproduce the results of this paper can be found at https://github.com/RUCAIBox/MPOP."
    }
  },
  {
    "id": "P10-1056",
    "result": [
      {
        "value": {
          "start": 458,
          "end": 485,
          "text": "a set of syntactic features",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1056:E0"
      },
      {
        "value": {
          "start": 736,
          "end": 744,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1056:E1"
      },
      {
        "from_id": "P10-1056:E0",
        "to_id": "P10-1056:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "To date, few attempts have been made to develop and validate methods for automatic evaluation of linguistic quality in text summarization. We present the first systematic assessment of several diverse classes of metrics designed to capture various aspects of well-written text. We train and test linguistic quality models on consecutive years of NIST evaluation data in order to show the generality of results. For grammaticality, the best results come from a set of syntactic features. Focus, coherence and referential clarity are best evaluated by a class of features measuring local coherence on the basis of cosine similarity between sentences, coreference information, and summarization specific features. Our best results are 90% accuracy for pairwise comparisons of competing systems over a test set of several inputs and 70% for ranking summaries of a specific input."
    }
  },
  {
    "id": "P11-1158",
    "result": [
      {
        "value": {
          "start": 913,
          "end": 952,
          "text": "Combining A* with adaptive supertagging",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1158:E0"
      },
      {
        "value": {
          "start": 685,
          "end": 693,
          "text": "CPU time",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1158:E1"
      },
      {
        "from_id": "P11-1158:E0",
        "to_id": "P11-1158:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "We present a systematic comparison and combination of two orthogonal techniques for efficient parsing of Combinatory Categorial Grammar (CCG). First we consider adaptive supertagging, a widely used approximate search technique that prunes most lexical categories from the parser's search space using a separate sequence model. Next we consider several variants on A*, a classic exact search technique which to our knowledge has not been applied to more expressive grammar formalisms like CCG. In addition to standard hardware-independent measures of parser effort we also present what we believe is the first evaluation of A* parsing on the more realistic but more stringent metric of CPU time. By itself, A* substantially reduces parser effort as measured by the number of edges considered during parsing, but we show that for CCG this does not always correspond to improvements in CPU time over a CKY baseline. Combining A* with adaptive supertagging decreases CPU time by 15% for our best model."
    }
  },
  {
    "id": "abstract-2021--acl-long--423",
    "result": [
      {
        "value": {
          "start": 855,
          "end": 900,
          "text": "hierarchical user interest matching framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--423:E0"
      },
      {
        "value": {
          "start": 1104,
          "end": 1115,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--423:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--423:E0",
        "to_id": "abstract-2021--acl-long--423:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "User interest modeling is critical for personalized news recommendation. Existing news recommendation methods usually learn a single user embedding for each user from their previous behaviors to represent their overall interest. However, user interest is usually diverse and multi-grained, which is difficult to be accurately modeled by a single user embedding. In this paper, we propose a news recommendation method with hierarchical user interest modeling, named HieRec. Instead of a single user embedding, in our method each user is represented in a hierarchical interest tree to better capture their diverse and multi-grained interest in news. We use a three-level hierarchy to represent 1) overall user interest; 2) user interest in coarse-grained topics like sports; and 3) user interest in fine-grained topics like football. Moreover, we propose a hierarchical user interest matching framework to match candidate news with different levels of user interest for more accurate user interest targeting. Extensive experiments on two real-world datasets validate our method can effectively improve the performance of user modeling for personalized news recommendation."
    }
  },
  {
    "id": "abstract-2020--acl-main--395",
    "result": [
      {
        "value": {
          "start": 605,
          "end": 619,
          "text": "using a BiLSTM",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--395:E0"
      },
      {
        "value": {
          "start": 487,
          "end": 495,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--395:E1"
      },
      {
        "value": {
          "start": 503,
          "end": 513,
          "text": "using BERT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--395:E2"
      },
      {
        "value": {
          "start": 487,
          "end": 495,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--395:E3"
      },
      {
        "value": {
          "start": 794,
          "end": 843,
          "text": "information retrieval and knowledge-based methods",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--395:E4"
      },
      {
        "value": {
          "start": 779,
          "end": 781,
          "text": "F1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--395:E5"
      },
      {
        "from_id": "abstract-2020--acl-main--395:E0",
        "to_id": "abstract-2020--acl-main--395:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--395:E2",
        "to_id": "abstract-2020--acl-main--395:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--395:E4",
        "to_id": "abstract-2020--acl-main--395:E5",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "The key to effortless end-user programming is natural language. We examine how to teach intelligent systems new functions, expressed in natural language. As a first step, we collected 3168 samples of teaching efforts in plain English. Then we built fuSE, a novel system that translates English function descriptions into code. Our approach is three-tiered and each task is evaluated separately. We first classify whether an intent to teach new functionality is present in the utterance (accuracy: 97.7% using BERT). Then we analyze the linguistic structure and construct a semantic model (accuracy: 97.6% using a BiLSTM). Finally, we synthesize the signature of the method, map the intermediate steps (instructions in the method body) to API calls and inject control structures (F1: 67.0% with information retrieval and knowledge-based methods). In an end-to-end evaluation on an unseen dataset fuSE synthesized 84.6% of the method signatures and 79.2% of the API calls correctly."
    }
  },
  {
    "id": "abstract-2020--acl-main--604",
    "result": [
      {
        "value": {
          "start": 158,
          "end": 165,
          "text": "RikiNet",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--604:E0"
      },
      {
        "value": {
          "start": 687,
          "end": 689,
          "text": "F1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--604:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--604:E0",
        "to_id": "abstract-2020--acl-main--604:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Reading long documents to answer open-domain questions remains challenging in natural language understanding. In this paper, we introduce a new model, called RikiNet, which reads Wikipedia pages for natural question answering. RikiNet contains a dynamic paragraph dual-attention reader and a multi-level cascaded answer predictor. The reader dynamically represents the document and question by utilizing a set of complementary attention mechanisms. The representations are then fed into the predictor to obtain the span of the short answer, the paragraph of the long answer, and the answer type in a cascaded manner. On the Natural Questions (NQ) dataset, a single RikiNet achieves 74.3 F1 and 57.9 F1 on long-answer and short-answer tasks. To our best knowledge, it is the first single model that outperforms the single human performance. Furthermore, an ensemble RikiNet obtains 76.1 F1 and 61.3 F1 on long-answer and short-answer tasks, achieving the best performance on the official NQ leaderboard."
    }
  },
  {
    "id": "abstract-2020--acl-main--579",
    "result": [
      {
        "value": {
          "start": 642,
          "end": 707,
          "text": "automatically generate “distractor” sentences to augment the bags",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--579:E0"
      },
      {
        "value": {
          "start": 228,
          "end": 236,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--579:E1"
      },
      {
        "value": {
          "start": 269,
          "end": 282,
          "text": "explanability",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--579:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--579:E0",
        "to_id": "abstract-2020--acl-main--579:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--579:E0",
        "to_id": "abstract-2020--acl-main--579:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recent neural models for relation extraction with distant supervision alleviate the impact of irrelevant sentences in a bag by learning importance weights for the sentences. Efforts thus far have focused on improving extraction accuracy but little is known about their explanability. In this work we annotate a test set with ground-truth sentence-level explanations to evaluate the quality of explanations afforded by the relation extraction models. We demonstrate that replacing the entity mentions in the sentences with their fine-grained entity types not only enhances extraction accuracy but also improves explanation. We also propose to automatically generate “distractor” sentences to augment the bags and train the model to ignore the distractors. Evaluations on the widely used FB-NYT dataset show that our methods achieve new state-of-the-art accuracy while improving model explanability."
    }
  },
  {
    "id": "abstract-2021--acl-long--427",
    "result": [
      {
        "value": {
          "start": 1013,
          "end": 1039,
          "text": "Combining these techniques",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--427:E0"
      },
      {
        "value": {
          "start": 1088,
          "end": 1100,
          "text": "memory usage",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--427:E1"
      },
      {
        "value": {
          "start": 171,
          "end": 181,
          "text": "perplexity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--427:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--427:E0",
        "to_id": "abstract-2021--acl-long--427:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--427:E0",
        "to_id": "abstract-2021--acl-long--427:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Increasing the input length has been a driver of progress in language modeling with transformers. We identify conditions where shorter inputs are not harmful, and achieve perplexity and efficiency improvements through two new methods that decrease input length. First, we show that initially training a model on short subsequences before moving on to longer ones both reduces overall training time and, surprisingly, substantially improves perplexity. Second, we show how to improve the efficiency of recurrence methods in transformers, which let models condition on previously processed tokens when generating sequences that exceed the maximal length the transformer can handle at once. Existing methods require computationally expensive relative position embeddings; we introduce a simple alternative of adding absolute position embeddings to queries and keys instead of to word embeddings, which efficiently produces superior results. We show that these recurrent models also benefit from short input lengths. Combining these techniques speeds up training by a factor of 1.65, reduces memory usage, and substantially improves perplexity on WikiText-103, without adding any parameters."
    }
  },
  {
    "id": "P11-1109",
    "result": [
      {
        "value": {
          "start": 463,
          "end": 535,
          "text": "regarding the sentences that define the same concept as parallel corpora",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1109:E0"
      },
      {
        "value": {
          "start": 450,
          "end": 459,
          "text": "precision",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1109:E1"
      },
      {
        "value": {
          "start": 676,
          "end": 690,
          "text": "precision rate",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1109:E2"
      },
      {
        "from_id": "P11-1109:E0",
        "to_id": "P11-1109:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P11-1109:E0",
        "to_id": "P11-1109:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We propose an automatic method of extracting paraphrases from definition sentences, which are also automatically acquired from the Web. We observe that a huge number of concepts are defined in Web documents, and that the sentences that define the same concept tend to convey mostly the same information using different expressions and thus contain many paraphrases. We show that a large number of paraphrases can be automatically extracted with high precision by regarding the sentences that define the same concept as parallel corpora. Experimental results indicated that with our method it was possible to extract about 300,000 paraphrases from 6 x 108 Web documents with a precision rate of about 94%."
    }
  },
  {
    "id": "abstract-2020--acl-main--133",
    "result": [
      {
        "value": {
          "start": 454,
          "end": 515,
          "text": "introducing a novel approach to dialogue coherence assessment",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--133:E0"
      },
      {
        "value": {
          "start": 841,
          "end": 856,
          "text": "accuracy points",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--133:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--133:E0",
        "to_id": "abstract-2020--acl-main--133:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recent dialogue coherence models use the coherence features designed for monologue texts, e.g. nominal entities, to represent utterances and then explicitly augment them with dialogue-relevant features, e.g., dialogue act labels. It indicates two drawbacks, (a) semantics of utterances are limited to entity mentions, and (b) the performance of coherence models strongly relies on the quality of the input dialogue act labels. We address these issues by introducing a novel approach to dialogue coherence assessment. We use dialogue act prediction as an auxiliary task in a multi-task learning scenario to obtain informative utterance representations for coherence assessment. Our approach alleviates the need for explicit dialogue act labels during evaluation. The results of our experiments show that our model substantially (more than 20 accuracy points) outperforms its strong competitors on the DailyDialogue corpus, and performs on par with them on the SwitchBoard corpus for ranking dialogues concerning their coherence. We release our source code."
    }
  },
  {
    "id": "abstract-2020--acl-main--126",
    "result": [
      {
        "value": {
          "start": 633,
          "end": 638,
          "text": "CMADE",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--126:E0"
      },
      {
        "value": {
          "start": 969,
          "end": 977,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--126:E1"
      },
      {
        "value": {
          "start": 606,
          "end": 689,
          "text": "automatic evaluation model CMADE (Comparison Model for Automatic Dialog Evaluation)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--126:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--126:E0",
        "to_id": "abstract-2020--acl-main--126:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--126:E2",
        "to_id": "abstract-2020--acl-main--126:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Open Domain dialog system evaluation is one of the most important challenges in dialog research. Existing automatic evaluation metrics, such as BLEU are mostly reference-based. They calculate the difference between the generated response and a limited number of available references. Likert-score based self-reported user rating is widely adopted by social conversational systems, such as Amazon Alexa Prize chatbots. However, self-reported user rating suffers from bias and variance among different users. To alleviate this problem, we formulate dialog evaluation as a comparison task. We also propose an automatic evaluation model CMADE (Comparison Model for Automatic Dialog Evaluation) that automatically cleans self-reported user ratings as it trains on them. Specifically, we first use a self-supervised method to learn better dialog feature representation, and then use KNN and Shapley to remove confusing samples. Our experiments show that CMADE achieves 89.2% accuracy in the dialog comparison task."
    }
  },
  {
    "id": "abstract-2021--acl-long--202",
    "result": [
      {
        "value": {
          "start": 321,
          "end": 326,
          "text": "UNIMO",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--202:E0"
      },
      {
        "value": {
          "start": 1047,
          "end": 1058,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--202:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--202:E0",
        "to_id": "abstract-2021--acl-long--202:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Existed pre-training methods either focus on single-modal tasks or multi-modal tasks, and cannot effectively adapt to each other. They can only utilize single-modal data (i.e., text or image) or limited multi-modal data (i.e., image-text pairs). In this work, we propose a UNIfied-MOdal pre-training architecture, namely UNIMO, which can effectively adapt to both single-modal and multi-modal understanding and generation tasks. Large scale of free text corpus and image collections are utilized to improve the capability of visual and textual understanding, and cross-modal contrastive learning (CMCL) is leveraged to align the textual and visual information into a unified semantic space, over a corpus of image-text pairs augmented with related images and texts. With the help of rich non-paired single-modal data, our model is able to learn more generalizable representations, by allowing textual knowledge and visual knowledge to enhance each other in the unified semantic space. The experimental results show that UNIMO greatly improves the performance of several single-modal and multi-modal downstream tasks. Our code and pre-trained models are public at https://github.com/PaddlePaddle/Research/tree/master/NLP/UNIMO ."
    }
  },
  {
    "id": "abstract-2021--acl-long--102",
    "result": [
      {
        "value": {
          "start": 779,
          "end": 821,
          "text": "multilingual contrastive pretraining (MCP)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--102:E0"
      },
      {
        "value": {
          "start": 708,
          "end": 719,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--102:E1"
      },
      {
        "value": {
          "start": 941,
          "end": 949,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--102:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--102:E0",
        "to_id": "abstract-2021--acl-long--102:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--102:E0",
        "to_id": "abstract-2021--acl-long--102:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Commonsense reasoning research has so far been limited to English. We aim to evaluate and improve popular multilingual language models (ML-LMs) to help advance commonsense reasoning (CSR) beyond English. We collect the Mickey corpus, consisting of 561k sentences in 11 different languages, which can be used for analyzing and improving ML-LMs. We propose Mickey Probe, a language-general probing task for fairly evaluating the common sense of popular ML-LMs across different languages. In addition, we also create two new datasets, X-CSQA and X-CODAH, by translating their English versions to 14 other languages, so that we can evaluate popular ML-LMs for cross-lingual commonsense reasoning. To improve the performance beyond English, we propose a simple yet effective method — multilingual contrastive pretraining (MCP). It significantly enhances sentence representations, yielding a large performance gain on both benchmarks (e.g., +2.7% accuracy for X-CSQA over XLM-R_L)."
    }
  },
  {
    "id": "P10-1147",
    "result": [
      {
        "value": {
          "start": 138,
          "end": 147,
          "text": "Our model",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1147:E0"
      },
      {
        "value": {
          "start": 738,
          "end": 748,
          "text": "BLEU score",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1147:E1"
      },
      {
        "from_id": "P10-1147:E0",
        "to_id": "P10-1147:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We present a discriminative model that directly predicts which set of phrasal translation rules should be extracted from a sentence pair. Our model scores extraction sets: nested collections of all the overlapping phrase pairs consistent with an underlying word alignment. Extraction set models provide two principle advantages over word-factored alignment models. First, we can incorporate features on phrase pairs, in addition to word links. Second, we can optimize for an extraction-based loss function that relates directly to the end task of generating translations. Our model gives improvements in alignment quality relative to state-of-the-art unsupervised and supervised baselines, as well as providing up to a 1.4 improvement in BLEU score in Chinese-to-English translation experiments."
    }
  },
  {
    "id": "abstract-2021--acl-long--374",
    "result": [
      {
        "value": {
          "start": 1058,
          "end": 1083,
          "text": "regularization techniques",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--374:E0"
      },
      {
        "value": {
          "start": 220,
          "end": 231,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--374:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--374:E0",
        "to_id": "abstract-2021--acl-long--374:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We study the problem of event coreference resolution (ECR) that seeks to group coreferent event mentions into the same clusters. Deep learning methods have recently been applied for this task to deliver state-of-the-art performance. However, existing deep learning models for ECR are limited in that they cannot exploit important interactions between relevant objects for ECR, e.g., context words and entity mentions, to support the encoding of document-level context. In addition, consistency constraints between golden and predicted clusters of event mentions have not been considered to improve representation learning in prior deep learning models for ECR. This work addresses such limitations by introducing a novel deep learning model for ECR. At the core of our model are document structures to explicitly capture relevant objects for ECR. Our document structures introduce diverse knowledge sources (discourse, syntax, semantics) to compute edges/interactions between structure nodes for document-level representation learning. We also present novel regularization techniques based on consistencies of golden and predicted clusters for event mentions in documents. Extensive experiments show that our model achieve state-of-the-art performance on two benchmark datasets."
    }
  },
  {
    "id": "abstract-2020--acl-main--54",
    "result": [
      {
        "value": {
          "start": 899,
          "end": 929,
          "text": "end-to-end neural architecture",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--54:E0"
      },
      {
        "value": {
          "start": 1047,
          "end": 1059,
          "text": "success rate",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--54:E1"
      },
      {
        "value": {
          "start": 1075,
          "end": 1103,
          "text": "language understanding score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--54:E2"
      },
      {
        "value": {
          "start": 1122,
          "end": 1152,
          "text": "response appropriateness score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--54:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--54:E0",
        "to_id": "abstract-2020--acl-main--54:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--54:E0",
        "to_id": "abstract-2020--acl-main--54:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--54:E0",
        "to_id": "abstract-2020--acl-main--54:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "The goal-oriented dialogue system needs to be optimized for tracking the dialogue flow and carrying out an effective conversation under various situations to meet the user goal. The traditional approach to build such a dialogue system is to take a pipelined modular architecture, where its modules are optimized individually. However, such an optimization scheme does not necessarily yield the overall performance improvement of the whole system. On the other hand, end-to-end dialogue systems with monolithic neural architecture are often trained only with input-output utterances, without taking into account the entire annotations available in the corpus. This scheme makes it difficult for goal-oriented dialogues where the system needs to integrate with external systems or to provide interpretable information about why the system generated a particular response. In this paper, we present an end-to-end neural architecture for dialogue systems that addresses both challenges above. In the human evaluation, our dialogue system achieved the success rate of 68.32%, the language understanding score of 4.149, and the response appropriateness score of 4.287, which ranked the system at the top position in the end-to-end multi-domain dialogue system task in the 8th dialogue systems technology challenge (DSTC8)."
    }
  },
  {
    "id": "P11-1033",
    "result": [
      {
        "value": {
          "start": 263,
          "end": 340,
          "text": "augments available labeled data in each language with unlabeled parallel data",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1033:E0"
      },
      {
        "value": {
          "start": 673,
          "end": 681,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1033:E1"
      },
      {
        "value": {
          "start": 823,
          "end": 834,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1033:E2"
      },
      {
        "from_id": "P11-1033:E0",
        "to_id": "P11-1033:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P11-1033:E0",
        "to_id": "P11-1033:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Most previous work on multilingual sentiment analysis has focused on methods to adapt sentiment resources from resource-rich languages to resource-poor languages. We present a novel approach for joint bilingual sentiment classification at the sentence level that augments available labeled data in each language with unlabeled parallel data. We rely on the intuition that the sentiment labels for parallel sentences should be similar and present a model that jointly learns improved monolingual sentiment classifiers for each language. Experiments on multiple data sets show that the proposed approach (1) outperforms the monolingual baselines, significantly improving the accuracy for both languages by 3.44%--8.12%; (2) outperforms two standard approaches for leveraging unlabeled data; and (3) produces (albeit smaller) performance gains when employing pseudo-parallel data from machine translation engines."
    }
  },
  {
    "id": "P10-1146",
    "result": [
      {
        "value": {
          "start": 700,
          "end": 734,
          "text": "uses both source and target syntax",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1146:E0"
      },
      {
        "value": {
          "start": 779,
          "end": 787,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1146:E1"
      },
      {
        "from_id": "P10-1146:E0",
        "to_id": "P10-1146:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Statistical translation models that try to capture the recursive structure of language have been widely adopted over the last few years. These models make use of varying amounts of information from linguistic theory: some use none at all, some use information about the grammar of the target language, some use information about the grammar of the source language. But progress has been slower on translation models that are able to learn the relationship between the grammars of both the source and target language. We discuss the reasons why this has been a challenge, review existing attempts to meet this challenge, and show how some old and new ideas can be combined into a simple approach that uses both source and target syntax for significant improvements in translation accuracy."
    }
  },
  {
    "id": "abstract-2020--acl-main--563",
    "result": [
      {
        "value": {
          "start": 582,
          "end": 635,
          "text": "employing a contextual hierarchical attention network",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--563:E0"
      },
      {
        "value": {
          "start": 971,
          "end": 985,
          "text": "joint accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--563:E1"
      },
      {
        "value": {
          "start": 516,
          "end": 527,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--563:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--563:E0",
        "to_id": "abstract-2020--acl-main--563:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--563:E0",
        "to_id": "abstract-2020--acl-main--563:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Recent studies in dialogue state tracking (DST) leverage historical information to determine states which are generally represented as slot-value pairs. However, most of them have limitations to efficiently exploit relevant context due to the lack of a powerful mechanism for modeling interactions between the slot and the dialogue history. Besides, existing methods usually ignore the slot imbalance problem and treat all slots indiscriminately, which limits the learning of hard slots and eventually hurts overall performance. In this paper, we propose to enhance the DST through employing a contextual hierarchical attention network to not only discern relevant information at both word level and turn level but also learn contextual representations. We further propose an adaptive objective to alleviate the slot imbalance problem by dynamically adjust weights of different slots during training. Experimental results show that our approach reaches 52.68% and 58.55% joint accuracy on MultiWOZ 2.0 and MultiWOZ 2.1 datasets respectively and achieves new state-of-the-art performance with considerable improvements (+1.24% and +5.98%)."
    }
  },
  {
    "id": "abstract-2021--acl-long--485",
    "result": [
      {
        "value": {
          "start": 1188,
          "end": 1220,
          "text": "enhanced entity mention features",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--485:E0"
      },
      {
        "value": {
          "start": 479,
          "end": 490,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--485:E1"
      },
      {
        "value": {
          "start": 865,
          "end": 890,
          "text": "incremental task settings",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--485:E2"
      },
      {
        "value": {
          "start": 1018,
          "end": 1030,
          "text": "performances",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--485:E3"
      },
      {
        "from_id": "abstract-2021--acl-long--485:E0",
        "to_id": "abstract-2021--acl-long--485:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--485:E2",
        "to_id": "abstract-2021--acl-long--485:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Medical named entity recognition (NER) and normalization (NEN) are fundamental for constructing knowledge graphs and building QA systems. Existing implementations for medical NER and NEN are suffered from the error propagation between the two tasks. The mispredicted mentions from NER will directly influence the results of NEN. Therefore, the NER module is the bottleneck of the whole system. Besides, the learnable features for both tasks are beneficial to improving the model performance. To avoid the disadvantages of existing models and exploit the generalized representation across the two tasks, we design an end-to-end progressive multi-task learning model for jointly modeling medical NER and NEN in an effective way. There are three level tasks with progressive difficulty in the framework. The progressive tasks can reduce the error propagation with the incremental task settings which implies the lower level tasks gain the supervised signals other than errors from the higher level tasks to improve their performances. Besides, the context features are exploited to enrich the semantic information of entity mentions extracted by NER. The performance of NEN profits from the enhanced entity mention features. The standard entities from knowledge bases are introduced into the NER module for extracting corresponding entity mentions correctly. The empirical results on two publicly available medical literature datasets demonstrate the superiority of our method over nine typical methods."
    }
  },
  {
    "id": "abstract-2020--acl-main--555",
    "result": [
      {
        "value": {
          "start": 765,
          "end": 792,
          "text": "pre-trained language models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--555:E0"
      },
      {
        "value": {
          "start": 872,
          "end": 883,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--555:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--555:E0",
        "to_id": "abstract-2020--acl-main--555:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines."
    }
  },
  {
    "id": "abstract-2021--acl-long--22",
    "result": [
      {
        "value": {
          "start": 386,
          "end": 389,
          "text": "MBR",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--22:E0"
      },
      {
        "value": {
          "start": 159,
          "end": 169,
          "text": "robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--22:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--22:E0",
        "to_id": "abstract-2021--acl-long--22:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Neural Machine Translation (NMT) currently exhibits biases such as producing translations that are too short and overgenerating frequent words, and shows poor robustness to copy noise in training data or domain shift. Recent work has tied these shortcomings to beam search – the de facto standard inference algorithm in NMT – and Eikema & Aziz (2020) propose to use Minimum Bayes Risk (MBR) decoding on unbiased samples instead. In this paper, we empirically investigate the properties of MBR decoding on a number of previously reported biases and failure cases of beam search. We find that MBR still exhibits a length and token frequency bias, owing to the MT metrics used as utility functions, but that MBR also increases robustness against copy noise in the training data and domain shift."
    }
  },
  {
    "id": "abstract-2020--acl-main--147",
    "result": [
      {
        "value": {
          "start": 333,
          "end": 357,
          "text": "self-attention mechanism",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--147:E0"
      },
      {
        "value": {
          "start": 376,
          "end": 395,
          "text": "translation quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--147:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--147:E0",
        "to_id": "abstract-2020--acl-main--147:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Most neural machine translation models only rely on pairs of parallel sentences, assuming syntactic information is automatically learned by an attention mechanism. In this work, we investigate different approaches to incorporate syntactic knowledge in the Transformer model and also propose a novel, parameter-free, dependency-aware self-attention mechanism that improves its translation quality, especially for long sentences and in low-resource scenarios. We show the efficacy of each approach on WMT English-German and English-Turkish, and WAT English-Japanese translation tasks."
    }
  },
  {
    "id": "abstract-2020--acl-main--48",
    "result": [
      {
        "value": {
          "start": 458,
          "end": 462,
          "text": "GCAN",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--48:E0"
      },
      {
        "value": {
          "start": 623,
          "end": 631,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--48:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--48:E0",
        "to_id": "abstract-2020--acl-main--48:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This paper solves the fake news detection problem under a more realistic scenario on social media. Given the source short-text tweet and the corresponding sequence of retweet users without text comments, we aim at predicting whether the source tweet is fake or not, and generating explanation by highlighting the evidences on suspicious retweeters and the words they concern. We develop a novel neural network-based model, Graph-aware Co-Attention Networks (GCAN), to achieve the goal. Extensive experiments conducted on real tweet datasets exhibit that GCAN can significantly outperform state-of-the-art methods by 16% in accuracy on average. In addition, the case studies also show that GCAN can produce reasonable explanations."
    }
  },
  {
    "id": "abstract-2021--acl-long--189",
    "result": [
      {
        "value": {
          "start": 786,
          "end": 802,
          "text": "slot annotations",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--189:E0"
      },
      {
        "value": {
          "start": 669,
          "end": 680,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--189:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--189:E0",
        "to_id": "abstract-2021--acl-long--189:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Task-oriented dialogue systems typically require manual annotation of dialogue slots in training data, which is costly to obtain. We propose a method that eliminates this requirement: We use weak supervision from existing linguistic annotation models to identify potential slot candidates, then automatically identify domain-relevant slots by using clustering algorithms. Furthermore, we use the resulting slot annotation to train a neural-network-based tagger that is able to perform slot tagging with no human intervention. This tagger is trained solely on the outputs of our method and thus does not rely on any labeled data. Our model demonstrates state-of-the-art performance in slot tagging without labeled training data on four different dialogue domains. Moreover, we find that slot annotations discovered by our model significantly improve the performance of an end-to-end dialogue response generation model, compared to using no slot annotation at all."
    }
  },
  {
    "id": "abstract-2020--acl-main--624",
    "result": [
      {
        "value": {
          "start": 611,
          "end": 664,
          "text": "domain-agnostic Human-In-The-Loop annotation approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--624:E0"
      },
      {
        "value": {
          "start": 1004,
          "end": 1020,
          "text": "annotation speed",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--624:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--624:E0",
        "to_id": "abstract-2020--acl-main--624:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Entity linking (EL) is concerned with disambiguating entity mentions in a text against knowledge bases (KB). It is crucial in a considerable number of fields like humanities, technical writing and biomedical sciences to enrich texts with semantics and discover more knowledge. The use of EL in such domains requires handling noisy texts, low resource settings and domain-specific KBs. Existing approaches are mostly inappropriate for this, as they depend on training data. However, in the above scenario, there exists hardly annotated data, and it needs to be created from scratch. We therefore present a novel domain-agnostic Human-In-The-Loop annotation approach: we use recommenders that suggest potential concepts and adaptive candidate ranking, thereby speeding up the overall annotation process and making it less tedious for users. We evaluate our ranking approach in a simulation on difficult texts and show that it greatly outperforms a strong baseline in ranking accuracy. In a user study, the annotation speed improves by 35% compared to annotating without interactive support; users report that they strongly prefer our system. An open-source and ready-to-use implementation based on the text annotation platform INCEpTION (https://inception-project.github.io) is made available."
    }
  },
  {
    "id": "abstract-2020--acl-main--134",
    "result": [
      {
        "value": {
          "start": 13,
          "end": 31,
          "text": "graph-based method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--134:E0"
      },
      {
        "value": {
          "start": 544,
          "end": 549,
          "text": "times",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--134:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--134:E0",
        "to_id": "abstract-2020--acl-main--134:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We propose a graph-based method to tackle the dependency tree linearization task. We formulate the task as a Traveling Salesman Problem (TSP), and use a biaffine attention model to calculate the edge costs. We facilitate the decoding by solving the TSP for each subtree and combining the solution into a projective tree. We then design a transition system as post-processing, inspired by non-projective transition-based parsing, to obtain non-projective sentences. Our proposed method outperforms the state-of-the-art linearizer while being 10 times faster in training and decoding."
    }
  },
  {
    "id": "abstract-2020--acl-main--219",
    "result": [
      {
        "value": {
          "start": 420,
          "end": 473,
          "text": "using state-of-the-art image and text representations",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--219:E0"
      },
      {
        "value": {
          "start": 1004,
          "end": 1015,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--219:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--219:E0",
        "to_id": "abstract-2020--acl-main--219:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "To achieve the long-term goal of machines being able to engage humans in conversation, our models should captivate the interest of their speaking partners. Communication grounded in images, whereby a dialogue is conducted based on a given photo, is a setup naturally appealing to humans (Hu et al., 2014). In this work we study large-scale architectures and datasets for this goal. We test a set of neural architectures using state-of-the-art image and text representations, considering various ways to fuse the components. To test such models, we collect a dataset of grounded human-human conversations, where speakers are asked to play roles given a provided emotional mood or style, as the use of such traits is also a key factor in engagingness (Guo et al., 2019). Our dataset, Image-Chat, consists of 202k dialogues over 202k images using 215 possible style traits. Automatic metrics and human evaluations of engagingness show the efficacy of our approach; in particular, we obtain state-of-the-art performance on the existing IGC task, and our best performing model is almost on par with humans on the Image-Chat test set (preferred 47.7% of the time)."
    }
  },
  {
    "id": "P11-1150",
    "result": [
      {
        "value": {
          "start": 963,
          "end": 995,
          "text": "apply the aspect ranking results",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1150:E0"
      },
      {
        "value": {
          "start": 1075,
          "end": 1086,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1150:E1"
      },
      {
        "from_id": "P11-1150:E0",
        "to_id": "P11-1150:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In this paper, we dedicate to the topic of aspect ranking, which aims to automatically identify important product aspects from online consumer reviews. The important aspects are identified according to two observations: (a) the important aspects of a product are usually commented by a large number of consumers; and (b) consumers' opinions on the important aspects greatly influence their overall opinions on the product. In particular, given consumer reviews of a product, we first identify the product aspects by a shallow dependency parser and determine consumers' opinions on these aspects via a sentiment classifier. We then develop an aspect ranking algorithm to identify the important aspects by simultaneously considering the aspect frequency and the influence of consumers' opinions given to each aspect on their overall opinions. The experimental results on 11 popular products in four domains demonstrate the effectiveness of our approach. We further apply the aspect ranking results to the application of document-level sentiment classification, and improve the performance significantly."
    }
  },
  {
    "id": "abstract-2020--acl-main--614",
    "result": [
      {
        "value": {
          "start": 394,
          "end": 410,
          "text": "end-to-end model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--614:E0"
      },
      {
        "value": {
          "start": 847,
          "end": 854,
          "text": "F-score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--614:E1"
      },
      {
        "value": {
          "start": 872,
          "end": 891,
          "text": "Area Under PR curve",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--614:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--614:E0",
        "to_id": "abstract-2020--acl-main--614:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--614:E0",
        "to_id": "abstract-2020--acl-main--614:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Showing items that do not match search query intent degrades customer experience in e-commerce. These mismatches result from counterfactual biases of the ranking algorithms toward noisy behavioral signals such as clicks and purchases in the search logs. Mitigating the problem requires a large labeled dataset, which is expensive and time-consuming to obtain. In this paper, we develop a deep, end-to-end model that learns to effectively classify mismatches and to generate hard mismatched examples to improve the classifier. We train the model end-to-end by introducing a latent variable into the cross-entropy loss that alternates between using the real and generated samples. This not only makes the classifier more robust but also boosts the overall ranking performance. Our model achieves a relative gain compared to baselines by over 26% in F-score, and over 17% in Area Under PR curve. On live search traffic, our model gains significant improvement in multiple countries."
    }
  },
  {
    "id": "abstract-2020--acl-main--20",
    "result": [
      {
        "value": {
          "start": 408,
          "end": 464,
          "text": "hierarchical conditional variational autoencoder (HCVAE)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--20:E0"
      },
      {
        "value": {
          "start": 772,
          "end": 783,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--20:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--20:E0",
        "to_id": "abstract-2020--acl-main--20:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "One of the most crucial challenges in question answering (QA) is the scarcity of labeled data, since it is costly to obtain question-answer (QA) pairs for a target text domain with human annotation. An alternative approach to tackle the problem is to use automatically generated QA pairs from either the problem context or from large amount of unstructured texts (e.g. Wikipedia). In this work, we propose a hierarchical conditional variational autoencoder (HCVAE) for generating QA pairs given unstructured texts as contexts, while maximizing the mutual information between generated QA pairs to ensure their consistency. We validate our Information Maximizing Hierarchical Conditional Variational AutoEncoder (Info-HCVAE) on several benchmark datasets by evaluating the performance of the QA model (BERT-base) using only the generated QA pairs (QA-based evaluation) or by using both the generated and human-labeled pairs (semi-supervised learning) for training, against state-of-the-art baseline models. The results show that our model obtains impressive performance gains over all baselines on both tasks, using only a fraction of data for training."
    }
  },
  {
    "id": "abstract-2020--acl-main--564",
    "result": [
      {
        "value": {
          "start": 654,
          "end": 681,
          "text": "data manipulation framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--564:E0"
      },
      {
        "value": {
          "start": 1400,
          "end": 1411,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--564:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--564:E0",
        "to_id": "abstract-2020--acl-main--564:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Current state-of-the-art neural dialogue models learn from human conversations following the data-driven paradigm. As such, a reliable training corpus is the crux of building a robust and well-behaved dialogue model. However, due to the open-ended nature of human conversations, the quality of user-generated training data varies greatly, and effective training samples are typically insufficient while noisy samples frequently appear. This impedes the learning of those data-driven neural dialogue models. Therefore, effective dialogue learning requires not only more reliable learning samples, but also fewer noisy samples. In this paper, we propose a data manipulation framework to proactively reshape the data distribution towards reliable samples by augmenting and highlighting effective learning samples as well as reducing the effect of inefficient samples simultaneously. In particular, the data manipulation model selectively augments the training samples and assigns an importance weight to each instance to reform the training data. Note that, the proposed data manipulation framework is fully data-driven and learnable. It not only manipulates training samples to optimize the dialogue generation model, but also learns to increase its manipulation skills through gradient descent with validation samples. Extensive experiments show that our framework can improve the dialogue generation performance with respect to various automatic evaluation metrics and human judgments."
    }
  },
  {
    "id": "abstract-2021--acl-long--142",
    "result": [
      {
        "value": {
          "start": 231,
          "end": 267,
          "text": "find external contexts of a sentence",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--142:E0"
      },
      {
        "value": {
          "start": 116,
          "end": 127,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--142:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--142:E0",
        "to_id": "abstract-2021--acl-long--142:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recent advances in Named Entity Recognition (NER) show that document-level contexts can significantly improve model performance. In many application scenarios, however, such contexts are not available. In this paper, we propose to find external contexts of a sentence by retrieving and selecting a set of semantically relevant texts through a search engine, with the original sentence as the query. We find empirically that the contextual representations computed on the retrieval-based input view, constructed through the concatenation of a sentence and its external contexts, can achieve significantly improved performance compared to the original input view based only on the sentence. Furthermore, we can improve the model performance of both input views by Cooperative Learning, a training method that encourages the two input views to produce similar contextual representations or output label distributions. Experiments show that our approach can achieve new state-of-the-art performance on 8 NER data sets across 5 domains."
    }
  },
  {
    "id": "abstract-2020--acl-main--398",
    "result": [
      {
        "value": {
          "start": 472,
          "end": 477,
          "text": "TaPas",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--398:E0"
      },
      {
        "value": {
          "start": 1075,
          "end": 1083,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--398:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--398:E0",
        "to_id": "abstract-2020--acl-main--398:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Answering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this paper, we present TaPas, an approach to question answering over tables without generating logical forms. TaPas trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection. TaPas extends BERT’s architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end. We experiment with three different semantic parsing datasets, and find that TaPas outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WikiSQL and WikiTQ, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from WikiSQL to WikiTQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art."
    }
  },
  {
    "id": "abstract-2020--acl-main--686",
    "result": [
      {
        "value": {
          "start": 282,
          "end": 285,
          "text": "HAT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--686:E0"
      },
      {
        "value": {
          "start": 965,
          "end": 972,
          "text": "speedup",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--686:E1"
      },
      {
        "value": {
          "start": 987,
          "end": 991,
          "text": "size",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--686:E2"
      },
      {
        "value": {
          "start": 1113,
          "end": 1124,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--686:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--686:E0",
        "to_id": "abstract-2020--acl-main--686:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--686:E0",
        "to_id": "abstract-2020--acl-main--686:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--686:E0",
        "to_id": "abstract-2020--acl-main--686:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but they are difficult to be deployed on hardware due to the intensive computation. To enable low-latency inference on resource-constrained hardware platforms, we propose to design Hardware-Aware Transformers (HAT) with neural architecture search. We first construct a large design space with arbitrary encoder-decoder attention and heterogeneous layers. Then we train a SuperTransformer that covers all candidates in the design space, and efficiently produces many SubTransformers with weight sharing. Finally, we perform an evolutionary search with a hardware latency constraint to find a specialized SubTransformer dedicated to run fast on the target hardware. Extensive experiments on four machine translation tasks demonstrate that HAT can discover efficient models for different hardware (CPU, GPU, IoT device). When running WMT’14 translation task on Raspberry Pi-4, HAT can achieve 3× speedup, 3.7× smaller size over baseline Transformer; 2.7× speedup, 3.6× smaller size over Evolved Transformer with 12,041× less search cost and no performance loss. HAT is open-sourced at https://github.com/mit-han-lab/hardware-aware-transformers."
    }
  },
  {
    "id": "abstract-2020--acl-main--458",
    "result": [
      {
        "value": {
          "start": 259,
          "end": 276,
          "text": "general framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--458:E0"
      },
      {
        "value": {
          "start": 165,
          "end": 184,
          "text": "factual correctness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--458:E1"
      },
      {
        "value": {
          "start": 881,
          "end": 896,
          "text": "overall quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--458:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--458:E0",
        "to_id": "abstract-2020--acl-main--458:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--458:E0",
        "to_id": "abstract-2020--acl-main--458:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Neural abstractive summarization models are able to generate summaries which have high overlap with human references. However, existing models are not optimized for factual correctness, a critical metric in real-world applications. In this work, we develop a general framework where we evaluate the factual correctness of a generated summary by fact-checking it automatically against its reference using an information extraction module. We further propose a training strategy which optimizes a neural summarization model with a factual correctness reward via reinforcement learning. We apply the proposed method to the summarization of radiology reports, where factual correctness is a key requirement. On two separate datasets collected from hospitals, we show via both automatic and human evaluation that the proposed approach substantially improves the factual correctness and overall quality of outputs over a competitive neural summarization system, producing radiology summaries that approach the quality of human-authored ones."
    }
  },
  {
    "id": "abstract-2020--acl-main--758",
    "result": [
      {
        "value": {
          "start": 218,
          "end": 266,
          "text": "multi-perspective cross-lingual neural framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--758:E0"
      },
      {
        "value": {
          "start": 495,
          "end": 506,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--758:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--758:E0",
        "to_id": "abstract-2020--acl-main--758:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The ability to match pieces of code to their corresponding natural language descriptions and vice versa is fundamental for natural language search interfaces to software repositories. In this paper, we propose a novel multi-perspective cross-lingual neural framework for code–text matching, inspired in part by a previous model for monolingual text-to-text matching, to capture both global and local similarities. Our experiments on the CoNaLa dataset show that our proposed model yields better performance on this cross-lingual text-to-code matching task than previous approaches that map code and text to a single joint embedding space."
    }
  },
  {
    "id": "P10-1141",
    "result": [
      {
        "value": {
          "start": 176,
          "end": 201,
          "text": "feature weighting schemes",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1141:E0"
      },
      {
        "value": {
          "start": 256,
          "end": 264,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1141:E1"
      },
      {
        "value": {
          "start": 295,
          "end": 316,
          "text": "classic tf.idf scheme",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1141:E2"
      },
      {
        "value": {
          "start": 256,
          "end": 264,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1141:E3"
      },
      {
        "from_id": "P10-1141:E0",
        "to_id": "P10-1141:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "P10-1141:E2",
        "to_id": "P10-1141:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Most sentiment analysis approaches use as baseline a support vector machines (SVM) classifier with binary unigram weights. In this paper, we explore whether more sophisticated feature weighting schemes from Information Retrieval can enhance classification accuracy. We show that variants of the classic tf.idf scheme adapted to sentiment analysis provide significant increases in accuracy, especially when using a sublinear function for term frequency weights and document frequency smoothing. The techniques are tested on a wide selection of data sets and produce the best accuracy to our knowledge."
    }
  },
  {
    "id": "abstract-2020--acl-main--501",
    "result": [
      {
        "value": {
          "start": 56,
          "end": 139,
          "text": "using document-level distant super-vision, pairing questions and relevant documents",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--501:E0"
      },
      {
        "value": {
          "start": 688,
          "end": 690,
          "text": "F1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--501:E1"
      },
      {
        "value": {
          "start": 726,
          "end": 733,
          "text": "Rouge-L",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--501:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--501:E0",
        "to_id": "abstract-2020--acl-main--501:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--501:E0",
        "to_id": "abstract-2020--acl-main--501:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We address the problem of extractive question answering using document-level distant super-vision, pairing questions and relevant documents with answer strings. We compare previously used probability space and distant supervision assumptions (assumptions on the correspondence between the weak answer string labels and possible answer mention spans). We show that these assumptions interact, and that different configurations provide complementary benefits. We demonstrate that a multi-objective model can efficiently combine the advantages of multiple assumptions and outperform the best individual formulation. Our approach outperforms previous state-of-the-art models by 4.3 points in F1 on TriviaQA-Wiki and 1.7 points in Rouge-L on NarrativeQA summaries."
    }
  },
  {
    "id": "abstract-2020--acl-main--56",
    "result": [
      {
        "value": {
          "start": 575,
          "end": 638,
          "text": "Gated Convolutional Bidirectional Attention-based Model (GCBiA)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--56:E0"
      },
      {
        "value": {
          "start": 1114,
          "end": 1129,
          "text": "on-topic recall",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--56:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--56:E0",
        "to_id": "abstract-2020--acl-main--56:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Off-topic spoken response detection, the task aiming at predicting whether a response is off-topic for the corresponding prompt, is important for an automated speaking assessment system. In many real-world educational applications, off-topic spoken response detectors are required to achieve high recall for off-topic responses not only on seen prompts but also on prompts that are unseen during training. In this paper, we propose a novel approach for off-topic spoken response detection with high off-topic recall on both seen and unseen prompts. We introduce a new model, Gated Convolutional Bidirectional Attention-based Model (GCBiA), which applies bi-attention mechanism and convolutions to extract topic words of prompts and key-phrases of responses, and introduces gated unit and residual connections between major layers to better represent the relevance of responses and prompts. Moreover, a new negative sampling method is proposed to augment training data. Experiment results demonstrate that our novel approach can achieve significant improvements in detecting off-topic responses with extremely high on-topic recall, for both seen and unseen prompts."
    }
  },
  {
    "id": "P10-1007",
    "result": [
      {
        "value": {
          "start": 510,
          "end": 520,
          "text": "Our system",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1007:E0"
      },
      {
        "value": {
          "start": 552,
          "end": 568,
          "text": "word-error rates",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1007:E1"
      },
      {
        "from_id": "P10-1007:E0",
        "to_id": "P10-1007:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "We introduce a novel mechanism for incorporating articulatory dynamics into speech recognition with the theory of task dynamics. This system reranks sentence-level hypotheses by the likelihoods of their hypothetical articulatory realizations which are derived from relationships learned with aligned acoustic/articulatory data. Experiments compare this with two baseline systems, namely an acoustic hidden Markov model and a dynamic Bayes network augmented with discretized representations of the vocal tract. Our system based on task dynamics reduces word-error rates significantly by 10.2% relative to the best baseline models."
    }
  },
  {
    "id": "P10-1084",
    "result": [
      {
        "value": {
          "start": 224,
          "end": 242,
          "text": "a generative model",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1084:E0"
      },
      {
        "value": {
          "start": 682,
          "end": 694,
          "text": "ROUGE scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1084:E1"
      },
      {
        "value": {
          "start": 269,
          "end": 287,
          "text": "a regression model",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1084:E2"
      },
      {
        "from_id": "P10-1084:E0",
        "to_id": "P10-1084:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P10-1084:E2",
        "to_id": "P10-1084:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Scoring sentences in documents given abstract summaries created by humans is important in extractive multi-document summarization. In this paper, we formulate extractive summarization as a two step learning problem building a generative model for pattern discovery and a regression model for inference. We calculate scores for sentences in document clusters based on their latent characteristics using a hierarchical topic model. Then, using these scores, we train a regression model based on the lexical and structural characteristics of the sentences, and use the model to score sentences of new documents to form a summary. Our system advances current state-of-the-art improving ROUGE scores by ~7%. Generated summaries are less redundant and more coherent based upon manual quality evaluations."
    }
  },
  {
    "id": "abstract-2020--acl-main--583",
    "result": [
      {
        "value": {
          "start": 428,
          "end": 449,
          "text": "coherence annotations",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--583:E0"
      },
      {
        "value": {
          "start": 639,
          "end": 650,
          "text": "consistency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--583:E1"
      },
      {
        "value": {
          "start": 655,
          "end": 662,
          "text": "quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--583:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--583:E0",
        "to_id": "abstract-2020--acl-main--583:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--583:E0",
        "to_id": "abstract-2020--acl-main--583:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We use coherence relations inspired by computational models of discourse to study the information needs and goals of image captioning. Using an annotation protocol specifically devised for capturing image–caption coherence relations, we annotate 10,000 instances from publicly-available image–caption pairs. We introduce a new task for learning inferences in imagery and text, coherence relation prediction, and show that these coherence annotations can be exploited to learn relation classifiers as an intermediary step, and also train coherence-aware, controllable image captioning models. The results show a dramatic improvement in the consistency and quality of the generated captions with respect to information needs specified via coherence relations."
    }
  },
  {
    "id": "abstract-2021--acl-long--401",
    "result": [
      {
        "value": {
          "start": 646,
          "end": 674,
          "text": "adversarial training regimen",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--401:E0"
      },
      {
        "value": {
          "start": 684,
          "end": 694,
          "text": "robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--401:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--401:E0",
        "to_id": "abstract-2021--acl-long--401:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Due to recent pretrained multilingual representation models, it has become feasible to exploit labeled data from one language to train a cross-lingual model that can then be applied to multiple new languages. In practice, however, we still face the problem of scarce labeled data, leading to subpar results. In this paper, we propose a novel data augmentation strategy for better cross-lingual natural language inference by enriching the data to reflect more diversity in a semantically faithful way. To this end, we propose two methods of training a generative model to induce synthesized examples, and then leverage the resulting data using an adversarial training regimen for more robustness. In a series of detailed experiments, we show that this fruitful combination leads to substantial gains in cross-lingual inference."
    }
  },
  {
    "id": "abstract-2020--acl-main--144",
    "result": [
      {
        "value": {
          "start": 20,
          "end": 45,
          "text": "data augmentation methods",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--144:E0"
      },
      {
        "value": {
          "start": 728,
          "end": 736,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--144:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--144:E0",
        "to_id": "abstract-2020--acl-main--144:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This paper explores data augmentation methods for training Neural Machine Translation to make use of similar translations, in a comparable way a human translator employs fuzzy matches. In particular, we show how we can simply present the neural model with information of both source and target sides of the fuzzy matches, we also extend the similarity to include semantically related translations retrieved using sentence distributed representations. We show that translations based on fuzzy matching provide the model with “copy” information while translations based on embedding similarities tend to extend the translation “context”. Results indicate that the effect from both similar sentences are adding up to further boost accuracy, combine naturally with model fine-tuning and are providing dynamic adaptation for unseen translation pairs. Tests on multiple data sets and domains show consistent accuracy improvements. To foster research around these techniques, we also release an Open-Source toolkit with efficient and flexible fuzzy-match implementation."
    }
  },
  {
    "id": "abstract-2021--acl-long--355",
    "result": [
      {
        "value": {
          "start": 1154,
          "end": 1213,
          "text": "injecting noise to the latent features of the language flow",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--355:E0"
      },
      {
        "value": {
          "start": 324,
          "end": 335,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--355:E1"
      },
      {
        "value": {
          "start": 1118,
          "end": 1150,
          "text": "augment QA data with new context",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--355:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--355:E0",
        "to_id": "abstract-2021--acl-long--355:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--355:E2",
        "to_id": "abstract-2021--acl-long--355:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recent years have witnessed various types of generative models for natural language generation (NLG), especially RNNs or transformer based sequence-to-sequence models, as well as variational autoencoder (VAE) and generative adversarial network (GAN) based models. However, flow-based generative models, which achieve strong performance in image generation due to their invertibility and exact density estimation properties, have been less explored for NLG. In this paper, we propose a flow-based language generation model by adapting previous flow generative models to language generation via continuous input embeddings, adapted affine coupling structures, and a novel architecture for autoregressive text generation. We also apply our framework to Sequence-to-Sequence generation, including text- and video-based Question Generation (QG) and Neural Machine Translation (NMT), and data augmentation for Question Answering (QA). We use our language flow model to provide extra input features for QG and NMT, which achieves improvements over the strong QG baselines on SQuAD and TVQA and NMT baseline on WMT16. We also augment QA data with new context by injecting noise to the latent features of the language flow and show this augmentation leads to a large performance improvement from strong baselines on SQuAD and TVQA."
    }
  },
  {
    "id": "abstract-2020--acl-main--245",
    "result": [
      {
        "value": {
          "start": 416,
          "end": 421,
          "text": "RobEn",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--245:E0"
      },
      {
        "value": {
          "start": 276,
          "end": 286,
          "text": "robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--245:E1"
      },
      {
        "value": {
          "start": 1123,
          "end": 1162,
          "text": "instantiation of RobEn paired with BERT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--245:E2"
      },
      {
        "value": {
          "start": 1175,
          "end": 1198,
          "text": "average robust accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--245:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--245:E0",
        "to_id": "abstract-2020--acl-main--245:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--245:E2",
        "to_id": "abstract-2020--acl-main--245:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Despite excellent performance on many tasks, NLP systems are easily fooled by small adversarial perturbations of inputs. Existing procedures to defend against such perturbations are either (i) heuristic in nature and susceptible to stronger attacks or (ii) provide guaranteed robustness to worst-case attacks, but are incompatible with state-of-the-art models like BERT. In this work, we introduce robust encodings (RobEn): a simple framework that confers guaranteed robustness, without making compromises on model architecture. The core component of RobEn is an encoding function, which maps sentences to a smaller, discrete space of encodings. Systems using these encodings as a bottleneck confer guaranteed robustness with standard training, and the same encodings can be used across multiple tasks. We identify two desiderata to construct robust encoding functions: perturbations of a sentence should map to a small set of encodings (stability), and models using encodings should still perform well (fidelity). We instantiate RobEn to defend against a large family of adversarial typos. Across six tasks from GLUE, our instantiation of RobEn paired with BERT achieves an average robust accuracy of 71.3% against all adversarial typos in the family considered, while previous work using a typo-corrector achieves only 35.3% accuracy against a simple greedy attack."
    }
  },
  {
    "id": "abstract-2021--acl-long--241",
    "result": [
      {
        "value": {
          "start": 562,
          "end": 582,
          "text": "modular architecture",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--241:E0"
      },
      {
        "value": {
          "start": 993,
          "end": 1001,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--241:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--241:E0",
        "to_id": "abstract-2021--acl-long--241:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Neural models have shown impressive performance gains in answering queries from natural language text. However, existing works are unable to support database queries, such as “List/Count all female athletes who were born in 20th century”, which require reasoning over sets of relevant facts with operations such as join, filtering and aggregation. We show that while state-of-the-art transformer models perform very well for small databases, they exhibit limitations in processing noisy data, numerical operations, and queries that aggregate facts. We propose a modular architecture to answer these database-style queries over multiple spans from text and aggregating these at scale. We evaluate the architecture using WikiNLDB, a novel dataset for exploring such queries. Our architecture scales to databases containing thousands of facts whereas contemporary models are limited by how many facts can be encoded. In direct comparison on small databases, our approach increases overall answer accuracy from 85% to 90%. On larger databases, our approach retains its accuracy whereas transformer baselines could not encode the context."
    }
  },
  {
    "id": "P11-1144",
    "result": [
      {
        "value": {
          "start": 153,
          "end": 197,
          "text": "makes use of large amounts of unlabeled data",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1144:E0"
      },
      {
        "value": {
          "start": 608,
          "end": 644,
          "text": "full frame-semantic parsing F1 score",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1144:E1"
      },
      {
        "value": {
          "start": 541,
          "end": 570,
          "text": "frame identification accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1144:E2"
      },
      {
        "value": {
          "start": 203,
          "end": 249,
          "text": "graph-based semi-supervised learning framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1144:E3"
      },
      {
        "from_id": "P11-1144:E0",
        "to_id": "P11-1144:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P11-1144:E0",
        "to_id": "P11-1144:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P11-1144:E3",
        "to_id": "P11-1144:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P11-1144:E3",
        "to_id": "P11-1144:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We describe a new approach to disambiguating semantic frames evoked by lexical predicates previously unseen in a lexicon or annotated data. Our approach makes use of large amounts of unlabeled data in a graph-based semi-supervised learning framework. We construct a large graph where vertices correspond to potential predicates and use label propagation to learn possible semantic frames for new ones. The label-propagated graph is used within a frame-semantic parser and, for unknown predicates, results in over 15% absolute improvement in frame identification accuracy and over 13% absolute improvement in full frame-semantic parsing F1 score on a blind test set, over a state-of-the-art supervised baseline."
    }
  },
  {
    "id": "abstract-2020--acl-main--636",
    "result": [
      {
        "value": {
          "start": 521,
          "end": 586,
          "text": "enhance a neural model based DST generator with a reward manager,",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--636:E0"
      },
      {
        "value": {
          "start": 722,
          "end": 736,
          "text": "joint accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--636:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--636:E0",
        "to_id": "abstract-2020--acl-main--636:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "A Dialogue State Tracker (DST) is a core component of a modular task-oriented dialogue system. Tremendous progress has been made in recent years. However, the major challenges remain. The state-of-the-art accuracy for DST is below 50% for a multi-domain dialogue task. A learnable DST for any new domain requires a large amount of labeled in-domain data and training from scratch. In this paper, we propose a Meta-Reinforced Multi-Domain State Generator (MERET). Our first contribution is to improve the DST accuracy. We enhance a neural model based DST generator with a reward manager, which is built on policy gradient reinforcement learning (RL) to fine-tune the generator. With this change, we are able to improve the joint accuracy of DST from 48.79% to 50.91% on the MultiWOZ corpus. Second, we explore to train a DST meta-learning model with a few domains as source domains and a new domain as target domain. We apply the model-agnostic meta-learning algorithm (MAML) to DST and the obtained meta-learning model is used for new domain adaptation. Our experimental results show this solution is able to outperform the traditional training approach with extremely less training data in target domain."
    }
  },
  {
    "id": "abstract-2021--acl-long--503",
    "result": [
      {
        "value": {
          "start": 11,
          "end": 18,
          "text": "BERTGen",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--503:E0"
      },
      {
        "value": {
          "start": 566,
          "end": 577,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--503:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--503:E0",
        "to_id": "abstract-2021--acl-long--503:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We present BERTGen, a novel, generative, decoder-only model which extends BERT by fusing multimodal and multilingual pre-trained models VL-BERT and M-BERT, respectively. BERTGen is auto-regressively trained for language generation tasks, namely image captioning, machine translation and multimodal machine translation, under a multi-task setting. With a comprehensive set of evaluations, we show that BERTGen outperforms many strong baselines across the tasks explored. We also show BERTGen’s ability for zero-shot language generation, where it exhibits competitive performance to supervised counterparts. Finally, we conduct ablation studies which demonstrate that BERTGen substantially benefits from multi-tasking and effectively transfers relevant inductive biases from the pre-trained models."
    }
  },
  {
    "id": "abstract-2020--acl-main--108",
    "result": [
      {
        "value": {
          "start": 258,
          "end": 295,
          "text": "using a new neural segmentation model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--108:E0"
      },
      {
        "value": {
          "start": 306,
          "end": 314,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--108:E1"
      },
      {
        "value": {
          "start": 384,
          "end": 395,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--108:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--108:E0",
        "to_id": "abstract-2020--acl-main--108:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--108:E0",
        "to_id": "abstract-2020--acl-main--108:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "This paper introduces the Webis Gmane Email Corpus 2019, the largest publicly available and fully preprocessed email corpus to date. We crawled more than 153 million emails from 14,699 mailing lists and segmented them into semantically consistent components using a new neural segmentation model. With 96% accuracy on 15 classes of email segments, our model achieves state-of-the-art performance while being more efficient to train than previous ones. All data, code, and trained models are made freely available alongside the paper."
    }
  },
  {
    "id": "abstract-2021--acl-long--297",
    "result": [
      {
        "value": {
          "start": 619,
          "end": 677,
          "text": "Edge-enhanced Bayesian Graph Convolutional Network (EBGCN)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--297:E0"
      },
      {
        "value": {
          "start": 1042,
          "end": 1053,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--297:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--297:E0",
        "to_id": "abstract-2021--acl-long--297:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Detecting rumors on social media is a very critical task with significant implications to the economy, public health, etc. Previous works generally capture effective features from texts and the propagation structure. However, the uncertainty caused by unreliable relations in the propagation structure is common and inevitable due to wily rumor producers and the limited collection of spread data. Most approaches neglect it and may seriously limit the learning of features. Towards this issue, this paper makes the first attempt to explore propagation uncertainty for rumor detection. Specifically, we propose a novel Edge-enhanced Bayesian Graph Convolutional Network (EBGCN) to capture robust structural features. The model adaptively rethinks the reliability of latent relations by adopting a Bayesian approach. Besides, we design a new edge-wise consistency training framework to optimize the model by enforcing consistency on relations. Experiments on three public benchmark datasets demonstrate that the proposed model achieves better performance than baseline methods on both rumor detection and early rumor detection tasks."
    }
  },
  {
    "id": "abstract-2020--acl-main--414",
    "result": [
      {
        "value": {
          "start": 861,
          "end": 900,
          "text": "RoBERTa answer classification component",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--414:E0"
      },
      {
        "value": {
          "start": 97,
          "end": 108,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--414:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--414:E0",
        "to_id": "abstract-2020--acl-main--414:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Evidence retrieval is a critical stage of question answering (QA), necessary not only to improve performance, but also to explain the decisions of the QA method. We introduce a simple, fast, and unsupervised iterative evidence retrieval method, which relies on three ideas: (a) an unsupervised alignment approach to soft-align questions and answers with justification sentences using only GloVe embeddings, (b) an iterative process that reformulates queries focusing on terms that are not covered by existing justifications, which (c) stops when the terms in the given question and candidate answers are covered by the retrieved justifications. Despite its simplicity, our approach outperforms all the previous methods (including supervised methods) on the evidence selection task on two datasets: MultiRC and QASC. When these evidence sentences are fed into a RoBERTa answer classification component, we achieve state-of-the-art QA performance on these two datasets."
    }
  },
  {
    "id": "abstract-2020--acl-main--503",
    "result": [
      {
        "value": {
          "start": 278,
          "end": 336,
          "text": "setting of selective question answering under domain shift",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--503:E0"
      },
      {
        "value": {
          "start": 510,
          "end": 518,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--503:E1"
      },
      {
        "value": {
          "start": 1128,
          "end": 1168,
          "text": "directly using the model’s probabilities",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--503:E2"
      },
      {
        "value": {
          "start": 510,
          "end": 518,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--503:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--503:E0",
        "to_id": "abstract-2020--acl-main--503:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--503:E2",
        "to_id": "abstract-2020--acl-main--503:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "To avoid giving wrong answers, question answering (QA) models need to know when to abstain from answering. Moreover, users often ask questions that diverge from the model’s training data, making errors more likely and thus abstention more critical. In this work, we propose the setting of selective question answering under domain shift, in which a QA model is tested on a mixture of in-domain and out-of-domain data, and must answer (i.e., not abstain on) as many questions as possible while maintaining high accuracy. Abstention policies based solely on the model’s softmax probabilities fare poorly, since models are overconfident on out-of-domain inputs. Instead, we train a calibrator to identify inputs on which the QA model errs, and abstain when it predicts an error is likely. Crucially, the calibrator benefits from observing the model’s behavior on out-of-domain data, even if from a different domain than the test data. We combine this method with a SQuAD-trained QA model and evaluate on mixtures of SQuAD and five other QA datasets. Our method answers 56% of questions while maintaining 80% accuracy; in contrast, directly using the model’s probabilities only answers 48% at 80% accuracy."
    }
  },
  {
    "id": "abstract-2020--acl-main--525",
    "result": [
      {
        "value": {
          "start": 37,
          "end": 50,
          "text": "layered model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--525:E0"
      },
      {
        "value": {
          "start": 631,
          "end": 640,
          "text": "F1 scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--525:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--525:E0",
        "to_id": "abstract-2020--acl-main--525:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This paper presents Pyramid, a novel layered model for Nested Named Entity Recognition (nested NER). In our approach, token or text region embeddings are recursively inputted into L flat NER layers, from bottom to top, stacked in a pyramid shape. Each time an embedding passes through a layer of the pyramid, its length is reduced by one. Its hidden state at layer l represents an l-gram in the input text, which is labeled only if its corresponding text region represents a complete entity mention. We also design an inverse pyramid to allow bidirectional interaction between layers. The proposed method achieves state-of-the-art F1 scores in nested NER on ACE-2004, ACE-2005, GENIA, and NNE, which are 80.27, 79.42, 77.78, and 93.70 with conventional embeddings, and 87.74, 86.34, 79.31, and 94.68 with pre-trained contextualized embeddings. In addition, our model can be used for the more general task of Overlapping Named Entity Recognition. A preliminary experiment confirms the effectiveness of our method in overlapping NER."
    }
  },
  {
    "id": "abstract-2021--acl-long--47",
    "result": [
      {
        "value": {
          "start": 295,
          "end": 344,
          "text": "learn adapter parameters for all layers and tasks",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--47:E0"
      },
      {
        "value": {
          "start": 937,
          "end": 951,
          "text": "generalization",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--47:E1"
      },
      {
        "value": {
          "start": 309,
          "end": 319,
          "text": "parameters",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--47:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--47:E0",
        "to_id": "abstract-2021--acl-long--47:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--47:E0",
        "to_id": "abstract-2021--acl-long--47:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "State-of-the-art parameter-efficient fine-tuning methods rely on introducing adapter modules between the layers of a pretrained language model. However, such modules are trained separately for each task and thus do not enable sharing information across tasks. In this paper, we show that we can learn adapter parameters for all layers and tasks by generating them using shared hypernetworks, which condition on task, adapter position, and layer id in a transformer model. This parameter-efficient multi-task learning framework allows us to achieve the best of both worlds by sharing knowledge across tasks via hypernetworks while enabling the model to adapt to each individual task through task-specific adapters. Experiments on the well-known GLUE benchmark show improved performance in multi-task learning while adding only 0.29% parameters per task. We additionally demonstrate substantial performance improvements in few-shot domain generalization across a variety of tasks. Our code is publicly available in https://github.com/rabeehk/hyperformer."
    }
  },
  {
    "id": "abstract-2020--acl-main--620",
    "result": [
      {
        "value": {
          "start": 247,
          "end": 284,
          "text": "uncertainty-aware curriculum learning",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--620:E0"
      },
      {
        "value": {
          "start": 860,
          "end": 879,
          "text": "translation quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--620:E1"
      },
      {
        "value": {
          "start": 884,
          "end": 901,
          "text": "convergence speed",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--620:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--620:E0",
        "to_id": "abstract-2020--acl-main--620:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--620:E0",
        "to_id": "abstract-2020--acl-main--620:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule."
    }
  },
  {
    "id": "abstract-2020--acl-main--556",
    "result": [
      {
        "value": {
          "start": 359,
          "end": 456,
          "text": "employ attention mechanisms to interact between different granularity of semantic representations",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--556:E0"
      },
      {
        "value": {
          "start": 532,
          "end": 543,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--556:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--556:E0",
        "to_id": "abstract-2020--acl-main--556:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In this paper, we propose a multi-granularity interaction network for extractive and abstractive multi-document summarization, which jointly learn semantic representations for words, sentences, and documents. The word representations are used to generate an abstractive summary while the sentence representations are used to produce an extractive summary. We employ attention mechanisms to interact between different granularity of semantic representations, which helps to capture multi-granularity key information and improves the performance of both abstractive and extractive summarization. Experiment results show that our proposed model substantially outperforms all strong baseline methods and achieves the best results on the Multi-News dataset."
    }
  },
  {
    "id": "P10-1044",
    "result": [
      {
        "value": {
          "start": 153,
          "end": 159,
          "text": "LDA-SP",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1044:E0"
      },
      {
        "value": {
          "start": 656,
          "end": 662,
          "text": "recall",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1044:E1"
      },
      {
        "from_id": "P10-1044:E0",
        "to_id": "P10-1044:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The computation of selectional preferences, the admissible argument values for a relation, is a well-known NLP task with broad applicability. We present LDA-SP, which utilizes LinkLDA (Erosheva et al., 2004) to model selectional preferences. By simultaneously inferring latent topics and topic distributions over relations, LDA-SP combines the benefits of previous approaches: like traditional class-based approaches, it produces human-interpretable classes describing each relation's preferences, but it is competitive with non-class-based methods in predictive power. \n \nWe compare LDA-SP to several state-of-the-art methods achieving an 85% increase in recall at 0.9 precision over mutual information (Erk, 2007). We also evaluate LDA-SP's effectiveness at filtering improper applications of inference rules, where we show substantial improvement over Pantel et al.'s system (Pantel et al., 2007)."
    }
  },
  {
    "id": "abstract-2021--acl-long--238",
    "result": [
      {
        "value": {
          "start": 850,
          "end": 865,
          "text": "retrieval model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--238:E0"
      },
      {
        "value": {
          "start": 1122,
          "end": 1131,
          "text": "F 1 score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--238:E1"
      },
      {
        "value": {
          "start": 1243,
          "end": 1259,
          "text": "similarity score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--238:E2"
      },
      {
        "value": {
          "start": 879,
          "end": 916,
          "text": "GPT-2 based property generation model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--238:E3"
      },
      {
        "value": {
          "start": 813,
          "end": 865,
          "text": "latent representation based property retrieval model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--238:E4"
      },
      {
        "from_id": "abstract-2021--acl-long--238:E0",
        "to_id": "abstract-2021--acl-long--238:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--238:E0",
        "to_id": "abstract-2021--acl-long--238:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--238:E3",
        "to_id": "abstract-2021--acl-long--238:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--238:E4",
        "to_id": "abstract-2021--acl-long--238:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--238:E4",
        "to_id": "abstract-2021--acl-long--238:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "CommonsenseQA (CQA) (Talmor et al., 2019) dataset was recently released to advance the research on common-sense question answering (QA) task. Whereas the prior work has mostly focused on proposing QA models for this dataset, our aim is to retrieve as well as generate explanation for a given (question, correct answer choice, incorrect answer choices) tuple from this dataset. Our explanation definition is based on certain desiderata, and translates an explanation into a set of positive and negative common-sense properties (aka facts) which not only explain the correct answer choice but also refute the incorrect ones. We human-annotate a first-of-its-kind dataset (called ECQA) of positive and negative properties, as well as free-flow explanations, for 11K QA pairs taken from the CQA dataset. We propose a latent representation based property retrieval model as well as a GPT-2 based property generation model with a novel two step fine-tuning procedure. We also propose a free-flow explanation generation model. Extensive experiments show that our retrieval model beats BM25 baseline by a relative gain of 100% in F 1 score, property generation model achieves a respectable F 1 score of 36.4, and free-flow generation model achieves a similarity score of 61.9, where last two scores are based on a human correlated semantic similarity metric."
    }
  },
  {
    "id": "abstract-2020--acl-main--642",
    "result": [
      {
        "value": {
          "start": 426,
          "end": 475,
          "text": "dual channel graph convolutional network (DC-GCN)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--642:E0"
      },
      {
        "value": {
          "start": 886,
          "end": 897,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--642:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--642:E0",
        "to_id": "abstract-2020--acl-main--642:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Visual question answering aims to answer the natural language question about a given image. Existing graph-based methods only focus on the relations between objects in an image and neglect the importance of the syntactic dependency relations between words in a question. To simultaneously capture the relations between objects in an image and the syntactic dependency relations between words in a question, we propose a novel dual channel graph convolutional network (DC-GCN) for better combining visual and textual advantages. The DC-GCN model consists of three parts: an I-GCN module to capture the relations between objects in an image, a Q-GCN module to capture the syntactic dependency relations between words in a question, and an attention alignment module to align image representations and question representations. Experimental results show that our model achieves comparable performance with the state-of-the-art approaches."
    }
  },
  {
    "id": "abstract-2021--acl-long--174",
    "result": [
      {
        "value": {
          "start": 537,
          "end": 612,
          "text": "integrate the two types of information with a graph-driven generative model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--174:E0"
      },
      {
        "value": {
          "start": 1052,
          "end": 1063,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--174:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--174:E0",
        "to_id": "abstract-2021--acl-long--174:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "With the need of fast retrieval speed and small memory footprint, document hashing has been playing a crucial role in large-scale information retrieval. To generate high-quality hashing code, both semantics and neighborhood information are crucial. However, most existing methods leverage only one of them or simply combine them via some intuitive criteria, lacking a theoretical principle to guide the integration process. In this paper, we encode the neighborhood information with a graph-induced Gaussian distribution, and propose to integrate the two types of information with a graph-driven generative model. To deal with the complicated correlations among documents, we further propose a tree-structured approximation method for learning. Under the approximation, we prove that the training objective can be decomposed into terms involving only singleton or pairwise documents, enabling the model to be trained as efficiently as uncorrelated ones. Extensive experimental results on three benchmark datasets show that our method achieves superior performance over state-of-the-art methods, demonstrating the effectiveness of the proposed model for simultaneously preserving semantic and neighborhood information."
    }
  },
  {
    "id": "abstract-2020--acl-main--509",
    "result": [
      {
        "value": {
          "start": 1115,
          "end": 1129,
          "text": "SVR-RF-R model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--509:E0"
      },
      {
        "value": {
          "start": 1186,
          "end": 1194,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--509:E1"
      },
      {
        "value": {
          "start": 1224,
          "end": 1228,
          "text": "RMSE",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--509:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--509:E0",
        "to_id": "abstract-2020--acl-main--509:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--509:E0",
        "to_id": "abstract-2020--acl-main--509:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "In online debates, users express different levels of agreement/disagreement with one another’s arguments and ideas. Often levels of agreement/disagreement are implicit in the text, and must be predicted to analyze collective opinions. Existing stance detection methods predict the polarity of a post’s stance toward a topic or post, but don’t consider the stance’s degree of intensity. We introduce a new research problem, stance polarity and intensity prediction in response relationships between posts. This problem is challenging because differences in stance intensity are often subtle and require nuanced language understanding. Cyber argumentation research has shown that incorporating both stance polarity and intensity data in online debates leads to better discussion analysis. We explore five different learning models: Ridge-M regression, Ridge-S regression, SVR-RF-R, pkudblab-PIP, and T-PAN-PIP for predicting stance polarity and intensity in argumentation. These models are evaluated using a new dataset for stance polarity and intensity prediction collected using a cyber argumentation platform. The SVR-RF-R model performs best for prediction of stance polarity with an accuracy of 70.43% and intensity with RMSE of 0.596. This work is the first to train models for predicting a post’s stance polarity and intensity in one combined value in cyber argumentation with reasonably good accuracy."
    }
  },
  {
    "id": "abstract-2021--acl-long--176",
    "result": [
      {
        "value": {
          "start": 951,
          "end": 969,
          "text": "evaluation setting",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--176:E0"
      },
      {
        "value": {
          "start": 1037,
          "end": 1045,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--176:E1"
      },
      {
        "value": {
          "start": 974,
          "end": 1023,
          "text": "creative use of associated database documentation",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--176:E2"
      },
      {
        "value": {
          "start": 1076,
          "end": 1087,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--176:E3"
      },
      {
        "from_id": "abstract-2021--acl-long--176:E0",
        "to_id": "abstract-2021--acl-long--176:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--176:E2",
        "to_id": "abstract-2021--acl-long--176:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--176:E0",
        "to_id": "abstract-2021--acl-long--176:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--176:E2",
        "to_id": "abstract-2021--acl-long--176:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The goal of database question answering is to enable natural language querying of real-life relational databases in diverse application domains. Recently, large-scale datasets such as Spider and WikiSQL facilitated novel modeling techniques for text-to-SQL parsing, improving zero-shot generalization to unseen databases. In this work, we examine the challenges that still prevent these techniques from practical deployment. First, we present KaggleDBQA, a new cross-domain evaluation dataset of real Web databases, with domain-specific data types, original formatting, and unrestricted questions. Second, we re-examine the choice of evaluation tasks for text-to-SQL parsers as applied in real-life settings. Finally, we augment our in-domain evaluation task with database documentation, a naturally occurring source of implicit domain knowledge. We show that KaggleDBQA presents a challenge to state-of-the-art zero-shot parsers but a more realistic evaluation setting and creative use of associated database documentation boosts their accuracy by over 13.2%, doubling their performance."
    }
  },
  {
    "id": "abstract-2020--acl-main--753",
    "result": [
      {
        "value": {
          "start": 761,
          "end": 777,
          "text": "latent variables",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--753:E0"
      },
      {
        "value": {
          "start": 960,
          "end": 964,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--753:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--753:E0",
        "to_id": "abstract-2020--acl-main--753:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "This paper proposes a simple and effective approach to address the problem of posterior collapse in conditional variational autoencoders (CVAEs). It thus improves performance of machine translation models that use noisy or monolingual data, as well as in conventional settings. Extending Transformer and conditional VAEs, our proposed latent variable model measurably prevents posterior collapse by (1) using a modified evidence lower bound (ELBO) objective which promotes mutual information between the latent variable and the target, and (2) guiding the latent variable with an auxiliary bag-of-words prediction task. As a result, the proposed model yields improved translation quality compared to existing variational NMT models on WMT Ro↔En and De↔En. With latent variables being effectively utilized, our model demonstrates improved robustness over non-latent Transformer in handling uncertainty: exploiting noisy source-side monolingual data (up to +3.2 BLEU), and training with weakly aligned web-mined parallel data (up to +4.7 BLEU)."
    }
  },
  {
    "id": "P11-1135",
    "result": [
      {
        "value": {
          "start": 924,
          "end": 945,
          "text": "co-trained classifier",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1135:E0"
      },
      {
        "value": {
          "start": 968,
          "end": 976,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1135:E1"
      },
      {
        "value": {
          "start": 1014,
          "end": 1020,
          "text": "errors",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1135:E2"
      },
      {
        "from_id": "P11-1135:E0",
        "to_id": "P11-1135:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "P11-1135:E0",
        "to_id": "P11-1135:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Resolving coordination ambiguity is a classic hard problem. This paper looks at co-ordination disambiguation in complex noun phrases (NPs). Parsers trained on the Penn Treebank are reporting impressive numbers these days, but they don't do very well on this problem (79%). We explore systems trained using three types of corpora: (1) annotated (e.g. the Penn Treebank), (2) bitexts (e.g. Europarl), and (3) unannotated monolingual (e.g. Google N-grams). Size matters: (1) is a million words, (2) is potentially billions of words and (3) is potentially trillions of words. The unannotated monolingual data is helpful when the ambiguity can be resolved through associations among the lexical items. The bilingual data is helpful when the ambiguity can be resolved by the order of words in the translation. We train separate classifiers with monolingual and bilingual features and iteratively improve them via co-training. The co-trained classifier achieves close to 96% accuracy on Treebank data and makes 20% fewer errors than a supervised system trained with Treebank annotations."
    }
  },
  {
    "id": "abstract-2021--acl-long--14",
    "result": [
      {
        "value": {
          "start": 325,
          "end": 391,
          "text": "disentangling persona-based dialogue generation into two sub-tasks",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--14:E0"
      },
      {
        "value": {
          "start": 921,
          "end": 937,
          "text": "response quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--14:E1"
      },
      {
        "value": {
          "start": 942,
          "end": 961,
          "text": "persona consistency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--14:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--14:E0",
        "to_id": "abstract-2021--acl-long--14:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--14:E0",
        "to_id": "abstract-2021--acl-long--14:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Maintaining a consistent persona is essential for dialogue agents. Although tremendous advancements have been brought, the limited-scale of annotated personalized dialogue datasets is still a barrier towards training robust and consistent persona-based dialogue models. This work shows how this challenge can be addressed by disentangling persona-based dialogue generation into two sub-tasks with a novel BERT-over-BERT (BoB) model. Specifically, the model consists of a BERT-based encoder and two BERT-based decoders, where one decoder is for response generation, and another is for consistency understanding. In particular, to learn the ability of consistency understanding from large-scale non-dialogue inference data, we train the second decoder in an unlikelihood manner. Under different limited data settings, both automatic and human evaluations demonstrate that the proposed model outperforms strong baselines in response quality and persona consistency."
    }
  },
  {
    "id": "abstract-2020--acl-main--533",
    "result": [
      {
        "value": {
          "start": 150,
          "end": 168,
          "text": "multitask learning",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--533:E0"
      },
      {
        "value": {
          "start": 185,
          "end": 196,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--533:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--533:E0",
        "to_id": "abstract-2020--acl-main--533:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Speech translation (ST) aims to learn transformations from speech in the source language to the text in the target language. Previous works show that multitask learning improves the ST performance, in which the recognition decoder generates the text of the source language, and the translation decoder obtains the final translations based on the output of the recognition decoder. Because whether the output of the recognition decoder has the correct semantics is more critical than its accuracy, we propose to improve the multitask ST model by utilizing word embedding as the intermediate."
    }
  },
  {
    "id": "abstract-2021--acl-long--237",
    "result": [
      {
        "value": {
          "start": 636,
          "end": 640,
          "text": "SEQA",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--237:E0"
      },
      {
        "value": {
          "start": 1329,
          "end": 1340,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--237:E1"
      },
      {
        "value": {
          "start": 1071,
          "end": 1081,
          "text": "robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--237:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--237:E0",
        "to_id": "abstract-2021--acl-long--237:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--237:E0",
        "to_id": "abstract-2021--acl-long--237:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Unsupervised commonsense question answering is appealing since it does not rely on any labeled task data. Among existing work, a popular solution is to use pre-trained language models to score candidate choices directly conditioned on the question or context. However, such scores from language models can be easily affected by irrelevant factors, such as word frequencies, sentence structures, etc. These distracting factors may not only mislead the model to choose a wrong answer but also make it oversensitive to lexical perturbations in candidate answers. In this paper, we present a novel SEmantic-based Question Answering method (SEQA) for unsupervised commonsense question answering. Instead of directly scoring each answer choice, our method first generates a set of plausible answers with generative models (e.g., GPT-2), and then uses these plausible answers to select the correct choice by considering the semantic similarity between each plausible answer and each choice. We devise a simple, yet sound formalism for this idea and verify its effectiveness and robustness with extensive experiments. We evaluate the proposed method on four benchmark datasets, and our method achieves the best results in unsupervised settings. Moreover, when attacked by TextFooler with synonym replacement, SEQA demonstrates much less performance drops than baselines, thereby indicating stronger robustness."
    }
  },
  {
    "id": "abstract-2021--acl-long--64",
    "result": [
      {
        "value": {
          "start": 449,
          "end": 472,
          "text": "neuro-symbolic approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--64:E0"
      },
      {
        "value": {
          "start": 1155,
          "end": 1163,
          "text": "F1 score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--64:E1"
      },
      {
        "value": {
          "start": 1217,
          "end": 1288,
          "text": "inductive bias offered by using logic results in a set of learned rules",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--64:E2"
      },
      {
        "value": {
          "start": 1387,
          "end": 1395,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--64:E3"
      },
      {
        "from_id": "abstract-2021--acl-long--64:E0",
        "to_id": "abstract-2021--acl-long--64:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--64:E2",
        "to_id": "abstract-2021--acl-long--64:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Entity linking (EL) is the task of disambiguating mentions appearing in text by linking them to entities in a knowledge graph, a crucial task for text understanding, question answering or conversational systems. In the special case of short-text EL, which poses additional challenges due to limited context, prior approaches have reached good performance by employing heuristics-based methods or purely neural approaches. Here, we take a different, neuro-symbolic approach that combines the advantages of using interpretable rules based on first-order logic with the performance of neural learning. Even though constrained to use rules, we show that we reach competitive or better performance with SoTA black-box neural approaches. Furthermore, our framework has the benefits of extensibility and transferability. We show that we can easily blend existing rule templates given by a human expert, with multiple types of features (priors, BERT encodings, box embeddings, etc), and even with scores resulting from previous EL methods, thus improving on such methods. As an example of improvement, on the LC-QuAD-1.0 dataset, we show more than 3% increase in F1 score relative to previous SoTA. Finally, we show that the inductive bias offered by using logic results in a set of learned rules that transfers from one dataset to another, sometimes without finetuning, while still having high accuracy."
    }
  },
  {
    "id": "P10-1105",
    "result": [
      {
        "value": {
          "start": 116,
          "end": 147,
          "text": "a generative phylogenetic model",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1105:E0"
      },
      {
        "value": {
          "start": 705,
          "end": 713,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1105:E1"
      },
      {
        "from_id": "P10-1105:E0",
        "to_id": "P10-1105:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "A central problem in historical linguistics is the identification of historically related cognate words. We present a generative phylogenetic model for automatically inducing cognate group structure from unaligned word lists. Our model represents the process of transformation and transmission from ancestor word to daughter word, as well as the alignment between the words lists of the observed languages. We also present a novel method for simplifying complex weighted automata created during inference to counteract the otherwise exponential growth of message sizes. On the task of identifying cognates in a dataset of Romance words, our model significantly outperforms a baseline approach, increasing accuracy by as much as 80%. Finally, we demonstrate that our automatically induced groups can be used to successfully reconstruct ancestral words."
    }
  },
  {
    "id": "abstract-2020--acl-main--40",
    "result": [
      {
        "value": {
          "start": 928,
          "end": 974,
          "text": "extremely deep models (with 72-layer encoders)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--40:E0"
      },
      {
        "value": {
          "start": 1013,
          "end": 1024,
          "text": "BLEU points",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--40:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--40:E0",
        "to_id": "abstract-2020--acl-main--40:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recent evidence reveals that Neural Machine Translation (NMT) models with deeper neural networks can be more effective but are difficult to train. In this paper, we present a MultiScale Collaborative (MSC) framework to ease the training of NMT models that are substantially deeper than those used previously. We explicitly boost the gradient back-propagation from top to bottom levels by introducing a block-scale collaboration mechanism into deep NMT models. Then, instead of forcing the whole encoder stack directly learns a desired representation, we let each encoder block learns a fine-grained representation and enhance it by encoding spatial dependencies using a context-scale collaboration. We provide empirical evidence showing that the MSC nets are easy to optimize and can obtain improvements of translation quality from considerably increased depth. On IWSLT translation tasks with three translation directions, our extremely deep models (with 72-layer encoders) surpass strong baselines by +2.2~+3.1 BLEU points. In addition, our deep MSC achieves a BLEU score of 30.56 on WMT14 English-to-German task that significantly outperforms state-of-the-art deep NMT models. We have included the source code in supplementary materials."
    }
  },
  {
    "id": "abstract-2021--acl-long--106",
    "result": [
      {
        "value": {
          "start": 820,
          "end": 872,
          "text": "simplification of the K-best spanning tree algorithm",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--106:E0"
      },
      {
        "value": {
          "start": 955,
          "end": 963,
          "text": "speed-up",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--106:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--106:E0",
        "to_id": "abstract-2021--acl-long--106:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "The connection between the maximum spanning tree in a directed graph and the best dependency tree of a sentence has been exploited by the NLP community. However, for many dependency parsing schemes, an important detail of this approach is that the spanning tree must have exactly one edge emanating from the root. While work has been done to efficiently solve this problem for finding the one-best dependency tree, no research has attempted to extend this solution to finding the K-best dependency trees. This is arguably a more important extension as a larger proportion of decoded trees will not be subject to the root constraint of dependency trees. Indeed, we show that the rate of root constraint violations increases by an average of 13 times when decoding with K=50 as opposed to K=1. In this paper, we provide a simplification of the K-best spanning tree algorithm of Camerini et al. (1980). Our simplification allows us to obtain a constant time speed-up over the original algorithm. Furthermore, we present a novel extension of the algorithm for decoding the K-best dependency trees of a graph which are subject to a root constraint."
    }
  },
  {
    "id": "abstract-2021--acl-long--479",
    "result": [
      {
        "value": {
          "start": 714,
          "end": 735,
          "text": "image retrieval model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--479:E0"
      },
      {
        "value": {
          "start": 751,
          "end": 759,
          "text": "recall@1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--479:E1"
      },
      {
        "value": {
          "start": 843,
          "end": 851,
          "text": "F1 score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--479:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--479:E0",
        "to_id": "abstract-2021--acl-long--479:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--479:E0",
        "to_id": "abstract-2021--acl-long--479:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We present a new human-human dialogue dataset - PhotoChat, the first dataset that casts light on the photo sharing behavior in online messaging. PhotoChat contains 12k dialogues, each of which is paired with a user photo that is shared during the conversation. Based on this dataset, we propose two tasks to facilitate research on image-text modeling: a photo-sharing intent prediction task that predicts whether one intends to share a photo in the next conversation turn, and a photo retrieval task that retrieves the most relevant photo according to the dialogue context. In addition, for both tasks, we provide baseline models using the state-of-the-art models and report their benchmark performances. The best image retrieval model achieves 10.4% recall@1 (out of 1000 candidates) and the best photo intent prediction model achieves 58.1% F1 score, indicating that the dataset presents interesting yet challenging real-world problems. We are releasing PhotoChat to facilitate future research work among the community."
    }
  },
  {
    "id": "abstract-2021--acl-long--364",
    "result": [
      {
        "value": {
          "start": 1348,
          "end": 1387,
          "text": "limiting the number of tracked mentions",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--364:E0"
      },
      {
        "value": {
          "start": 1064,
          "end": 1075,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--364:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--364:E0",
        "to_id": "abstract-2021--acl-long--364:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Streaming cross document entity coreference (CDC) systems disambiguate mentions of named entities in a scalable manner via incremental clustering. Unlike other approaches for named entity disambiguation (e.g., entity linking), streaming CDC allows for the disambiguation of entities that are unknown at inference time. Thus, it is well-suited for processing streams of data where new entities are frequently introduced. Despite these benefits, this task is currently difficult to study, as existing approaches are either evaluated on datasets that are no longer available, or omit other crucial details needed to ensure fair comparison. In this work, we address this issue by compiling a large benchmark adapted from existing free datasets, and performing a comprehensive evaluation of a number of novel and existing baseline models. We investigate: how to best encode mentions, which clustering algorithms are most effective for grouping mentions, how models transfer to different domains, and how bounding the number of mentions tracked during inference impacts performance. Our results show that the relative performance of neural and feature-based mention encoders varies across different domains, and in most cases the best performance is achieved using a combination of both approaches. We also find that performance is minimally impacted by limiting the number of tracked mentions."
    }
  },
  {
    "id": "abstract-2021--acl-long--318",
    "result": [
      {
        "value": {
          "start": 889,
          "end": 976,
          "text": "maximizing the mutual information between question-answer pairs and predicted solutions",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--318:E0"
      },
      {
        "value": {
          "start": 354,
          "end": 365,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--318:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--318:E0",
        "to_id": "abstract-2021--acl-long--318:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Weakly supervised question answering usually has only the final answers as supervision signals while the correct solutions to derive the answers are not provided. This setting gives rise to the spurious solution problem: there may exist many spurious solutions that coincidentally derive the correct answer, but training on such solutions can hurt model performance (e.g., producing wrong solutions or answers). For example, for discrete reasoning tasks as on DROP, there may exist many equations to derive a numeric answer, and typically only one of them is correct. Previous learning methods mostly filter out spurious solutions with heuristics or using model confidence, but do not explicitly exploit the semantic correlations between a question and its solution. In this paper, to alleviate the spurious solution problem, we propose to explicitly exploit such semantic correlations by maximizing the mutual information between question-answer pairs and predicted solutions. Extensive experiments on four question answering datasets show that our method significantly outperforms previous learning methods in terms of task performance and is more effective in training models to produce correct solutions."
    }
  },
  {
    "id": "abstract-2020--acl-main--350",
    "result": [
      {
        "value": {
          "start": 25,
          "end": 36,
          "text": "SimulSpeech",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--350:E0"
      },
      {
        "value": {
          "start": 1308,
          "end": 1319,
          "text": "BLEU scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--350:E1"
      },
      {
        "value": {
          "start": 1330,
          "end": 1335,
          "text": "delay",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--350:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--350:E0",
        "to_id": "abstract-2020--acl-main--350:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--350:E0",
        "to_id": "abstract-2020--acl-main--350:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "In this work, we develop SimulSpeech, an end-to-end simultaneous speech to text translation system which translates speech in source language to text in target language concurrently. SimulSpeech consists of a speech encoder, a speech segmenter and a text decoder, where 1) the segmenter builds upon the encoder and leverages a connectionist temporal classification (CTC) loss to split the input streaming speech in real time, 2) the encoder-decoder attention adopts a wait- k  strategy for simultaneous translation. SimulSpeech is more challenging than previous cascaded systems (with simultaneous automatic speech recognition (ASR) and simultaneous neural machine translation (NMT)). We introduce two novel knowledge distillation methods to ensure the performance: 1) Attention-level knowledge distillation transfers the knowledge from the multiplication of the attention matrices of simultaneous NMT and ASR models to help the training of the attention mechanism in SimulSpeech; 2) Data-level knowledge distillation transfers the knowledge from the full-sentence NMT model and also reduces the complexity of data distribution to help on the optimization of SimulSpeech. Experiments on MuST-C English-Spanish and English-German spoken language translation datasets show that SimulSpeech achieves reasonable BLEU scores and lower delay compared to full-sentence end-to-end speech to text translation (without simultaneous translation), and better performance than the two-stage cascaded simultaneous translation model in terms of BLEU scores and translation delay."
    }
  },
  {
    "id": "abstract-2020--acl-main--528",
    "result": [
      {
        "value": {
          "start": 441,
          "end": 506,
          "text": "incorporating the word lexicon into the character representations",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--528:E0"
      },
      {
        "value": {
          "start": 817,
          "end": 832,
          "text": "inference speed",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--528:E1"
      },
      {
        "value": {
          "start": 47,
          "end": 58,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--528:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--528:E0",
        "to_id": "abstract-2020--acl-main--528:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--528:E0",
        "to_id": "abstract-2020--acl-main--528:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recently, many works have tried to augment the performance of Chinese named entity recognition (NER) using word lexicons. As a representative, Lattice-LSTM has achieved new benchmark results on several public Chinese NER datasets. However, Lattice-LSTM has a complex model architecture. This limits its application in many industrial areas where real-time NER responses are needed. In this work, we propose a simple but effective method for incorporating the word lexicon into the character representations. This method avoids designing a complicated sequence modeling architecture, and for any neural NER model, it requires only subtle adjustment of the character representation layer to introduce the lexicon information. Experimental studies on four benchmark Chinese NER datasets show that our method achieves an inference speed up to 6.15 times faster than those of state-of-the-art methods, along with a better performance. The experimental results also show that the proposed method can be easily incorporated with pre-trained models like BERT."
    }
  },
  {
    "id": "abstract-2021--acl-long--232",
    "result": [
      {
        "value": {
          "start": 21,
          "end": 81,
          "text": "extractive summarization-based collaborative filtering model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--232:E0"
      },
      {
        "value": {
          "start": 716,
          "end": 724,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--232:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--232:E0",
        "to_id": "abstract-2021--acl-long--232:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We pioneer the first extractive summarization-based collaborative filtering model called ESCOFILT. Our proposed model specifically produces extractive summaries for each item and user. Unlike other types of explanations, summary-level explanations closely resemble real-life explanations. The strength of ESCOFILT lies in the fact that it unifies representation and explanation. In other words, extractive summaries both represent and explain the items and users. Our model uniquely integrates BERT, K-Means embedding clustering, and multilayer perceptron to learn sentence embeddings, representation-explanations, and user-item interactions, respectively. We argue that our approach enhances both rating prediction accuracy and user/item explainability. Our experiments illustrate that ESCOFILT’s prediction accuracy is better than the other state-of-the-art recommender models. Furthermore, we propose a comprehensive set of criteria that assesses the real-life explainability of explanations. Our explainability study demonstrates the superiority of and preference for summary-level explanations over other explanation types."
    }
  },
  {
    "id": "abstract-2020--acl-main--647",
    "result": [
      {
        "value": {
          "start": 191,
          "end": 229,
          "text": "Iterative Null-space Projection (INLP)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--647:E0"
      },
      {
        "value": {
          "start": 675,
          "end": 679,
          "text": "bias",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--647:E1"
      },
      {
        "value": {
          "start": 684,
          "end": 692,
          "text": "fairness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--647:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--647:E0",
        "to_id": "abstract-2020--acl-main--647:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--647:E0",
        "to_id": "abstract-2020--acl-main--647:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations. Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it. While applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification."
    }
  },
  {
    "id": "abstract-2020--acl-main--161",
    "result": [
      {
        "value": {
          "start": 757,
          "end": 799,
          "text": "uncertainty reduction over representations",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--161:E0"
      },
      {
        "value": {
          "start": 847,
          "end": 855,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--161:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--161:E0",
        "to_id": "abstract-2020--acl-main--161:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Suspense is a crucial ingredient of narrative fiction, engaging readers and making stories compelling. While there is a vast theoretical literature on suspense, it is computationally not well understood. We compare two ways for modelling suspense: surprise, a backward-looking measure of how unexpected the current state is given the story so far; and uncertainty reduction, a forward-looking measure of how unexpected the continuation of the story is. Both can be computed either directly over story representations or over their probability distributions. We propose a hierarchical language model that encodes stories and computes surprise and uncertainty reduction. Evaluating against short stories annotated with human suspense judgements, we find that uncertainty reduction over representations is the best predictor, resulting in near human accuracy. We also show that uncertainty reduction can be used to predict suspenseful events in movie synopses."
    }
  },
  {
    "id": "abstract-2020--acl-main--89",
    "result": [
      {
        "value": {
          "start": 610,
          "end": 617,
          "text": "GenBERT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--89:E0"
      },
      {
        "value": {
          "start": 655,
          "end": 666,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--89:E1"
      },
      {
        "value": {
          "start": 586,
          "end": 608,
          "text": "pre-training our model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--89:E2"
      },
      {
        "value": {
          "start": 655,
          "end": 666,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--89:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--89:E0",
        "to_id": "abstract-2020--acl-main--89:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--89:E2",
        "to_id": "abstract-2020--acl-main--89:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information. However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only. Consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility. In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup. We show that pre-training our model, GenBERT, on this data, dramatically improves performance on DROP (49.3 –> 72.3 F1), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture. Moreover, GenBERT generalizes well to math word problem datasets, while maintaining high performance on standard RC tasks. Our approach provides a general recipe for injecting skills into large pre-trained LMs, whenever the skill is amenable to automatic data augmentation."
    }
  },
  {
    "id": "abstract-2020--acl-main--472",
    "result": [
      {
        "value": {
          "start": 646,
          "end": 683,
          "text": "using both word and message attention",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--472:E0"
      },
      {
        "value": {
          "start": 612,
          "end": 622,
          "text": "accuracies",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--472:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--472:E0",
        "to_id": "abstract-2020--acl-main--472:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Not all documents are equally important. Language processing is increasingly finding use as a supplement for questionnaires to assess psychological attributes of consenting individuals, but most approaches neglect to consider whether all documents of an individual are equally informative. In this paper, we present a novel model that uses message-level attention to learn the relative weight of users’ social media posts for assessing their five factor personality traits. We demonstrate that models with message-level attention outperform those with word-level attention, and ultimately yield state-of-the-art accuracies for all five traits by using both word and message attention in combination with past approaches (an average increase in Pearson r of 2.5%). In addition, examination of the high-signal posts identified by our model provides insight into the relationship between language and personality, helping to inform future work."
    }
  },
  {
    "id": "abstract-2021--acl-long--451",
    "result": [
      {
        "value": {
          "start": 474,
          "end": 543,
          "text": "formulate the NER subtasks as an entity span sequence generation task",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--451:E0"
      },
      {
        "value": {
          "start": 998,
          "end": 1009,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--451:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--451:E0",
        "to_id": "abstract-2021--acl-long--451:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Named Entity Recognition (NER) is the task of identifying spans that represent entities in sentences. Whether the entity spans are nested or discontinuous, the NER task can be categorized into the flat NER, nested NER, and discontinuous NER subtasks. These subtasks have been mainly solved by the token-level sequence labelling or span-level classification. However, these solutions can hardly tackle the three kinds of NER subtasks concurrently. To that end, we propose to formulate the NER subtasks as an entity span sequence generation task, which can be solved by a unified sequence-to-sequence (Seq2Seq) framework. Based on our unified framework, we can leverage the pre-trained Seq2Seq model to solve all three kinds of NER subtasks without the special design of the tagging schema or ways to enumerate spans. We exploit three types of entity representations to linearize entities into a sequence. Our proposed framework is easy-to-implement and achieves state-of-the-art (SoTA) or near SoTA performance on eight English NER datasets, including two flat NER datasets, three nested NER datasets, and three discontinuous NER datasets."
    }
  },
  {
    "id": "P10-1062",
    "result": [
      {
        "value": {
          "start": 313,
          "end": 332,
          "text": "linguistic features",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1062:E0"
      },
      {
        "value": {
          "start": 907,
          "end": 917,
          "text": "error rate",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1062:E1"
      },
      {
        "from_id": "P10-1062:E0",
        "to_id": "P10-1062:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Automatic error detection is desired in the post-processing to improve machine translation quality. The previous work is largely based on confidence estimation using system-based features, such as word posterior probabilities calculated from N-best lists or word lattices. We propose to incorporate two groups of linguistic features, which convey information from outside machine translation systems, into error detection: lexical and syntactic features. We use a maximum entropy classifier to predict translation errors by integrating word posterior probability feature and linguistic features. The experimental results show that 1) linguistic features alone outperform word posterior probability based confidence estimation in error detection; and 2) linguistic features can further provide complementary information when combined with word confidence scores, which collectively reduce the classification error rate by 18.52% and improve the F measure by 16.37%."
    }
  },
  {
    "id": "P11-1122",
    "result": [
      {
        "value": {
          "start": 180,
          "end": 203,
          "text": "a variety of mechanisms",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1122:E0"
      },
      {
        "value": {
          "start": 222,
          "end": 241,
          "text": "translation quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1122:E1"
      },
      {
        "value": {
          "start": 402,
          "end": 472,
          "text": "a set of features that model both the translations and the translators",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1122:E2"
      },
      {
        "value": {
          "start": 1005,
          "end": 1015,
          "text": "total cost",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1122:E3"
      },
      {
        "from_id": "P11-1122:E0",
        "to_id": "P11-1122:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P11-1122:E2",
        "to_id": "P11-1122:E3",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Naively collecting translations by crowd-sourcing the task to non-professional translators yields disfluent, low-quality results if no quality control is exercised. We demonstrate a variety of mechanisms that increase the translation quality to near professional levels. Specifically, we solicit redundant translations and edits to them, and automatically select the best output among them. We propose a set of features that model both the translations and the translators, such as country of residence, LM perplexity of the translation, edit rate from the other translations, and (optionally) calibration against professional translators. Using these features to score the collected translations, we are able to discriminate between acceptable and unacceptable translations. We recreate the NIST 2009 Urdu-to-English evaluation set with Mechanical Turk, and quantitatively show that our models are able to select translations within the range of quality that we expect from professional translators. The total cost is more than an order of magnitude lower than professional translation."
    }
  },
  {
    "id": "abstract-2020--acl-main--141",
    "result": [
      {
        "value": {
          "start": 595,
          "end": 703,
          "text": "empowers the relational reasoning across sentences by automatically inducing the latent document-level graph",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--141:E0"
      },
      {
        "value": {
          "start": 880,
          "end": 888,
          "text": "F1 score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--141:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--141:E0",
        "to_id": "abstract-2020--acl-main--141:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale document-level dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations."
    }
  },
  {
    "id": "abstract-2021--acl-long--373",
    "result": [
      {
        "value": {
          "start": 297,
          "end": 341,
          "text": "Multi-Layer Bidirectional Net work (MLBiNet)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--373:E0"
      },
      {
        "value": {
          "start": 968,
          "end": 979,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--373:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--373:E0",
        "to_id": "abstract-2021--acl-long--373:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We consider the problem of collectively detecting multiple events, particularly in cross-sentence settings. The key to dealing with the problem is to encode semantic information and model event inter-dependency at a document-level. In this paper, we reformulate it as a Seq2Seq task and propose a Multi-Layer Bidirectional Net work (MLBiNet) to capture the document-level association of events and semantic information simultaneously. Specifically, a bidirectional decoder is firstly devised to model event inter-dependency within a sentence when decoding the event tag vector sequence. Secondly, an information aggregation module is employed to aggregate sentence-level semantic and event tag information. Finally, we stack multiple bidirectional decoders and feed cross-sentence information, forming a multi-layer bidirectional tagging architecture to iteratively propagate information across sentences. We show that our approach provides significant improvement in performance compared to the current state-of-the-art results."
    }
  },
  {
    "id": "P11-1056",
    "result": [
      {
        "value": {
          "start": 603,
          "end": 615,
          "text": "RE framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1056:E0"
      },
      {
        "value": {
          "start": 655,
          "end": 666,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1056:E1"
      },
      {
        "from_id": "P11-1056:E0",
        "to_id": "P11-1056:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In this paper, we observe that there exists a second dimension to the relation extraction (RE) problem that is orthogonal to the relation type dimension. We show that most of these second dimensional structures are relatively constrained and not difficult to identify. We propose a novel algorithmic approach to RE that starts by first identifying these structures and then, within these, identifying the semantic type of the relation. In the real RE problem where relation arguments need to be identified, exploiting these structures also allows reducing pipelined propagated errors. We show that this RE framework provides significant improvement in RE performance."
    }
  },
  {
    "id": "abstract-2020--acl-main--366",
    "result": [
      {
        "value": {
          "start": 342,
          "end": 407,
          "text": "builds a  graph autoencoder  (GAE) on a Keyword Correlation Graph",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--366:E0"
      },
      {
        "value": {
          "start": 885,
          "end": 896,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--366:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--366:E0",
        "to_id": "abstract-2020--acl-main--366:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Document clustering requires a deep understanding of the complex structure of long-text; in particular, the intra-sentential (local) and inter-sentential features (global). Existing representation learning models do not fully capture these features. To address this, we present a novel graph-based representation for document clustering that builds a  graph autoencoder  (GAE) on a Keyword Correlation Graph. The graph is constructed with topical keywords as nodes and multiple local and global features as edges. A GAE is employed to aggregate the two sets of features by learning a latent representation which can jointly reconstruct them. Clustering is then performed on the learned representations, using vector dimensions as features for inducing document classes. Extensive experiments on two datasets show that the features learned by our approach can achieve better clustering performance than other existing features, including term frequency-inverse document frequency and average embedding."
    }
  },
  {
    "id": "abstract-2021--acl-long--496",
    "result": [
      {
        "value": {
          "start": 447,
          "end": 503,
          "text": "attention-guided multi-layer multi-cross encoding scheme",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--496:E0"
      },
      {
        "value": {
          "start": 1040,
          "end": 1051,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--496:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--496:E0",
        "to_id": "abstract-2021--acl-long--496:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Argument pair extraction (APE) is a research task for extracting arguments from two passages and identifying potential argument pairs. Prior research work treats this task as a sequence labeling problem and a binary classification problem on two passages that are directly concatenated together, which has a limitation of not fully utilizing the unique characteristics and inherent relations of two different passages. This paper proposes a novel attention-guided multi-layer multi-cross encoding scheme to address the challenges. The new model processes two passages with two individual sequence encoders and updates their representations using each other’s representations through attention. In addition, the pair prediction part is formulated as a table-filling problem by updating the representations of two sequences’ Cartesian product. Furthermore, an auxiliary attention loss is introduced to guide each argument to align to its paired argument. An extensive set of experiments show that the new model significantly improves the APE performance over several alternatives."
    }
  },
  {
    "id": "abstract-2020--acl-main--60",
    "result": [
      {
        "value": {
          "start": 271,
          "end": 328,
          "text": "paraphrase augmented response generation (PARG) framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--60:E0"
      },
      {
        "value": {
          "start": 49,
          "end": 60,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--60:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--60:E0",
        "to_id": "abstract-2020--acl-main--60:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Neural generative models have achieved promising performance on dialog generation tasks if given a huge data set. However, the lack of high-quality dialog data and the expensive data annotation process greatly limit their application in real world settings. We propose a paraphrase augmented response generation (PARG) framework that jointly trains a paraphrase model and a response generation model to improve the dialog generation performance. We also design a method to automatically construct paraphrase training data set based on dialog state and dialog act labels. PARG is applicable to various dialog generation models, such as TSCP (Lei et al., 2018) and DAMD (Zhang et al., 2019). Experimental results show that the proposed framework improves these state-of-the-art dialog models further on CamRest676 and MultiWOZ. PARG also outperforms other data augmentation methods significantly in dialog generation tasks, especially under low resource settings."
    }
  },
  {
    "id": "abstract-2021--acl-long--442",
    "result": [
      {
        "value": {
          "start": 716,
          "end": 735,
          "text": "incorporating CoCLR",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--442:E0"
      },
      {
        "value": {
          "start": 668,
          "end": 676,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--442:E1"
      },
      {
        "value": {
          "start": 637,
          "end": 654,
          "text": "training on CoSQA",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--442:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--442:E0",
        "to_id": "abstract-2021--acl-long--442:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--442:E2",
        "to_id": "abstract-2021--acl-long--442:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Finding codes given natural language query is beneficial to the productivity of software developers. Future progress towards better semantic matching between query and code requires richer supervised training resources. To remedy this, we introduce CoSQA dataset. It includes 20,604 labels for pairs of natural language queries and codes, each annotated by at least 3 human annotators. We further introduce a contrastive learning method dubbed CoCLR to enhance text-code matching, which works as a data augmenter to bring more artificially generated training instances. We show that, evaluated on CodeXGLUE with the same CodeBERT model, training on CoSQA improves the accuracy of code question answering by 5.1% and incorporating CoCLR brings a further improvement of 10.5%."
    }
  },
  {
    "id": "abstract-2020--acl-main--631",
    "result": [
      {
        "value": {
          "start": 550,
          "end": 584,
          "text": "masked sequence-to-sequence method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--631:E0"
      },
      {
        "value": {
          "start": 882,
          "end": 894,
          "text": "performances",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--631:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--631:E0",
        "to_id": "abstract-2020--acl-main--631:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Aspect term extraction aims to extract aspect terms from review texts as opinion targets for sentiment analysis. One of the big challenges with this task is the lack of sufficient annotated data. While data augmentation is potentially an effective technique to address the above issue, it is uncontrollable as it may change aspect words and aspect labels unexpectedly. In this paper, we formulate the data augmentation as a conditional generation task: generating a new sentence while preserving the original opinion targets and labels. We propose a masked sequence-to-sequence method for conditional augmentation of aspect term extraction. Unlike existing augmentation approaches, ours is controllable and allows to generate more diversified sentences. Experimental results confirm that our method alleviates the data scarcity problem significantly. It also effectively boosts the performances of several current models for aspect term extraction."
    }
  },
  {
    "id": "abstract-2021--acl-long--338",
    "result": [
      {
        "value": {
          "start": 447,
          "end": 456,
          "text": "HiddenCut",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--338:E0"
      },
      {
        "value": {
          "start": 813,
          "end": 827,
          "text": "generalization",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--338:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--338:E0",
        "to_id": "abstract-2021--acl-long--338:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Fine-tuning large pre-trained models with task-specific data has achieved great success in NLP. However, it has been demonstrated that the majority of information within the self-attention networks is redundant and not utilized effectively during the fine-tuning stage. This leads to inferior results when generalizing the obtained models to out-of-domain distributions. To this end, we propose a simple yet effective data augmentation technique, HiddenCut, to better regularize the model and encourage it to learn more generalizable features. Specifically, contiguous spans within the hidden space are dynamically and strategically dropped during training. Experiments show that our HiddenCut method outperforms the state-of-the-art augmentation methods on the GLUE benchmark, and consistently exhibits superior generalization performances on out-of-distribution and challenging counterexamples. We have publicly released our code at https://github.com/GT-SALT/HiddenCut."
    }
  },
  {
    "id": "abstract-2021--acl-long--478",
    "result": [
      {
        "value": {
          "start": 356,
          "end": 362,
          "text": "ExCorD",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--478:E0"
      },
      {
        "value": {
          "start": 814,
          "end": 825,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--478:E1"
      },
      {
        "value": {
          "start": 839,
          "end": 841,
          "text": "F1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--478:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--478:E0",
        "to_id": "abstract-2021--acl-long--478:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--478:E0",
        "to_id": "abstract-2021--acl-long--478:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "One of the main challenges in conversational question answering (CQA) is to resolve the conversational dependency, such as anaphora and ellipsis. However, existing approaches do not explicitly train QA models on how to resolve the dependency, and thus these models are limited in understanding human dialogues. In this paper, we propose a novel framework, ExCorD (Explicit guidance on how to resolve Conversational Dependency) to enhance the abilities of QA models in comprehending conversational context. ExCorD first generates self-contained questions that can be understood without the conversation history, then trains a QA model with the pairs of original and self-contained questions using a consistency-based regularizer. In our experiments, we demonstrate that ExCorD significantly improves the QA models’ performance by up to 1.2 F1 on QuAC, and 5.2 F1 on CANARD, while addressing the limitations of the existing approaches."
    }
  },
  {
    "id": "abstract-2020--acl-main--303",
    "result": [
      {
        "value": {
          "start": 727,
          "end": 776,
          "text": "fine-tuning on a small amount of constructed data",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--303:E0"
      },
      {
        "value": {
          "start": 652,
          "end": 672,
          "text": "syntactic robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--303:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--303:E0",
        "to_id": "abstract-2020--acl-main--303:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Sequence-based neural networks show significant sensitivity to syntactic structure, but they still perform less well on syntactic tasks than tree-based networks. Such tree-based networks can be provided with a constituency parse, a dependency parse, or both. We evaluate which of these two representational schemes more effectively introduces biases for syntactic structure that increase performance on the subject-verb agreement prediction task. We find that a constituency-based network generalizes more robustly than a dependency-based one, and that combining the two types of structure does not yield further improvement. Finally, we show that the syntactic robustness of sequential models can be substantially improved by fine-tuning on a small amount of constructed data, suggesting that data augmentation is a viable alternative to explicit constituency structure for imparting the syntactic biases that sequential models are lacking."
    }
  },
  {
    "id": "abstract-2021--acl-long--221",
    "result": [
      {
        "value": {
          "start": 1106,
          "end": 1165,
          "text": "emphasizing the learning on uncertain monolingual sentences",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--221:E0"
      },
      {
        "value": {
          "start": 1199,
          "end": 1218,
          "text": "translation quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--221:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--221:E0",
        "to_id": "abstract-2021--acl-long--221:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Self-training has proven effective for improving NMT performance by augmenting model training with synthetic parallel data. The common practice is to construct synthetic data based on a randomly sampled subset of large-scale monolingual data, which we empirically show is sub-optimal. In this work, we propose to improve the sampling procedure by selecting the most informative monolingual sentences to complement the parallel data. To this end, we compute the uncertainty of monolingual sentences using the bilingual dictionary extracted from the parallel data. Intuitively, monolingual sentences with lower uncertainty generally correspond to easy-to-translate patterns which may not provide additional gains. Accordingly, we design an uncertainty-based sampling strategy to efficiently exploit the monolingual data for self-training, in which monolingual sentences with higher uncertainty would be sampled with higher probability. Experimental results on large-scale WMT English⇒German and English⇒Chinese datasets demonstrate the effectiveness of the proposed approach. Extensive analyses suggest that emphasizing the learning on uncertain monolingual sentences by our approach does improve the translation quality of high-uncertainty sentences and also benefits the prediction of low-frequency words at the target side."
    }
  },
  {
    "id": "abstract-2020--acl-main--241",
    "result": [
      {
        "value": {
          "start": 304,
          "end": 327,
          "text": "distance-based approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--241:E0"
      },
      {
        "value": {
          "start": 1065,
          "end": 1073,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--241:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--241:E0",
        "to_id": "abstract-2020--acl-main--241:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First, we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes."
    }
  },
  {
    "id": "P10-1003",
    "result": [
      {
        "value": {
          "start": 22,
          "end": 47,
          "text": "dependency parsing method",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1003:E0"
      },
      {
        "value": {
          "start": 95,
          "end": 103,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1003:E1"
      },
      {
        "value": {
          "start": 58,
          "end": 79,
          "text": "bilingual constraints",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1003:E2"
      },
      {
        "from_id": "P10-1003:E0",
        "to_id": "P10-1003:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P10-1003:E2",
        "to_id": "P10-1003:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This paper proposes a dependency parsing method that uses bilingual constraints to improve the accuracy of parsing bilingual texts (bitexts). In our method, a target-side tree fragment that corresponds to a source-side tree fragment is identified via word alignment and mapping rules that are automatically learned. Then it is verified by checking the subtree list that is collected from large scale automatically parsed data on the target side. Our method, thus, requires gold standard trees only on the source side of a bilingual corpus in the training phase, unlike the joint parsing model, which requires gold standard trees on the both sides. Compared to the reordering constraint model, which requires the same training data as ours, our method achieved higher accuracy because of richer bilingual constraints. Experiments on the translated portion of the Chinese Treebank show that our system outperforms monolingual parsers by 2.93 points for Chinese and 1.64 points for English."
    }
  },
  {
    "id": "abstract-2020--acl-main--23",
    "result": [
      {
        "value": {
          "start": 478,
          "end": 483,
          "text": "PPVAE",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--23:E0"
      },
      {
        "value": {
          "start": 942,
          "end": 956,
          "text": "conditionality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--23:E1"
      },
      {
        "value": {
          "start": 961,
          "end": 970,
          "text": "diversity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--23:E2"
      },
      {
        "value": {
          "start": 980,
          "end": 995,
          "text": "training effort",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--23:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--23:E0",
        "to_id": "abstract-2020--acl-main--23:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--23:E0",
        "to_id": "abstract-2020--acl-main--23:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--23:E0",
        "to_id": "abstract-2020--acl-main--23:E3",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Conditional Text Generation has drawn much attention as a topic of Natural Language Generation (NLG) which provides the possibility for humans to control the properties of generated contents. Current conditional generation models cannot handle emerging conditions due to their joint end-to-end learning fashion. When a new condition added, these techniques require full retraining. In this paper, we present a new framework named Pre-train and Plug-in Variational Auto-Encoder (PPVAE) towards flexible conditional text generation. PPVAE decouples the text generation module from the condition representation module to allow “one-to-many” conditional generation. When a fresh condition emerges, only a lightweight network needs to be trained and works as a plug-in for PPVAE, which is efficient and desirable for real-world applications. Extensive experiments demonstrate the superiority of PPVAE against the existing alternatives with better conditionality and diversity but less training effort."
    }
  },
  {
    "id": "abstract-2021--acl-long--222",
    "result": [
      {
        "value": {
          "start": 372,
          "end": 394,
          "text": "two pre-training tasks",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--222:E0"
      },
      {
        "value": {
          "start": 932,
          "end": 955,
          "text": "translation performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--222:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--222:E0",
        "to_id": "abstract-2021--acl-long--222:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Context-aware neural machine translation (NMT) remains challenging due to the lack of large-scale document-level parallel corpora. To break the corpus bottleneck, in this paper we aim to improve context-aware NMT by taking the advantage of the availability of both large-scale sentence-level parallel dataset and source-side monolingual documents. To this end, we propose two pre-training tasks. One learns to translate a sentence from source language to target language on the sentence-level parallel dataset while the other learns to translate a document from deliberately noised to original on the monolingual documents. Importantly, the two pre-training tasks are jointly and simultaneously learned via the same model, thereafter fine-tuned on scale-limited parallel documents from both sentence-level and document-level perspectives. Experimental results on four translation tasks show that our approach significantly improves translation performance. One nice property of our approach is that the fine-tuned model can be used to translate both sentences and documents."
    }
  },
  {
    "id": "abstract-2021--acl-long--141",
    "result": [
      {
        "value": {
          "start": 805,
          "end": 835,
          "text": "automatically generated labels",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--141:E0"
      },
      {
        "value": {
          "start": 841,
          "end": 852,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--141:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--141:E0",
        "to_id": "abstract-2021--acl-long--141:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Recently, there is an effort to extend fine-grained entity typing by using a richer and ultra-fine set of types, and labeling noun phrases including pronouns and nominal nouns instead of just named entity mentions. A key challenge for this ultra-fine entity typing task is that human annotated data are extremely scarce, and the annotation ability of existing distant or weak supervision approaches is very limited. To remedy this problem, in this paper, we propose to obtain training data for ultra-fine entity typing by using a BERT Masked Language Model (MLM). Given a mention in a sentence, our approach constructs an input for the BERT MLM so that it predicts context dependent hypernyms of the mention, which can be used as type labels. Experimental results demonstrate that, with the help of these automatically generated labels, the performance of an ultra-fine entity typing model can be improved substantially. We also show that our approach can be applied to improve traditional fine-grained entity typing after performing simple type mapping."
    }
  },
  {
    "id": "abstract-2020--acl-main--252",
    "result": [
      {
        "value": {
          "start": 585,
          "end": 607,
          "text": "Using monolingual data",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--252:E0"
      },
      {
        "value": {
          "start": 633,
          "end": 652,
          "text": "translation quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--252:E1"
      },
      {
        "value": {
          "start": 794,
          "end": 843,
          "text": "Leveraging monolingual data with self-supervision",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--252:E2"
      },
      {
        "value": {
          "start": 937,
          "end": 941,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--252:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--252:E0",
        "to_id": "abstract-2020--acl-main--252:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--252:E2",
        "to_id": "abstract-2020--acl-main--252:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Over the last few years two promising research directions in low-resource neural machine translation (NMT) have emerged. The first focuses on utilizing high-resource languages to improve the quality of low-resource languages via multilingual NMT. The second direction employs monolingual data with self-supervision to pre-train translation models, followed by fine-tuning on small amounts of supervised data. In this work, we join these two lines of research and demonstrate the efficacy of monolingual data with self-supervision in multilingual NMT. We offer three major results: (i) Using monolingual data significantly boosts the translation quality of low-resource languages in multilingual models. (ii) Self-supervision improves zero-shot translation quality in multilingual models. (iii) Leveraging monolingual data with self-supervision provides a viable path towards adding new languages to multilingual models, getting up to 33 BLEU on ro-en translation without any parallel data or back-translation."
    }
  },
  {
    "id": "abstract-2021--acl-long--136",
    "result": [
      {
        "value": {
          "start": 956,
          "end": 1003,
          "text": "use of dialog structure as background knowledge",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--136:E0"
      },
      {
        "value": {
          "start": 1030,
          "end": 1050,
          "text": "multi-turn coherence",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--136:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--136:E0",
        "to_id": "abstract-2021--acl-long--136:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Learning discrete dialog structure graph from human-human dialogs yields basic insights into the structure of conversation, and also provides background knowledge to facilitate dialog generation. However, this problem is less studied in open-domain dialogue. In this paper, we conduct unsupervised discovery of discrete dialog structure from chitchat corpora, and then leverage it to facilitate coherent dialog generation in downstream systems. To this end, we present an unsupervised model, Discrete Variational Auto-Encoder with Graph Neural Network (DVAE-GNN), to discover discrete hierarchical latent dialog states (at the level of both session and utterance) and their transitions from corpus as a dialog structure graph. Then we leverage it as background knowledge to facilitate dialog management in a RL based dialog system. Experimental results on two benchmark corpora confirm that DVAE-GNN can discover meaningful dialog structure graph, and the use of dialog structure as background knowledge can significantly improve multi-turn coherence."
    }
  },
  {
    "id": "abstract-2020--acl-main--357",
    "result": [
      {
        "value": {
          "start": 476,
          "end": 531,
          "text": "casts a plausibility ranking task in a full-text format",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--357:E0"
      },
      {
        "value": {
          "start": 892,
          "end": 900,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--357:E1"
      },
      {
        "value": {
          "start": 536,
          "end": 615,
          "text": "leverages the masked language modeling head tuned during the pre-training phase",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--357:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--357:E0",
        "to_id": "abstract-2020--acl-main--357:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--357:E2",
        "to_id": "abstract-2020--acl-main--357:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Fine-tuning of pre-trained transformer models has become the standard approach for solving common NLP tasks. Most of the existing approaches rely on a randomly initialized classifier on top of such networks. We argue that this fine-tuning procedure is sub-optimal as the pre-trained model has no prior on the specific classifier labels, while it might have already learned an intrinsic textual representation of the task. In this paper, we introduce a new scoring method that casts a plausibility ranking task in a full-text format and leverages the masked language modeling head tuned during the pre-training phase. We study commonsense reasoning tasks where the model must rank a set of hypotheses given a premise, focusing on the COPA, Swag, HellaSwag and CommonsenseQA datasets. By exploiting our scoring method without fine-tuning, we are able to produce strong baselines (e.g. 80% test accuracy on COPA) that are comparable to supervised approaches. Moreover, when fine-tuning directly on the proposed scoring function, we show that our method provides a much more stable training phase across random restarts (e.g x10 standard deviation reduction on COPA test accuracy) and requires less annotated data than the standard classifier approach to reach equivalent performances."
    }
  },
  {
    "id": "abstract-2021--acl-long--12",
    "result": [
      {
        "value": {
          "start": 543,
          "end": 560,
          "text": "two-stage DSS-DST",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--12:E0"
      },
      {
        "value": {
          "start": 1299,
          "end": 1313,
          "text": "joint accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--12:E1"
      },
      {
        "value": {
          "start": 1420,
          "end": 1431,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--12:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--12:E0",
        "to_id": "abstract-2021--acl-long--12:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--12:E0",
        "to_id": "abstract-2021--acl-long--12:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "The goal of dialogue state tracking (DST) is to predict the current dialogue state given all previous dialogue contexts. Existing approaches generally predict the dialogue state at every turn from scratch. However, the overwhelming majority of the slots in each turn should simply inherit the slot values from the previous turn. Therefore, the mechanism of treating slots equally in each turn not only is inefficient but also may lead to additional errors because of the redundant slot value generation. To address this problem, we devise the two-stage DSS-DST which consists of the Dual Slot Selector based on the current turn dialogue, and the Slot Value Generator based on the dialogue history. The Dual Slot Selector determines each slot whether to update slot value or to inherit the slot value from the previous turn from two aspects: (1) if there is a strong relationship between it and the current turn dialogue utterances; (2) if a slot value with high reliability can be obtained for it through the current turn dialogue. The slots selected to be updated are permitted to enter the Slot Value Generator to update values by a hybrid method, while the other slots directly inherit the values from the previous turn. Empirical results show that our method achieves 56.93%, 60.73%, and 58.04% joint accuracy on MultiWOZ 2.0, MultiWOZ 2.1, and MultiWOZ 2.2 datasets respectively and achieves a new state-of-the-art performance with significant improvements."
    }
  },
  {
    "id": "abstract-2021--acl-long--551",
    "result": [
      {
        "value": {
          "start": 386,
          "end": 454,
          "text": "introducing two powerful deep bidirectional transformer-based models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--551:E0"
      },
      {
        "value": {
          "start": 965,
          "end": 976,
          "text": "ARLUE score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--551:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--551:E0",
        "to_id": "abstract-2021--acl-long--551:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Pre-trained language models (LMs) are currently integral to many natural language processing systems. Although multilingual LMs were also introduced to serve many languages, these have limitations such as being costly at inference time and the size and diversity of non-English data involved in their pre-training. We remedy these issues for a collection of diverse Arabic varieties by introducing two powerful deep bidirectional transformer-based models, ARBERT and MARBERT. To evaluate our models, we also introduce ARLUE, a new benchmark for multi-dialectal Arabic language understanding evaluation. ARLUE is built using 42 datasets targeting six different task clusters, allowing us to offer a series of standardized experiments under rich conditions. When fine-tuned on ARLUE, our models collectively achieve new state-of-the-art results across the majority of tasks (37 out of 48 classification tasks, on the 42 datasets). Our best model acquires the highest ARLUE score (77.40) across all six task clusters, outperforming all other models including XLM-R Large ( 3.4x larger size). Our models are publicly available at https://github.com/UBC-NLP/marbert and ARLUE will be released through the same repository."
    }
  },
  {
    "id": "P11-1069",
    "result": [
      {
        "value": {
          "start": 490,
          "end": 509,
          "text": "shift-reduce parser",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1069:E0"
      },
      {
        "value": {
          "start": 528,
          "end": 538,
          "text": "accuracies",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1069:E1"
      },
      {
        "from_id": "P11-1069:E0",
        "to_id": "P11-1069:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "CCGs are directly compatible with binary-branching bottom-up parsing algorithms, in particular CKY and shift-reduce algorithms. While the chart-based approach has been the dominant approach for CCG, the shift-reduce method has been little explored. In this paper, we develop a shift-reduce CCG parser using a discriminative model and beam search, and compare its strengths and weaknesses with the chart-based C&C parser. We study different errors made by the two parsers, and show that the shift-reduce parser gives competitive accuracies compared to C&C. Considering our use of a small beam, and given the high ambiguity levels in an automatically-extracted grammar and the amount of information in the CCG lexical categories which form the shift actions, this is a surprising result."
    }
  },
  {
    "id": "abstract-2021--acl-long--127",
    "result": [
      {
        "value": {
          "start": 710,
          "end": 756,
          "text": "self-training and domain adaptation approaches",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--127:E0"
      },
      {
        "value": {
          "start": 628,
          "end": 639,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--127:E1"
      },
      {
        "value": {
          "start": 795,
          "end": 848,
          "text": "unlabeled data and existing stance detection datasets",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--127:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--127:E0",
        "to_id": "abstract-2021--acl-long--127:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--127:E2",
        "to_id": "abstract-2021--acl-long--127:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "The prevalence of the COVID-19 pandemic in day-to-day life has yielded large amounts of stance detection data on social media sites, as users turn to social media to share their views regarding various issues related to the pandemic, e.g. stay at home mandates and wearing face masks when out in public. We set out to make use of this data by collecting the stance expressed by Twitter users, with respect to topics revolving around the pandemic. We annotate a new stance detection dataset, called COVID-19-Stance. Using this newly annotated dataset, we train several established stance detection models to ascertain a baseline performance for this specific task. To further improve the performance, we employ self-training and domain adaptation approaches to take advantage of large amounts of unlabeled data and existing stance detection datasets. The dataset, code, and other resources are available on GitHub."
    }
  },
  {
    "id": "abstract-2020--acl-main--696",
    "result": [
      {
        "value": {
          "start": 306,
          "end": 343,
          "text": "statistical schwa deletion classifier",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--696:E0"
      },
      {
        "value": {
          "start": 598,
          "end": 609,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--696:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--696:E0",
        "to_id": "abstract-2020--acl-main--696:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Hindi grapheme-to-phoneme (G2P) conversion is mostly trivial, with one exception: whether a schwa represented in the orthography is pronounced or unpronounced (deleted). Previous work has attempted to predict schwa deletion in a rule-based fashion using prosodic or phonetic analysis. We present the first statistical schwa deletion classifier for Hindi, which relies solely on the orthography as the input and outperforms previous approaches. We trained our model on a newly-compiled pronunciation lexicon extracted from various online dictionaries. Our best Hindi model achieves state of the art performance, and also achieves good performance on a closely related language, Punjabi, without modification."
    }
  },
  {
    "id": "P10-1128",
    "result": [
      {
        "value": {
          "start": 580,
          "end": 677,
          "text": "incorporate extra-linguistic information into an existing corpus-based reference resolution model",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1128:E0"
      },
      {
        "value": {
          "start": 839,
          "end": 847,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1128:E1"
      },
      {
        "from_id": "P10-1128:E0",
        "to_id": "P10-1128:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "This paper proposes an approach to reference resolution in situated dialogues by exploiting extra-linguistic information. Recently, investigations of referential behaviours involved in situations in the real world have received increasing attention by researchers (Di Eugenio et al., 2000; Byron, 2005; van Deemter, 2007; Spanger et al., 2009). In order to create an accurate reference resolution model, we need to handle extra-linguistic information as well as textual information examined by existing approaches (Soon et al., 2001; Ng and Cardie, 2002, etc.). In this paper, we incorporate extra-linguistic information into an existing corpus-based reference resolution model, and investigate its effects on reference resolution problems within a corpus of Japanese dialogues. The results demonstrate that our proposed model achieves an accuracy of 79.0% for this task."
    }
  },
  {
    "id": "P10-1013",
    "result": [
      {
        "value": {
          "start": 418,
          "end": 421,
          "text": "WOE",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1013:E0"
      },
      {
        "value": {
          "start": 485,
          "end": 494,
          "text": "precision",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1013:E1"
      },
      {
        "value": {
          "start": 499,
          "end": 505,
          "text": "recall",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1013:E2"
      },
      {
        "value": {
          "start": 961,
          "end": 986,
          "text": "dependency-parse features",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1013:E3"
      },
      {
        "value": {
          "start": 485,
          "end": 494,
          "text": "precision",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1013:E4"
      },
      {
        "value": {
          "start": 499,
          "end": 505,
          "text": "recall",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1013:E5"
      },
      {
        "from_id": "P10-1013:E0",
        "to_id": "P10-1013:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P10-1013:E0",
        "to_id": "P10-1013:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P10-1013:E3",
        "to_id": "P10-1013:E4",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P10-1013:E3",
        "to_id": "P10-1013:E5",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Information-extraction (IE) systems seek to distill semantic relations from natural-language text, but most systems use supervised learning of relation-specific examples and are thus limited by the availability of training data. Open IE systems such as TextRunner, on the other hand, aim to handle the unbounded number of relations found on the Web. But how well can these open systems perform? \n \nThis paper presents WOE, an open IE system which improves dramatically on TextRunner's precision and recall. The key to WOE's performance is a novel form of self-supervised learning for open extractors -- using heuristic matches between Wikipedia infobox attribute values and corresponding sentences to construct training data. Like TextRunner, WOE's extractor eschews lexicalized features and handles an unbounded set of semantic relations. WOE can operate in two modes: when restricted to POS tag features, it runs as quickly as TextRunner, but when set to use dependency-parse features its precision and recall rise even higher."
    }
  },
  {
    "id": "abstract-2021--acl-long--11",
    "result": [
      {
        "value": {
          "start": 895,
          "end": 905,
          "text": "Flow score",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--11:E0"
      },
      {
        "value": {
          "start": 1052,
          "end": 1077,
          "text": "chatbot-level correlation",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--11:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--11:E0",
        "to_id": "abstract-2021--acl-long--11:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Nowadays, open-domain dialogue models can generate acceptable responses according to the historical context based on the large-scale pre-trained language models. However, they generally concatenate the dialogue history directly as the model input to predict the response, which we named as the flat pattern and ignores the dynamic information flow across dialogue utterances. In this work, we propose the DialoFlow model, in which we introduce a dynamic flow mechanism to model the context flow, and design three training objectives to capture the information dynamics across dialogue utterances by addressing the semantic influence brought about by each utterance in large-scale pre-training. Experiments on the multi-reference Reddit Dataset and DailyDialog Dataset demonstrate that our DialoFlow significantly outperforms the DialoGPT on the dialogue generation task. Besides, we propose the Flow score , an effective automatic metric for evaluating interactive human-bot conversation quality based on the pre-trained DialoFlow, which presents high chatbot-level correlation ( r=0.9 ) with human ratings among 11 chatbots. Code and pre-trained models will be public."
    }
  },
  {
    "id": "abstract-2020--acl-main--591",
    "result": [
      {
        "value": {
          "start": 605,
          "end": 673,
          "text": "ground truth parse trees are provided as additional training signals",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--591:E0"
      },
      {
        "value": {
          "start": 710,
          "end": 720,
          "text": "perplexity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--591:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--591:E0",
        "to_id": "abstract-2020--acl-main--591:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "It is commonly believed that knowledge of syntactic structure should improve language modeling. However, effectively and computationally efficiently incorporating syntactic structure into neural language models has been a challenging topic. In this paper, we make use of a multi-task objective, i.e., the models simultaneously predict words as well as ground truth parse trees in a form called “syntactic distances”, where information between these two separate objectives shares the same intermediate representation. Experimental results on the Penn Treebank and Chinese Treebank datasets show that when ground truth parse trees are provided as additional training signals, the model is able to achieve lower perplexity and induce trees with better quality."
    }
  },
  {
    "id": "abstract-2021--acl-long--86",
    "result": [
      {
        "value": {
          "start": 337,
          "end": 378,
          "text": "text-based adversarial training algorithm",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--86:E0"
      },
      {
        "value": {
          "start": 142,
          "end": 153,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--86:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--86:E0",
        "to_id": "abstract-2021--acl-long--86:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The advent of large pre-trained language models has given rise to rapid progress in the field of Natural Language Processing (NLP). While the performance of these models on standard benchmarks has scaled with size, compression techniques such as knowledge distillation have been key in making them practical. We present MATE-KD, a novel text-based adversarial training algorithm which improves the performance of knowledge distillation. MATE-KD first trains a masked language model-based generator to perturb text by maximizing the divergence between teacher and student logits. Then using knowledge distillation a student is trained on both the original and the perturbed training samples. We evaluate our algorithm, using BERT-based models, on the GLUE benchmark and demonstrate that MATE-KD outperforms competitive adversarial learning and data augmentation baselines. On the GLUE test set our 6 layer RoBERTa based model outperforms BERT-large."
    }
  },
  {
    "id": "abstract-2020--acl-main--755",
    "result": [
      {
        "value": {
          "start": 647,
          "end": 677,
          "text": "subword regularization methods",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--755:E0"
      },
      {
        "value": {
          "start": 441,
          "end": 451,
          "text": "robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--755:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--755:E0",
        "to_id": "abstract-2020--acl-main--755:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Neural Machine Translation (NMT) models are sensitive to small perturbations in the input. Robustness to such perturbations is typically measured using translation quality metrics such as BLEU on the noisy input. This paper proposes additional metrics which measure the relative degradation and changes in translation when small perturbations are added to the input. We focus on a class of models employing subword regularization to address robustness and perform extensive evaluations of these models using the robustness measures proposed. Results show that our proposed metrics reveal a clear trend of improved robustness to perturbations when subword regularization methods are used."
    }
  },
  {
    "id": "abstract-2021--acl-long--225",
    "result": [
      {
        "value": {
          "start": 275,
          "end": 349,
          "text": "meta-learning algorithm for unsupervised neural machine translation (UNMT)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--225:E0"
      },
      {
        "value": {
          "start": 769,
          "end": 780,
          "text": "BLEU scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--225:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--225:E0",
        "to_id": "abstract-2021--acl-long--225:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Unsupervised machine translation, which utilizes unpaired monolingual corpora as training data, has achieved comparable performance against supervised machine translation. However, it still suffers from data-scarce domains. To address this issue, this paper presents a novel meta-learning algorithm for unsupervised neural machine translation (UNMT) that trains the model to adapt to another domain by utilizing only a small amount of training data. We assume that domain-general knowledge is a significant factor in handling data-scarce domains. Hence, we extend the meta-learning algorithm, which utilizes knowledge learned from high-resource domains, to boost the performance of low-resource UNMT. Our model surpasses a transfer learning-based approach by up to 2-3 BLEU scores. Extensive experimental results show that our proposed algorithm is pertinent for fast adaptation and consistently outperforms other baselines."
    }
  },
  {
    "id": "abstract-2020--acl-main--297",
    "result": [
      {
        "value": {
          "start": 739,
          "end": 760,
          "text": "syntactic information",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--297:E0"
      },
      {
        "value": {
          "start": 836,
          "end": 844,
          "text": "F1 score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--297:E1"
      },
      {
        "value": {
          "start": 374,
          "end": 422,
          "text": "dependency graph convolutional networks (DEPGCN)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--297:E2"
      },
      {
        "value": {
          "start": 836,
          "end": 844,
          "text": "F1 score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--297:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--297:E0",
        "to_id": "abstract-2020--acl-main--297:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--297:E2",
        "to_id": "abstract-2020--acl-main--297:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Opinion role labeling (ORL) is a fine-grained opinion analysis task and aims to answer “who expressed what kind of sentiment towards what?”. Due to the scarcity of labeled data, ORL remains challenging for data-driven methods. In this work, we try to enhance neural ORL models with syntactic knowledge by comparing and integrating different representations. We also propose dependency graph convolutional networks (DEPGCN) to encode parser information at different processing levels. In order to compensate for parser inaccuracy and reduce error propagation, we introduce multi-task learning (MTL) to train the parser and the ORL model simultaneously. We verify our methods on the benchmark MPQA corpus. The experimental results show that syntactic information is highly valuable for ORL, and our final MTL model effectively boosts the F1 score by 9.29 over the syntax-agnostic baseline. In addition, we find that the contributions from syntactic knowledge do not fully overlap with contextualized word representations (BERT). Our best model achieves 4.34 higher F1 score than the current state-ofthe-art."
    }
  },
  {
    "id": "P10-1085",
    "result": [
      {
        "value": {
          "start": 217,
          "end": 274,
          "text": "improving word alignment for various kinds of SMT systems",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1085:E0"
      },
      {
        "value": {
          "start": 531,
          "end": 541,
          "text": "BLEU score",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1085:E1"
      },
      {
        "value": {
          "start": 279,
          "end": 322,
          "text": "improving phrase table for phrase-based SMT",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1085:E2"
      },
      {
        "value": {
          "start": 531,
          "end": 541,
          "text": "BLEU score",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1085:E3"
      },
      {
        "from_id": "P10-1085:E0",
        "to_id": "P10-1085:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P10-1085:E2",
        "to_id": "P10-1085:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P10-1085:E0",
        "to_id": "P10-1085:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P10-1085:E2",
        "to_id": "P10-1085:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This paper proposes to use monolingual collocations to improve Statistical Machine Translation (SMT). We make use of the collocation probabilities, which are estimated from monolingual corpora, in two aspects, namely improving word alignment for various kinds of SMT systems and improving phrase table for phrase-based SMT. The experimental results show that our method improves the performance of both word alignment and translation quality significantly. As compared to baseline systems, we achieve absolute improvements of 2.40 BLEU score on a phrase-based SMT system and 1.76 BLEU score on a parsing-based SMT system."
    }
  },
  {
    "id": "abstract-2020--acl-main--748",
    "result": [
      {
        "value": {
          "start": 763,
          "end": 794,
          "text": "concept normalization framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--748:E0"
      },
      {
        "value": {
          "start": 821,
          "end": 832,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--748:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--748:E0",
        "to_id": "abstract-2020--acl-main--748:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Concept normalization, the task of linking textual mentions of concepts to concepts in an ontology, is challenging because ontologies are large. In most cases, annotated datasets cover only a small sample of the concepts, yet concept normalizers are expected to predict all concepts in the ontology. In this paper, we propose an architecture consisting of a candidate generator and a list-wise ranker based on BERT. The ranker considers pairings of concept mentions and candidate concepts, allowing it to make predictions for any concept, not just those seen during training. We further enhance this list-wise approach with a semantic type regularizer that allows the model to incorporate semantic type information from the ontology during training. Our proposed concept normalization framework achieves state-of-the-art performance on multiple datasets."
    }
  },
  {
    "id": "abstract-2020--acl-main--321",
    "result": [
      {
        "value": {
          "start": 545,
          "end": 560,
          "text": "unified encoder",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--321:E0"
      },
      {
        "value": {
          "start": 636,
          "end": 640,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--321:E1"
      },
      {
        "value": {
          "start": 645,
          "end": 658,
          "text": "METEOR scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--321:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--321:E0",
        "to_id": "abstract-2020--acl-main--321:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--321:E0",
        "to_id": "abstract-2020--acl-main--321:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Most of the existing models for document-level machine translation adopt dual-encoder structures. The representation of the source sentences and the document-level contexts are modeled with two separate encoders. Although these models can make use of the document-level contexts, they do not fully model the interaction between the contexts and the source sentences, and can not directly adapt to the recent pre-training models (e.g., BERT) which encodes multiple sentences with a single encoder. In this work, we propose a simple and effective unified encoder that can outperform the baseline models of dual-encoder models in terms of BLEU and METEOR scores. Moreover, the pre-training models can further boost the performance of our proposed model."
    }
  },
  {
    "id": "abstract-2021--acl-long--124",
    "result": [
      {
        "value": {
          "start": 221,
          "end": 225,
          "text": "CARI",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--124:E0"
      },
      {
        "value": {
          "start": 608,
          "end": 647,
          "text": "regular pre-trained models’ performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--124:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--124:E0",
        "to_id": "abstract-2021--acl-long--124:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Models pre-trained on large-scale regular text corpora often do not work well for user-generated data where the language styles differ significantly from the mainstream text. Here we present Context-Aware Rule Injection (CARI), an innovative method for formality style transfer (FST) by injecting multiple rules into an end-to-end BERT-based encoder and decoder model. CARI is able to learn to select optimal rules based on context. The intrinsic evaluation showed that CARI achieved the new highest performance on the FST benchmark dataset. Our extrinsic evaluation showed that CARI can greatly improve the regular pre-trained models’ performance on several tweet sentiment analysis tasks. Our contributions are as follows: 1.We propose a new method, CARI, to integrate rules for pre-trained language models. CARI is context-aware and can trained end-to-end with the downstream NLP applications. 2.We have achieved new state-of-the-art results for FST on the benchmark GYAFC dataset. 3.We are the first to evaluate FST methods with extrinsic evaluation and specifically on sentiment classification tasks. We show that CARI outperformed existing rule-based FST approaches for sentiment classification."
    }
  },
  {
    "id": "abstract-2021--acl-long--349",
    "result": [
      {
        "value": {
          "start": 987,
          "end": 990,
          "text": "EBR",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--349:E0"
      },
      {
        "value": {
          "start": 1178,
          "end": 1189,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--349:E1"
      },
      {
        "value": {
          "start": 1225,
          "end": 1236,
          "text": "BLEU points",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--349:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--349:E0",
        "to_id": "abstract-2021--acl-long--349:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--349:E0",
        "to_id": "abstract-2021--acl-long--349:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The discrepancy between maximum likelihood estimation (MLE) and task measures such as BLEU score has been studied before for autoregressive neural machine translation (NMT) and resulted in alternative training algorithms (Ranzato et al., 2016; Norouzi et al., 2016; Shen et al., 2016; Wu et al., 2018). However, MLE training remains the de facto approach for autoregressive NMT because of its computational efficiency and stability. Despite this mismatch between the training objective and task measure, we notice that the samples drawn from an MLE-based trained NMT support the desired distribution – there are samples with much higher BLEU score comparing to the beam decoding output. To benefit from this observation, we train an energy-based model to mimic the behavior of the task measure (i.e., the energy-based model assigns lower energy to samples with higher BLEU score), which is resulted in a re-ranking algorithm based on the samples drawn from NMT: energy-based re-ranking (EBR). We use both marginal energy models (over target sentence) and joint energy models (over both source and target sentences). Our EBR with the joint energy model consistently improves the performance of the Transformer-based NMT: +3.7 BLEU points on IWSLT’14 German-English, +3.37 BELU points on Sinhala-English, +1.4 BLEU points on WMT’16 English-German tasks."
    }
  },
  {
    "id": "abstract-2020--acl-main--773",
    "result": [
      {
        "value": {
          "start": 1178,
          "end": 1200,
          "text": "orthogonality approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--773:E0"
      },
      {
        "value": {
          "start": 157,
          "end": 165,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--773:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--773:E0",
        "to_id": "abstract-2020--acl-main--773:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "While deep learning models are making fast progress on the task of Natural Language Inference, recent studies have also shown that these models achieve high accuracy by exploiting several dataset biases, and without deep understanding of the language semantics. Using contradiction-word bias and word-overlapping bias as our two bias examples, this paper explores both data-level and model-level debiasing methods to robustify models against lexical dataset biases. First, we debias the dataset through data augmentation and enhancement, but show that the model bias cannot be fully removed via this method. Next, we also compare two ways of directly debiasing the model without knowing what the dataset biases are in advance. The first approach aims to remove the label bias at the embedding level. The second approach employs a bag-of-words sub-model to capture the features that are likely to exploit the bias and prevents the original model from learning these biased features by forcing orthogonality between these two sub-models. We performed evaluations on new balanced datasets extracted from the original MNLI dataset as well as the NLI stress tests, and show that the orthogonality approach is better at debiasing the model while maintaining competitive overall accuracy."
    }
  },
  {
    "id": "abstract-2021--acl-long--461",
    "result": [
      {
        "value": {
          "start": 1351,
          "end": 1361,
          "text": "MCR method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--461:E0"
      },
      {
        "value": {
          "start": 1376,
          "end": 1387,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--461:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--461:E0",
        "to_id": "abstract-2021--acl-long--461:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "As more and more product reviews are posted in both text and images, Multimodal Review Analysis (MRA) becomes an attractive research topic. Among the existing review analysis tasks, helpfulness prediction on review text has become predominant due to its importance for e-commerce platforms and online shops, i.e. helping customers quickly acquire useful product information. This paper proposes a new task Multimodal Review Helpfulness Prediction (MRHP) aiming to analyze the review helpfulness from text and visual modalities. Meanwhile, a novel Multi-perspective Coherent Reasoning method (MCR) is proposed to solve the MRHP task, which conducts joint reasoning over texts and images from both the product and the review, and aggregates the signals to predict the review helpfulness. Concretely, we first propose a product-review coherent reasoning module to measure the intra- and inter-modal coherence between the target product and the review. In addition, we also devise an intra-review coherent reasoning module to identify the coherence between the text content and images of the review, which is a piece of strong evidence for review helpfulness prediction. To evaluate the effectiveness of MCR, we present two newly collected multimodal review datasets as benchmark evaluation resources for the MRHP task. Experimental results show that our MCR method can lead to a performance increase of up to 8.5% as compared to the best performing text-only model. The source code and datasets can be obtained from https://github.com/jhliu17/MCR."
    }
  },
  {
    "id": "abstract-2020--acl-main--3",
    "result": [
      {
        "value": {
          "start": 555,
          "end": 587,
          "text": "template regularization approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--3:E0"
      },
      {
        "value": {
          "start": 614,
          "end": 624,
          "text": "robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--3:E1"
      },
      {
        "value": {
          "start": 628,
          "end": 702,
          "text": "regularizing the representation of utterances based on utterance templates",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--3:E2"
      },
      {
        "value": {
          "start": 289,
          "end": 320,
          "text": "Coarse-to-fine approach (Coach)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--3:E3"
      },
      {
        "value": {
          "start": 944,
          "end": 955,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--3:E4"
      },
      {
        "from_id": "abstract-2020--acl-main--3:E0",
        "to_id": "abstract-2020--acl-main--3:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--3:E2",
        "to_id": "abstract-2020--acl-main--3:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--3:E3",
        "to_id": "abstract-2020--acl-main--3:E4",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "As an essential task in task-oriented dialog systems, slot filling requires extensive training data in a certain domain. However, such data are not always available. Hence, cross-domain slot filling has naturally arisen to cope with this data scarcity problem. In this paper, we propose a Coarse-to-fine approach (Coach) for cross-domain slot filling. Our model first learns the general pattern of slot entities by detecting whether the tokens are slot entities or not. It then predicts the specific types for the slot entities. In addition, we propose a template regularization approach to improve the adaptation robustness by regularizing the representation of utterances based on utterance templates. Experimental results show that our model significantly outperforms state-of-the-art approaches in slot filling. Furthermore, our model can also be applied to the cross-domain named entity recognition task, and it achieves better adaptation performance than other existing baselines. The code is available at https://github.com/zliucr/coach."
    }
  },
  {
    "id": "abstract-2021--acl-long--391",
    "result": [
      {
        "value": {
          "start": 863,
          "end": 910,
          "text": "constraining all label angle variances balanced",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--391:E0"
      },
      {
        "value": {
          "start": 369,
          "end": 377,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--391:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--391:E0",
        "to_id": "abstract-2021--acl-long--391:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Semi-Supervised Text Classification (SSTC) mainly works under the spirit of self-training. They initialize the deep classifier by training over labeled texts; and then alternatively predict unlabeled texts as their pseudo-labels and train the deep classifier over the mixture of labeled and pseudo-labeled texts. Naturally, their performance is largely affected by the accuracy of pseudo-labels for unlabeled texts. Unfortunately, they often suffer from low accuracy because of the margin bias problem caused by the large difference between representation distributions of labels in SSTC. To alleviate this problem, we apply the angular margin loss, and perform Gaussian linear transformation to achieve balanced label angle variances, i.e., the variance of label angles of texts within the same label. More accuracy of predicted pseudo-labels can be achieved by constraining all label angle variances balanced, where they are estimated over both labeled and pseudo-labeled texts during self-training loops. With this insight, we propose a novel SSTC method, namely Semi-Supervised Text Classification with Balanced Deep representation Distributions (S2TC-BDD). To evaluate S2TC-BDD, we compare it against the state-of-the-art SSTC methods. Empirical results demonstrate the effectiveness of S2TC-BDD, especially when the labeled texts are scarce."
    }
  },
  {
    "id": "abstract-2020--acl-main--85",
    "result": [
      {
        "value": {
          "start": 310,
          "end": 375,
          "text": "augmenting it with a contextualized sparse representation (Sparc)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--85:E0"
      },
      {
        "value": {
          "start": 745,
          "end": 762,
          "text": "CuratedTREC score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--85:E1"
      },
      {
        "value": {
          "start": 845,
          "end": 860,
          "text": "inference speed",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--85:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--85:E0",
        "to_id": "abstract-2020--acl-main--85:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--85:E0",
        "to_id": "abstract-2020--acl-main--85:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Open-domain question answering can be formulated as a phrase retrieval problem, in which we can expect huge scalability and speed benefit but often suffer from low accuracy due to the limitation of existing phrase representation models. In this paper, we aim to improve the quality of each phrase embedding by augmenting it with a contextualized sparse representation (Sparc). Unlike previous sparse vectors that are term-frequency-based (e.g., tf-idf) or directly learned (only few thousand dimensions), we leverage rectified self-attention to indirectly learn sparse vectors in n-gram vocabulary space. By augmenting the previous phrase retrieval model (Seo et al., 2019) with Sparc, we show 4%+ improvement in CuratedTREC and SQuAD-Open. Our CuratedTREC score is even better than the best known retrieve & read model with at least 45x faster inference speed."
    }
  },
  {
    "id": "P11-1090",
    "result": [
      {
        "value": {
          "start": 24,
          "end": 60,
          "text": "unsupervised dynamic graphical model",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1090:E0"
      },
      {
        "value": {
          "start": 652,
          "end": 660,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1090:E1"
      },
      {
        "from_id": "P11-1090:E0",
        "to_id": "P11-1090:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This paper describes an unsupervised dynamic graphical model for morphological segmentation and bilingual morpheme alignment for statistical machine translation. The model extends Hidden Semi-Markov chain models by using factored output nodes and special structures for its conditional probability distributions. It relies on morpho-syntactic and lexical source-side information (part-of-speech, morphological segmentation) while learning a morpheme segmentation over the target language. Our model outperforms a competitive word alignment system in alignment quality. Used in a monolingual morphological segmentation setting it substantially improves accuracy over previous state-of-the-art models on three Arabic and Hebrew datasets."
    }
  },
  {
    "id": "abstract-2021--acl-long--481",
    "result": [
      {
        "value": {
          "start": 399,
          "end": 403,
          "text": "MASN",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--481:E0"
      },
      {
        "value": {
          "start": 1021,
          "end": 1032,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--481:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--481:E0",
        "to_id": "abstract-2021--acl-long--481:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Video Question Answering is a task which requires an AI agent to answer questions grounded in video. This task entails three key challenges: (1) understand the intention of various questions, (2) capturing various elements of the input video (e.g., object, action, causality), and (3) cross-modal grounding between language and vision information. We propose Motion-Appearance Synergistic Networks (MASN), which embed two cross-modal features grounded on motion and appearance information and selectively utilize them depending on the question’s intentions. MASN consists of a motion module, an appearance module, and a motion-appearance fusion module. The motion module computes the action-oriented cross-modal joint representations, while the appearance module focuses on the appearance aspect of the input video. Finally, the motion-appearance fusion module takes each output of the motion module and the appearance module as input, and performs question-guided fusion. As a result, MASN achieves new state-of-the-art performance on the TGIF-QA and MSVD-QA datasets. We also conduct qualitative analysis by visualizing the inference results of MASN."
    }
  },
  {
    "id": "P10-1156",
    "result": [
      {
        "value": {
          "start": 846,
          "end": 867,
          "text": "using a voting scheme",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1156:E0"
      },
      {
        "value": {
          "start": 827,
          "end": 836,
          "text": "F measure",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1156:E1"
      },
      {
        "from_id": "P10-1156:E0",
        "to_id": "P10-1156:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Word Sense Disambiguation remains one of the most complex problems facing computational linguists to date. In this paper we present a system that combines evidence from a monolingual WSD system together with that from a multilingual WSD system to yield state of the art performance on standard All-Words data sets. The monolingual system is based on a modification of the graph based state of the art algorithm In-Degree. The multilingual system is an improvement over an All-Words unsupervised approach, SALAAM. SALAAM exploits multilingual evidence as a means of disambiguation. In this paper, we present modifications to both of the original approaches and then their combination. We finally report the highest results obtained to date on the SENSEVAL 2 standard data set using an unsupervised method, we achieve an overall F measure of 64.58 using a voting scheme."
    }
  },
  {
    "id": "abstract-2020--acl-main--750",
    "result": [
      {
        "value": {
          "start": 534,
          "end": 600,
          "text": "using shared and private domain parameters and multi-task learning",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--750:E0"
      },
      {
        "value": {
          "start": 757,
          "end": 767,
          "text": "average F1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--750:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--750:E0",
        "to_id": "abstract-2020--acl-main--750:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Named entity recognition is a key component of many text processing pipelines and it is thus essential for this component to be robust to different types of input. However, domain transfer of NER models with data from multiple genres has not been widely studied. To this end, we conduct NER experiments in three predictive setups on data from: a) multiple domains; b) multiple domains where the genre label is unknown at inference time; c) domains not encountered in training. We introduce a new architecture tailored to this task by using shared and private domain parameters and multi-task learning. This consistently outperforms all other baseline and competitive methods on all three experimental setups, with differences ranging between +1.95 to +3.11 average F1 across multiple genres when compared to standard approaches. These results illustrate the challenges that need to be taken into account when building real-world NLP applications that are robust to various types of text and the methods that can help, at least partially, alleviate these issues."
    }
  },
  {
    "id": "abstract-2021--acl-long--168",
    "result": [
      {
        "value": {
          "start": 769,
          "end": 820,
          "text": "context-aware and model-agnostic debiasing strategy",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--168:E0"
      },
      {
        "value": {
          "start": 306,
          "end": 317,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--168:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--168:E0",
        "to_id": "abstract-2021--acl-long--168:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The element of repetition in cyberbullying behavior has directed recent computational studies toward detecting cyberbullying based on a social media session. In contrast to a single text, a session may consist of an initial post and an associated sequence of comments. Yet, emerging efforts to enhance the performance of session-based cyberbullying detection have largely overlooked unintended social biases in existing cyberbullying datasets. For example, a session containing certain demographic-identity terms (e.g., “gay” or “black”) is more likely to be classified as an instance of cyberbullying. In this paper, we first show evidence of such bias in models trained on sessions collected from different social media platforms (e.g., Instagram). We then propose a context-aware and model-agnostic debiasing strategy that leverages a reinforcement learning technique, without requiring any extra resources or annotations apart from a pre-defined set of sensitive triggers commonly used for identifying cyberbullying instances. Empirical evaluations show that the proposed strategy can simultaneously alleviate the impacts of the unintended biases and improve the detection performance."
    }
  },
  {
    "id": "abstract-2020--acl-main--667",
    "result": [
      {
        "value": {
          "start": 283,
          "end": 328,
          "text": "decomposing the problem into two sub-problems",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--667:E0"
      },
      {
        "value": {
          "start": 480,
          "end": 491,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--667:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--667:E0",
        "to_id": "abstract-2020--acl-main--667:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "In this work, we explore the implicit event argument detection task, which studies event arguments beyond sentence boundaries. The addition of cross-sentence argument candidates imposes great challenges for modeling. To reduce the number of candidates, we adopt a two-step approach, decomposing the problem into two sub-problems: argument head-word detection and head-to-span expansion. Evaluated on the recent RAMS dataset (Ebner et al., 2020), our model achieves overall better performance than a strong sequence labeling baseline. We further provide detailed error analysis, presenting where the model mainly makes errors and indicating directions for future improvements. It remains a challenge to detect implicit arguments, calling for more future work of document-level modeling for this task."
    }
  },
  {
    "id": "abstract-2020--acl-main--254",
    "result": [
      {
        "value": {
          "start": 240,
          "end": 257,
          "text": "adaptive policies",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--254:E0"
      },
      {
        "value": {
          "start": 582,
          "end": 593,
          "text": "BLEU points",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--254:E1"
      },
      {
        "value": {
          "start": 661,
          "end": 671,
          "text": "BLEU score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--254:E2"
      },
      {
        "value": {
          "start": 156,
          "end": 163,
          "text": "latency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--254:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--254:E0",
        "to_id": "abstract-2020--acl-main--254:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--254:E0",
        "to_id": "abstract-2020--acl-main--254:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--254:E0",
        "to_id": "abstract-2020--acl-main--254:E3",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Adaptive policies are better than fixed policies for simultaneous translation, since they can flexibly balance the tradeoff between translation quality and latency based on the current context information. But previous methods on obtaining adaptive policies either rely on complicated training process, or underperform simple fixed policies. We design an algorithm to achieve adaptive policies via a simple heuristic composition of a set of fixed policies. Experiments on Chinese -> English and German -> English show that our adaptive policies can outperform fixed ones by up to 4 BLEU points for the same latency, and more surprisingly, it even surpasses the BLEU score of full-sentence translation in the greedy mode (and very close to beam mode), but with much lower latency."
    }
  },
  {
    "id": "P10-1065",
    "result": [
      {
        "value": {
          "start": 528,
          "end": 554,
          "text": "mapping type ratio feature",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1065:E0"
      },
      {
        "value": {
          "start": 868,
          "end": 875,
          "text": "F-score",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1065:E1"
      },
      {
        "from_id": "P10-1065:E0",
        "to_id": "P10-1065:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We observe that (1) how a given named entity (NE) is translated (i.e., either semantically or phonetically) depends greatly on its associated entity type, and (2) entities within an aligned pair should share the same type. Also, (3) those initially detected NEs are anchors, whose information should be used to give certainty scores when selecting candidates. From this basis, an integrated model is thus proposed in this paper to jointly identify and align bilingual named entities between Chinese and English. It adopts a new mapping type ratio feature (which is the proportion of NE internal tokens that are semantically translated), enforces an entity type consistency constraint, and utilizes additional monolingual candidate certainty factors (based on those NE anchors). The experiments show that this novel approach has substantially raised the type-sensitive F-score of identified NE-pairs from 68.4% to 81.7% (42.1% F-score imperfection reduction) in our Chinese-English NE alignment task."
    }
  },
  {
    "id": "P10-1136",
    "result": [
      {
        "value": {
          "start": 496,
          "end": 543,
          "text": "use of semantic features and syntactic features",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1136:E0"
      },
      {
        "value": {
          "start": 600,
          "end": 611,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1136:E1"
      },
      {
        "from_id": "P10-1136:E0",
        "to_id": "P10-1136:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Determining the semantic intent of web queries not only involves identifying their semantic class, which is a primary focus of previous works, but also understanding their semantic structure. In this work, we formally define the semantic structure of noun phrase queries as comprised of intent heads and intent modifiers. We present methods that automatically identify these constituents as well as their semantic roles based on Markov and semi-Markov conditional random fields. We show that the use of semantic features and syntactic features significantly contribute to improving the understanding performance."
    }
  },
  {
    "id": "abstract-2020--acl-main--587",
    "result": [
      {
        "value": {
          "start": 485,
          "end": 488,
          "text": "MAE",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--587:E0"
      },
      {
        "value": {
          "start": 873,
          "end": 877,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--587:E1"
      },
      {
        "value": {
          "start": 897,
          "end": 917,
          "text": "number of parameters",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--587:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--587:E0",
        "to_id": "abstract-2020--acl-main--587:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--587:E0",
        "to_id": "abstract-2020--acl-main--587:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Multi-head attentive neural architectures have achieved state-of-the-art results on a variety of natural language processing tasks. Evidence has shown that they are overparameterized; attention heads can be pruned without significant performance loss. In this work, we instead “reallocate” them—the model learns to activate different heads on different inputs. Drawing connections between multi-head attention and mixture of experts, we propose the mixture of attentive experts model (MAE). MAE is trained using a block coordinate descent algorithm that alternates between updating (1) the responsibilities of the experts and (2) their parameters. Experiments on machine translation and language modeling show that MAE outperforms strong baselines on both tasks. Particularly, on the WMT14 English to German translation dataset, MAE improves over “transformer-base” by 0.8 BLEU, with a comparable number of parameters. Our analysis shows that our model learns to specialize different experts to different inputs."
    }
  },
  {
    "id": "abstract-2020--acl-main--593",
    "result": [
      {
        "value": {
          "start": 442,
          "end": 485,
          "text": "add classifiers to different layers of BERT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--593:E0"
      },
      {
        "value": {
          "start": 748,
          "end": 762,
          "text": "speed/accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--593:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--593:E0",
        "to_id": "abstract-2020--acl-main--593:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--593:E0",
        "to_id": "abstract-2020--acl-main--593:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "As NLP models become larger, executing a trained model requires significant computational resources incurring monetary and environmental costs. To better respect a given inference budget, we propose a modification to contextual representation fine-tuning which, during inference, allows for an early (and fast) “exit” from neural network calculations for simple instances, and late (and accurate) exit for hard instances. To achieve this, we add classifiers to different layers of BERT and use their calibrated confidence scores to make early exit decisions. We test our proposed modification on five different datasets in two tasks: three text classification datasets and two natural language inference benchmarks. Our method presents a favorable speed/accuracy tradeoff in almost all cases, producing models which are up to five times faster than the state of the art, while preserving their accuracy. Our method also requires almost no additional training resources (in either time or parameters) compared to the baseline BERT model. Finally, our method alleviates the need for costly retraining of multiple models at different levels of efficiency; we allow users to control the inference speed/accuracy tradeoff using a single trained model, by setting a single variable at inference time. We publicly release our code."
    }
  },
  {
    "id": "abstract-2021--acl-long--504",
    "result": [
      {
        "value": {
          "start": 903,
          "end": 942,
          "text": "batch-level and global-level selections",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--504:E0"
      },
      {
        "value": {
          "start": 1184,
          "end": 1195,
          "text": "BLEU points",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--504:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--504:E0",
        "to_id": "abstract-2021--acl-long--504:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Neural Machine Translation (NMT) models achieve state-of-the-art performance on many translation benchmarks. As an active research field in NMT, knowledge distillation is widely applied to enhance the model’s performance by transferring teacher model’s knowledge on each training sample. However, previous work rarely discusses the different impacts and connections among these samples, which serve as the medium for transferring teacher knowledge. In this paper, we design a novel protocol that can effectively analyze the different impacts of samples by comparing various samples’ partitions. Based on above protocol, we conduct extensive experiments and find that the teacher’s knowledge is not the more, the better. Knowledge over specific samples may even hurt the whole performance of knowledge distillation. Finally, to address these issues, we propose two simple yet effective strategies, i.e., batch-level and global-level selections, to pick suitable samples for distillation. We evaluate our approaches on two large-scale machine translation tasks, WMT’14 English-German and WMT’19 Chinese-English. Experimental results show that our approaches yield up to +1.28 and +0.89 BLEU points improvements over the Transformer baseline, respectively."
    }
  },
  {
    "id": "abstract-2021--acl-long--564",
    "result": [
      {
        "value": {
          "start": 1034,
          "end": 1101,
          "text": "number of collective outliers in the active learning pool decreases",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--564:E0"
      },
      {
        "value": {
          "start": 969,
          "end": 1002,
          "text": "active learning sample efficiency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--564:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--564:E0",
        "to_id": "abstract-2021--acl-long--564:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Active learning promises to alleviate the massive data needs of supervised machine learning: it has successfully improved sample efficiency by an order of magnitude on traditional tasks like topic classification and object recognition. However, we uncover a striking contrast to this promise: across 5 models and 4 datasets on the task of visual question answering, a wide variety of active learning approaches fail to outperform random selection. To understand this discrepancy, we profile 8 active learning methods on a per-example basis, and identify the problem as collective outliers – groups of examples that active learning methods prefer to acquire but models fail to learn (e.g., questions that ask about text in images or require external knowledge). Through systematic ablation experiments and qualitative visualizations, we verify that collective outliers are a general phenomenon responsible for degrading pool-based active learning. Notably, we show that active learning sample efficiency increases significantly as the number of collective outliers in the active learning pool decreases. We conclude with a discussion and prescriptive recommendations for mitigating the effects of these outliers in future work."
    }
  },
  {
    "id": "abstract-2021--acl-long--205",
    "result": [
      {
        "value": {
          "start": 704,
          "end": 731,
          "text": "recursive semi-Markov model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--205:E0"
      },
      {
        "value": {
          "start": 1122,
          "end": 1124,
          "text": "F1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--205:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--205:E0",
        "to_id": "abstract-2021--acl-long--205:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In this paper, we study the task of graph-based constituent parsing in the setting that binarization is not conducted as a pre-processing step, where a constituent tree may consist of nodes with more than two children. Previous graph-based methods on this setting typically generate hidden nodes with the dummy label inside the n-ary nodes, in order to transform the tree into a binary tree for prediction. The limitation is that the hidden nodes break the sibling relations of the n-ary node’s children. Consequently, the dependencies of such sibling constituents might not be accurately modeled and is being ignored. To solve this limitation, we propose a novel graph-based framework, which is called “recursive semi-Markov model”. The main idea is to utilize 1-order semi-Markov model to predict the immediate children sequence of a constituent candidate, which then recursively serves as a child candidate of its parent. In this manner, the dependencies of sibling constituents can be described by 1-order transition features, which solves the above limitation. Through experiments, the proposed framework obtains the F1 of 95.92% and 92.50% on the datasets of PTB and CTB 5.1 respectively. Specially, the recursive semi-Markov model shows advantages in modeling nodes with more than two children, whose average F1 can be improved by 0.3-1.1 points in PTB and 2.3-6.8 points in CTB 5.1."
    }
  },
  {
    "id": "abstract-2020--acl-main--331",
    "result": [
      {
        "value": {
          "start": 983,
          "end": 1010,
          "text": "text representation methods",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--331:E0"
      },
      {
        "value": {
          "start": 799,
          "end": 810,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--331:E1"
      },
      {
        "value": {
          "start": 1015,
          "end": 1042,
          "text": "pre-trained language models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--331:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--331:E0",
        "to_id": "abstract-2020--acl-main--331:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--331:E2",
        "to_id": "abstract-2020--acl-main--331:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "News recommendation is an important technique for personalized news service. Compared with product and movie recommendations which have been comprehensively studied, the research on news recommendation is much more limited, mainly due to the lack of a high-quality benchmark dataset. In this paper, we present a large-scale dataset named MIND for news recommendation. Constructed from the user click logs of Microsoft News, MIND contains 1 million users and more than 160k English news articles, each of which has rich textual content such as title, abstract and body. We demonstrate MIND a good testbed for news recommendation through a comparative study of several state-of-the-art news recommendation methods which are originally developed on different proprietary datasets. Our results show the performance of news recommendation highly relies on the quality of news content understanding and user interest modeling. Many natural language processing techniques such as effective text representation methods and pre-trained language models can effectively improve the performance of news recommendation. The MIND dataset will be available at https://msnews.github.io."
    }
  },
  {
    "id": "abstract-2020--acl-main--267",
    "result": [
      {
        "value": {
          "start": 583,
          "end": 637,
          "text": "Attentive Pooling with Learnable Norms (APLN) approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--267:E0"
      },
      {
        "value": {
          "start": 1370,
          "end": 1402,
          "text": "performance of attentive pooling",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--267:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--267:E0",
        "to_id": "abstract-2020--acl-main--267:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Pooling is an important technique for learning text representations in many neural NLP models. In conventional pooling methods such as average, max and attentive pooling, text representations are weighted summations of the L1 or L∞ norm of input features. However, their pooling norms are always fixed and may not be optimal for learning accurate text representations in different tasks. In addition, in many popular pooling methods such as max and attentive pooling some features may be over-emphasized, while other useful ones are not fully exploited. In this paper, we propose an Attentive Pooling with Learnable Norms (APLN) approach for text representation. Different from existing pooling methods that use a fixed pooling norm, we propose to learn the norm in an end-to-end manner to automatically find the optimal ones for text representation in different tasks. In addition, we propose two methods to ensure the numerical stability of the model training. The first one is scale limiting, which re-scales the input to ensure non-negativity and alleviate the risk of exponential explosion. The second one is re-formulation, which decomposes the exponent operation to avoid computing the real-valued powers of the input and further accelerate the pooling operation. Experimental results on four benchmark datasets show that our approach can effectively improve the performance of attentive pooling."
    }
  },
  {
    "id": "abstract-2021--acl-long--216",
    "result": [
      {
        "value": {
          "start": 1036,
          "end": 1099,
          "text": "many low-quality seed spans are filtered out in the first stage",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--216:E0"
      },
      {
        "value": {
          "start": 1119,
          "end": 1147,
          "text": "time complexity of inference",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--216:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--216:E0",
        "to_id": "abstract-2021--acl-long--216:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Named entity recognition (NER) is a well-studied task in natural language processing. Traditional NER research only deals with flat entities and ignores nested entities. The span-based methods treat entity recognition as a span classification task. Although these methods have the innate ability to handle nested NER, they suffer from high computational cost, ignorance of boundary information, under-utilization of the spans that partially match with entities, and difficulties in long entity recognition. To tackle these issues, we propose a two-stage entity identifier. First we generate span proposals by filtering and boundary regression on the seed spans to locate the entities, and then label the boundary-adjusted span proposals with the corresponding categories. Our method effectively utilizes the boundary information of entities and partially matched spans during training. Through boundary regression, entities of any length can be covered theoretically, which improves the ability to recognize long entities. In addition, many low-quality seed spans are filtered out in the first stage, which reduces the time complexity of inference. Experiments on nested NER datasets demonstrate that our proposed method outperforms previous state-of-the-art models."
    }
  },
  {
    "id": "abstract-2020--acl-main--204",
    "result": [
      {
        "value": {
          "start": 280,
          "end": 287,
          "text": "DeeBERT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--204:E0"
      },
      {
        "value": {
          "start": 462,
          "end": 476,
          "text": "inference time",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--204:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--204:E0",
        "to_id": "abstract-2020--acl-main--204:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications. However, they are also notorious for being slow in inference, which makes them difficult to deploy in real-time applications. We propose a simple but effective method, DeeBERT, to accelerate BERT inference. Our approach allows samples to exit earlier without passing through the entire model. Experiments show that DeeBERT is able to save up to ~40% inference time with minimal degradation in model quality. Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy. Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks. Code is available at https://github.com/castorini/DeeBERT."
    }
  },
  {
    "id": "abstract-2021--acl-long--505",
    "result": [
      {
        "value": {
          "start": 430,
          "end": 466,
          "text": "conditional cross-mutual information",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--505:E0"
      },
      {
        "value": {
          "start": 957,
          "end": 970,
          "text": "context usage",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--505:E1"
      },
      {
        "value": {
          "start": 994,
          "end": 1013,
          "text": "translation quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--505:E2"
      },
      {
        "value": {
          "start": 1043,
          "end": 1047,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--505:E3"
      },
      {
        "value": {
          "start": 1052,
          "end": 1057,
          "text": "COMET",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--505:E4"
      },
      {
        "value": {
          "start": 819,
          "end": 845,
          "text": "context-aware word dropout",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--505:E5"
      },
      {
        "from_id": "abstract-2021--acl-long--505:E0",
        "to_id": "abstract-2021--acl-long--505:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--505:E0",
        "to_id": "abstract-2021--acl-long--505:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--505:E0",
        "to_id": "abstract-2021--acl-long--505:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--505:E0",
        "to_id": "abstract-2021--acl-long--505:E4",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--505:E5",
        "to_id": "abstract-2021--acl-long--505:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--505:E5",
        "to_id": "abstract-2021--acl-long--505:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--505:E5",
        "to_id": "abstract-2021--acl-long--505:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--505:E5",
        "to_id": "abstract-2021--acl-long--505:E4",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recent work in neural machine translation has demonstrated both the necessity and feasibility of using inter-sentential context, context from sentences other than those currently being translated. However, while many current methods present model architectures that theoretically can use this extra context, it is often not clear how much they do actually utilize it at translation time. In this paper, we introduce a new metric, conditional cross-mutual information, to quantify usage of context by these models. Using this metric, we measure how much document-level machine translation systems use particular varieties of context. We find that target context is referenced more than source context, and that including more context has a diminishing affect on results. We then introduce a new, simple training method, context-aware word dropout, to increase the usage of context by context-aware models. Experiments show that our method not only increases context usage, but also improves the translation quality according to metrics such as BLEU and COMET, as well as performance on anaphoric pronoun resolution and lexical cohesion contrastive datasets."
    }
  },
  {
    "id": "abstract-2021--acl-long--229",
    "result": [
      {
        "value": {
          "start": 873,
          "end": 906,
          "text": "unsupervised rationale generation",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--229:E0"
      },
      {
        "value": {
          "start": 955,
          "end": 966,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--229:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--229:E0",
        "to_id": "abstract-2021--acl-long--229:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Lifelong learning (LL) aims to train a neural network on a stream of tasks while retaining knowledge from previous tasks. However, many prior attempts in NLP still suffer from the catastrophic forgetting issue, where the model completely forgets what it just learned in the previous tasks. In this paper, we introduce Rational LAMOL, a novel end-to-end LL framework for language models. In order to alleviate catastrophic forgetting, Rational LAMOL enhances LAMOL, a recent LL model, by applying critical freezing guided by human rationales. When the human rationales are not available, we propose exploiting unsupervised generated rationales as substitutions. In the experiment, we tested Rational LAMOL on permutations of three datasets from the ERASER benchmark. The results show that our proposed framework outperformed vanilla LAMOL on most permutations. Furthermore, unsupervised rationale generation was able to consistently improve the overall LL performance from the baseline without relying on human-annotated rationales."
    }
  },
  {
    "id": "P10-1131",
    "result": [
      {
        "value": {
          "start": 492,
          "end": 517,
          "text": "the multilingual approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1131:E0"
      },
      {
        "value": {
          "start": 524,
          "end": 529,
          "text": "error",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1131:E1"
      },
      {
        "from_id": "P10-1131:E0",
        "to_id": "P10-1131:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "We present an approach to multilingual grammar induction that exploits a phylogeny-structured model of parameter drift. Our method does not require any translated texts or token-level alignments. Instead, the phylogenetic prior couples languages at a parameter level. Joint induction in the multilingual model substantially outperforms independent learning, with larger gains both from more articulated phylogenies and as well as from increasing numbers of languages. Across eight languages, the multilingual approach gives error reductions over the standard monolingual DMV averaging 21.1% and reaching as high as 39%."
    }
  },
  {
    "id": "abstract-2021--acl-long--359",
    "result": [
      {
        "value": {
          "start": 938,
          "end": 973,
          "text": "straightforward variant of RE model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--359:E0"
      },
      {
        "value": {
          "start": 1015,
          "end": 1018,
          "text": "AUC",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--359:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--359:E0",
        "to_id": "abstract-2021--acl-long--359:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Knowledge Graph (KG) and attention mechanism have been demonstrated effective in introducing and selecting useful information for weakly supervised methods. However, only qualitative analysis and ablation study are provided as evidence. In this paper, we contribute a dataset and propose a paradigm to quantitatively evaluate the effect of attention and KG on bag-level relation extraction (RE). We find that (1) higher attention accuracy may lead to worse performance as it may harm the model’s ability to extract entity mention features; (2) the performance of attention is largely influenced by various noise distribution patterns, which is closely related to real-world datasets; (3) KG-enhanced attention indeed improves RE performance, while not through enhanced attention but by incorporating entity prior; and (4) attention mechanism may exacerbate the issue of insufficient training data. Based on these findings, we show that a straightforward variant of RE model can achieve significant improvements (6% AUC on average) on two real-world datasets as compared with three state-of-the-art baselines. Our codes and datasets are available at https://github.com/zig-kwin-hu/how-KG-ATT-help."
    }
  },
  {
    "id": "P11-1088",
    "result": [
      {
        "value": {
          "start": 588,
          "end": 622,
          "text": "lemma (i.e., citation form) models",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1088:E0"
      },
      {
        "value": {
          "start": 475,
          "end": 482,
          "text": "F-score",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1088:E1"
      },
      {
        "value": {
          "start": 352,
          "end": 371,
          "text": "linguistic features",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1088:E2"
      },
      {
        "from_id": "P11-1088:E0",
        "to_id": "P11-1088:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P11-1088:E2",
        "to_id": "P11-1088:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Arabic handwriting recognition (HR) is a challenging problem due to Arabic's connected letter forms, consonantal diacritics and rich morphology. In this paper we isolate the task of identification of erroneous words in HR from the task of producing corrections for these words. We consider a variety of linguistic (morphological and syntactic) and non-linguistic features to automatically identify these errors. Our best approach achieves a roughly ~15% absolute increase in F-score over a simple but reasonable baseline. A detailed error analysis shows that linguistic features, such as lemma (i.e., citation form) models, help improve HR-error detection precisely where we expect them to: semantically incoherent error words."
    }
  },
  {
    "id": "abstract-2020--acl-main--263",
    "result": [
      {
        "value": {
          "start": 450,
          "end": 499,
          "text": "adversarially fine-tuning them for a single epoch",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--263:E0"
      },
      {
        "value": {
          "start": 523,
          "end": 533,
          "text": "robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--263:E1"
      },
      {
        "value": {
          "start": 554,
          "end": 565,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--263:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--263:E0",
        "to_id": "abstract-2020--acl-main--263:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--263:E0",
        "to_id": "abstract-2020--acl-main--263:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Training on only perfect Standard English corpora predisposes pre-trained neural networks to discriminate against minorities from non-standard linguistic backgrounds (e.g., African American Vernacular English, Colloquial Singapore English, etc.). We perturb the inflectional morphology of words to craft plausible and semantically similar adversarial examples that expose these biases in popular NLP models, e.g., BERT and Transformer, and show that adversarially fine-tuning them for a single epoch significantly improves robustness without sacrificing performance on clean data."
    }
  },
  {
    "id": "abstract-2020--acl-main--754",
    "result": [
      {
        "value": {
          "start": 419,
          "end": 489,
          "text": "automatically learns how to weight training data through a data scorer",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--754:E0"
      },
      {
        "value": {
          "start": 725,
          "end": 744,
          "text": "average performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--754:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--754:E0",
        "to_id": "abstract-2020--acl-main--754:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "When training multilingual machine translation (MT) models that can translate to/from multiple languages, we are faced with imbalanced training sets: some languages have much more training data than others. Standard practice is to up-sample less resourced languages to increase representation, and the degree of up-sampling has a large effect on the overall performance. In this paper, we propose a method that instead automatically learns how to weight training data through a data scorer that is optimized to maximize performance on all test languages. Experiments on two sets of languages under both one-to-many and many-to-one MT settings show our method not only consistently outperforms heuristic baselines in terms of average performance, but also offers flexible control over the performance of which languages are optimized."
    }
  },
  {
    "id": "abstract-2021--acl-long--101",
    "result": [
      {
        "value": {
          "start": 575,
          "end": 624,
          "text": "removing residual connections in an encoder layer",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--101:E0"
      },
      {
        "value": {
          "start": 669,
          "end": 680,
          "text": "BLEU points",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--101:E1"
      },
      {
        "value": {
          "start": 982,
          "end": 1002,
          "text": "translation coverage",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--101:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--101:E0",
        "to_id": "abstract-2021--acl-long--101:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--101:E0",
        "to_id": "abstract-2021--acl-long--101:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Multilingual neural machine translation has shown the capability of directly translating between language pairs unseen in training, i.e. zero-shot translation. Despite being conceptually attractive, it often suffers from low output quality. The difficulty of generalizing to new translation directions suggests the model representations are highly specific to those language pairs seen in training. We demonstrate that a main factor causing the language-specific representations is the positional correspondence to input tokens. We show that this can be easily alleviated by removing residual connections in an encoder layer. With this modification, we gain up to 18.5 BLEU points on zero-shot translation while retaining quality on supervised directions. The improvements are particularly prominent between related languages, where our proposed model outperforms pivot-based translation. Moreover, our approach allows easy integration of new languages, which substantially expands translation coverage. By thorough inspections of the hidden layer outputs, we show that our approach indeed leads to more language-independent representations."
    }
  },
  {
    "id": "abstract-2020--acl-main--734",
    "result": [
      {
        "value": {
          "start": 696,
          "end": 712,
          "text": "memory mechanism",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--734:E0"
      },
      {
        "value": {
          "start": 817,
          "end": 828,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--734:E1"
      },
      {
        "value": {
          "start": 506,
          "end": 511,
          "text": "WMSeg",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--734:E2"
      },
      {
        "value": {
          "start": 906,
          "end": 916,
          "text": "robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--734:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--734:E0",
        "to_id": "abstract-2020--acl-main--734:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--734:E2",
        "to_id": "abstract-2020--acl-main--734:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Contextual features always play an important role in Chinese word segmentation (CWS). Wordhood information, being one of the contextual features, is proved to be useful in many conventional character-based segmenters. However, this feature receives less attention in recent neural models and it is also challenging to design a framework that can properly integrate wordhood information from different wordhood measures to existing neural frameworks. In this paper, we therefore propose a neural framework, WMSeg, which uses memory networks to incorporate wordhood information with several popular encoder-decoder combinations for CWS. Experimental results on five benchmark datasets indicate the memory mechanism successfully models wordhood information for neural segmenters and helps WMSeg achieve state-of-the-art performance on all those datasets. Further experiments and analyses also demonstrate the robustness of our proposed framework with respect to different wordhood measures and the efficiency of wordhood information in cross-domain experiments."
    }
  },
  {
    "id": "P11-1149",
    "result": [
      {
        "value": {
          "start": 526,
          "end": 536,
          "text": "our system",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1149:E0"
      },
      {
        "value": {
          "start": 550,
          "end": 558,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1149:E1"
      },
      {
        "from_id": "P11-1149:E0",
        "to_id": "P11-1149:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Current approaches for semantic parsing take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain. This supervision bottleneck is one of the major difficulties in scaling up semantic parsing. \n \nWe argue that a semantic parser can be trained effectively without annotated data, and introduce an unsupervised learning algorithm. The algorithm takes a self training approach driven by confidence estimation. Evaluated over Geoquery, a standard dataset for this task, our system achieved 66% accuracy, compared to 80% of its fully supervised counterpart, demonstrating the promise of unsupervised approaches for this task."
    }
  },
  {
    "id": "abstract-2020--acl-main--342",
    "result": [
      {
        "value": {
          "start": 316,
          "end": 338,
          "text": "transition-based model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--342:E0"
      },
      {
        "value": {
          "start": 861,
          "end": 871,
          "text": "F1 measure",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--342:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--342:E0",
        "to_id": "abstract-2020--acl-main--342:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Emotion-cause pair extraction aims to extract all potential pairs of emotions and corresponding causes from unannotated emotion text. Most existing methods are pipelined framework, which identifies emotions and extracts causes separately, leading to a drawback of error propagation. Towards this issue, we propose a transition-based model to transform the task into a procedure of parsing-like directed graph construction. The proposed model incrementally generates the directed graph with labeled edges based on a sequence of actions, from which we can recognize emotions with the corresponding causes simultaneously, thereby optimizing separate subtasks jointly and maximizing mutual benefits of tasks interdependently. Experimental results show that our approach achieves the best performance, outperforming the state-of-the-art methods by 6.71% (p<0.01) in F1 measure."
    }
  },
  {
    "id": "abstract-2020--acl-main--520",
    "result": [
      {
        "value": {
          "start": 321,
          "end": 343,
          "text": "transition-based model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--520:E0"
      },
      {
        "value": {
          "start": 553,
          "end": 561,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--520:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--520:E0",
        "to_id": "abstract-2020--acl-main--520:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Unlike widely used Named Entity Recognition (NER) data sets in generic domains, biomedical NER data sets often contain mentions consisting of discontinuous spans. Conventional sequence tagging techniques encode Markov assumptions that are efficient but preclude recovery of these mentions. We propose a simple, effective transition-based model with generic neural encoding for discontinuous NER. Through extensive experiments on three biomedical data sets, we show that our model can effectively recognize discontinuous mentions without sacrificing the accuracy on continuous mentions."
    }
  },
  {
    "id": "abstract-2020--acl-main--511",
    "result": [
      {
        "value": {
          "start": 24,
          "end": 65,
          "text": "annotation framework for argument quality",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--511:E0"
      },
      {
        "value": {
          "start": 565,
          "end": 569,
          "text": "cost",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--511:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--511:E0",
        "to_id": "abstract-2020--acl-main--511:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "We present an efficient annotation framework for argument quality, a feature difficult to be measured reliably as per previous work. A stochastic transitivity model is combined with an effective sampling strategy to infer high-quality labels with low effort from crowdsourced pairwise judgments. The model’s capabilities are showcased by compiling Webis-ArgQuality-20, an argument quality corpus that comprises scores for rhetorical, logical, dialectical, and overall quality inferred from a total of 41,859 pairwise judgments among 1,271 arguments. With up to 93% cost savings, our approach significantly outperforms existing annotation procedures. Furthermore, novel insight into argument quality is provided through statistical analysis, and a new aggregation method to infer overall quality from individual quality dimensions is proposed."
    }
  },
  {
    "id": "abstract-2021--acl-long--353",
    "result": [
      {
        "value": {
          "start": 253,
          "end": 266,
          "text": "prefix-tuning",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--353:E0"
      },
      {
        "value": {
          "start": 820,
          "end": 831,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--353:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--353:E0",
        "to_id": "abstract-2021--acl-long--353:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were “virtual tokens”. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training."
    }
  },
  {
    "id": "abstract-2021--acl-long--390",
    "result": [
      {
        "value": {
          "start": 254,
          "end": 267,
          "text": "MetaAdaptRank",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--390:E0"
      },
      {
        "value": {
          "start": 652,
          "end": 668,
          "text": "ranking accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--390:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--390:E0",
        "to_id": "abstract-2021--acl-long--390:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The effectiveness of Neural Information Retrieval (Neu-IR) often depends on a large scale of in-domain relevance training signals, which are not always available in real-world ranking scenarios. To democratize the benefits of Neu-IR, this paper presents MetaAdaptRank, a domain adaptive learning method that generalizes Neu-IR models from label-rich source domains to few-shot target domains. Drawing on source-domain massive relevance supervision, MetaAdaptRank contrastively synthesizes a large number of weak supervision signals for target domains and meta-learns to reweight these synthetic “weak” data based on their benefits to the target-domain ranking accuracy of Neu-IR models. Experiments on three TREC benchmarks in the web, news, and biomedical domains show that MetaAdaptRank significantly improves the few-shot ranking accuracy of Neu-IR models. Further analyses indicate that MetaAdaptRank thrives from both its contrastive weak data synthesis and meta-reweighted data selection. The code and data of this paper can be obtained from https://github.com/thunlp/MetaAdaptRank."
    }
  },
  {
    "id": "abstract-2021--acl-long--207",
    "result": [
      {
        "value": {
          "start": 499,
          "end": 519,
          "text": "multi-view framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--207:E0"
      },
      {
        "value": {
          "start": 290,
          "end": 301,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--207:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--207:E0",
        "to_id": "abstract-2021--acl-long--207:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In structured prediction problems, cross-lingual transfer learning is an efficient way to train quality models for low-resource languages, and further improvement can be obtained by learning from multiple source languages. However, not all source models are created equal and some may hurt performance on the target language. Previous work has explored the similarity between source and target sentences as an approximate measure of strength for different source models. In this paper, we propose a multi-view framework, by leveraging a small number of labeled target sentences, to effectively combine multiple source models into an aggregated source view at different granularity levels (language, sentence, or sub-structure), and transfer it to a target view based on a task-specific model. By encouraging the two views to interact with each other, our framework can dynamically adjust the confidence level of each source model and improve the performance of both views during training. Experiments for three structured prediction tasks on sixteen data sets show that our framework achieves significant improvement over all existing approaches, including these with access to additional source language data."
    }
  },
  {
    "id": "P11-1116",
    "result": [
      {
        "value": {
          "start": 108,
          "end": 137,
          "text": "nonlinear probabilistic model",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1116:E0"
      },
      {
        "value": {
          "start": 505,
          "end": 516,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1116:E1"
      },
      {
        "value": {
          "start": 541,
          "end": 544,
          "text": "P@5",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1116:E2"
      },
      {
        "value": {
          "start": 546,
          "end": 549,
          "text": "MAP",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1116:E3"
      },
      {
        "value": {
          "start": 554,
          "end": 565,
          "text": "R-Precision",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1116:E4"
      },
      {
        "from_id": "P11-1116:E0",
        "to_id": "P11-1116:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P11-1116:E0",
        "to_id": "P11-1116:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P11-1116:E0",
        "to_id": "P11-1116:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P11-1116:E0",
        "to_id": "P11-1116:E4",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This paper focuses on mining the hyponymy (or is-a) relation from large-scale, open-domain web documents. A nonlinear probabilistic model is exploited to model the correlation between sentences in the aggregation of pattern matching results. Based on the model, we design a set of evidence combination and propagation algorithms. These significantly improve the result quality of existing approaches. Experimental results conducted on 500 million web pages and hypernym labels for 300 terms show over 20% performance improvement in terms of P@5, MAP and R-Precision."
    }
  },
  {
    "id": "abstract-2021--acl-long--239",
    "result": [
      {
        "value": {
          "start": 478,
          "end": 502,
          "text": "recurring span selection",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--239:E0"
      },
      {
        "value": {
          "start": 929,
          "end": 931,
          "text": "F1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--239:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--239:E0",
        "to_id": "abstract-2021--acl-long--239:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "In several question answering benchmarks, pretrained models have reached human parity through fine-tuning on an order of 100,000 annotated questions and answers. We explore the more realistic few-shot setting, where only a few hundred training examples are available, and observe that standard models perform poorly, highlighting the discrepancy between current pretraining objectives and question answering. We propose a new pretraining scheme tailored for question answering: recurring span selection. Given a passage with multiple sets of recurring spans, we mask in each set all recurring spans but one, and ask the model to select the correct span in the passage for each masked span. Masked spans are replaced with a special token, viewed as a question representation, that is later used during fine-tuning to select the answer span. The resulting model obtains surprisingly good results on multiple benchmarks (e.g., 72.7 F1 on SQuAD with only 128 training examples), while maintaining competitive performance in the high-resource setting."
    }
  },
  {
    "id": "abstract-2020--acl-main--611",
    "result": [
      {
        "value": {
          "start": 372,
          "end": 376,
          "text": "FLAT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--611:E0"
      },
      {
        "value": {
          "start": 829,
          "end": 840,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--611:E1"
      },
      {
        "value": {
          "start": 845,
          "end": 855,
          "text": "efficiency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--611:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--611:E0",
        "to_id": "abstract-2020--acl-main--611:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--611:E0",
        "to_id": "abstract-2020--acl-main--611:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recently, the character-word lattice structure has been proved to be effective for Chinese named entity recognition (NER) by incorporating the word information. However, since the lattice structure is complex and dynamic, the lattice-based models are hard to fully utilize the parallel computation of GPUs and usually have a low inference speed. In this paper, we propose FLAT: Flat-LAttice Transformer for Chinese NER, which converts the lattice structure into a flat structure consisting of spans. Each span corresponds to a character or latent word and its position in the original lattice. With the power of Transformer and well-designed position encoding, FLAT can fully leverage the lattice information and has an excellent parallel ability. Experiments on four datasets show FLAT outperforms other lexicon-based models in performance and efficiency."
    }
  },
  {
    "id": "abstract-2021--acl-long--363",
    "result": [
      {
        "value": {
          "start": 541,
          "end": 545,
          "text": "CoRI",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--363:E0"
      },
      {
        "value": {
          "start": 970,
          "end": 973,
          "text": "AUC",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--363:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--363:E0",
        "to_id": "abstract-2021--acl-long--363:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Integrating extracted knowledge from the Web to knowledge graphs (KGs) can facilitate tasks like question answering. We study relation integration that aims to align free-text relations in subject-relation-object extractions to relations in a target KG. To address the challenge that free-text relations are ambiguous, previous methods exploit neighbor entities and relations for additional context. However, the predictions are made independently, which can be mutually inconsistent. We propose a two-stage Collective Relation Integration (CoRI) model, where the first stage independently makes candidate predictions, and the second stage employs a collective model that accesses all candidate predictions to make globally coherent predictions. We further improve the collective model with augmented data from the portion of the target KG that is otherwise unused. Experiment results on two datasets show that CoRI can significantly outperform the baselines, improving AUC from .677 to .748 and from .716 to .780, respectively."
    }
  },
  {
    "id": "P11-1143",
    "result": [
      {
        "value": {
          "start": 471,
          "end": 507,
          "text": "information need prediction approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1143:E0"
      },
      {
        "value": {
          "start": 524,
          "end": 535,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1143:E1"
      },
      {
        "from_id": "P11-1143:E0",
        "to_id": "P11-1143:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In this paper we address the problem of question recommendation from large archives of community question answering data by exploiting the users' information needs. Our experimental results indicate that questions based on the same or similar information need can provide excellent question recommendation. We show that translation model can be effectively utilized to predict the information need given only the user's query question. Experiments show that the proposed information need prediction approach can improve the performance of question recommendation."
    }
  },
  {
    "id": "abstract-2020--acl-main--404",
    "result": [
      {
        "value": {
          "start": 557,
          "end": 616,
          "text": "mask proper names and pronouns during training of the model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--404:E0"
      },
      {
        "value": {
          "start": 441,
          "end": 455,
          "text": "frequency bias",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--404:E1"
      },
      {
        "value": {
          "start": 248,
          "end": 259,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--404:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--404:E0",
        "to_id": "abstract-2020--acl-main--404:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--404:E0",
        "to_id": "abstract-2020--acl-main--404:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "A central concern in Computational Social Sciences (CSS) is fairness: where the role of NLP is to scale up text analysis to large corpora, the quality of automatic analyses should be as independent as possible of textual properties. We analyze the performance of a state-of-the-art neural model on the task of political claims detection (i.e., the identification of forward-looking statements made by political actors) and identify a strong frequency bias: claims made by frequent actors are recognized better. We propose two simple debiasing methods which mask proper names and pronouns during training of the model, thus removing personal information bias. We find that (a) these methods significantly decrease frequency bias while keeping the overall performance stable; and (b) the resulting models improve when evaluated in an out-of-domain setting."
    }
  },
  {
    "id": "abstract-2021--acl-long--155",
    "result": [
      {
        "value": {
          "start": 416,
          "end": 420,
          "text": "GLAT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--155:E0"
      },
      {
        "value": {
          "start": 275,
          "end": 282,
          "text": "speedup",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--155:E1"
      },
      {
        "value": {
          "start": 416,
          "end": 420,
          "text": "GLAT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--155:E2"
      },
      {
        "value": {
          "start": 872,
          "end": 883,
          "text": "BLEU points",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--155:E3"
      },
      {
        "from_id": "abstract-2021--acl-long--155:E0",
        "to_id": "abstract-2021--acl-long--155:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--155:E2",
        "to_id": "abstract-2021--acl-long--155:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recent work on non-autoregressive neural machine translation (NAT) aims at improving the efficiency by parallel decoding without sacrificing the quality. However, existing NAT methods are either inferior to Transformer or require multiple decoding passes, leading to reduced speedup. We propose the Glancing Language Model (GLM) for single-pass parallel generation models. With GLM, we develop Glancing Transformer (GLAT) for machine translation. With only single-pass parallel decoding, GLAT is able to generate high-quality translation with 8×-15× speedup. Note that GLAT does not modify the network architecture, which is a training method to learn word interdependency. Experiments on multiple WMT language directions show that GLAT outperforms all previous single pass non-autoregressive methods, and is nearly comparable to Transformer, reducing the gap to 0.25-0.9 BLEU points."
    }
  },
  {
    "id": "abstract-2020--acl-main--18",
    "result": [
      {
        "value": {
          "start": 554,
          "end": 587,
          "text": "content selection from input data",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--18:E0"
      },
      {
        "value": {
          "start": 794,
          "end": 806,
          "text": "performances",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--18:E1"
      },
      {
        "value": {
          "start": 872,
          "end": 883,
          "text": "BLEU points",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--18:E2"
      },
      {
        "value": {
          "start": 592,
          "end": 609,
          "text": "language modeling",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--18:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--18:E0",
        "to_id": "abstract-2020--acl-main--18:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--18:E0",
        "to_id": "abstract-2020--acl-main--18:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--18:E0",
        "to_id": "abstract-2020--acl-main--18:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--18:E3",
        "to_id": "abstract-2020--acl-main--18:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Neural-based end-to-end approaches to natural language generation (NLG) from structured data or knowledge are data-hungry, making their adoption for real-world applications difficult with limited data. In this work, we propose the new task of few-shot natural language generation. Motivated by how humans tend to summarize tabular data, we propose a simple yet effective approach and show that it not only demonstrates strong performance but also provides good generalization across domains. The design of the model architecture is based on two aspects: content selection from input data and language modeling to compose coherent sentences, which can be acquired from prior knowledge. With just 200 training examples, across multiple domains, we show that our approach achieves very reasonable performances and outperforms the strongest baseline by an average of over 8.0 BLEU points improvement. Our code and data can be found at https://github.com/czyssrs/Few-Shot-NLG"
    }
  },
  {
    "id": "abstract-2021--acl-long--119",
    "result": [
      {
        "value": {
          "start": 490,
          "end": 517,
          "text": "data augmentation algorithm",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--119:E0"
      },
      {
        "value": {
          "start": 926,
          "end": 938,
          "text": "ROUGE scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--119:E1"
      },
      {
        "value": {
          "start": 940,
          "end": 952,
          "text": "RQE accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--119:E2"
      },
      {
        "value": {
          "start": 821,
          "end": 832,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--119:E3"
      },
      {
        "from_id": "abstract-2021--acl-long--119:E0",
        "to_id": "abstract-2021--acl-long--119:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--119:E0",
        "to_id": "abstract-2021--acl-long--119:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--119:E0",
        "to_id": "abstract-2021--acl-long--119:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Users of medical question answering systems often submit long and detailed questions, making it hard to achieve high recall in answer retrieval. To alleviate this problem, we propose a novel Multi-Task Learning (MTL) method with data augmentation for medical question understanding. We first establish an equivalence between the tasks of question summarization and Recognizing Question Entailment (RQE) using their definitions in the medical domain. Based on this equivalence, we propose a data augmentation algorithm to use just one dataset to optimize for both tasks, with a weighted MTL loss. We introduce gradually soft parameter-sharing: a constraint for decoder parameters to be close, that is gradually loosened as we move to the highest layer. We show through ablation studies that our proposed novelties improve performance. Our method outperforms existing MTL methods across 4 datasets of medical question pairs, in ROUGE scores, RQE accuracy and human evaluation. Finally, we show that our method fares better than single-task learning under 4 low-resource settings."
    }
  },
  {
    "id": "P10-1037",
    "result": [
      {
        "value": {
          "start": 15,
          "end": 38,
          "text": "active learning methods",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1037:E0"
      },
      {
        "value": {
          "start": 509,
          "end": 517,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1037:E1"
      },
      {
        "from_id": "P10-1037:E0",
        "to_id": "P10-1037:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We investigate active learning methods for Japanese dependency parsing. We propose active learning methods of using partial dependency relations in a given sentence for parsing and evaluate their effectiveness empirically. Furthermore, we utilize syntactic constraints of Japanese to obtain more labeled examples from precious labeled ones that annotators give. Experimental results show that our proposed methods improve considerably the learning curve of Japanese dependency parsing. In order to achieve an accuracy of over 88.3%, one of our methods requires only 34.4% of labeled examples as compared to passive learning."
    }
  },
  {
    "id": "P10-1076",
    "result": [
      {
        "value": {
          "start": 547,
          "end": 566,
          "text": "boosting algorithms",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1076:E0"
      },
      {
        "value": {
          "start": 912,
          "end": 932,
          "text": "translation accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1076:E1"
      },
      {
        "from_id": "P10-1076:E0",
        "to_id": "P10-1076:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In this paper, we present a simple and effective method to address the issue of how to generate diversified translation systems from a single Statistical Machine Translation (SMT) engine for system combination. Our method is based on the framework of boosting. First, a sequence of weak translation systems is generated from a baseline system in an iterative manner. Then, a strong translation system is built from the ensemble of these weak translation systems. To adapt boosting to SMT system combination, several key components of the original boosting algorithms are redesigned in this work. We evaluate our method on Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrase-based system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems."
    }
  },
  {
    "id": "abstract-2021--acl-long--545",
    "result": [
      {
        "value": {
          "start": 103,
          "end": 117,
          "text": "meta-knowledge",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--545:E0"
      },
      {
        "value": {
          "start": 749,
          "end": 776,
          "text": "false acceptance rate (FAR)",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--545:E1"
      },
      {
        "value": {
          "start": 514,
          "end": 536,
          "text": "equal error rate (EER)",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--545:E2"
      },
      {
        "value": {
          "start": 772,
          "end": 775,
          "text": "FAR",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--545:E3"
      },
      {
        "from_id": "abstract-2021--acl-long--545:E0",
        "to_id": "abstract-2021--acl-long--545:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--545:E0",
        "to_id": "abstract-2021--acl-long--545:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--545:E0",
        "to_id": "abstract-2021--acl-long--545:E3",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "In this paper we explore the improvement of intent recognition in conversational systems by the use of meta-knowledge embedded in intent identifiers. Developers often include such knowledge, structure as taxonomies, in the documentation of chatbots. By using neuro-symbolic algorithms to incorporate those taxonomies into embeddings of the output space, we were able to improve accuracy in intent recognition. In datasets with intents and example utterances from 200 professional chatbots, we saw decreases in the equal error rate (EER) in more than 40% of the chatbots in comparison to the baseline of the same algorithm without the meta-knowledge. The meta-knowledge proved also to be effective in detecting out-of-scope utterances, improving the false acceptance rate (FAR) in two thirds of the chatbots, with decreases of 0.05 or more in FAR in almost 40% of the chatbots. When considering only the well-developed workspaces with a high level use of taxonomies, FAR decreased more than 0.05 in 77% of them, and more than 0.1 in 39% of the chatbots."
    }
  },
  {
    "id": "abstract-2021--acl-long--67",
    "result": [
      {
        "value": {
          "start": 361,
          "end": 408,
          "text": "applying a pipeline that uses recent algorithms",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--67:E0"
      },
      {
        "value": {
          "start": 453,
          "end": 476,
          "text": "induced lexicon quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--67:E1"
      },
      {
        "value": {
          "start": 613,
          "end": 628,
          "text": "Our final model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--67:E2"
      },
      {
        "value": {
          "start": 697,
          "end": 706,
          "text": "F1 points",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--67:E3"
      },
      {
        "from_id": "abstract-2021--acl-long--67:E0",
        "to_id": "abstract-2021--acl-long--67:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--67:E2",
        "to_id": "abstract-2021--acl-long--67:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Bilingual lexicons map words in one language to their translations in another, and are typically induced by learning linear projections to align monolingual word embedding spaces. In this paper, we show it is possible to produce much higher quality lexicons with methods that combine (1) unsupervised bitext mining and (2) unsupervised word alignment. Directly applying a pipeline that uses recent algorithms for both subproblems significantly improves induced lexicon quality and further gains are possible by learning to filter the resulting lexical entries, with both unsupervised and semi-supervised schemes. Our final model outperforms the state of the art on the BUCC 2020 shared task by 14 F1 points averaged over 12 language pairs, while also providing a more interpretable approach that allows for rich reasoning of word meaning in context. Further analysis of our output and the standard reference lexicons suggests they are of comparable quality, and new benchmarks may be needed to measure further progress on this task."
    }
  },
  {
    "id": "abstract-2021--acl-long--395",
    "result": [
      {
        "value": {
          "start": 339,
          "end": 344,
          "text": "Coins",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--395:E0"
      },
      {
        "value": {
          "start": 1122,
          "end": 1133,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--395:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--395:E0",
        "to_id": "abstract-2021--acl-long--395:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Despite recent successes of large pre-trained language models in solving reasoning tasks, their inference capabilities remain opaque. We posit that such models can be made more interpretable by explicitly generating interim inference rules, and using them to guide the generation of task-specific textual outputs. In this paper we present Coins, a recursive inference framework that i) iteratively reads context sentences, ii) dynamically generates contextualized inference rules, encodes them, and iii) uses them to guide task-specific output generation. We apply to a Narrative Story Completion task that asks a model to complete a story with missing sentences, to produce a coherent story with plausible logical connections, causal relationships, and temporal dependencies. By modularizing inference and sentence generation steps in a recurrent model, we aim to make reasoning steps and their effects on next sentence generation transparent. Our automatic and manual evaluations show that the model generates better story sentences than SOTA baselines, especially in terms of coherence. We further demonstrate improved performance over strong pre-trained LMs in generating commonsense inference rules. The recursive nature of holds the potential for controlled generation of longer sequences."
    }
  },
  {
    "id": "abstract-2020--acl-main--648",
    "result": [
      {
        "value": {
          "start": 611,
          "end": 630,
          "text": "Our proposed method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--648:E0"
      },
      {
        "value": {
          "start": 707,
          "end": 715,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--648:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--648:E0",
        "to_id": "abstract-2020--acl-main--648:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Simplified Chinese to Traditional Chinese character conversion is a common preprocessing step in Chinese NLP. Despite this, current approaches have insufficient performance because they do not take into account that a simplified Chinese character can correspond to multiple traditional characters. Here, we propose a model that can disambiguate between mappings and convert between the two scripts. The model is based on subword segmentation, two language models, as well as a method for mapping between subword sequences. We further construct benchmark datasets for topic classification and script conversion. Our proposed method outperforms previous Chinese Character conversion approaches by 6 points in accuracy. These results are further confirmed in a downstream application, where 2kenize is used to convert pretraining dataset for topic classification. An error analysis reveals that our method’s particular strengths are in dealing with code mixing and named entities."
    }
  },
  {
    "id": "P10-1079",
    "result": [
      {
        "value": {
          "start": 290,
          "end": 369,
          "text": "shares similarities with both spell checking and machine translation approaches",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1079:E0"
      },
      {
        "value": {
          "start": 239,
          "end": 254,
          "text": "Word Error Rate",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1079:E1"
      },
      {
        "value": {
          "start": 562,
          "end": 572,
          "text": "BLEU score",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1079:E2"
      },
      {
        "from_id": "P10-1079:E0",
        "to_id": "P10-1079:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "P10-1079:E0",
        "to_id": "P10-1079:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "In recent years, research in natural language processing has increasingly focused on normalizing SMS messages. Different well-defined approaches have been proposed, but the problem remains far from being solved: best systems achieve a 11% Word Error Rate. This paper presents a method that shares similarities with both spell checking and machine translation approaches. The normalization part of the system is entirely based on models trained from a corpus. Evaluated in French by 10-fold-cross validation, the system achieves a 9.3% Word Error Rate and a 0.83 BLEU score."
    }
  },
  {
    "id": "abstract-2021--acl-long--294",
    "result": [
      {
        "value": {
          "start": 25,
          "end": 44,
          "text": "hierarchical method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--294:E0"
      },
      {
        "value": {
          "start": 676,
          "end": 686,
          "text": "perplexity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--294:E1"
      },
      {
        "value": {
          "start": 729,
          "end": 745,
          "text": "model parameters",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--294:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--294:E0",
        "to_id": "abstract-2021--acl-long--294:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--294:E0",
        "to_id": "abstract-2021--acl-long--294:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We describe an efficient hierarchical method to compute attention in the Transformer architecture. The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity. We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks. Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark. It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models."
    }
  },
  {
    "id": "abstract-2021--acl-long--344",
    "result": [
      {
        "value": {
          "start": 645,
          "end": 650,
          "text": "A-GCN",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--344:E0"
      },
      {
        "value": {
          "start": 1265,
          "end": 1276,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--344:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--344:E0",
        "to_id": "abstract-2021--acl-long--344:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Syntactic information, especially dependency trees, has been widely used by existing studies to improve relation extraction with better semantic guidance for analyzing the context information associated with the given entities. However, most existing studies suffer from the noise in the dependency trees, especially when they are automatically generated, so that intensively leveraging dependency information may introduce confusions to relation classification and necessary pruning is of great importance in this task. In this paper, we propose a dependency-driven approach for relation extraction with attentive graph convolutional networks (A-GCN). In this approach, an attention mechanism upon graph convolutional networks is applied to different contextual words in the dependency tree obtained from an off-the-shelf dependency parser, to distinguish the importance of different word dependencies. Consider that dependency types among words also contain important contextual guidance, which is potentially helpful for relation extraction, we also include the type information in A-GCN modeling. Experimental results on two English benchmark datasets demonstrate the effectiveness of our A-GCN, which outperforms previous studies and achieves state-of-the-art performance on both datasets."
    }
  },
  {
    "id": "abstract-2021--acl-long--164",
    "result": [
      {
        "value": {
          "start": 729,
          "end": 774,
          "text": "combination of the CNN with ALBERT or RoBERTa",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--164:E0"
      },
      {
        "value": {
          "start": 92,
          "end": 103,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--164:E1"
      },
      {
        "value": {
          "start": 539,
          "end": 584,
          "text": "combination of our pretrained CNN with ALBERT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--164:E2"
      },
      {
        "value": {
          "start": 92,
          "end": 103,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--164:E3"
      },
      {
        "from_id": "abstract-2021--acl-long--164:E0",
        "to_id": "abstract-2021--acl-long--164:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--164:E2",
        "to_id": "abstract-2021--acl-long--164:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Transformer-based language models (TLMs), such as BERT, ALBERT and GPT-3, have shown strong performance in a wide range of NLP tasks and currently dominate the field of NLP. However, many researchers wonder whether these models can maintain their dominance forever. Of course, we do not have answers now, but, as an attempt to find better neural architectures and training schemes, we pretrain a simple CNN using a GAN-style learning scheme and Wikipedia data, and then integrate it with standard TLMs. We show that on the GLUE tasks, the combination of our pretrained CNN with ALBERT outperforms the original ALBERT and achieves a similar performance to that of SOTA. Furthermore, on open-domain QA (Quasar-T and SearchQA), the combination of the CNN with ALBERT or RoBERTa achieved stronger performance than SOTA and the original TLMs. We hope that this work provides a hint for developing a novel strong network architecture along with its training scheme. Our source code and models are available at https://github.com/nict-wisdom/bertac."
    }
  },
  {
    "id": "abstract-2021--acl-long--315",
    "result": [
      {
        "value": {
          "start": 1098,
          "end": 1111,
          "text": "T5-base model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--315:E0"
      },
      {
        "value": {
          "start": 1057,
          "end": 1068,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--315:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--315:E0",
        "to_id": "abstract-2021--acl-long--315:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The current state-of-the-art generative models for open-domain question answering (ODQA) have focused on generating direct answers from unstructured textual information. However, a large amount of world’s knowledge is stored in structured databases, and need to be accessed using query languages such as SQL. Furthermore, query languages can answer questions that require complex reasoning, as well as offering full explainability. In this paper, we propose a hybrid framework that takes both textual and tabular evidences as input and generates either direct answers or SQL queries depending on which form could better answer the question. The generated SQL queries can then be executed on the associated databases to obtain the final answers. To the best of our knowledge, this is the first paper that applies Text2SQL to ODQA tasks. Empirically, we demonstrate that on several ODQA datasets, the hybrid methods consistently outperforms the baseline models that only takes homogeneous input by a large margin. Specifically we achieve the state-of-the-art performance on OpenSQuAD dataset using a T5-base model. In a detailed analysis, we demonstrate that the being able to generate structural SQL queries can always bring gains, especially for those questions that requires complex reasoning."
    }
  },
  {
    "id": "abstract-2020--acl-main--496",
    "result": [
      {
        "value": {
          "start": 124,
          "end": 187,
          "text": "extend this selective rationalization approach to text matching",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--496:E0"
      },
      {
        "value": {
          "start": 985,
          "end": 993,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--496:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--496:E0",
        "to_id": "abstract-2020--acl-main--496:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Selecting input features of top relevance has become a popular method for building self-explaining models. In this work, we extend this selective rationalization approach to text matching, where the goal is to jointly select and align text pieces, such as tokens or sentences, as a justification for the downstream prediction. Our approach employs optimal transport (OT) to find a minimal cost alignment between the inputs. However, directly applying OT often produces dense and therefore uninterpretable alignments. To overcome this limitation, we introduce novel constrained variants of the OT problem that result in highly sparse alignments with controllable sparsity. Our model is end-to-end differentiable using the Sinkhorn algorithm for OT and can be trained without any alignment annotations. We evaluate our model on the StackExchange, MultiNews, e-SNLI, and MultiRC datasets. Our model achieves very sparse rationale selections with high fidelity while preserving prediction accuracy compared to strong attention baseline models."
    }
  },
  {
    "id": "abstract-2020--acl-main--53",
    "result": [
      {
        "value": {
          "start": 857,
          "end": 931,
          "text": "SOM-DST (Selectively Overwriting Memory for Dialogue State Tracking) model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--53:E0"
      },
      {
        "value": {
          "start": 958,
          "end": 977,
          "text": "joint goal accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--53:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--53:E0",
        "to_id": "abstract-2020--acl-main--53:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recent works in dialogue state tracking (DST) focus on an open vocabulary-based setting to resolve scalability and generalization issues of the predefined ontology-based approaches. However, they are inefficient in that they predict the dialogue state at every turn from scratch. Here, we consider dialogue state as an explicit fixed-sized memory and propose a selectively overwriting mechanism for more efficient DST. This mechanism consists of two steps: (1) predicting state operation on each of the memory slots, and (2) overwriting the memory with new values, of which only a few are generated according to the predicted state operations. Our method decomposes DST into two sub-tasks and guides the decoder to focus only on one of the tasks, thus reducing the burden of the decoder. This enhances the effectiveness of training and DST performance. Our SOM-DST (Selectively Overwriting Memory for Dialogue State Tracking) model achieves state-of-the-art joint goal accuracy with 51.72% in MultiWOZ 2.0 and 53.01% in MultiWOZ 2.1 in an open vocabulary-based DST setting. In addition, we analyze the accuracy gaps between the current and the ground truth-given situations and suggest that it is a promising direction to improve state operation prediction to boost the DST performance."
    }
  },
  {
    "id": "P10-1026",
    "result": [
      {
        "value": {
          "start": 202,
          "end": 217,
          "text": "Bayesian method",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1026:E0"
      },
      {
        "value": {
          "start": 768,
          "end": 778,
          "text": "accuracies",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1026:E1"
      },
      {
        "from_id": "P10-1026:E0",
        "to_id": "P10-1026:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Existing word similarity measures are not robust to data sparseness since they rely only on the point estimation of words' context profiles obtained from a limited amount of data. This paper proposes a Bayesian method for robust distributional word similarities. The method uses a distribution of context profiles obtained by Bayesian estimation and takes the expectation of a base similarity measure under that distribution. When the context profiles are multinomial distributions, the priors are Dirichlet, and the base measure is the Bhattacharyya coefficient, we can derive an analytical form that allows efficient calculation. For the task of word similarity estimation using a large amount of Web data in Japanese, we show that the proposed measure gives better accuracies than other well-known similarity measures."
    }
  },
  {
    "id": "abstract-2020--acl-main--188",
    "result": [
      {
        "value": {
          "start": 517,
          "end": 535,
          "text": "calibration method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--188:E0"
      },
      {
        "value": {
          "start": 917,
          "end": 928,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--188:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--188:E0",
        "to_id": "abstract-2020--acl-main--188:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We address the problem of calibrating prediction confidence for output entities of interest in natural language processing (NLP) applications. It is important that NLP applications such as named entity recognition and question answering produce calibrated confidence scores for their predictions, especially if the applications are to be deployed in a safety-critical domain such as healthcare. However the output space of such structured prediction models are often too large to directly adapt binary or multi-class calibration methods. In this study, we propose a general calibration scheme for output entities of interest in neural network based structured prediction models. Our proposed method can be used with any binary class calibration scheme and a neural network model. Additionally, we show that our calibration method can also be used as an uncertainty-aware, entity-specific decoding step to improve the performance of the underlying model at no additional training cost or data requirements. We show that our method outperforms current calibration techniques for Named Entity Recognition, Part-of-speech tagging and Question Answering systems. We also observe an improvement in model performance from our decoding step across several tasks and benchmark datasets. Our method improves the calibration and model performance on out-of-domain test scenarios as well."
    }
  },
  {
    "id": "abstract-2021--acl-long--398",
    "result": [
      {
        "value": {
          "start": 363,
          "end": 387,
          "text": "training objective MiSAD",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--398:E0"
      },
      {
        "value": {
          "start": 799,
          "end": 807,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--398:E1"
      },
      {
        "value": {
          "start": 885,
          "end": 896,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--398:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--398:E0",
        "to_id": "abstract-2021--acl-long--398:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--398:E0",
        "to_id": "abstract-2021--acl-long--398:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Despite the well-developed cut-edge representation learning for language, most language representation models usually focus on specific levels of linguistic units. This work introduces universal language representation learning, i.e., embeddings of different levels of linguistic units or text with quite diverse lengths in a uniform vector space. We propose the training objective MiSAD that utilizes meaningful n-grams extracted from large unlabeled corpus by a simple but effective algorithm for pre-trained language models. Then we empirically verify that well designed pre-training scheme may effectively yield universal language representation, which will bring great convenience when handling multiple layers of linguistic objects in a unified way. Especially, our model achieves the highest accuracy on analogy tasks in different language levels and significantly improves the performance on downstream tasks in the GLUE benchmark and a question answering dataset."
    }
  },
  {
    "id": "P11-1128",
    "result": [
      {
        "value": {
          "start": 468,
          "end": 499,
          "text": "adjoining tree-to-string system",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1128:E0"
      },
      {
        "value": {
          "start": 509,
          "end": 528,
          "text": "translation quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1128:E1"
      },
      {
        "value": {
          "start": 537,
          "end": 541,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1128:E2"
      },
      {
        "from_id": "P11-1128:E0",
        "to_id": "P11-1128:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P11-1128:E0",
        "to_id": "P11-1128:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We introduce synchronous tree adjoining grammars (TAG) into tree-to-string translation, which converts a source tree to a target string. Without reconstructing TAG derivations explicitly, our rule extraction algorithm directly learns tree-to-string rules from aligned Treebank-style trees. As tree-to-string translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-to-string system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST Chinese-English test sets."
    }
  },
  {
    "id": "P11-1152",
    "result": [
      {
        "value": {
          "start": 502,
          "end": 572,
          "text": "both training on rare events and preferable application to rare events",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1152:E0"
      },
      {
        "value": {
          "start": 581,
          "end": 591,
          "text": "perplexity",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1152:E1"
      },
      {
        "from_id": "P11-1152:E0",
        "to_id": "P11-1152:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Building on earlier work that integrates different factors in language modeling, we view (i) backing off to a shorter history and (ii) class-based generalization as two complementary mechanisms of using a larger equivalence class for prediction when the default equivalence class is too small for reliable estimation. This view entails that the classes in a language model should be learned from rare events only and should be preferably applied to rare events. We construct such a model and show that both training on rare events and preferable application to rare events improve perplexity when compared to a simple direct interpolation of class-based with standard language models."
    }
  },
  {
    "id": "P11-1010",
    "result": [
      {
        "value": {
          "start": 488,
          "end": 520,
          "text": "(semi-)supervised hashing method",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1010:E0"
      },
      {
        "value": {
          "start": 889,
          "end": 900,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1010:E1"
      },
      {
        "from_id": "P11-1010:E0",
        "to_id": "P11-1010:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Searching documents that are similar to a query document is an important component in modern information retrieval. Some existing hashing methods can be used for efficient document similarity search. However, unsupervised hashing methods cannot incorporate prior knowledge for better hashing. Although some supervised hashing methods can derive effective hash functions from prior knowledge, they are either computationally expensive or poorly discriminative. This paper proposes a novel (semi-)supervised hashing method named Semi-Supervised SimHash (S3H) for high-dimensional data similarity search. The basic idea of S3H is to learn the optimal feature weights from prior knowledge to relocate the data such that similar data have similar hash codes. We evaluate our method with several state-of-the-art methods on two large datasets. All the results show that our method gets the best performance."
    }
  },
  {
    "id": "abstract-2020--acl-main--302",
    "result": [
      {
        "value": {
          "start": 953,
          "end": 991,
          "text": "techniques developed before the DL era",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--302:E0"
      },
      {
        "value": {
          "start": 91,
          "end": 102,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--302:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--302:E0",
        "to_id": "abstract-2020--acl-main--302:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In the deep learning (DL) era, parsing models are extremely simplified with little hurt on performance, thanks to the remarkable capability of multi-layer BiLSTMs in context representation. As the most popular graph-based dependency parser due to its high efficiency and performance, the biaffine parser directly scores single dependencies under the arc-factorization assumption, and adopts a very simple local token-wise cross-entropy training loss. This paper for the first time presents a second-order TreeCRF extension to the biaffine parser. For a long time, the complexity and inefficiency of the inside-outside algorithm hinder the popularity of TreeCRF. To address this issue, we propose an effective way to batchify the inside and Viterbi algorithms for direct large matrix operation on GPUs, and to avoid the complex outside algorithm via efficient back-propagation. Experiments and analysis on 27 datasets from 13 languages clearly show that techniques developed before the DL era, such as structural learning (global TreeCRF loss) and high-order modeling are still useful, and can further boost parsing performance over the state-of-the-art biaffine parser, especially for partially annotated training data. We release our code at https://github.com/yzhangcs/crfpar."
    }
  },
  {
    "id": "abstract-2020--acl-main--41",
    "result": [
      {
        "value": {
          "start": 692,
          "end": 708,
          "text": "norm-based model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--41:E0"
      },
      {
        "value": {
          "start": 1059,
          "end": 1069,
          "text": "BLEU score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--41:E1"
      },
      {
        "value": {
          "start": 1097,
          "end": 1104,
          "text": "speedup",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--41:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--41:E0",
        "to_id": "abstract-2020--acl-main--41:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--41:E0",
        "to_id": "abstract-2020--acl-main--41:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "A neural machine translation (NMT) system is expensive to train, especially with high-resource settings. As the NMT architectures become deeper and wider, this issue gets worse and worse. In this paper, we aim to improve the efficiency of training an NMT by introducing a novel norm-based curriculum learning method. We use the norm (aka length or module) of a word embedding as a measure of 1) the difficulty of the sentence, 2) the competence of the model, and 3) the weight of the sentence. The norm-based sentence difficulty takes the advantages of both linguistically motivated and model-based sentence difficulties. It is easy to determine and contains learning-dependent features. The norm-based model competence makes NMT learn the curriculum in a fully automated way, while the norm-based sentence weight further enhances the learning of the vector representation of the NMT. Experimental results for the WMT’14 English-German and WMT’17 Chinese-English translation tasks demonstrate that the proposed method outperforms strong baselines in terms of BLEU score (+1.17/+1.56) and training speedup (2.22x/3.33x)."
    }
  },
  {
    "id": "abstract-2021--acl-long--163",
    "result": [
      {
        "value": {
          "start": 1035,
          "end": 1051,
          "text": "increasing depth",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--163:E0"
      },
      {
        "value": {
          "start": 1069,
          "end": 1083,
          "text": "generalization",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--163:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--163:E0",
        "to_id": "abstract-2021--acl-long--163:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "It is a common belief that training deep transformers from scratch requires large datasets. Consequently, for small datasets, people usually use shallow and simple additional layers on top of pre-trained models during fine-tuning. This work shows that this does not always need to be the case: with proper initialization and optimization, the benefits of very deep transformers can carry over to challenging tasks with small datasets, including Text-to-SQL semantic parsing and logical reading comprehension. In particular, we successfully train 48 layers of transformers, comprising 24 fine-tuned layers from pre-trained RoBERTa and 24 relation-aware layers trained from scratch. With fewer training steps and no task-specific pre-training, we obtain the state of the art performance on the challenging cross-domain Text-to-SQL parsing benchmark Spider. We achieve this by deriving a novel Data dependent Transformer Fixed-update initialization scheme (DT-Fixup), inspired by the prior T-Fixup work. Further error analysis shows that increasing depth can help improve generalization on small datasets for hard cases that require reasoning and structural understanding."
    }
  },
  {
    "id": "abstract-2021--acl-long--352",
    "result": [
      {
        "value": {
          "start": 414,
          "end": 428,
          "text": "compound rules",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--352:E0"
      },
      {
        "value": {
          "start": 481,
          "end": 490,
          "text": "precision",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--352:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--352:E0",
        "to_id": "abstract-2021--acl-long--352:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We study the problem of building entity tagging systems by using a few rules as weak supervision. Previous methods mostly focus on disambiguating entity types based on contexts and expert-provided rules, while assuming entity spans are given. In this work, we propose a novel method TALLOR that bootstraps high-quality logical rules to train a neural tagger in a fully automated manner. Specifically, we introduce compound rules that are composed from simple rules to increase the precision of boundary detection and generate more diverse pseudo labels. We further design a dynamic label selection strategy to ensure pseudo label quality and therefore avoid overfitting the neural tagger. Experiments on three datasets demonstrate that our method outperforms other weakly supervised methods and even rivals a state-of-the-art distantly supervised tagger with a lexicon of over 2,000 terms when starting from only 20 simple rules. Our method can serve as a tool for rapidly building taggers in emerging domains and tasks. Case studies show that learned rules can potentially explain the predicted entities."
    }
  },
  {
    "id": "abstract-2020--acl-main--221",
    "result": [
      {
        "value": {
          "start": 587,
          "end": 634,
          "text": "introduction of these models into SDS pipelines",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--221:E0"
      },
      {
        "value": {
          "start": 654,
          "end": 691,
          "text": "perceived naturalness of interactions",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--221:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--221:E0",
        "to_id": "abstract-2020--acl-main--221:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The timings of spoken response offsets in human dialogue have been shown to vary based on contextual elements of the dialogue. We propose neural models that simulate the distributions of these response offsets, taking into account the response turn as well as the preceding turn. The models are designed to be integrated into the pipeline of an incremental spoken dialogue system (SDS). We evaluate our models using offline experiments as well as human listening tests. We show that human listeners consider certain response timings to be more natural based on the dialogue context. The introduction of these models into SDS pipelines could increase the perceived naturalness of interactions."
    }
  },
  {
    "id": "abstract-2021--acl-long--408",
    "result": [
      {
        "value": {
          "start": 434,
          "end": 453,
          "text": "CBOW-based training",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--408:E0"
      },
      {
        "value": {
          "start": 487,
          "end": 511,
          "text": "computational efficiency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--408:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--408:E0",
        "to_id": "abstract-2021--acl-long--408:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The advent of contextual word embeddings — representations of words which incorporate semantic and syntactic information from their context—has led to tremendous improvements on a wide variety of NLP tasks. However, recent contextual models have prohibitively high computational cost in many use-cases and are often hard to interpret. In this work, we demonstrate that our proposed distillation method, which is a simple extension of CBOW-based training, allows to significantly improve computational efficiency of NLP applications, while outperforming the quality of existing static embeddings trained from scratch as well as those distilled from previously proposed methods. As a side-effect, our approach also allows a fair comparison of both contextual and static embeddings via standard lexical evaluation tasks."
    }
  },
  {
    "id": "abstract-2020--acl-main--612",
    "result": [
      {
        "value": {
          "start": 437,
          "end": 443,
          "text": "FGS2EE",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--612:E0"
      },
      {
        "value": {
          "start": 898,
          "end": 909,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--612:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--612:E0",
        "to_id": "abstract-2020--acl-main--612:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Entity embeddings, which represent different aspects of each entity with a single vector like word embeddings, are a key component of neural entity linking models. Existing entity embeddings are learned from canonical Wikipedia articles and local contexts surrounding target entities. Such entity embeddings are effective, but too distinctive for linking models to learn contextual commonality. We propose a simple yet effective method, FGS2EE, to inject fine-grained semantic information into entity embeddings to reduce the distinctiveness and facilitate the learning of contextual commonality. FGS2EE first uses the embeddings of semantic type words to generate semantic embeddings, and then combines them with existing entity embeddings through linear aggregation. Extensive experiments show the effectiveness of such embeddings. Based on our entity embeddings, we achieved new sate-of-the-art performance on entity linking."
    }
  },
  {
    "id": "abstract-2020--acl-main--250",
    "result": [
      {
        "value": {
          "start": 804,
          "end": 812,
          "text": "schuBERT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--250:E0"
      },
      {
        "value": {
          "start": 831,
          "end": 847,
          "text": "average accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--250:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--250:E0",
        "to_id": "abstract-2020--acl-main--250:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Transformers have gradually become a key component for many state-of-the-art natural language representation models. A recent Transformer based model- BERTachieved state-of-the-art results on various natural language processing tasks, including GLUE, SQuAD v1.1, and SQuAD v2.0. This model however is computationally prohibitive and has a huge number of parameters. In this work we revisit the architecture choices of BERT in efforts to obtain a lighter model. We focus on reducing the number of parameters yet our methods can be applied towards other objectives such FLOPs or latency. We show that much efficient light BERT models can be obtained by reducing algorithmically chosen correct architecture design dimensions rather than reducing the number of Transformer encoder layers. In particular, our schuBERT gives 6.6% higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number of parameters."
    }
  },
  {
    "id": "abstract-2021--acl-long--507",
    "result": [
      {
        "value": {
          "start": 904,
          "end": 946,
          "text": "English/German and English/Russian systems",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--507:E0"
      },
      {
        "value": {
          "start": 989,
          "end": 1000,
          "text": "BLEU points",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--507:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--507:E0",
        "to_id": "abstract-2021--acl-long--507:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We show that margin-based bitext mining in a multilingual sentence space can be successfully scaled to operate on monolingual corpora of billions of sentences. We use 32 snapshots of a curated common crawl corpus (Wenzel et al, 2019) totaling 71 billion unique sentences. Using one unified approach for 90 languages, we were able to mine 10.8 billion parallel sentences, out of which only 2.9 billions are aligned with English. We illustrate the capability of our scalable mining system to create high quality training sets from one language to any other by training hundreds of different machine translation models and evaluating them on the many-to-many TED benchmark. Further, we evaluate on competitive translation benchmarks such as WMT and WAT. Using only mined bitext, we set a new state of the art for a single system on the WMT’19 test set for English-German/Russian/Chinese. In particular, our English/German and English/Russian systems outperform the best single ones by over 4 BLEU points and are on par with best WMT’19 systems, which train on the WMT training data and augment it with backtranslation. We also achieve excellent results for distant languages pairs like Russian/Japanese, outperforming the best submission at the 2020 WAT workshop. All of the mined bitext will be freely available."
    }
  },
  {
    "id": "abstract-2020--acl-main--655",
    "result": [
      {
        "value": {
          "start": 238,
          "end": 242,
          "text": "KGAT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--655:E0"
      },
      {
        "value": {
          "start": 669,
          "end": 680,
          "text": "FEVER score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--655:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--655:E0",
        "to_id": "abstract-2020--acl-main--655:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Fact Verification requires fine-grained natural language inference capability that finds subtle clues to identify the syntactical and semantically correct but not well-supported claims. This paper presents Kernel Graph Attention Network (KGAT), which conducts more fine-grained fact verification with kernel-based attentions. Given a claim and a set of potential evidence sentences that form an evidence graph, KGAT introduces node kernels, which better measure the importance of the evidence node, and edge kernels, which conduct fine-grained evidence propagation in the graph, into Graph Attention Networks for more accurate fact verification. KGAT achieves a 70.38% FEVER score and significantly outperforms existing fact verification models on FEVER, a large-scale benchmark for fact verification. Our analyses illustrate that, compared to dot-product attentions, the kernel-based attention concentrates more on relevant evidence sentences and meaningful clues in the evidence graph, which is the main source of KGAT’s effectiveness. All source codes of this work are available at https://github.com/thunlp/KernelGAT."
    }
  },
  {
    "id": "P11-1100",
    "result": [
      {
        "value": {
          "start": 387,
          "end": 396,
          "text": "our model",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1100:E0"
      },
      {
        "value": {
          "start": 514,
          "end": 519,
          "text": "error",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1100:E1"
      },
      {
        "value": {
          "start": 387,
          "end": 396,
          "text": "our model",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1100:E2"
      },
      {
        "value": {
          "start": 514,
          "end": 524,
          "text": "error rate",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1100:E3"
      },
      {
        "from_id": "P11-1100:E0",
        "to_id": "P11-1100:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "P11-1100:E2",
        "to_id": "P11-1100:E3",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "We present a novel model to represent and assess the discourse coherence of text. Our model assumes that coherent text implicitly favors certain types of discourse relation transitions. We implement this model and apply it towards the text ordering ranking task, which aims to discern an original text from a permuted ordering of its sentences. The experimental results demonstrate that our model is able to significantly outperform the state-of-the-art coherence model by Barzilay and Lapata (2005), reducing the error rate of the previous approach by an average of 29% over three data sets against human upper bounds. We further show that our model is synergistic with the previous approach, demonstrating an error reduction of 73% when the features from both models are combined for the task."
    }
  },
  {
    "id": "abstract-2020--acl-main--628",
    "result": [
      {
        "value": {
          "start": 127,
          "end": 151,
          "text": "sentence meta-embeddings",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--628:E0"
      },
      {
        "value": {
          "start": 622,
          "end": 633,
          "text": "Pearson’s r",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--628:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--628:E0",
        "to_id": "abstract-2020--acl-main--628:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We address the task of unsupervised Semantic Textual Similarity (STS) by ensembling diverse pre-trained sentence encoders into sentence meta-embeddings. We apply, extend and evaluate different meta-embedding methods from the word embedding literature at the sentence level, including dimensionality reduction (Yin and Schütze, 2016), generalized Canonical Correlation Analysis (Rastogi et al., 2015) and cross-view auto-encoders (Bollegala and Bao, 2018). Our sentence meta-embeddings set a new unsupervised State of The Art (SoTA) on the STS Benchmark and on the STS12-STS16 datasets, with gains of between 3.7% and 6.4% Pearson’s r over single-source systems."
    }
  },
  {
    "id": "abstract-2021--acl-long--438",
    "result": [
      {
        "value": {
          "start": 371,
          "end": 428,
          "text": "Reinforcement Iterative Sequence Editing (RISE) framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--438:E0"
      },
      {
        "value": {
          "start": 448,
          "end": 476,
          "text": "minimum Levenshtein distance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--438:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--438:E0",
        "to_id": "abstract-2021--acl-long--438:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Conversational Question Simplification (CQS) aims to simplify self-contained questions into conversational ones by incorporating some conversational characteristics, e.g., anaphora and ellipsis. Existing maximum likelihood estimation based methods often get trapped in easily learned tokens as all tokens are treated equally during training. In this work, we introduce a Reinforcement Iterative Sequence Editing (RISE) framework that optimizes the minimum Levenshtein distance through explicit editing actions. RISE is able to pay attention to tokens that are related to conversational characteristics. To train RISE, we devise an Iterative Reinforce Training (IRT) algorithm with a Dynamic Programming based Sampling (DPS) process to improve exploration. Experimental results on two benchmark datasets show that RISE significantly outperforms state-of-the-art methods and generalizes well on unseen data."
    }
  },
  {
    "id": "abstract-2020--acl-main--7",
    "result": [
      {
        "value": {
          "start": 644,
          "end": 696,
          "text": "conditional variational autoencoder based deep model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--7:E0"
      },
      {
        "value": {
          "start": 909,
          "end": 920,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--7:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--7:E0",
        "to_id": "abstract-2020--acl-main--7:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Leveraging persona information of users in Neural Response Generators (NRG) to perform personalized conversations has been considered as an attractive and important topic in the research of conversational agents over the past few years. Despite of the promising progress achieved by recent studies in this field, persona information tends to be incorporated into neural networks in the form of user embeddings, with the expectation that the persona can be involved via End-to-End learning. This paper proposes to adopt the personality-related characteristics of human conversations into variational response generators, by designing a specific conditional variational autoencoder based deep model with two new regularization terms employed to the loss function, so as to guide the optimization towards the direction of generating both persona-aware and relevant responses. Besides, to reasonably evaluate the performances of various persona modeling approaches, this paper further presents three direct persona-oriented metrics from different perspectives. The experimental results have shown that our proposed methodology can notably improve the performance of persona-aware response generation, and the metrics are reasonable to evaluate the results."
    }
  },
  {
    "id": "abstract-2021--acl-long--417",
    "result": [
      {
        "value": {
          "start": 399,
          "end": 403,
          "text": "AREC",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--417:E0"
      },
      {
        "value": {
          "start": 859,
          "end": 867,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--417:E1"
      },
      {
        "value": {
          "start": 923,
          "end": 939,
          "text": "model robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--417:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--417:E0",
        "to_id": "abstract-2021--acl-long--417:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--417:E0",
        "to_id": "abstract-2021--acl-long--417:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Deep learning models have achieved great success on the task of Natural Language Inference (NLI), though only a few attempts try to explain their behaviors. Existing explanation methods usually pick prominent features such as words or phrases from the input text. However, for NLI, alignments among words or phrases are more enlightening clues to explain the model. To this end, this paper presents AREC, a post-hoc approach to generate alignment rationale explanations for co-attention based models in NLI. The explanation is based on feature selection, which keeps few but sufficient alignments while maintaining the same prediction of the target model. Experimental results show that our method is more faithful and human-readable compared with many existing approaches. We further study and re-evaluate three typical models through our explanation beyond accuracy, and propose a simple method that greatly improves the model robustness."
    }
  },
  {
    "id": "abstract-2021--acl-long--199",
    "result": [
      {
        "value": {
          "start": 843,
          "end": 871,
          "text": "Simplified LXMERT (LXMERT-S)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--199:E0"
      },
      {
        "value": {
          "start": 892,
          "end": 902,
          "text": "parameters",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--199:E1"
      },
      {
        "value": {
          "start": 304,
          "end": 315,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--199:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--199:E0",
        "to_id": "abstract-2021--acl-long--199:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--199:E0",
        "to_id": "abstract-2021--acl-long--199:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Multimodal pre-training models, such as LXMERT, have achieved excellent results in downstream tasks. However, current pre-trained models require large amounts of training data and have huge model sizes, which make them impossible to apply in low-resource situations. How to obtain similar or even better performance than a larger model under the premise of less pre-training data and smaller model size has become an important problem. In this paper, we propose a new Multi-stage Pre-training (MSP) method, which uses information at different granularities from word, phrase to sentence in both texts and images to pre-train a model in stages. We also design several different pre-training tasks suitable for the information granularity in different stage in order to efficiently capture the diverse knowledge from a limited corpus. We take a Simplified LXMERT (LXMERT-S) which is with 45.9% parameters of the original LXMERT model and only 11.44% of the original pre-training data as the testbed of our MSP method. Experimental results show that our method achieves comparable performance to the original LXMERT model in all downstream tasks, and even outperforms the original model in Image-Text Retrieval task."
    }
  },
  {
    "id": "abstract-2021--acl-long--283",
    "result": [
      {
        "value": {
          "start": 400,
          "end": 406,
          "text": "HERALD",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--283:E0"
      },
      {
        "value": {
          "start": 843,
          "end": 864,
          "text": "annotation efficiency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--283:E1"
      },
      {
        "value": {
          "start": 896,
          "end": 933,
          "text": "user disengagement detection accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--283:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--283:E0",
        "to_id": "abstract-2021--acl-long--283:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--283:E0",
        "to_id": "abstract-2021--acl-long--283:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Open-domain dialog systems have a user-centric goal: to provide humans with an engaging conversation experience. User engagement is one of the most important metrics for evaluating open-domain dialog systems, and could also be used as real-time feedback to benefit dialog policy learning. Existing work on detecting user disengagement typically requires hand-labeling many dialog samples. We propose HERALD, an efficient annotation framework that reframes the training data annotation process as a denoising problem. Specifically, instead of manually labeling training samples, we first use a set of labeling heuristics to label training samples automatically. We then denoise the weakly labeled data using the Shapley algorithm. Finally, we use the denoised data to train a user engagement detector. Our experiments show that HERALD improves annotation efficiency significantly and achieves 86% user disengagement detection accuracy in two dialog corpora."
    }
  },
  {
    "id": "abstract-2021--acl-long--389",
    "result": [
      {
        "value": {
          "start": 201,
          "end": 213,
          "text": "VisualSparta",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--389:E0"
      },
      {
        "value": {
          "start": 323,
          "end": 331,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--389:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--389:E0",
        "to_id": "abstract-2021--acl-long--389:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Text-to-image retrieval is an essential task in cross-modal information retrieval, i.e., retrieving relevant images from a large and unlabelled dataset given textual queries. In this paper, we propose VisualSparta, a novel (Visual-text Sparse Transformer Matching) model that shows significant improvement in terms of both accuracy and efficiency. VisualSparta is capable of outperforming previous state-of-the-art scalable methods in MSCOCO and Flickr30K. We also show that it achieves substantial retrieving speed advantages, i.e., for a 1 million image index, VisualSparta using CPU gets ~391X speedup compared to CPU vector search and ~5.4X speedup compared to vector search with GPU acceleration. Experiments show that this speed advantage even gets bigger for larger datasets because VisualSparta can be efficiently implemented as an inverted index. To the best of our knowledge, VisualSparta is the first transformer-based text-to-image retrieval model that can achieve real-time searching for large-scale datasets, with significant accuracy improvement compared to previous state-of-the-art methods."
    }
  },
  {
    "id": "abstract-2020--acl-main--542",
    "result": [
      {
        "value": {
          "start": 498,
          "end": 526,
          "text": "Curriculum Learning approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--542:E0"
      },
      {
        "value": {
          "start": 817,
          "end": 828,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--542:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--542:E0",
        "to_id": "abstract-2020--acl-main--542:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "With the great success of pre-trained language models, the pretrain-finetune paradigm now becomes the undoubtedly dominant solution for natural language understanding (NLU) tasks. At the fine-tune stage, target task data is usually introduced in a completely random order and treated equally. However, examples in NLU tasks can vary greatly in difficulty, and similar to human learning procedure, language models can benefit from an easy-to-difficult curriculum. Based on this idea, we propose our Curriculum Learning approach. By reviewing the trainset in a crossed way, we are able to distinguish easy examples from difficult ones, and arrange a curriculum for language models. Without any manual model architecture design or use of external data, our Curriculum Learning approach obtains significant and universal performance improvements on a wide range of NLU tasks."
    }
  },
  {
    "id": "abstract-2021--acl-long--132",
    "result": [
      {
        "value": {
          "start": 13,
          "end": 48,
          "text": "human-and-model-in-the-loop process",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--132:E0"
      },
      {
        "value": {
          "start": 531,
          "end": 542,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--132:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--132:E0",
        "to_id": "abstract-2021--acl-long--132:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We present a human-and-model-in-the-loop process for dynamically generating datasets and training better performing and more robust hate detection models. We provide a new dataset of 40,000 entries, generated and labelled by trained annotators over four rounds of dynamic data creation. It includes 15,000 challenging perturbations and each hateful entry has fine-grained labels for the type and target of hate. Hateful entries make up 54% of the dataset, which is substantially higher than comparable datasets. We show that model performance is substantially improved using this approach. Models trained on later rounds of data collection perform better on test sets and are harder for annotators to trick. They also have better performance on HateCheck, a suite of functional tests for online hate detection. We provide the code, dataset and annotation guidelines for other researchers to use."
    }
  },
  {
    "id": "abstract-2021--acl-long--488",
    "result": [
      {
        "value": {
          "start": 552,
          "end": 556,
          "text": "KECI",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--488:E0"
      },
      {
        "value": {
          "start": 1474,
          "end": 1483,
          "text": "F1 scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--488:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--488:E0",
        "to_id": "abstract-2021--acl-long--488:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Compared to the general news domain, information extraction (IE) from biomedical text requires much broader domain knowledge. However, many previous IE methods do not utilize any external knowledge during inference. Due to the exponential growth of biomedical publications, models that do not go beyond their fixed set of parameters will likely fall behind. Inspired by how humans look up relevant information to comprehend a scientific text, we present a novel framework that utilizes external knowledge for joint entity and relation extraction named KECI (Knowledge-Enhanced Collective Inference). Given an input text, KECI first constructs an initial span graph representing its initial understanding of the text. It then uses an entity linker to form a knowledge graph containing relevant background knowledge for the the entity mentions in the text. To make the final predictions, KECI fuses the initial span graph and the knowledge graph into a more refined graph using an attention mechanism. KECI takes a collective approach to link mention spans to entities by integrating global relational information into local representations using graph convolutional networks. Our experimental results show that the framework is highly effective, achieving new state-of-the-art results in two different benchmark datasets: BioRelEx (binding interaction detection) and ADE (adverse drug event extraction). For example, KECI achieves absolute improvements of 4.59% and 4.91% in F1 scores over the state-of-the-art on the BioRelEx entity and relation extraction tasks"
    }
  },
  {
    "id": "abstract-2020--acl-main--119",
    "result": [
      {
        "value": {
          "start": 491,
          "end": 527,
          "text": "a model based on iterative inference",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--119:E0"
      },
      {
        "value": {
          "start": 620,
          "end": 628,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--119:E1"
      },
      {
        "value": {
          "start": 17,
          "end": 33,
          "text": "end-to-end model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--119:E2"
      },
      {
        "value": {
          "start": 704,
          "end": 717,
          "text": "Smatch scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--119:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--119:E0",
        "to_id": "abstract-2020--acl-main--119:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--119:E2",
        "to_id": "abstract-2020--acl-main--119:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We propose a new end-to-end model that treats AMR parsing as a series of dual decisions on the input sequence and the incrementally constructed graph. At each time step, our model performs multiple rounds of attention, reasoning, and composition that aim to answer two critical questions: (1) which part of the input  sequence  to abstract; and (2) where in the output  graph  to construct the new concept. We show that the answers to these two questions are mutually causalities. We design a model based on iterative inference that helps achieve better answers in both perspectives, leading to greatly improved parsing accuracy. Our experimental results significantly outperform all previously reported Smatch scores by large margins. Remarkably, without the help of any large-scale pre-trained language model (e.g., BERT), our model already surpasses previous state-of-the-art using BERT. With the help of BERT, we can push the state-of-the-art results to 80.2% on LDC2017T10 (AMR 2.0) and 75.4% on LDC2014T12 (AMR 1.0)."
    }
  },
  {
    "id": "P11-1065",
    "result": [
      {
        "value": {
          "start": 567,
          "end": 643,
          "text": "introduce linguistic information of varying granularity from the source side",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1065:E0"
      },
      {
        "value": {
          "start": 1016,
          "end": 1020,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1065:E1"
      },
      {
        "from_id": "P11-1065:E0",
        "to_id": "P11-1065:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "While it is generally accepted that many translation phenomena are correlated with linguistic structures, employing linguistic syntax for translation has proven a highly non-trivial task. The key assumption behind many approaches is that translation is guided by the source and/or target language parse, employing rules extracted from the parse tree or performing tree transformations. These approaches enforce strict constraints and might overlook important translation phenomena that cross linguistic constituents. We propose a novel flexible modelling approach to introduce linguistic information of varying granularity from the source side. Our method induces joint probability synchronous grammars and estimates their parameters, by selecting and weighing together linguistically motivated rules according to an objective function directly targeting generalisation over future data. We obtain statistically significant improvements across 4 different language pairs with English as source, mounting up to +1.92 BLEU for Chinese as target."
    }
  },
  {
    "id": "abstract-2021--acl-long--85",
    "result": [
      {
        "value": {
          "start": 658,
          "end": 696,
          "text": "domain-specific fine-tuning approaches",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--85:E0"
      },
      {
        "value": {
          "start": 727,
          "end": 745,
          "text": "detection accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--85:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--85:E0",
        "to_id": "abstract-2021--acl-long--85:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Deployed real-world machine learning applications are often subject to uncontrolled and even potentially malicious inputs. Those out-of-domain inputs can lead to unpredictable outputs and sometimes catastrophic safety issues. Prior studies on out-of-domain detection require in-domain task labels and are limited to supervised classification scenarios. Our work tackles the problem of detecting out-of-domain samples with only unsupervised in-domain data. We utilize the latent representations of pre-trained transformers and propose a simple yet effective method to transform features across all layers to construct out-of-domain detectors efficiently. Two domain-specific fine-tuning approaches are further proposed to boost detection accuracy. Our empirical evaluations of related methods on two datasets validate that our method greatly improves out-of-domain detection ability in a more general scenario."
    }
  },
  {
    "id": "abstract-2021--acl-long--25",
    "result": [
      {
        "value": {
          "start": 295,
          "end": 299,
          "text": "LaSS",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--25:E0"
      },
      {
        "value": {
          "start": 618,
          "end": 622,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--25:E1"
      },
      {
        "value": {
          "start": 295,
          "end": 299,
          "text": "LaSS",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--25:E2"
      },
      {
        "value": {
          "start": 655,
          "end": 681,
          "text": "generalization performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--25:E3"
      },
      {
        "from_id": "abstract-2021--acl-long--25:E0",
        "to_id": "abstract-2021--acl-long--25:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--25:E2",
        "to_id": "abstract-2021--acl-long--25:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Multilingual neural machine translation aims at learning a single translation model for multiple languages. These jointly trained models often suffer from performance degradationon rich-resource language pairs. We attribute this degeneration to parameter interference. In this paper, we propose LaSS to jointly train a single unified multilingual MT model. LaSS learns Language Specific Sub-network (LaSS) for each language pair to counter parameter interference. Comprehensive experiments on IWSLT and WMT datasets with various Transformer architectures show that LaSS obtains gains on 36 language pairs by up to 1.2 BLEU. Besides, LaSS shows its strong generalization performance at easy adaptation to new language pairs and zero-shot translation. LaSS boosts zero-shot translation with an average of 8.3 BLEU on 30 language pairs. Codes and trained models are available at https://github.com/NLP-Playground/LaSS."
    }
  },
  {
    "id": "abstract-2020--acl-main--347",
    "result": [
      {
        "value": {
          "start": 490,
          "end": 521,
          "text": "two-stage pre-training approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--347:E0"
      },
      {
        "value": {
          "start": 534,
          "end": 556,
          "text": "demands of speech data",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--347:E1"
      },
      {
        "value": {
          "start": 572,
          "end": 582,
          "text": "efficiency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--347:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--347:E0",
        "to_id": "abstract-2020--acl-main--347:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--347:E0",
        "to_id": "abstract-2020--acl-main--347:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Pre-trained language models have achieved huge improvement on many NLP tasks. However, these methods are usually designed for written text, so they do not consider the properties of spoken language. Therefore, this paper aims at generalizing the idea of language model pre-training to lattices generated by recognition systems. We propose a framework that trains neural lattice language models to provide contextualized representations for spoken language understanding tasks. The proposed two-stage pre-training approach reduces the demands of speech data and has better efficiency. Experiments on intent detection and dialogue act recognition datasets demonstrate that our proposed method consistently outperforms strong baselines when evaluated on spoken inputs. The code is available at https://github.com/MiuLab/Lattice-ELMo."
    }
  },
  {
    "id": "abstract-2020--acl-main--598",
    "result": [
      {
        "value": {
          "start": 835,
          "end": 845,
          "text": "Our system",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--598:E0"
      },
      {
        "value": {
          "start": 933,
          "end": 941,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--598:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--598:E0",
        "to_id": "abstract-2020--acl-main--598:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We propose the task of unsupervised morphological paradigm completion. Given only raw text and a lemma list, the task consists of generating the morphological paradigms, i.e., all inflected forms, of the lemmas. From a natural language processing (NLP) perspective, this is a challenging unsupervised task, and high-performing systems have the potential to improve tools for low-resource languages or to assist linguistic annotators. From a cognitive science perspective, this can shed light on how children acquire morphological knowledge. We further introduce a system for the task, which generates morphological paradigms via the following steps: (i) EDIT TREE retrieval, (ii) additional lemma retrieval, (iii) paradigm size discovery, and (iv) inflection generation. We perform an evaluation on 14 typologically diverse languages. Our system outperforms trivial baselines with ease and, for some languages, even obtains a higher accuracy than minimally supervised systems."
    }
  },
  {
    "id": "abstract-2021--acl-long--424",
    "result": [
      {
        "value": {
          "start": 1138,
          "end": 1167,
          "text": "popularity-aware user encoder",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--424:E0"
      },
      {
        "value": {
          "start": 1334,
          "end": 1342,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--424:E1"
      },
      {
        "value": {
          "start": 585,
          "end": 624,
          "text": "incorporate news popularity information",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--424:E2"
      },
      {
        "value": {
          "start": 657,
          "end": 666,
          "text": "diversity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--424:E3"
      },
      {
        "from_id": "abstract-2021--acl-long--424:E0",
        "to_id": "abstract-2021--acl-long--424:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--424:E2",
        "to_id": "abstract-2021--acl-long--424:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--424:E2",
        "to_id": "abstract-2021--acl-long--424:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--424:E0",
        "to_id": "abstract-2021--acl-long--424:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Personalized news recommendation methods are widely used in online news services. These methods usually recommend news based on the matching between news content and user interest inferred from historical behaviors. However, these methods usually have difficulties in making accurate recommendations to cold-start users, and tend to recommend similar news with those users have read. In general, popular news usually contain important information and can attract users with different interests. Besides, they are usually diverse in content and topic. Thus, in this paper we propose to incorporate news popularity information to alleviate the cold-start and diversity problems for personalized news recommendation. In our method, the ranking score for recommending a candidate news to a target user is the combination of a personalized matching score and a news popularity score. The former is used to capture the personalized user interest in news. The latter is used to measure time-aware popularity of candidate news, which is predicted based on news content, recency, and real-time CTR using a unified framework. Besides, we propose a popularity-aware user encoder to eliminate the popularity bias in user behaviors for accurate interest modeling. Experiments on two real-world datasets show our method can effectively improve the accuracy and diversity for news recommendation."
    }
  },
  {
    "id": "P11-1021",
    "result": [
      {
        "value": {
          "start": 45,
          "end": 93,
          "text": "large scale distributed composite language model",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1021:E0"
      },
      {
        "value": {
          "start": 619,
          "end": 637,
          "text": "drastic perplexity",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1021:E1"
      },
      {
        "value": {
          "start": 695,
          "end": 714,
          "text": "translation quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1021:E2"
      },
      {
        "value": {
          "start": 731,
          "end": 741,
          "text": "BLEU score",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1021:E3"
      },
      {
        "value": {
          "start": 747,
          "end": 758,
          "text": "readability",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1021:E4"
      },
      {
        "from_id": "P11-1021:E0",
        "to_id": "P11-1021:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "P11-1021:E0",
        "to_id": "P11-1021:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P11-1021:E0",
        "to_id": "P11-1021:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P11-1021:E0",
        "to_id": "P11-1021:E4",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This paper presents an attempt at building a large scale distributed composite language model that simultaneously accounts for local word lexical information, mid-range sentence syntactic structure, and long-span document semantic content under a directed Markov random field paradigm. The composite language model has been trained by performing a convergent N-best list approximate EM algorithm that has linear time complexity and a follow-up EM algorithm to improve word prediction power on corpora with up to a billion tokens and stored on a supercomputer. The large scale distributed composite language model gives drastic perplexity reduction over n-grams and achieves significantly better translation quality measured by the BLEU score and \"readability\" when applied to the task of re-ranking the N-best list from a state-of-the-art parsing-based machine translation system."
    }
  },
  {
    "id": "abstract-2020--acl-main--713",
    "result": [
      {
        "value": {
          "start": 988,
          "end": 1010,
          "text": "adding global features",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--713:E0"
      },
      {
        "value": {
          "start": 1024,
          "end": 1035,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--713:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--713:E0",
        "to_id": "abstract-2020--acl-main--713:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Most existing joint neural models for Information Extraction (IE) use local task-specific classifiers to predict labels for individual instances (e.g., trigger, relation) regardless of their interactions. For example, a victim of a die event is likely to be a victim of an attack event in the same sentence. In order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural framework, OneIE, that aims to extract the globally optimal IE result as a graph from an input sentence. OneIE performs end-to-end IE in four stages: (1) Encoding a given sentence as contextualized word representations; (2) Identifying entity mentions and event triggers as nodes; (3) Computing label scores for all nodes and their pairwise links using local classifiers; (4) Searching for the globally optimal graph with a beam decoder. At the decoding stage, we incorporate global features to capture the cross-subtask and cross-instance interactions. Experiments show that adding global features improves the performance of our model and achieves new state of-the-art on all subtasks. In addition, as OneIE does not use any language-specific feature, we prove it can be easily applied to new languages or trained in a multilingual manner."
    }
  },
  {
    "id": "abstract-2020--acl-main--45",
    "result": [
      {
        "value": {
          "start": 582,
          "end": 595,
          "text": "use dice loss",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--45:E0"
      },
      {
        "value": {
          "start": 1272,
          "end": 1283,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--45:E1"
      },
      {
        "value": {
          "start": 1229,
          "end": 1247,
          "text": "training objective",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--45:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--45:E0",
        "to_id": "abstract-2020--acl-main--45:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--45:E2",
        "to_id": "abstract-2020--acl-main--45:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Many NLP tasks such as tagging and machine reading comprehension are faced with the severe data imbalance issue: negative examples significantly outnumber positive examples, and the huge number of easy-negative examples overwhelms the training. The most commonly used cross entropy (CE) criteria is actually an accuracy-oriented objective, and thus creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples. In this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks. Dice loss is based on the Sørensen--Dice coefficient or Tversky index , which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue. To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples. Theoretical analysis shows that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training. With the proposed training objective, we observe significant performance boost on a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task; SOTA results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task; along with competitive results on the tasks of machine reading comprehension and paraphrase identification."
    }
  },
  {
    "id": "abstract-2021--acl-long--443",
    "result": [
      {
        "value": {
          "start": 1003,
          "end": 1029,
          "text": "evaluator trained with PGD",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--443:E0"
      },
      {
        "value": {
          "start": 1088,
          "end": 1108,
          "text": "numbers of rewriting",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--443:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--443:E0",
        "to_id": "abstract-2021--acl-long--443:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "A few approaches have been developed to improve neural machine translation (NMT) models with multiple passes of decoding. However, their performance gains are limited because of lacking proper policies to terminate the multi-pass process. To address this issue, we introduce a novel architecture of Rewriter-Evaluator. Translating a source sentence involves multiple rewriting passes. In every pass, a rewriter generates a new translation to improve the past translation. Termination of this multi-pass process is determined by a score of translation quality estimated by an evaluator. We also propose prioritized gradient descent (PGD) to jointly and efficiently train the rewriter and the evaluator. Extensive experiments on three machine translation tasks show that our architecture notably improves the performances of NMT models and significantly outperforms prior methods. An oracle experiment reveals that it can largely reduce performance gaps to the oracle policy. Experiments confirm that the evaluator trained with PGD is more accurate than prior methods in determining proper numbers of rewriting."
    }
  },
  {
    "id": "abstract-2021--acl-long--571",
    "result": [
      {
        "value": {
          "start": 502,
          "end": 506,
          "text": "VOLT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--571:E0"
      },
      {
        "value": {
          "start": 787,
          "end": 802,
          "text": "vocabulary size",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--571:E1"
      },
      {
        "value": {
          "start": 821,
          "end": 825,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--571:E2"
      },
      {
        "value": {
          "start": 502,
          "end": 506,
          "text": "VOLT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--571:E3"
      },
      {
        "value": {
          "start": 909,
          "end": 920,
          "text": "search time",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--571:E4"
      },
      {
        "value": {
          "start": 930,
          "end": 939,
          "text": "GPU hours",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--571:E5"
      },
      {
        "from_id": "abstract-2021--acl-long--571:E0",
        "to_id": "abstract-2021--acl-long--571:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--571:E0",
        "to_id": "abstract-2021--acl-long--571:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--571:E3",
        "to_id": "abstract-2021--acl-long--571:E4",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--571:E3",
        "to_id": "abstract-2021--acl-long--571:E5",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "The choice of token vocabulary affects the performance of machine translation. This paper aims to figure out what is a good vocabulary and whether we can find the optimal vocabulary without trial training. To answer these questions, we first provide an alternative understanding of vocabulary from the perspective of information theory. It motivates us to formulate the quest of vocabularization – finding the best token dictionary with a proper size – as an optimal transport (OT) problem. We propose VOLT, a simple and efficient solution without trial training. Empirical results show that VOLT beats widely-used vocabularies in diverse scenarios, including WMT-14 English-German translation, TED bilingual translation, and TED multilingual translation. For example, VOLT achieves 70% vocabulary size reduction and 0.5 BLEU gain on English-German translation. Also, compared to BPE-search, VOLT reduces the search time from 384 GPU hours to 30 GPU hours on English-German translation. Codes are available at https://github.com/Jingjing-NLP/VOLT."
    }
  },
  {
    "id": "P10-1078",
    "result": [
      {
        "value": {
          "start": 625,
          "end": 635,
          "text": "our method",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1078:E0"
      },
      {
        "value": {
          "start": 648,
          "end": 660,
          "text": "ROUGE scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1078:E1"
      },
      {
        "from_id": "P10-1078:E0",
        "to_id": "P10-1078:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "This paper presents a framework for automatically processing information coming from community Question Answering (cQA) portals with the purpose of generating a trustful, complete, relevant and succinct summary in response to a question. We exploit the metadata intrinsically present in User Generated Content (UGC) to bias automatic multi-document summarization techniques toward high quality information. We adopt a representation of concepts alternative to n-grams and propose two concept-scoring functions based on semantic overlap. Experimental results on data drawn from Yahoo! Answers demonstrate the effectiveness of our method in terms of ROUGE scores. We show that the information contained in the best answers voted by users of cQA portals can be successfully complemented by our method."
    }
  },
  {
    "id": "abstract-2020--acl-main--76",
    "result": [
      {
        "value": {
          "start": 359,
          "end": 363,
          "text": "T-TA",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--76:E0"
      },
      {
        "value": {
          "start": 783,
          "end": 793,
          "text": "accuracies",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--76:E1"
      },
      {
        "value": {
          "start": 323,
          "end": 364,
          "text": "Transformer-based Text Autoencoder (T-TA)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--76:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--76:E0",
        "to_id": "abstract-2020--acl-main--76:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--76:E2",
        "to_id": "abstract-2020--acl-main--76:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Even though BERT has achieved successful performance improvements in various supervised learning tasks, BERT is still limited by repetitive inferences on unsupervised tasks for the computation of contextual language representations. To resolve this limitation, we propose a novel deep bidirectional language model called a Transformer-based Text Autoencoder (T-TA). The T-TA computes contextual language representations without repetition and displays the benefits of a deep bidirectional architecture, such as that of BERT. In computation time experiments in a CPU environment, the proposed T-TA performs over six times faster than the BERT-like model on a reranking task and twelve times faster on a semantic similarity task. Furthermore, the T-TA shows competitive or even better accuracies than those of BERT on the above tasks. Code is available at https://github.com/joongbo/tta."
    }
  },
  {
    "id": "abstract-2020--acl-main--36",
    "result": [
      {
        "value": {
          "start": 243,
          "end": 284,
          "text": "jointly masked sequence-to-sequence model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--36:E0"
      },
      {
        "value": {
          "start": 1056,
          "end": 1067,
          "text": "BLEU scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--36:E1"
      },
      {
        "value": {
          "start": 1129,
          "end": 1134,
          "text": "speed",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--36:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--36:E0",
        "to_id": "abstract-2020--acl-main--36:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--36:E0",
        "to_id": "abstract-2020--acl-main--36:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The masked language model has received remarkable attention due to its effectiveness on various natural language processing tasks. However, few works have adopted this technique in the sequence-to-sequence models. In this work, we introduce a jointly masked sequence-to-sequence model and explore its application on non-autoregressive neural machine translation~(NAT). Specifically, we first empirically study the functionalities of the encoder and the decoder in NAT models, and find that the encoder takes a more important role than the decoder regarding the translation quality. Therefore, we propose to train the encoder more rigorously by masking the encoder input while training. As for the decoder, we propose to train it based on the consecutive masking of the decoder input with an  n -gram loss function to alleviate the problem of translating duplicate words. The two types of masks are applied to the model jointly at the training stage. We conduct experiments on five benchmark machine translation tasks, and our model can achieve 27.69/32.24 BLEU scores on WMT14 English-German/German-English tasks with  5+  times speed up compared with an autoregressive model."
    }
  },
  {
    "id": "abstract-2020--acl-main--673",
    "result": [
      {
        "value": {
          "start": 9,
          "end": 36,
          "text": "disentangled representation",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--673:E0"
      },
      {
        "value": {
          "start": 965,
          "end": 972,
          "text": "quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--673:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--673:E0",
        "to_id": "abstract-2020--acl-main--673:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Learning disentangled representations of natural language is essential for many NLP tasks, e.g., conditional text generation, style transfer, personalized dialogue systems, etc. Similar problems have been studied extensively for other forms of data, such as images and videos. However, the discrete nature of natural language makes the disentangling of textual representations more challenging (e.g., the manipulation over the data space cannot be easily achieved). Inspired by information theory, we propose a novel method that effectively manifests disentangled representations of text, without any supervision on semantics. A new mutual information upper bound is derived and leveraged to measure dependence between style and content. By minimizing this upper bound, the proposed method induces style and content embeddings into two independent low-dimensional spaces. Experiments on both conditional text generation and text-style transfer demonstrate the high quality of our disentangled representation in terms of content and style preservation."
    }
  },
  {
    "id": "P10-1112",
    "result": [
      {
        "value": {
          "start": 46,
          "end": 102,
          "text": "exploits both large tree fragments and symbol refinement",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1112:E0"
      },
      {
        "value": {
          "start": 548,
          "end": 558,
          "text": "accuracies",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1112:E1"
      },
      {
        "value": {
          "start": 571,
          "end": 573,
          "text": "F1",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1112:E2"
      },
      {
        "from_id": "P10-1112:E0",
        "to_id": "P10-1112:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "P10-1112:E0",
        "to_id": "P10-1112:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We present a simple but accurate parser which exploits both large tree fragments and symbol refinement. We parse with all fragments of the training set, in contrast to much recent work on tree selection in data-oriented parsing and tree-substitution grammar learning. We require only simple, deterministic grammar symbol refinement, in contrast to recent work on latent symbol refinement. Moreover, our parser requires no explicit lexicon machinery, instead parsing input sentences as character streams. Despite its simplicity, our parser achieves accuracies of over 88% F1 on the standard English WSJ task, which is competitive with substantially more complicated state-of-the-art lexicalized and latent-variable parsers. Additional specific contributions center on making implicit all-fragments parsing efficient, including a coarse-to-fine inference scheme and a new graph encoding."
    }
  },
  {
    "id": "P10-1099",
    "result": [
      {
        "value": {
          "start": 531,
          "end": 569,
          "text": "leverage recently-developed techniques",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1099:E0"
      },
      {
        "value": {
          "start": 802,
          "end": 807,
          "text": "error",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1099:E1"
      },
      {
        "from_id": "P10-1099:E0",
        "to_id": "P10-1099:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Most supervised language processing systems show a significant drop-off in performance when they are tested on text that comes from a domain significantly different from the domain of the training data. Semantic role labeling techniques are typically trained on newswire text, and in tests their performance on fiction is as much as 19% worse than their performance on newswire text. We investigate techniques for building open-domain semantic role labeling systems that approach the ideal of a train-once, use-anywhere system. We leverage recently-developed techniques for learning representations of text using latent-variable language models, and extend these techniques to ones that provide the kinds of features that are useful for semantic role labeling. In experiments, our novel system reduces error by 16% relative to the previous state of the art on out-of-domain text."
    }
  },
  {
    "id": "abstract-2021--acl-long--21",
    "result": [
      {
        "value": {
          "start": 446,
          "end": 452,
          "text": "mRASP2",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--21:E0"
      },
      {
        "value": {
          "start": 409,
          "end": 420,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--21:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--21:E0",
        "to_id": "abstract-2021--acl-long--21:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Existing multilingual machine translation approaches mainly focus on English-centric directions, while the non-English directions still lag behind. In this work, we aim to build a many-to-many translation system with an emphasis on the quality of non-English language directions. Our intuition is based on the hypothesis that a universal cross-language representation leads to better multilingual translation performance. To this end, we propose mRASP2, a training method to obtain a single unified multilingual translation model. mRASP2 is empowered by two techniques: a) a contrastive learning scheme to close the gap among representations of different languages, and b) data augmentation on both multiple parallel and monolingual data to further align token representations. For English-centric directions, mRASP2 achieves competitive or even better performance than a strong pre-trained model mBART on tens of WMT benchmarks. For non-English directions, mRASP2 achieves an improvement of average 10+ BLEU compared with the multilingual baseline"
    }
  },
  {
    "id": "abstract-2020--acl-main--504",
    "result": [
      {
        "value": {
          "start": 447,
          "end": 466,
          "text": "Cascade Transformer",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--504:E0"
      },
      {
        "value": {
          "start": 776,
          "end": 784,
          "text": "speed-up",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--504:E1"
      },
      {
        "value": {
          "start": 122,
          "end": 133,
          "text": "computation",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--504:E2"
      },
      {
        "value": {
          "start": 906,
          "end": 914,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--504:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--504:E0",
        "to_id": "abstract-2020--acl-main--504:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--504:E0",
        "to_id": "abstract-2020--acl-main--504:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--504:E0",
        "to_id": "abstract-2020--acl-main--504:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Large transformer-based language models have been shown to be very effective in many classification tasks. However, their computational complexity prevents their use in applications requiring the classification of a large set of candidates. While previous works have investigated approaches to reduce model size, relatively little attention has been paid to techniques to improve batch throughput during inference. In this paper, we introduce the Cascade Transformer, a simple yet effective technique to adapt transformer-based models into a cascade of rankers. Each ranker is used to prune a subset of candidates in a batch, thus dramatically increasing throughput at inference time. Partial encodings from the transformer model are shared among rerankers, providing further speed-up. When compared to a state-of-the-art transformer model, our approach reduces computation by 37% with almost no impact on accuracy, as measured on two English Question Answering datasets."
    }
  },
  {
    "id": "abstract-2021--acl-long--137",
    "result": [
      {
        "value": {
          "start": 232,
          "end": 250,
          "text": "learning framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--137:E0"
      },
      {
        "value": {
          "start": 918,
          "end": 929,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--137:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--137:E0",
        "to_id": "abstract-2021--acl-long--137:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We study the learning of a matching model for dialogue response selection. Motivated by the recent finding that models trained with random negative samples are not ideal in real-world scenarios, we propose a hierarchical curriculum learning framework that trains the matching model in an “easy-to-difficult” scheme. Our learning framework consists of two complementary curricula: (1) corpus-level curriculum (CC); and (2) instance-level curriculum (IC). In CC, the model gradually increases its ability in finding the matching clues between the dialogue context and a response candidate. As for IC, it progressively strengthens the model’s ability in identifying the mismatching information between the dialogue context and a response candidate. Empirical studies on three benchmark datasets with three state-of-the-art matching models demonstrate that the proposed learning framework significantly improves the model performance across various evaluation metrics."
    }
  },
  {
    "id": "P11-1147",
    "result": [
      {
        "value": {
          "start": 900,
          "end": 931,
          "text": "classes and probabilistic model",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1147:E0"
      },
      {
        "value": {
          "start": 981,
          "end": 992,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1147:E1"
      },
      {
        "from_id": "P11-1147:E0",
        "to_id": "P11-1147:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Learning by Reading (LbR) aims at enabling machines to acquire knowledge from and reason about textual input. This requires knowledge about the domain structure (such as entities, classes, and actions) in order to do inference. We present a method to infer this implicit knowledge from unlabeled text. Unlike previous approaches, we use automatically extracted classes with a probability distribution over entities to allow for context-sensitive labeling. From a corpus of 1.4m sentences, we learn about 250k simple propositions about American football in the form of predicate-argument structures like \"quarterbacks throw passes to receivers\". Using several statistical measures, we show that our model is able to generalize and explain the data statistically significantly better than various baseline approaches. Human subjects judged up to 96.6% of the resulting propositions to be sensible. The classes and probabilistic model can be used in textual enrichment to improve the performance of LbR end-to-end systems."
    }
  },
  {
    "id": "P11-1006",
    "result": [
      {
        "value": {
          "start": 289,
          "end": 313,
          "text": "a probabilistic approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1006:E0"
      },
      {
        "value": {
          "start": 955,
          "end": 963,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1006:E1"
      },
      {
        "from_id": "P11-1006:E0",
        "to_id": "P11-1006:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This paper proposes a new method for approximate string search, specifically candidate generation in spelling error correction, which is a task as follows. Given a misspelled word, the system finds words in a dictionary, which are most \"similar\" to the misspelled word. The paper proposes a probabilistic approach to the task, which is both accurate and efficient. The approach includes the use of a log linear model, a method for training the model, and an algorithm for finding the top k candidates. The log linear model is defined as a conditional probability distribution of a corrected word and a rule set for the correction conditioned on the misspelled word. The learning method employs the criterion in candidate generation as loss function. The retrieval algorithm is efficient and is guaranteed to find the optimal k candidates. Experimental results on large scale data show that the proposed approach improves upon existing methods in terms of accuracy in different settings."
    }
  },
  {
    "id": "abstract-2020--acl-main--356",
    "result": [
      {
        "value": {
          "start": 68,
          "end": 72,
          "text": "BHWR",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--356:E0"
      },
      {
        "value": {
          "start": 325,
          "end": 356,
          "text": "quality of such representations",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--356:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--356:E0",
        "to_id": "abstract-2020--acl-main--356:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This paper presents the Bayesian Hierarchical Words Representation (BHWR) learning algorithm. BHWR facilitates Variational Bayes word representation learning combined with semantic taxonomy modeling via hierarchical priors. By propagating relevant information between related words, BHWR utilizes the taxonomy to improve the quality of such representations. Evaluation of several linguistic datasets demonstrates the advantages of BHWR over suitable alternatives that facilitate Bayesian modeling with or without semantic priors. Finally, we further show that BHWR produces better representations for rare words."
    }
  },
  {
    "id": "abstract-2020--acl-main--551",
    "result": [
      {
        "value": {
          "start": 473,
          "end": 505,
          "text": "reinforcement learning mechanism",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--551:E0"
      },
      {
        "value": {
          "start": 601,
          "end": 612,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--551:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--551:E0",
        "to_id": "abstract-2020--acl-main--551:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In this paper, we argue that elementary discourse unit (EDU) is a more appropriate textual unit of content selection than the sentence unit in abstractive summarization. To well handle the problem of composing EDUs into an informative and fluent summary, we propose a novel summarization method that first designs an EDU selection model to extract and group informative EDUs and then an EDU fusion model to fuse the EDUs in each group into one sentence. We also design the reinforcement learning mechanism to use EDU fusion results to reward the EDU selection action, boosting the final summarization performance. Experiments on CNN/Daily Mail have demonstrated the effectiveness of our model."
    }
  },
  {
    "id": "abstract-2020--acl-main--666",
    "result": [
      {
        "value": {
          "start": 13,
          "end": 42,
          "text": "sentence-level language model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--666:E0"
      },
      {
        "value": {
          "start": 694,
          "end": 702,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--666:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--666:E0",
        "to_id": "abstract-2020--acl-main--666:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We propose a sentence-level language model which selects the next sentence in a story from a finite set of fluent alternatives. Since it does not need to model fluency, the sentence-level language model can focus on longer range dependencies, which are crucial for multi-sentence coherence. Rather than dealing with individual words, our method treats the story so far as a list of pre-trained sentence embeddings and predicts an embedding for the next sentence, which is more efficient than predicting word embeddings. Notably this allows us to consider a large number of candidates for the next sentence during training. We demonstrate the effectiveness of our approach with state-of-the-art accuracy on the unsupervised Story Cloze task and with promising results on larger-scale next sentence prediction tasks."
    }
  },
  {
    "id": "abstract-2021--acl-long--306",
    "result": [
      {
        "value": {
          "start": 770,
          "end": 826,
          "text": "discourse structures among the context path of the claim",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--306:E0"
      },
      {
        "value": {
          "start": 866,
          "end": 877,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--306:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--306:E0",
        "to_id": "abstract-2021--acl-long--306:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Discourse relations among arguments reveal logical structures of a debate conversation. However, no prior work has explicitly studied how the sequence of discourse relations influence a claim’s impact. This paper empirically shows that the discourse relations between two arguments along the context path are essential factors for identifying the persuasive power of an argument. We further propose DisCOC to inject and fuse the sentence-level structural discourse information with contextualized features derived from large-scale language models. Experimental results and extensive analysis show that the attention and gate mechanisms that explicitly model contexts and texts can indeed help the argument impact classification task defined by Durmus et al. (2019), and discourse structures among the context path of the claim to be classified can further boost the performance."
    }
  },
  {
    "id": "P10-1042",
    "result": [
      {
        "value": {
          "start": 347,
          "end": 362,
          "text": "HL-SOT approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1042:E0"
      },
      {
        "value": {
          "start": 629,
          "end": 640,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1042:E1"
      },
      {
        "from_id": "P10-1042:E0",
        "to_id": "P10-1042:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Existing works on sentiment analysis on product reviews suffer from the following limitations: (1) The knowledge of hierarchical relationships of products attributes is not fully utilized. (2) Reviews or sentences mentioning several attributes associated with complicated sentiments are not dealt with very well. In this paper, we propose a novel HL-SOT approach to labeling a product's attributes and their associated sentiments in product reviews by a Hierarchical Learning (HL) process with a defined Sentiment Ontology Tree (SOT). The empirical analysis against a human-labeled data set demonstrates promising and reasonable performance of the proposed HL-SOT approach. While this paper is mainly on sentiment analysis on reviews of one product, our proposed HL-SOT approach is easily generalized to labeling a mix of reviews of more than one products."
    }
  },
  {
    "id": "abstract-2021--acl-long--235",
    "result": [
      {
        "value": {
          "start": 170,
          "end": 228,
          "text": "incorporate linguistic features into neural network models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--235:E0"
      },
      {
        "value": {
          "start": 661,
          "end": 673,
          "text": "performances",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--235:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--235:E0",
        "to_id": "abstract-2021--acl-long--235:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Deep learning models for automatic readability assessment generally discard linguistic features traditionally used in machine learning models for the task. We propose to incorporate linguistic features into neural network models by learning syntactic dense embeddings based on linguistic features. To cope with the relationships between the features, we form a correlation graph among features and use it to learn their embeddings so that similar features will be represented by similar embeddings. Experiments with six data sets of two proficiency levels demonstrate that our proposed methodology can complement BERT-only model to achieve significantly better performances for automatic readability assessment."
    }
  },
  {
    "id": "abstract-2020--acl-main--59",
    "result": [
      {
        "value": {
          "start": 753,
          "end": 784,
          "text": "uses the actor-critic framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--59:E0"
      },
      {
        "value": {
          "start": 823,
          "end": 834,
          "text": "scalability",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--59:E1"
      },
      {
        "value": {
          "start": 1106,
          "end": 1116,
          "text": "two agents",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--59:E2"
      },
      {
        "value": {
          "start": 1136,
          "end": 1153,
          "text": "task success rate",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--59:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--59:E0",
        "to_id": "abstract-2020--acl-main--59:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--59:E2",
        "to_id": "abstract-2020--acl-main--59:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Many studies have applied reinforcement learning to train a dialog policy and show great promise these years. One common approach is to employ a user simulator to obtain a large number of simulated user experiences for reinforcement learning algorithms. However, modeling a realistic user simulator is challenging. A rule-based simulator requires heavy domain expertise for complex tasks, and a data-driven simulator requires considerable data and it is even unclear how to evaluate a simulator. To avoid explicitly building a user simulator beforehand, we propose Multi-Agent Dialog Policy Learning, which regards both the system and the user as the dialog agents. Two agents interact with each other and are jointly learned simultaneously. The method uses the actor-critic framework to facilitate pretraining and improve scalability. We also propose Hybrid Value Network for the role-aware reward decomposition to integrate role-specific domain knowledge of each agent in the task-oriented dialog. Results show that our method can successfully build a system policy and a user policy simultaneously, and two agents can achieve a high task success rate through conversational interaction."
    }
  },
  {
    "id": "P10-1001",
    "result": [
      {
        "value": {
          "start": 333,
          "end": 344,
          "text": "our parsers",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1001:E0"
      },
      {
        "value": {
          "start": 408,
          "end": 435,
          "text": "unlabeled attachment scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1001:E1"
      },
      {
        "from_id": "P10-1001:E0",
        "to_id": "P10-1001:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We present algorithms for higher-order dependency parsing that are \"third-order\" in the sense that they can evaluate substructures containing three dependencies, and \"efficient\" in the sense that they require only O(n4) time. Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions. We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively."
    }
  },
  {
    "id": "abstract-2020--acl-main--170",
    "result": [
      {
        "value": {
          "start": 1076,
          "end": 1105,
          "text": "standard BPE during inference",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--170:E0"
      },
      {
        "value": {
          "start": 1145,
          "end": 1149,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--170:E1"
      },
      {
        "value": {
          "start": 1145,
          "end": 1149,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--170:E2"
      },
      {
        "value": {
          "start": 1040,
          "end": 1067,
          "text": "BPE-dropout during training",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--170:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--170:E0",
        "to_id": "abstract-2020--acl-main--170:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--170:E0",
        "to_id": "abstract-2020--acl-main--170:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--170:E3",
        "to_id": "abstract-2020--acl-main--170:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--170:E3",
        "to_id": "abstract-2020--acl-main--170:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Subword segmentation is widely used to address the open vocabulary problem in machine translation. The dominant approach to subword segmentation is Byte Pair Encoding (BPE), which keeps the most frequent words intact while splitting the rare ones into multiple tokens. While multiple segmentations are possible even with the same vocabulary, BPE splits words into unique sequences; this may prevent a model from better learning the compositionality of words and being robust to segmentation errors. So far, the only way to overcome this BPE imperfection, its deterministic nature, was to create another subword segmentation algorithm (Kudo, 2018). In contrast, we show that BPE itself incorporates the ability to produce multiple segmentations of the same word. We introduce BPE-dropout - simple and effective subword regularization method based on and compatible with conventional BPE. It stochastically corrupts the segmentation procedure of BPE, which leads to producing multiple segmentations within the same fixed BPE framework. Using BPE-dropout during training and the standard BPE during inference improves translation quality up to 2.3 BLEU compared to BPE and up to 0.9 BLEU compared to the previous subword regularization."
    }
  },
  {
    "id": "abstract-2020--acl-main--69",
    "result": [
      {
        "value": {
          "start": 752,
          "end": 768,
          "text": "back-translation",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--69:E0"
      },
      {
        "value": {
          "start": 1014,
          "end": 1028,
          "text": "grammaticality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--69:E1"
      },
      {
        "value": {
          "start": 1041,
          "end": 1045,
          "text": "cost",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--69:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--69:E0",
        "to_id": "abstract-2020--acl-main--69:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--69:E0",
        "to_id": "abstract-2020--acl-main--69:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Question Generation (QG) is fundamentally a simple syntactic transformation; however, many aspects of semantics influence what questions are good to form. We implement this observation by developing Syn-QG, a set of transparent syntactic rules leveraging universal dependencies, shallow semantic parsing, lexical resources, and custom rules which transform declarative sentences into question-answer pairs. We utilize PropBank argument descriptions and VerbNet state predicates to incorporate shallow semantic content, which helps generate questions of a descriptive nature and produce inferential and semantically richer questions than existing systems. In order to improve syntactic fluency and eliminate grammatically incorrect questions, we employ back-translation over the output of these syntactic rules. A set of crowd-sourced evaluations shows that our system can generate a larger number of highly grammatical and relevant questions than previous QG systems and that back-translation drastically improves grammaticality at a slight cost of generating irrelevant questions."
    }
  },
  {
    "id": "abstract-2021--acl-long--484",
    "result": [
      {
        "value": {
          "start": 908,
          "end": 938,
          "text": "performs a re-labeling process",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--484:E0"
      },
      {
        "value": {
          "start": 319,
          "end": 330,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--484:E1"
      },
      {
        "value": {
          "start": 844,
          "end": 866,
          "text": "filters the noisy data",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--484:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--484:E0",
        "to_id": "abstract-2021--acl-long--484:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--484:E2",
        "to_id": "abstract-2021--acl-long--484:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Distant supervision for relation extraction provides uniform bag labels for each sentence inside the bag, while accurate sentence labels are important for downstream applications that need the exact relation type. Directly using bag labels for sentence-level training will introduce much noise, thus severely degrading performance. In this work, we propose the use of negative training (NT), in which a model is trained using complementary labels regarding that “the instance does not belong to these complementary labels”. Since the probability of selecting a true label as a complementary label is low, NT provides less noisy information. Furthermore, the model trained with NT is able to separate the noisy data from the training data. Based on NT, we propose a sentence-level framework, SENT, for distant relation extraction. SENT not only filters the noisy data to construct a cleaner dataset, but also performs a re-labeling process to transform the noisy data into useful training data, thus further benefiting the model’s performance. Experimental results show the significant improvement of the proposed method over previous methods on sentence-level evaluation and de-noise effect."
    }
  },
  {
    "id": "abstract-2021--acl-long--56",
    "result": [
      {
        "value": {
          "start": 503,
          "end": 522,
          "text": "using ToM inference",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--56:E0"
      },
      {
        "value": {
          "start": 545,
          "end": 566,
          "text": "dialog agreement rate",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--56:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--56:E0",
        "to_id": "abstract-2021--acl-long--56:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In this paper, we explore the ability to model and infer personality types of opponents, predict their responses, and use this information to adapt a dialog agent’s high-level strategy in negotiation tasks. Inspired by the idea of incorporating a theory of mind (ToM) into machines, we introduce a probabilistic formulation to encapsulate the opponent’s personality type during both learning and inference. We test our approach on the CraigslistBargain dataset (He et al. 2018) and show that our method using ToM inference achieves a 20% higher dialog agreement rate compared to baselines on a mixed population of opponents. We also demonstrate that our model displays diverse negotiation behavior with different types of opponents."
    }
  },
  {
    "id": "abstract-2021--acl-long--284",
    "result": [
      {
        "value": {
          "start": 433,
          "end": 505,
          "text": "provides a compact encoding of dialogue histories and predicted programs",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--284:E0"
      },
      {
        "value": {
          "start": 517,
          "end": 531,
          "text": "generalization",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--284:E1"
      },
      {
        "value": {
          "start": 536,
          "end": 560,
          "text": "computational efficiency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--284:E2"
      },
      {
        "value": {
          "start": 914,
          "end": 931,
          "text": "absolute accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--284:E3"
      },
      {
        "from_id": "abstract-2021--acl-long--284:E0",
        "to_id": "abstract-2021--acl-long--284:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--284:E0",
        "to_id": "abstract-2021--acl-long--284:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--284:E0",
        "to_id": "abstract-2021--acl-long--284:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Conversational semantic parsers map user utterances to executable programs given dialogue histories composed of previous utterances, programs, and system responses. Existing parsers typically condition on rich representations of history that include the complete set of values and computations previously discussed. We propose a model that abstracts over values to focus prediction on type- and function-level context. This approach provides a compact encoding of dialogue histories and predicted programs, improving generalization and computational efficiency. Our model incorporates several other components, including an atomic span copy operation and structural enforcement of well-formedness constraints on predicted programs, that are particularly advantageous in the low-data regime. Trained on the SMCalFlow and TreeDST datasets, our model outperforms prior work by 7.3% and 10.6% respectively in terms of absolute accuracy. Trained on only a thousand examples from each dataset, it outperforms strong baselines by 12.4% and 6.4%. These results indicate that simple representations are key to effective generalization in conversational semantic parsing."
    }
  },
  {
    "id": "abstract-2020--acl-main--535",
    "result": [
      {
        "value": {
          "start": 271,
          "end": 293,
          "text": "retrieval-based method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--535:E0"
      },
      {
        "value": {
          "start": 958,
          "end": 967,
          "text": "relevance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--535:E1"
      },
      {
        "value": {
          "start": 969,
          "end": 983,
          "text": "grammaticality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--535:E2"
      },
      {
        "value": {
          "start": 214,
          "end": 223,
          "text": "diversity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--535:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--535:E0",
        "to_id": "abstract-2020--acl-main--535:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--535:E0",
        "to_id": "abstract-2020--acl-main--535:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--535:E0",
        "to_id": "abstract-2020--acl-main--535:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Neural sequence to sequence text generation has been proved to be a viable approach to paraphrase generation. Despite promising results, paraphrases generated by these models mostly suffer from lack of quality and diversity. To address these problems, we propose a novel retrieval-based method for paraphrase generation. Our model first retrieves a paraphrase pair similar to the input sentence from a pre-defined index. With its novel editor module, the model then paraphrases the input sequence by editing it using the extracted relations between the retrieved pair of sentences. In order to have fine-grained control over the editing process, our model uses the newly introduced concept of Micro Edit Vectors. It both extracts and exploits these vectors using the attention mechanism in the Transformer architecture. Experimental results show the superiority of our paraphrase generation method in terms of both automatic metrics, and human evaluation of relevance, grammaticality, and diversity of generated paraphrases."
    }
  },
  {
    "id": "P11-1111",
    "result": [
      {
        "value": {
          "start": 109,
          "end": 143,
          "text": "maximum entropy reranking approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1111:E0"
      },
      {
        "value": {
          "start": 292,
          "end": 303,
          "text": "error rates",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1111:E1"
      },
      {
        "value": {
          "start": 386,
          "end": 399,
          "text": "maximum error",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1111:E2"
      },
      {
        "value": {
          "start": 423,
          "end": 436,
          "text": "average error",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1111:E3"
      },
      {
        "from_id": "P11-1111:E0",
        "to_id": "P11-1111:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "P11-1111:E0",
        "to_id": "P11-1111:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "P11-1111:E0",
        "to_id": "P11-1111:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "In this work, we present a novel approach to the generation task of ordering prenominal modifiers. We take a maximum entropy reranking approach to the problem which admits arbitrary features on a permutation of modifiers, exploiting hundreds of thousands of features in total. We compare our error rates to the state-of-the-art and to a strong Google n-gram count baseline. We attain a maximum error reduction of 69.8% and average error reduction across all test sets of 59.1% compared to the state-of-the-art and a maximum error reduction of 68.4% and average error reduction across all test sets of 41.8% compared to our Google n-gram count baseline."
    }
  },
  {
    "id": "abstract-2021--acl-long--268",
    "result": [
      {
        "value": {
          "start": 858,
          "end": 898,
          "text": "Margin-based Token-level Objective (MTO)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--268:E0"
      },
      {
        "value": {
          "start": 1205,
          "end": 1209,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--268:E1"
      },
      {
        "value": {
          "start": 905,
          "end": 948,
          "text": "Margin-based Sentence-level Objective (MSO)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--268:E2"
      },
      {
        "value": {
          "start": 1342,
          "end": 1362,
          "text": "translation adequacy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--268:E3"
      },
      {
        "value": {
          "start": 1374,
          "end": 1381,
          "text": "fluency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--268:E4"
      },
      {
        "from_id": "abstract-2021--acl-long--268:E0",
        "to_id": "abstract-2021--acl-long--268:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--268:E2",
        "to_id": "abstract-2021--acl-long--268:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--268:E0",
        "to_id": "abstract-2021--acl-long--268:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--268:E0",
        "to_id": "abstract-2021--acl-long--268:E4",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--268:E2",
        "to_id": "abstract-2021--acl-long--268:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--268:E2",
        "to_id": "abstract-2021--acl-long--268:E4",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The Neural Machine Translation (NMT) model is essentially a joint language model conditioned on both the source sentence and partial translation. Therefore, the NMT model naturally involves the mechanism of the Language Model (LM) that predicts the next token only based on partial translation. Despite its success, NMT still suffers from the hallucination problem, generating fluent but inadequate translations. The main reason is that NMT pays excessive attention to the partial translation while neglecting the source sentence to some extent, namely overconfidence of the LM. Accordingly, we define the Margin between the NMT and the LM, calculated by subtracting the predicted probability of the LM from that of the NMT model for each token. The Margin is negatively correlated to the overconfidence degree of the LM. Based on the property, we propose a Margin-based Token-level Objective (MTO) and a Margin-based Sentence-level Objective (MSO) to maximize the Margin for preventing the LM from being overconfident. Experiments on WMT14 English-to-German, WMT19 Chinese-to-English, and WMT14 English-to-French translation tasks demonstrate the effectiveness of our approach, with 1.36, 1.50, and 0.63 BLEU improvements, respectively, compared to the Transformer baseline. The human evaluation further verifies that our approaches improve translation adequacy as well as fluency."
    }
  },
  {
    "id": "abstract-2020--acl-main--205",
    "result": [
      {
        "value": {
          "start": 853,
          "end": 932,
          "text": "combination of the auxiliary task and the additional input of class-definitions",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--205:E0"
      },
      {
        "value": {
          "start": 974,
          "end": 982,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--205:E1"
      },
      {
        "value": {
          "start": 1075,
          "end": 1095,
          "text": "number of parameters",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--205:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--205:E0",
        "to_id": "abstract-2020--acl-main--205:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--205:E0",
        "to_id": "abstract-2020--acl-main--205:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "In hierarchical text classification, we perform a sequence of inference steps to predict the category of a document from top to bottom of a given class taxonomy. Most of the studies have focused on developing novels neural network architectures to deal with the hierarchical structure, but we prefer to look for efficient ways to strengthen a baseline model. We first define the task as a sequence-to-sequence problem. Afterwards, we propose an auxiliary synthetic task of bottom-up-classification. Then, from external dictionaries, we retrieve textual definitions for the classes of all the hierarchy’s layers, and map them into the word vector space. We use the class-definition embeddings as an additional input to condition the prediction of the next layer and in an adapted beam search. Whereas the modified search did not provide large gains, the combination of the auxiliary task and the additional input of class-definitions significantly enhance the classification accuracy. With our efficient approaches, we outperform previous studies, using a drastically reduced number of parameters, in two well-known English datasets."
    }
  },
  {
    "id": "abstract-2021--acl-long--258",
    "result": [
      {
        "value": {
          "start": 1152,
          "end": 1183,
          "text": "similarity-driven meta-learning",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--258:E0"
      },
      {
        "value": {
          "start": 1196,
          "end": 1222,
          "text": "generalization performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--258:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--258:E0",
        "to_id": "abstract-2021--acl-long--258:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Natural language is compositional; the meaning of a sentence is a function of the meaning of its parts. This property allows humans to create and interpret novel sentences, generalizing robustly outside their prior experience. Neural networks have been shown to struggle with this kind of generalization, in particular performing poorly on tasks designed to assess compositional generalization (i.e. where training and testing distributions differ in ways that would be trivial for a compositional strategy to resolve). Their poor performance on these tasks may in part be due to the nature of supervised learning which assumes training and testing data to be drawn from the same distribution. We implement a meta-learning augmented version of supervised learning whose objective directly optimizes for out-of-distribution generalization. We construct pairs of tasks for meta-learning by sub-sampling existing training data. Each pair of tasks is constructed to contain relevant examples, as determined by a similarity metric, in an effort to inhibit models from memorizing their input. Experimental results on the COGS and SCAN datasets show that our similarity-driven meta-learning can improve generalization performance."
    }
  },
  {
    "id": "abstract-2021--acl-long--357",
    "result": [
      {
        "value": {
          "start": 989,
          "end": 1005,
          "text": "BERTbased models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--357:E0"
      },
      {
        "value": {
          "start": 1050,
          "end": 1058,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--357:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--357:E0",
        "to_id": "abstract-2021--acl-long--357:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Event forecasting is a challenging, yet important task, as humans seek to constantly plan for the future. Existing automated forecasting studies rely mostly on structured data, such as time-series or event-based knowledge graphs, to help predict future events. In this work, we aim to formulate a task, construct a dataset, and provide benchmarks for developing methods for event forecasting with large volumes of unstructured text data. To simulate the forecasting scenario on temporal news documents, we formulate the problem as a restricted-domain, multiple-choice, question-answering (QA) task. Unlike existing QA tasks, our task limits accessible information, and thus a model has to make a forecasting judgement. To showcase the usefulness of this task formulation, we introduce ForecastQA, a question-answering dataset consisting of 10,392 event forecasting questions, which have been collected and verified via crowdsourcing efforts. We present our experiments on ForecastQA using BERTbased models and find that our best model achieves 61.0% accuracy on the dataset, which still lags behind human performance by about 19%. We hope ForecastQA will support future research efforts in bridging this gap."
    }
  },
  {
    "id": "P10-1006",
    "result": [
      {
        "value": {
          "start": 579,
          "end": 601,
          "text": "knowledge-based method",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1006:E0"
      },
      {
        "value": {
          "start": 962,
          "end": 973,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1006:E1"
      },
      {
        "from_id": "P10-1006:E0",
        "to_id": "P10-1006:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Name ambiguity problem has raised urgent demands for efficient, high-quality named entity disambiguation methods. In recent years, the increasing availability of large-scale, rich semantic knowledge sources (such as Wikipedia and WordNet) creates new opportunities to enhance the named entity disambiguation by developing algorithms which can exploit these knowledge sources at best. The problem is that these knowledge sources are heterogeneous and most of the semantic knowledge within them is embedded in complex structures, such as graphs and networks. This paper proposes a knowledge-based method, called Structural Semantic Relatedness (SSR), which can enhance the named entity disambiguation by capturing and leveraging the structural semantic knowledge in multiple knowledge sources. Empirical results show that, in comparison with the classical BOW based methods and social network based methods, our method can significantly improve the disambiguation performance by respectively 8.7% and 14.7%."
    }
  },
  {
    "id": "abstract-2021--acl-long--44",
    "result": [
      {
        "value": {
          "start": 334,
          "end": 365,
          "text": "develop novel pretraining tasks",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--44:E0"
      },
      {
        "value": {
          "start": 387,
          "end": 398,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--44:E1"
      },
      {
        "value": {
          "start": 11,
          "end": 55,
          "text": "Knowledge Enhanced Multimodal BART (KM-BART)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--44:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--44:E0",
        "to_id": "abstract-2021--acl-long--44:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--44:E2",
        "to_id": "abstract-2021--acl-long--44:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We present Knowledge Enhanced Multimodal BART (KM-BART), which is a Transformer-based sequence-to-sequence model capable of reasoning about commonsense knowledge from multimodal inputs of images and texts. We adapt the generative BART architecture (Lewis et al., 2020) to a multimodal model with visual and textual inputs. We further develop novel pretraining tasks to improve the model performance on the Visual Commonsense Generation (VCG) task. In particular, our pretraining task of Knowledge-based Commonsense Generation (KCG) boosts model performance on the VCG task by leveraging commonsense knowledge from a large language model pretrained on external commonsense knowledge graphs. To the best of our knowledge, we are the first to propose a dedicated task for improving model performance on the VCG task. Experimental results show that our model reaches state-of-the-art performance on the VCG task (Park et al., 2020) by applying these novel pretraining tasks."
    }
  },
  {
    "id": "abstract-2020--acl-main--163",
    "result": [
      {
        "value": {
          "start": 899,
          "end": 924,
          "text": "utilising unlabelled data",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--163:E0"
      },
      {
        "value": {
          "start": 728,
          "end": 739,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--163:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--163:E0",
        "to_id": "abstract-2020--acl-main--163:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Natural language understanding (NLU) and natural language generation (NLG) are two fundamental and related tasks in building task-oriented dialogue systems with opposite objectives: NLU tackles the transformation from natural language to formal representations, whereas NLG does the reverse. A key to success in either task is parallel training data which is expensive to obtain at a large scale. In this work, we propose a generative model which couples NLU and NLG through a shared latent variable. This approach allows us to explore both spaces of natural language and formal representations, and facilitates information sharing through the latent space to eventually benefit NLU and NLG. Our model achieves state-of-the-art performance on two dialogue datasets with both flat and tree-structured formal representations. We also show that the model can be trained in a semi-supervised fashion by utilising unlabelled data to boost its performance."
    }
  },
  {
    "id": "abstract-2021--acl-long--129",
    "result": [
      {
        "value": {
          "start": 581,
          "end": 601,
          "text": "augmented BERT model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--129:E0"
      },
      {
        "value": {
          "start": 756,
          "end": 764,
          "text": "macro-F1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--129:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--129:E0",
        "to_id": "abstract-2021--acl-long--129:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We introduce the well-established social scientific concept of social solidarity and its contestation, anti-solidarity, as a new problem setting to supervised machine learning in NLP to assess how European solidarity discourses changed before and after the COVID-19 outbreak was declared a global pandemic. To this end, we annotate 2.3k English and German tweets for (anti-)solidarity expressions, utilizing multiple human annotators and two annotation approaches (experts vs. crowds). We use these annotations to train a BERT model with multiple data augmentation strategies. Our augmented BERT model that combines both expert and crowd annotations outperforms the baseline BERT classifier trained with expert annotations only by over 25 points, from 58% macro-F1 to almost 85%. We use this high-quality model to automatically label over 270k tweets between September 2019 and December 2020. We then assess the automatically labeled data for how statements related to European (anti-)solidarity discourses developed over time and in relation to one another, before and during the COVID-19 crisis. Our results show that solidarity became increasingly salient and contested during the crisis. While the number of solidarity tweets remained on a higher level and dominated the discourse in the scrutinized time frame, anti-solidarity tweets initially spiked, then decreased to (almost) pre-COVID-19 values before rising to a stable higher level until the end of 2020."
    }
  },
  {
    "id": "abstract-2021--acl-long--475",
    "result": [
      {
        "value": {
          "start": 521,
          "end": 526,
          "text": "MaRGE",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--475:E0"
      },
      {
        "value": {
          "start": 862,
          "end": 873,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--475:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--475:E0",
        "to_id": "abstract-2021--acl-long--475:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The availability of large-scale datasets has driven the development of neural models that create generic summaries from single or multiple documents. In this work we consider query focused summarization (QFS), a task for which training data in the form of queries, documents, and summaries is not readily available. We propose to decompose QFS into (1) query modeling (i.e., finding supportive evidence within a set of documents for a query) and (2) conditional language modeling (i.e., summary generation). We introduce MaRGE, a Masked ROUGE Regression framework for evidence estimation and ranking which relies on a unified representation for summaries and queries, so that summaries in generic data can be converted into proxy queries for learning a query model. Experiments across QFS benchmarks and query types show that our model achieves state-of-the-art performance despite learning from weak supervision."
    }
  },
  {
    "id": "abstract-2021--acl-long--87",
    "result": [
      {
        "value": {
          "start": 318,
          "end": 345,
          "text": "information gain filtration",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--87:E0"
      },
      {
        "value": {
          "start": 869,
          "end": 886,
          "text": "median perplexity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--87:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--87:E0",
        "to_id": "abstract-2021--acl-long--87:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Language model fine-tuning is essential for modern natural language processing, but is computationally expensive and time-consuming. Further, the effectiveness of fine-tuning is limited by the inclusion of training examples that negatively affect performance. Here we present a general fine-tuning method that we call information gain filtration for improving the overall training efficiency and final performance of language model fine-tuning. We define the information gain of an example as the improvement on a validation metric after training on that example. A secondary learner is then trained to approximate this quantity. During fine-tuning, this learner selects informative examples and skips uninformative ones. We show that our method has consistent improvement across datasets, fine-tuning tasks, and language model architectures. For example, we achieve a median perplexity of 54.0 on a books dataset compared to 57.3 for standard fine-tuning. We present statistical evidence that offers insight into the improvements of our method over standard fine-tuning. The generality of our method leads us to propose a new paradigm for language model fine-tuning — we encourage researchers to release pretrained secondary learners on common corpora to promote efficient and effective fine-tuning, thereby improving the performance and reducing the overall energy footprint of language model fine-tuning."
    }
  },
  {
    "id": "abstract-2021--acl-long--518",
    "result": [
      {
        "value": {
          "start": 820,
          "end": 832,
          "text": "DensePhrases",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--518:E0"
      },
      {
        "value": {
          "start": 900,
          "end": 908,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--518:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--518:E0",
        "to_id": "abstract-2021--acl-long--518:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (Seo et al., 2019). However, current phrase retrieval models heavily depend on sparse representations and still underperform retriever-reader approaches. In this work, we show for the first time that we can learn dense representations of phrases alone that achieve much stronger performance in open-domain QA. We present an effective method to learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods. We also propose a query-side fine-tuning strategy, which can support transfer learning and reduce the discrepancy between training and inference. On five popular open-domain QA datasets, our model DensePhrases improves over previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs. Finally, we directly use our pre-indexed dense phrase representations for two slot filling tasks, showing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks."
    }
  },
  {
    "id": "abstract-2021--acl-long--43",
    "result": [
      {
        "value": {
          "start": 1006,
          "end": 1022,
          "text": "IAIS regularizer",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--43:E0"
      },
      {
        "value": {
          "start": 523,
          "end": 534,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--43:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--43:E0",
        "to_id": "abstract-2021--acl-long--43:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Despite the achievements of large-scale multimodal pre-training approaches, cross-modal retrieval, e.g., image-text retrieval, remains a challenging task. To bridge the semantic gap between the two modalities, previous studies mainly focus on word-region alignment at the object level, lacking the matching between the linguistic relation among the words and the visual relation among the regions. The neglect of such relation consistency impairs the contextualized representation of image-text pairs and hinders the model performance and the interpretability. In this paper, we first propose a novel metric, Intra-modal Self-attention Distance (ISD), to quantify the relation consistency by measuring the semantic distance between linguistic and visual relations. In response, we present Inter-modal Alignment on Intra-modal Self-attentions (IAIS), a regularized training method to optimize the ISD and calibrate intra-modal self-attentions from the two modalities mutually via inter-modal alignment. The IAIS regularizer boosts the performance of prevailing models on Flickr30k and MS COCO datasets by a considerable margin, which demonstrates the superiority of our approach."
    }
  },
  {
    "id": "P11-1131",
    "result": [
      {
        "value": {
          "start": 38,
          "end": 70,
          "text": "phrase-to-phrase alignment model",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1131:E0"
      },
      {
        "value": {
          "start": 716,
          "end": 733,
          "text": "alignment quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1131:E1"
      },
      {
        "value": {
          "start": 738,
          "end": 757,
          "text": "translation quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1131:E2"
      },
      {
        "value": {
          "start": 840,
          "end": 847,
          "text": "runtime",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1131:E3"
      },
      {
        "from_id": "P11-1131:E0",
        "to_id": "P11-1131:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P11-1131:E0",
        "to_id": "P11-1131:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P11-1131:E0",
        "to_id": "P11-1131:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We propose a principled and efficient phrase-to-phrase alignment model, useful in machine translation as well as other related natural language processing problems. In a hidden semi-Markov model, word-to-phrase and phrase-to-word translations are modeled directly by the system. Agreement between two directional models encourages the selection of parsimonious phrasal alignments, avoiding the overfitting commonly encountered in unsupervised training with multi-word units. Expanding the state space to include \"gappy phrases\" (such as French ne * pas) makes the alignment space more symmetric; thus, it allows agreement between discontinuous alignments. The resulting system shows substantial improvements in both alignment quality and translation quality over word-based Hidden Markov Models, while maintaining asymptotically equivalent runtime."
    }
  },
  {
    "id": "P10-1030",
    "result": [
      {
        "value": {
          "start": 374,
          "end": 379,
          "text": "LUCHS",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1030:E0"
      },
      {
        "value": {
          "start": 547,
          "end": 555,
          "text": "F1 score",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1030:E1"
      },
      {
        "from_id": "P10-1030:E0",
        "to_id": "P10-1030:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Many researchers are trying to use information extraction (IE) to create large-scale knowledge bases from natural language text on the Web. However, the primary approach (supervised learning of relation-specific extractors) requires manually-labeled training data for each relation and doesn't scale to the thousands of relations encoded in Web text. \n \nThis paper presents LUCHS, a self-supervised, relation-specific IE system which learns 5025 relations --- more than an order of magnitude greater than any previous approach --- with an average F1 score of 61%. Crucial to LUCHS's performance is an automated system for dynamic lexicon learning, which allows it to learn accurately from heuristically-generated training data, which is often noisy and sparse."
    }
  },
  {
    "id": "abstract-2020--acl-main--394",
    "result": [
      {
        "value": {
          "start": 840,
          "end": 872,
          "text": "incorporating affective features",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--394:E0"
      },
      {
        "value": {
          "start": 926,
          "end": 937,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--394:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--394:E0",
        "to_id": "abstract-2020--acl-main--394:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The rise of online communication platforms has been accompanied by some undesirable effects, such as the proliferation of aggressive and abusive behaviour online. Aiming to tackle this problem, the natural language processing (NLP) community has experimented with a range of techniques for abuse detection. While achieving substantial success, these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users, disregarding the emotional state of the users and how this might affect their language. The latter is, however, inextricably linked to abusive behaviour. In this paper, we present the first joint model of emotion and abusive language detection, experimenting in a multi-task learning framework that allows one task to inform the other. Our results demonstrate that incorporating affective features leads to significant improvements in abuse detection performance across datasets."
    }
  },
  {
    "id": "abstract-2021--acl-long--317",
    "result": [
      {
        "value": {
          "start": 574,
          "end": 619,
          "text": "select-and-rerank (SAR) progressive framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--317:E0"
      },
      {
        "value": {
          "start": 1038,
          "end": 1046,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--317:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--317:E0",
        "to_id": "abstract-2021--acl-long--317:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "While sophisticated neural-based models have achieved remarkable success in Visual Question Answering (VQA), these models tend to answer questions only according to superficial correlations between question and answer. Several recent approaches have been developed to address this language priors problem. However, most of them predict the correct answer according to one best output without checking the authenticity of answers. Besides, they only explore the interaction between image and question, ignoring the semantics of candidate answers. In this paper, we propose a select-and-rerank (SAR) progressive framework based on Visual Entailment. Specifically, we first select the candidate answers relevant to the question or the image, then we rerank the candidate answers by a visual entailment task, which verifies whether the image semantically entails the synthetic statement of the question and each candidate answer. Experimental results show the effectiveness of our proposed framework, which establishes a new state-of-the-art accuracy on VQA-CP v2 with a 7.55% improvement."
    }
  },
  {
    "id": "abstract-2021--acl-long--204",
    "result": [
      {
        "value": {
          "start": 487,
          "end": 538,
          "text": "Stacked Acoustic-and-Textual Encoding (SATE) method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--204:E0"
      },
      {
        "value": {
          "start": 1163,
          "end": 1174,
          "text": "BLEU scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--204:E1"
      },
      {
        "value": {
          "start": 1242,
          "end": 1262,
          "text": "end-to-end ST system",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--204:E2"
      },
      {
        "value": {
          "start": 1163,
          "end": 1167,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--204:E3"
      },
      {
        "from_id": "abstract-2021--acl-long--204:E0",
        "to_id": "abstract-2021--acl-long--204:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--204:E2",
        "to_id": "abstract-2021--acl-long--204:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Encoder pre-training is promising in end-to-end Speech Translation (ST), given the fact that speech-to-translation data is scarce. But ST encoders are not simple instances of Automatic Speech Recognition (ASR) or Machine Translation (MT) encoders. For example, we find that ASR encoders lack the global context representation, which is necessary for translation, whereas MT encoders are not designed to deal with long but locally attentive acoustic sequences. In this work, we propose a Stacked Acoustic-and-Textual Encoding (SATE) method for speech translation. Our encoder begins with processing the acoustic sequence as usual, but later behaves more like an MT encoder for a global representation of the input sequence. In this way, it is straightforward to incorporate the pre-trained models into the system. Also, we develop an adaptor module to alleviate the representation inconsistency between the pre-trained ASR encoder and MT encoder, and develop a multi-teacher knowledge distillation method to preserve the pre-training knowledge. Experimental results on the LibriSpeech En-Fr and MuST-C En-De ST tasks show that our method achieves state-of-the-art BLEU scores of 18.3 and 25.2. To our knowledge, we are the first to develop an end-to-end ST system that achieves comparable or even better BLEU performance than the cascaded ST counterpart when large-scale ASR and MT data is available."
    }
  },
  {
    "id": "abstract-2020--acl-main--709",
    "result": [
      {
        "value": {
          "start": 426,
          "end": 445,
          "text": "CRF alignment model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--709:E0"
      },
      {
        "value": {
          "start": 756,
          "end": 758,
          "text": "F1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--709:E1"
      },
      {
        "value": {
          "start": 773,
          "end": 784,
          "text": "CRF aligner",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--709:E2"
      },
      {
        "value": {
          "start": 67,
          "end": 74,
          "text": "quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--709:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--709:E0",
        "to_id": "abstract-2020--acl-main--709:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--709:E2",
        "to_id": "abstract-2020--acl-main--709:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "The success of a text simplification system heavily depends on the quality and quantity of complex-simple sentence pairs in the training corpus, which are extracted by aligning sentences between parallel articles. To evaluate and improve sentence alignment quality, we create two manually annotated sentence-aligned datasets from two commonly used text simplification corpora, Newsela and Wikipedia. We propose a novel neural CRF alignment model which not only leverages the sequential nature of sentences in parallel documents but also utilizes a neural sentence pair model to capture semantic similarity. Experiments demonstrate that our proposed approach outperforms all the previous work on monolingual sentence alignment task by more than 5 points in F1. We apply our CRF aligner to construct two new text simplification datasets, Newsela-Auto and Wiki-Auto, which are much larger and of better quality compared to the existing datasets. A Transformer-based seq2seq model trained on our datasets establishes a new state-of-the-art for text simplification in both automatic and human evaluation."
    }
  },
  {
    "id": "abstract-2020--acl-main--721",
    "result": [
      {
        "value": {
          "start": 542,
          "end": 634,
          "text": "from websites with little overlap with existing sources of knowledge for distant supervision",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--721:E0"
      },
      {
        "value": {
          "start": 919,
          "end": 921,
          "text": "F1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--721:E1"
      },
      {
        "value": {
          "start": 639,
          "end": 681,
          "text": "websites in entirely new subject verticals",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--721:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--721:E0",
        "to_id": "abstract-2020--acl-main--721:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--721:E2",
        "to_id": "abstract-2020--acl-main--721:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "In many documents, such as semi-structured webpages, textual semantics are augmented with additional information conveyed using visual elements including layout, font size, and color. Prior work on information extraction from semi-structured websites has required learning an extraction model specific to a given template via either manually labeled or distantly supervised data from that template. In this work, we propose a solution for “zero-shot” open-domain relation extraction from webpages with a previously unseen template, including from websites with little overlap with existing sources of knowledge for distant supervision and websites in entirely new subject verticals. Our model uses a graph neural network-based approach to build a rich representation of text fields on a webpage and the relationships between them, enabling generalization to new templates. Experiments show this approach provides a 31% F1 gain over a baseline for zero-shot extraction in a new subject vertical."
    }
  },
  {
    "id": "P11-1022",
    "result": [
      {
        "value": {
          "start": 815,
          "end": 840,
          "text": "a visualization prototype",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1022:E0"
      },
      {
        "value": {
          "start": 928,
          "end": 940,
          "text": "productivity",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1022:E1"
      },
      {
        "value": {
          "start": 324,
          "end": 341,
          "text": "a novel framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1022:E2"
      },
      {
        "value": {
          "start": 487,
          "end": 495,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1022:E3"
      },
      {
        "value": {
          "start": 530,
          "end": 537,
          "text": "F-score",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1022:E4"
      },
      {
        "from_id": "P11-1022:E0",
        "to_id": "P11-1022:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P11-1022:E2",
        "to_id": "P11-1022:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P11-1022:E2",
        "to_id": "P11-1022:E4",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "State-of-the-art statistical machine translation (MT) systems have made significant progress towards producing user-acceptable translation output. However, there is still no efficient way for MT systems to inform users which words are likely translated correctly and how confident it is about the whole sentence. We propose a novel framework to predict word-level and sentence-level MT errors with a large number of novel features. Experimental results show that the MT error prediction accuracy is increased from 69.1 to 72.2 in F-score. The Pearson correlation between the proposed confidence measure and the human-targeted translation edit rate (HTER) is 0.6. Improvements between 0.4 and 0.9 TER reduction are obtained with the n-best list reranking task using the proposed confidence measure. Also, we present a visualization prototype of MT errors at the word and sentence levels with the objective to improve post-editor productivity."
    }
  },
  {
    "id": "abstract-2021--acl-long--519",
    "result": [
      {
        "value": {
          "start": 358,
          "end": 436,
          "text": "unsupervised pre-training with the Inverse Cloze Task and masked salient spans",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--519:E0"
      },
      {
        "value": {
          "start": 590,
          "end": 615,
          "text": "top-20 retrieval accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--519:E1"
      },
      {
        "value": {
          "start": 450,
          "end": 500,
          "text": "supervised finetuning using question-context pairs",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--519:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--519:E0",
        "to_id": "abstract-2021--acl-long--519:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--519:E2",
        "to_id": "abstract-2021--acl-long--519:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recent work on training neural retrievers for open-domain question answering (OpenQA) has employed both supervised and unsupervised approaches. However, it remains unclear how unsupervised and supervised methods can be used most effectively for neural retrievers. In this work, we systematically study retriever pre-training. We first propose an approach of unsupervised pre-training with the Inverse Cloze Task and masked salient spans, followed by supervised finetuning using question-context pairs. This approach leads to absolute gains of 2+ points over the previous best result in the top-20 retrieval accuracy on Natural Questions and TriviaQA datasets. We next explore two approaches for end-to-end training of the reader and retriever components in OpenQA models, which differ in the manner the reader ingests the retrieved documents. Our experiments demonstrate the effectiveness of these approaches as we obtain state-of-the-art results. On the Natural Questions dataset, we obtain a top-20 retrieval accuracy of 84%, an improvement of 5 points over the recent DPR model. We also achieve good results on answer extraction, outperforming recent models like REALM and RAG by 3+ points."
    }
  },
  {
    "id": "abstract-2021--acl-long--520",
    "result": [
      {
        "value": {
          "start": 775,
          "end": 783,
          "text": "CRONKGQA",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--520:E0"
      },
      {
        "value": {
          "start": 713,
          "end": 724,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--520:E1"
      },
      {
        "value": {
          "start": 819,
          "end": 869,
          "text": "exploits recent advances in Temporal KG embeddings",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--520:E2"
      },
      {
        "value": {
          "start": 951,
          "end": 959,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--520:E3"
      },
      {
        "from_id": "abstract-2021--acl-long--520:E0",
        "to_id": "abstract-2021--acl-long--520:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--520:E2",
        "to_id": "abstract-2021--acl-long--520:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--520:E0",
        "to_id": "abstract-2021--acl-long--520:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--520:E2",
        "to_id": "abstract-2021--acl-long--520:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Temporal Knowledge Graphs (Temporal KGs) extend regular Knowledge Graphs by providing temporal scopes (start and end times) on each edge in the KG. While Question Answering over KG (KGQA) has received some attention from the research community, QA over Temporal KGs (Temporal KGQA) is a relatively unexplored area. Lack of broad coverage datasets has been another factor limiting progress in this area. We address this challenge by presenting CRONQUESTIONS, the largest known Temporal KGQA dataset, clearly stratified into buckets of structural complexity. CRONQUESTIONS expands the only known previous dataset by a factor of 340x. We find that various state-of-the-art KGQA methods fall far short of the desired performance on this new dataset. In response, we also propose CRONKGQA, a transformer-based solution that exploits recent advances in Temporal KG embeddings, and achieves performance superior to all baselines, with an increase of 120% in accuracy over the next best performing method. Through extensive experiments, we give detailed insights into the workings of CRONKGQA, as well as situations where significant further improvements appear possible. In addition to the dataset, we have released our code as well."
    }
  },
  {
    "id": "abstract-2020--acl-main--15",
    "result": [
      {
        "value": {
          "start": 1424,
          "end": 1446,
          "text": "Knowledge distillation",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--15:E0"
      },
      {
        "value": {
          "start": 196,
          "end": 204,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--15:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--15:E0",
        "to_id": "abstract-2020--acl-main--15:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Non-autoregressive (NAR) models generate all the tokens of a sequence in parallel, resulting in faster generation speed compared to their autoregressive (AR) counterparts but at the cost of lower accuracy. Different techniques including knowledge distillation and source-target alignment have been proposed to bridge the gap between AR and NAR models in various tasks such as neural machine translation (NMT), automatic speech recognition (ASR), and text to speech (TTS). With the help of those techniques, NAR models can catch up with the accuracy of AR models in some tasks but not in some others. In this work, we conduct a study to understand the difficulty of NAR sequence generation and try to answer: (1) Why NAR models can catch up with AR models in some tasks but not all? (2) Why techniques like knowledge distillation and source-target alignment can help NAR models. Since the main difference between AR and NAR models is that NAR models do not use dependency among target tokens while AR models do, intuitively the difficulty of NAR sequence generation heavily depends on the strongness of dependency among target tokens. To quantify such dependency, we propose an analysis model called CoMMA to characterize the difficulty of different NAR sequence generation tasks. We have several interesting findings: 1) Among the NMT, ASR and TTS tasks, ASR has the most target-token dependency while TTS has the least. 2) Knowledge distillation reduces the target-token dependency in target sequence and thus improves the accuracy of NAR models. 3) Source-target alignment constraint encourages dependency of a target token on source tokens and thus eases the training of NAR models."
    }
  },
  {
    "id": "P11-1124",
    "result": [
      {
        "value": {
          "start": 13,
          "end": 43,
          "text": "discriminative learning method",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1124:E0"
      },
      {
        "value": {
          "start": 764,
          "end": 774,
          "text": "BLEU score",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1124:E1"
      },
      {
        "from_id": "P11-1124:E0",
        "to_id": "P11-1124:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We present a discriminative learning method to improve the consistency of translations in phrase-based Statistical Machine Translation (SMT) systems. Our method is inspired by Translation Memory (TM) systems which are widely used by human translators in industrial settings. We constrain the translation of an input sentence using the most similar 'translation example' retrieved from the TM. Differently from previous research which used simple fuzzy match thresholds, these constraints are imposed using discriminative learning to optimise the translation performance. We observe that using this method can benefit the SMT system by not only producing consistent translations, but also improved translation outputs. We report a 0.9 point improvement in terms of BLEU score on English--Chinese technical documents."
    }
  },
  {
    "id": "P10-1130",
    "result": [
      {
        "value": {
          "start": 448,
          "end": 503,
          "text": "Klein and Manning's Dependency Model with Valence (DMV)",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1130:E0"
      },
      {
        "value": {
          "start": 530,
          "end": 538,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1130:E1"
      },
      {
        "from_id": "P10-1130:E0",
        "to_id": "P10-1130:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We show how web mark-up can be used to improve unsupervised dependency parsing. Starting from raw bracketings of four common HTML tags (anchors, bold, italics and underlines), we refine approximate partial phrase boundaries to yield accurate parsing constraints. Conversion procedures fall out of our linguistic analysis of a newly available million-word hyper-text corpus. We demonstrate that derived constraints aid grammar induction by training Klein and Manning's Dependency Model with Valence (DMV) on this data set: parsing accuracy on Section 23 (all sentences) of the Wall Street Journal corpus jumps to 50.4%, beating previous state-of-the-art by more than 5%. Web-scale experiments show that the DMV, perhaps because it is unlexicalized, does not benefit from orders of magnitude more annotated but noisier data. Our model, trained on a single blog, generalizes to 53.3% accuracy out-of-domain, against the Brown corpus --- nearly 10% higher than the previous published best. The fact that web mark-up strongly correlates with syntactic structure may have broad applicability in NLP."
    }
  },
  {
    "id": "P11-1080",
    "result": [
      {
        "value": {
          "start": 788,
          "end": 852,
          "text": "combination of the hierarchical model with distributed inference",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1080:E0"
      },
      {
        "value": {
          "start": 874,
          "end": 882,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1080:E1"
      },
      {
        "value": {
          "start": 889,
          "end": 894,
          "text": "error",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1080:E2"
      },
      {
        "from_id": "P11-1080:E0",
        "to_id": "P11-1080:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P11-1080:E0",
        "to_id": "P11-1080:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Cross-document coreference, the task of grouping all the mentions of each entity in a document collection, arises in information extraction and automated knowledge base construction. For large collections, it is clearly impractical to consider all possible groupings of mentions into distinct entities. To solve the problem we propose two ideas: (a) a distributed inference technique that uses parallelism to enable large scale processing, and (b) a hierarchical model of coreference that represents uncertainty over multiple granularities of entities to facilitate more effective approximate inference. To evaluate these ideas, we constructed a labeled corpus of 1.5 million disambiguated mentions in Web pages by selecting link anchors referring to Wikipedia entities. We show that the combination of the hierarchical model with distributed inference quickly obtains high accuracy (with error reduction of 38%) on this large dataset, demonstrating the scalability of our approach."
    }
  },
  {
    "id": "abstract-2021--acl-long--243",
    "result": [
      {
        "value": {
          "start": 1128,
          "end": 1216,
          "text": "replacing the original multilingual tokenizer with the specialized monolingual tokenizer",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--243:E0"
      },
      {
        "value": {
          "start": 902,
          "end": 924,
          "text": "downstream performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--243:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--243:E0",
        "to_id": "abstract-2021--acl-long--243:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In this work, we provide a systematic and comprehensive empirical comparison of pretrained multilingual language models versus their monolingual counterparts with regard to their monolingual task performance. We study a set of nine typologically diverse languages with readily available pretrained monolingual models on a set of five diverse monolingual downstream tasks. We first aim to establish, via fair and controlled comparisons, if a gap between the multilingual and the corresponding monolingual representation of that language exists, and subsequently investigate the reason for any performance difference. To disentangle conflating factors, we train new monolingual models on the same data, with monolingually and multilingually trained tokenizers. We find that while the pretraining data size is an important factor, a designated monolingual tokenizer plays an equally important role in the downstream performance. Our results show that languages that are adequately represented in the multilingual model’s vocabulary exhibit negligible performance decreases over their monolingual counterparts. We further find that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language."
    }
  },
  {
    "id": "abstract-2021--acl-long--274",
    "result": [
      {
        "value": {
          "start": 389,
          "end": 392,
          "text": "GIT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--274:E0"
      },
      {
        "value": {
          "start": 838,
          "end": 840,
          "text": "F1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--274:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--274:E0",
        "to_id": "abstract-2021--acl-long--274:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Document-level event extraction aims to recognize event information from a whole piece of article. Existing methods are not effective due to two challenges of this task: a) the target event arguments are scattered across sentences; b) the correlation among events in a document is non-trivial to model. In this paper, we propose Heterogeneous Graph-based Interaction Model with a Tracker (GIT) to solve the aforementioned two challenges. For the first challenge, GIT constructs a heterogeneous graph interaction network to capture global interactions among different sentences and entity mentions. For the second, GIT introduces a Tracker module to track the extracted events and hence capture the interdependency among the events. Experiments on a large-scale dataset (Zheng et al, 2019) show GIT outperforms the previous methods by 2.8 F1. Further analysis reveals is effective in extracting multiple correlated events and event arguments that scatter across the document."
    }
  },
  {
    "id": "abstract-2020--acl-main--128",
    "result": [
      {
        "value": {
          "start": 403,
          "end": 494,
          "text": "introduce a collapsed dependency transfer mechanism into the conditional random field (CRF)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--128:E0"
      },
      {
        "value": {
          "start": 1041,
          "end": 1050,
          "text": "F1 scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--128:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--128:E0",
        "to_id": "abstract-2020--acl-main--128:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In this paper, we explore the slot tagging with only a few labeled support sentences (a.k.a. few-shot). Few-shot slot tagging faces a unique challenge compared to the other fewshot classification problems as it calls for modeling the dependencies between labels. But it is hard to apply previously learned label dependencies to an unseen domain, due to the discrepancy of label sets. To tackle this, we introduce a collapsed dependency transfer mechanism into the conditional random field (CRF) to transfer abstract label dependency patterns as transition scores. In the few-shot setting, the emission score of CRF can be calculated as a word’s similarity to the representation of each label. To calculate such similarity, we propose a Label-enhanced Task-Adaptive Projection Network (L-TapNet) based on the state-of-the-art few-shot classification model – TapNet, by leveraging label name semantics in representing labels. Experimental results show that our model significantly outperforms the strongest few-shot learning baseline by 14.64 F1 scores in the one-shot setting."
    }
  },
  {
    "id": "abstract-2021--acl-long--412",
    "result": [
      {
        "value": {
          "start": 568,
          "end": 597,
          "text": "cyclic consistency constraint",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--412:E0"
      },
      {
        "value": {
          "start": 626,
          "end": 649,
          "text": "translation performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--412:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--412:E0",
        "to_id": "abstract-2021--acl-long--412:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Multimodal sentiment analysis is the challenging research area that attends to the fusion of multiple heterogeneous modalities. The main challenge is the occurrence of some missing modalities during the multimodal fusion procedure. However, the existing techniques require all modalities as input, thus are sensitive to missing modalities at predicting time. In this work, the coupled-translation fusion network (CTFN) is firstly proposed to model bi-direction interplay via couple learning, ensuring the robustness in respect to missing modalities. Specifically, the cyclic consistency constraint is presented to improve the translation performance, allowing us directly to discard decoder and only embraces encoder of Transformer. This could contribute to a much lighter model. Due to the couple learning, CTFN is able to conduct bi-direction cross-modality intercorrelation parallelly. Based on CTFN, a hierarchical architecture is further established to exploit multiple bi-direction translations, leading to double multimodal fusing embeddings compared with traditional translation methods. Moreover, the convolution block is utilized to further highlight explicit interactions among those translations. For evaluation, CTFN was verified on two multimodal benchmarks with extensive ablation studies. The experiments demonstrate that the proposed framework achieves state-of-the-art or often competitive performance. Additionally, CTFN still maintains robustness when considering missing modality."
    }
  },
  {
    "id": "abstract-2021--acl-long--474",
    "result": [
      {
        "value": {
          "start": 696,
          "end": 733,
          "text": "models augmented with Focus Attention",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--474:E0"
      },
      {
        "value": {
          "start": 871,
          "end": 876,
          "text": "ROUGE",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--474:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--474:E0",
        "to_id": "abstract-2021--acl-long--474:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Professional summaries are written with document-level information, such as the theme of the document, in mind. This is in contrast with most seq2seq decoders which simultaneously learn to focus on salient content, while deciding what to generate, at each decoding step. With the motivation to narrow this gap, we introduce Focus Attention Mechanism, a simple yet effective method to encourage decoders to proactively generate tokens that are similar or topical to the input document. Further, we propose a Focus Sampling method to enable generation of diverse summaries, an area currently understudied in summarization. When evaluated on the BBC extreme summarization task, two state-of-the-art models augmented with Focus Attention generate summaries that are closer to the target and more faithful to their input documents, outperforming their vanilla counterparts on ROUGE and multiple faithfulness measures. We also empirically demonstrate that Focus Sampling is more effective in generating diverse and faithful summaries than top-k or nucleus sampling-based decoding methods."
    }
  },
  {
    "id": "abstract-2020--acl-main--749",
    "result": [
      {
        "value": {
          "start": 70,
          "end": 139,
          "text": "embraces ontological structure at both training and during prediction",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--749:E0"
      },
      {
        "value": {
          "start": 488,
          "end": 503,
          "text": "strict accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--749:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--749:E0",
        "to_id": "abstract-2020--acl-main--749:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We propose a novel method for hierarchical entity classification that embraces ontological structure at both training and during prediction. At training, our novel multi-level learning-to-rank loss compares positive types against negative siblings according to the type tree. During prediction, we define a coarse-to-fine decoder that restricts viable candidates at each level of the ontology based on already predicted parent type(s). Our approach significantly outperform prior work on strict accuracy, demonstrating the effectiveness of our method."
    }
  },
  {
    "id": "abstract-2021--acl-long--108",
    "result": [
      {
        "value": {
          "start": 641,
          "end": 651,
          "text": "our method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--108:E0"
      },
      {
        "value": {
          "start": 692,
          "end": 714,
          "text": "mean average precision",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--108:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--108:E0",
        "to_id": "abstract-2021--acl-long--108:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Research on the application of NLP in symbol-based Augmentative and Alternative Communication (AAC) tools for improving social interaction support is scarce. We contribute a novel method for generating context-related vocabulary from photographs of personally relevant events aimed at supporting people with language impairments in retelling their past experiences. Performance was calculated with information retrieval concepts on the relevance of vocabulary generated for communicating a corpus of 9730 narrative phrases about events depicted in 1946 photographs. In comparison to a baseline generation composed of frequent English words, our method generated vocabulary with a 4.6 gain in mean average precision, regardless of the level of contextual information in the input photographs, and 6.9 for photographs in which contextual information was extracted correctly. We conclude by discussing how our findings provide insights for system optimization and usage."
    }
  },
  {
    "id": "abstract-2020--acl-main--438",
    "result": [
      {
        "value": {
          "start": 701,
          "end": 720,
          "text": "learned constraints",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--438:E0"
      },
      {
        "value": {
          "start": 263,
          "end": 271,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--438:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--438:E0",
        "to_id": "abstract-2020--acl-main--438:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Various natural language processing tasks are structured prediction problems where outputs are constructed with multiple interdependent decisions. Past work has shown that domain knowledge, framed as constraints over the output space, can help improve predictive accuracy. However, designing good constraints often relies on domain expertise. In this paper, we study the problem of learning such constraints. We frame the problem as that of training a two-layer rectifier network to identify valid structures or substructures, and show a construction for converting a trained network into a system of linear constraints over the inference variables. Our experiments on several NLP tasks show that the learned constraints can improve the prediction accuracy, especially when the number of training examples is small."
    }
  },
  {
    "id": "abstract-2021--acl-long--348",
    "result": [
      {
        "value": {
          "start": 570,
          "end": 577,
          "text": "SemFace",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--348:E0"
      },
      {
        "value": {
          "start": 1194,
          "end": 1198,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--348:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--348:E0",
        "to_id": "abstract-2021--acl-long--348:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "While pre-training techniques are working very well in natural language processing, how to pre-train a decoder and effectively use it for neural machine translation (NMT) still remains a tricky issue. The main reason is that the cross-attention module between the encoder and decoder cannot be pre-trained, and the combined encoder-decoder model cannot work well in the fine-tuning stage because the inputs of the decoder cross-attention come from unknown encoder outputs. In this paper, we propose a better pre-training method for NMT by defining a semantic interface (SemFace) between the pre-trained encoder and the pre-trained decoder. Specifically, we propose two types of semantic interfaces, including CL-SemFace which regards cross-lingual embeddings as an interface, and VQ-SemFace which employs vector quantized embeddings to constrain the encoder outputs and decoder inputs in the same language-independent space. We conduct massive experiments on six supervised translation pairs and three unsupervised pairs. Experimental results demonstrate that our proposed SemFace can effectively connect the pre-trained encoder and decoder, and achieves significant improvement by 3.7 and 1.5 BLEU points on the two tasks respectively compared with previous pre-training-based NMT models."
    }
  },
  {
    "id": "abstract-2020--acl-main--372",
    "result": [
      {
        "value": {
          "start": 696,
          "end": 712,
          "text": "BERT-based model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--372:E0"
      },
      {
        "value": {
          "start": 733,
          "end": 741,
          "text": "F1-score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--372:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--372:E0",
        "to_id": "abstract-2020--acl-main--372:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Understanding emotion expressed in language has a wide range of applications, from building empathetic chatbots to detecting harmful online behavior. Advancement in this area can be improved using large-scale datasets with a fine-grained typology, adaptable to multiple downstream tasks. We introduce GoEmotions, the largest manually annotated dataset of 58k English Reddit comments, labeled for 27 emotion categories or Neutral. We demonstrate the high quality of the annotations via Principal Preserved Component Analysis. We conduct transfer learning experiments with existing emotion benchmarks to show that our dataset generalizes well to other domains and different emotion taxonomies. Our BERT-based model achieves an average F1-score of .46 across our proposed taxonomy, leaving much room for improvement."
    }
  },
  {
    "id": "abstract-2021--acl-long--79",
    "result": [
      {
        "value": {
          "start": 465,
          "end": 481,
          "text": "turn-based model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--79:E0"
      },
      {
        "value": {
          "start": 911,
          "end": 919,
          "text": "F1 score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--79:E1"
      },
      {
        "value": {
          "start": 641,
          "end": 661,
          "text": "the turn-based model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--79:E2"
      },
      {
        "value": {
          "start": 707,
          "end": 724,
          "text": "parse performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--79:E3"
      },
      {
        "from_id": "abstract-2021--acl-long--79:E0",
        "to_id": "abstract-2021--acl-long--79:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--79:E2",
        "to_id": "abstract-2021--acl-long--79:E3",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Parsing spoken dialogue poses unique difficulties, including disfluencies and unmarked boundaries between sentence-like units. Previous work has shown that prosody can help with parsing disfluent speech (Tran et al. 2018), but has assumed that the input to the parser is already segmented into sentence-like units (SUs), which isn’t true in existing speech applications. We investigate how prosody affects a parser that receives an entire dialogue turn as input (a turn-based model), instead of gold standard pre-segmented SUs (an SU-based model). In experiments on the English Switchboard corpus, we find that when using transcripts alone, the turn-based model has trouble segmenting SUs, leading to worse parse performance than the SU-based model. However, prosody can effectively replace gold standard SU boundaries: with prosody, the turn-based model performs as well as the SU-based model (91.38 vs. 91.06 F1 score, respectively), despite performing two tasks (SU segmentation and parsing) rather than one (parsing alone). Analysis shows that pitch and intensity features are the most important for this corpus, since they allow the model to correctly distinguish an SU boundary from a speech disfluency – a distinction that the model otherwise struggles to make."
    }
  },
  {
    "id": "abstract-2020--acl-main--449",
    "result": [
      {
        "value": {
          "start": 752,
          "end": 769,
          "text": "relative encoding",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--449:E0"
      },
      {
        "value": {
          "start": 797,
          "end": 822,
          "text": "summarization performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--449:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--449:E0",
        "to_id": "abstract-2020--acl-main--449:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Generating a readable summary that describes the functionality of a program is known as source code summarization. In this task, learning code representation by modeling the pairwise relationship between code tokens to capture their long-range dependencies is crucial. To learn code representation for summarization, we explore the Transformer model that uses a self-attention mechanism and has shown to be effective in capturing long-range dependencies. In this work, we show that despite the approach is simple, it outperforms the state-of-the-art techniques by a significant margin. We perform extensive analysis and ablation studies that reveal several important findings, e.g., the absolute encoding of source code tokens’ position hinders, while relative encoding significantly improves the summarization performance. We have made our code publicly available to facilitate future research."
    }
  },
  {
    "id": "abstract-2020--acl-main--135",
    "result": [
      {
        "value": {
          "start": 646,
          "end": 655,
          "text": "our model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--135:E0"
      },
      {
        "value": {
          "start": 673,
          "end": 684,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--135:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--135:E0",
        "to_id": "abstract-2020--acl-main--135:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This paper proposes the problem of Deep Question Generation (DQG), which aims to generate complex questions that require reasoning over multiple pieces of information about the input passage. In order to capture the global structure of the document and facilitate reasoning, we propose a novel framework that first constructs a semantic-level graph for the input document and then encodes the semantic graph by introducing an attention-based GGNN (Att-GGNN). Afterward, we fuse the document-level and graph-level representations to perform joint training of content selection and question decoding. On the HotpotQA deep-question centric dataset, our model greatly improves performance over questions requiring reasoning over multiple facts, leading to state-of-the-art performance. The code is publicly available at https://github.com/WING-NUS/SG-Deep-Question-Generation."
    }
  },
  {
    "id": "abstract-2021--acl-long--181",
    "result": [
      {
        "value": {
          "start": 781,
          "end": 841,
          "text": "Contrastive Learning with semantIc Negative Examples (CLINE)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--181:E0"
      },
      {
        "value": {
          "start": 200,
          "end": 210,
          "text": "robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--181:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--181:E0",
        "to_id": "abstract-2021--acl-long--181:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Despite pre-trained language models have proven useful for learning high-quality semantic representations, these models are still vulnerable to simple perturbations. Recent works aimed to improve the robustness of pre-trained models mainly focus on adversarial training from perturbed examples with similar semantics, neglecting the utilization of different or even opposite semantics. Different from the image processing field, the text is discrete and few word substitutions can cause significant semantic changes. To study the impact of semantics caused by small perturbations, we conduct a series of pilot experiments and surprisingly find that adversarial training is useless or even harmful for the model to detect these semantic changes. To address this problem, we propose Contrastive Learning with semantIc Negative Examples (CLINE), which constructs semantic negative examples unsupervised to improve the robustness under semantically adversarial attacking. By comparing with similar and opposite semantic examples, the model can effectively perceive the semantic changes caused by small perturbations. Empirical results show that our approach yields substantial improvements on a range of sentiment analysis, reasoning, and reading comprehension tasks. And CLINE also ensures the compactness within the same semantics and separability across different semantics in sentence-level."
    }
  },
  {
    "id": "abstract-2020--acl-main--740",
    "result": [
      {
        "value": {
          "start": 932,
          "end": 964,
          "text": "multi-phase adaptive pretraining",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--740:E0"
      },
      {
        "value": {
          "start": 491,
          "end": 502,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--740:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--740:E0",
        "to_id": "abstract-2020--acl-main--740:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Language models pretrained on text from a wide variety of sources form the foundation of today’s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task’s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance."
    }
  },
  {
    "id": "abstract-2021--acl-long--257",
    "result": [
      {
        "value": {
          "start": 128,
          "end": 189,
          "text": "leverage unsupervised learning in combination with heuristics",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--257:E0"
      },
      {
        "value": {
          "start": 474,
          "end": 501,
          "text": "coverage of nodes and edges",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--257:E1"
      },
      {
        "value": {
          "start": 527,
          "end": 535,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--257:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--257:E0",
        "to_id": "abstract-2021--acl-long--257:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--257:E0",
        "to_id": "abstract-2021--acl-long--257:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We present algorithms for aligning components of Abstract Meaning Representation (AMR) graphs to spans in English sentences. We leverage unsupervised learning in combination with heuristics, taking the best of both worlds from previous AMR aligners. Our unsupervised models, however, are more sensitive to graph substructures, without requiring a separate syntactic parse. Our approach covers a wider variety of AMR substructures than previously considered, achieves higher coverage of nodes and edges, and does so with higher accuracy. We will release our LEAMR datasets and aligner for use in research on AMR parsing, generation, and evaluation."
    }
  },
  {
    "id": "abstract-2020--acl-main--278",
    "result": [
      {
        "value": {
          "start": 903,
          "end": 935,
          "text": "graduated label smoothing method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--278:E0"
      },
      {
        "value": {
          "start": 958,
          "end": 979,
          "text": "inference calibration",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--278:E1"
      },
      {
        "value": {
          "start": 655,
          "end": 678,
          "text": "translation performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--278:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--278:E0",
        "to_id": "abstract-2020--acl-main--278:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--278:E0",
        "to_id": "abstract-2020--acl-main--278:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Confidence calibration, which aims to make model predictions equal to the true correctness measures, is important for neural machine translation (NMT) because it is able to offer useful indicators of translation errors in the generated output. While prior studies have shown that NMT models trained with label smoothing are well-calibrated on the ground-truth training data, we find that miscalibration still remains a severe challenge for NMT during inference due to the discrepancy between training and inference. By carefully designing experiments on three language pairs, our work provides in-depth analyses of the correlation between calibration and translation performance as well as linguistic properties of miscalibration and reports a number of interesting findings that might help humans better analyze, understand and improve NMT models. Based on these observations, we further propose a new graduated label smoothing method that can improve both inference calibration and translation performance."
    }
  },
  {
    "id": "abstract-2020--acl-main--567",
    "result": [
      {
        "value": {
          "start": 229,
          "end": 306,
          "text": "Dialogue State Tracker with Slot Attention and Slot Information Sharing (SAS)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--567:E0"
      },
      {
        "value": {
          "start": 616,
          "end": 627,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--567:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--567:E0",
        "to_id": "abstract-2020--acl-main--567:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Dialogue state tracker is responsible for inferring user intentions through dialogue history. Previous methods have difficulties in handling dialogues with long interaction context, due to the excessive information. We propose a Dialogue State Tracker with Slot Attention and Slot Information Sharing (SAS) to reduce redundant information’s interference and improve long dialogue context tracking. Specially, we first apply a Slot Attention to learn a set of slot-specific features from the original dialogue and then integrate them using a slot information sharing module. Our model yields a significantly improved performance compared to previous state-of the-art models on the MultiWOZ dataset."
    }
  },
  {
    "id": "abstract-2020--acl-main--277",
    "result": [
      {
        "value": {
          "start": 437,
          "end": 473,
          "text": "semi-autoregressive model RecoverSAT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--277:E0"
      },
      {
        "value": {
          "start": 906,
          "end": 913,
          "text": "speedup",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--277:E1"
      },
      {
        "value": {
          "start": 943,
          "end": 954,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--277:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--277:E0",
        "to_id": "abstract-2020--acl-main--277:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--277:E0",
        "to_id": "abstract-2020--acl-main--277:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Non-autoregressive neural machine translation (NAT) predicts the entire target sequence simultaneously and significantly accelerates inference process. However, NAT discards the dependency information in a sentence, and thus inevitably suffers from the multi-modality problem: the target tokens may be provided by different possible translations, often causing token repetitions or missing. To alleviate this problem, we propose a novel semi-autoregressive model RecoverSAT in this work, which generates a translation as a sequence of segments. The segments are generated simultaneously while each segment is predicted token-by-token. By dynamically determining segment length and deleting repetitive segments, RecoverSAT is capable of recovering from repetitive and missing token errors. Experimental results on three widely-used benchmark datasets show that our proposed model achieves more than 4 times speedup while maintaining comparable performance compared with the corresponding autoregressive model."
    }
  },
  {
    "id": "P11-1032",
    "result": [
      {
        "value": {
          "start": 572,
          "end": 582,
          "text": "classifier",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1032:E0"
      },
      {
        "value": {
          "start": 602,
          "end": 610,
          "text": "accurate",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1032:E1"
      },
      {
        "from_id": "P11-1032:E0",
        "to_id": "P11-1032:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Consumers increasingly rate, review and research products online (Jansen, 2010; Litvin et al., 2008). Consequently, websites containing consumer reviews are becoming targets of opinion spam. While recent work has focused primarily on manually identifiable instances of opinion spam, in this work we study deceptive opinion spam---fictitious opinions that have been deliberately written to sound authentic. Integrating work from psychology and computational linguistics, we develop and compare three approaches to detecting deceptive opinion spam, and ultimately develop a classifier that is nearly 90% accurate on our gold-standard opinion spam dataset. Based on feature analysis of our learned models, we additionally make several theoretical contributions, including revealing a relationship between deceptive opinions and imaginative writing."
    }
  },
  {
    "id": "abstract-2020--acl-main--457",
    "result": [
      {
        "value": {
          "start": 425,
          "end": 431,
          "text": "ASGARD",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--457:E0"
      },
      {
        "value": {
          "start": 907,
          "end": 919,
          "text": "ROUGE scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--457:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--457:E0",
        "to_id": "abstract-2020--acl-main--457:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Sequence-to-sequence models for abstractive summarization have been studied extensively, yet the generated summaries commonly suffer from fabricated content, and are often found to be near-extractive. We argue that, to address these issues, the summarizer should acquire semantic interpretation over input, e.g., via structured representation, to allow the generation of more informative summaries. In this paper, we present ASGARD, a novel framework for Abstractive Summarization with Graph-Augmentation and semantic-driven RewarD. We propose the use of dual encoders—a sequential document encoder and a graph-structured encoder—to maintain the global context and local characteristics of entities, complementing each other. We further design a reward based on a multiple choice cloze test to drive the model to better capture entity interactions. Results show that our models produce significantly higher ROUGE scores than a variant without knowledge graph as input on both New York Times and CNN/Daily Mail datasets. We also obtain better or comparable performance compared to systems that are fine-tuned from large pretrained language models. Human judges further rate our model outputs as more informative and containing fewer unfaithful errors."
    }
  },
  {
    "id": "abstract-2020--acl-main--751",
    "result": [
      {
        "value": {
          "start": 341,
          "end": 348,
          "text": "TXtract",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--751:E0"
      },
      {
        "value": {
          "start": 839,
          "end": 841,
          "text": "F1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--751:E1"
      },
      {
        "value": {
          "start": 853,
          "end": 861,
          "text": "coverage",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--751:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--751:E0",
        "to_id": "abstract-2020--acl-main--751:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--751:E0",
        "to_id": "abstract-2020--acl-main--751:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Extracting structured knowledge from product profiles is crucial for various applications in e-Commerce. State-of-the-art approaches for knowledge extraction were each designed for a single category of product, and thus do not apply to real-life e-Commerce scenarios, which often contain thousands of diverse categories. This paper proposes TXtract, a taxonomy-aware knowledge extraction model that applies to thousands of product categories organized in a hierarchical taxonomy. Through category conditional self-attention and multi-task learning, our approach is both scalable, as it trains a single model for thousands of categories, and effective, as it extracts category-specific attribute values. Experiments on products from a taxonomy with 4,000 categories show that TXtract outperforms state-of-the-art approaches by up to 10% in F1 and 15% in coverage across all categories."
    }
  },
  {
    "id": "abstract-2020--acl-main--57",
    "result": [
      {
        "value": {
          "start": 323,
          "end": 326,
          "text": "MDS",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--57:E0"
      },
      {
        "value": {
          "start": 682,
          "end": 701,
          "text": "per-turn accuracies",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--57:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--57:E0",
        "to_id": "abstract-2020--acl-main--57:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Existing end-to-end dialog systems perform less effectively when data is scarce. To obtain an acceptable success in real-life online services with only a handful of training examples, both fast adaptability and reliable performance are highly desirable for dialog systems. In this paper, we propose the Meta-Dialog System (MDS), which combines the advantages of both meta-learning approaches and human-machine collaboration. We evaluate our methods on a new extended-bAbI dataset and a transformed MultiWOZ dataset for low-resource goal-oriented dialog learning. Experimental results show that MDS significantly outperforms non-meta-learning baselines and can achieve more than 90% per-turn accuracies with only 10 dialogs on the extended-bAbI dataset."
    }
  },
  {
    "id": "abstract-2021--acl-long--68",
    "result": [
      {
        "value": {
          "start": 210,
          "end": 252,
          "text": "minimalistic LNA (LayerNorm and Attention)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--68:E0"
      },
      {
        "value": {
          "start": 717,
          "end": 721,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--68:E1"
      },
      {
        "value": {
          "start": 115,
          "end": 182,
          "text": "transfer learning from a pretrained speech encoder and text decoder",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--68:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--68:E0",
        "to_id": "abstract-2021--acl-long--68:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--68:E2",
        "to_id": "abstract-2021--acl-long--68:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We present a simple yet effective approach to build multilingual speech-to-text (ST) translation through efficient transfer learning from a pretrained speech encoder and text decoder. Our key finding is that a minimalistic LNA (LayerNorm and Attention) finetuning can achieve zero-shot crosslingual and cross-modality transfer ability by only finetuning 10 50% of the pretrained parameters. This effectively leverages large pretrained models at low training cost such as wav2vec 2.0 for acoustic modeling, and mBART for multilingual text generation. This sets a new state-of-the-art for 36 translation directions (and surpassing cascaded ST for 26 of them) on the large-scale multilingual ST benchmark CoVoST 2 (+6.4 BLEU on average for En-X directions and +6.7 BLEU for X-En directions). Our approach demonstrates strong zero-shot performance in a many-to-many multilingual model (+5.6 BLEU on average across 28 non-English directions), making it an appealing approach for attaining high-quality speech translation with improved parameter and data efficiency."
    }
  },
  {
    "id": "abstract-2020--acl-main--32",
    "result": [
      {
        "value": {
          "start": 1024,
          "end": 1036,
          "text": "Gaussian-BAT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--32:E0"
      },
      {
        "value": {
          "start": 1514,
          "end": 1522,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--32:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--32:E0",
        "to_id": "abstract-2020--acl-main--32:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recent years have witnessed a surge of interests of using neural topic models for automatic topic extraction from text, since they avoid the complicated mathematical derivations for model inference as in traditional topic models such as Latent Dirichlet Allocation (LDA). However, these models either typically assume improper prior (e.g. Gaussian or Logistic Normal) over latent topic space or could not infer topic distribution for a given document. To address these limitations, we propose a neural topic modeling approach, called Bidirectional Adversarial Topic (BAT) model, which represents the first attempt of applying bidirectional adversarial training for neural topic modeling. The proposed BAT builds a two-way projection between the document-topic distribution and the document-word distribution. It uses a generator to capture the semantic patterns from texts and an encoder for topic inference. Furthermore, to incorporate word relatedness information, the Bidirectional Adversarial Topic model with Gaussian (Gaussian-BAT) is extended from BAT. To verify the effectiveness of BAT and Gaussian-BAT, three benchmark corpora are used in our experiments. The experimental results show that BAT and Gaussian-BAT obtain more coherent topics, outperforming several competitive baselines. Moreover, when performing text clustering based on the extracted topics, our models outperform all the baselines, with more significant improvements achieved by Gaussian-BAT where an increase of near 6% is observed in accuracy."
    }
  },
  {
    "id": "P10-1160",
    "result": [
      {
        "value": {
          "start": 313,
          "end": 331,
          "text": "implicit arguments",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1160:E0"
      },
      {
        "value": {
          "start": 469,
          "end": 488,
          "text": "coverage of NomBank",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1160:E1"
      },
      {
        "from_id": "P10-1160:E0",
        "to_id": "P10-1160:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Despite its substantial coverage, NomBank does not account for all within-sentence arguments and ignores extra-sentential arguments altogether. These arguments, which we call implicit, are important to semantic processing, and their recovery could potentially benefit many NLP applications. We present a study of implicit arguments for a select group of frequent nominal predicates. We show that implicit arguments are pervasive for these predicates, adding 65% to the coverage of NomBank. We demonstrate the feasibility of recovering implicit arguments with a supervised classification model. Our results and analyses provide a baseline for future work on this emerging task."
    }
  },
  {
    "id": "abstract-2020--acl-main--590",
    "result": [
      {
        "value": {
          "start": 855,
          "end": 929,
          "text": "crafting high-quality adversaries and including them in the training stage",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--590:E0"
      },
      {
        "value": {
          "start": 28,
          "end": 39,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--590:E1"
      },
      {
        "value": {
          "start": 807,
          "end": 817,
          "text": "robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--590:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--590:E0",
        "to_id": "abstract-2020--acl-main--590:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--590:E0",
        "to_id": "abstract-2020--acl-main--590:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Despite achieving prominent performance on many important tasks, it has been reported that neural networks are vulnerable to adversarial examples. Previously studies along this line mainly focused on semantic tasks such as sentiment analysis, question answering and reading comprehension. In this study, we show that adversarial examples also exist in dependency parsing: we propose two approaches to study where and how parsers make mistakes by searching over perturbations to existing texts at sentence and phrase levels, and design algorithms to construct such examples in both of the black-box and white-box settings. Our experiments with one of state-of-the-art parsers on the English Penn Treebank (PTB) show that up to 77% of input examples admit adversarial perturbations, and we also show that the robustness of parsing models can be improved by crafting high-quality adversaries and including them in the training stage, while suffering little to no performance drop on the clean input data."
    }
  },
  {
    "id": "abstract-2020--acl-main--292",
    "result": [
      {
        "value": {
          "start": 648,
          "end": 733,
          "text": "Conditioning a popular domain-adversarial baseline method with these learned concepts",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--292:E0"
      },
      {
        "value": {
          "start": 752,
          "end": 763,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--292:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--292:E0",
        "to_id": "abstract-2020--acl-main--292:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Cross-domain sentiment analysis has received significant attention in recent years, prompted by the need to combat the domain gap between different applications that make use of sentiment analysis. In this paper, we take a novel perspective on this task by exploring the role of external commonsense knowledge. We introduce a new framework, KinGDOM, which utilizes the ConceptNet knowledge graph to enrich the semantics of a document by providing both domain-specific and domain-general background concepts. These concepts are learned by training a graph convolutional autoencoder that leverages inter-domain concepts in a domain-invariant manner. Conditioning a popular domain-adversarial baseline method with these learned concepts helps improve its performance over state-of-the-art approaches, demonstrating the efficacy of our proposed framework."
    }
  },
  {
    "id": "abstract-2021--acl-long--384",
    "result": [
      {
        "value": {
          "start": 515,
          "end": 527,
          "text": "Cluster2Sent",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--384:E0"
      },
      {
        "value": {
          "start": 782,
          "end": 796,
          "text": "ROUGE-1 points",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--384:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--384:E0",
        "to_id": "abstract-2021--acl-long--384:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Following each patient visit, physicians draft long semi-structured clinical summaries called SOAP notes. While invaluable to clinicians and researchers, creating digital SOAP notes is burdensome, contributing to physician burnout. In this paper, we introduce the first complete pipelines to leverage deep summarization models to generate these notes based on transcripts of conversations between physicians and patients. After exploring a spectrum of methods across the extractive-abstractive spectrum, we propose Cluster2Sent, an algorithm that (i) extracts important utterances relevant to each summary section; (ii) clusters together related utterances; and then (iii) generates one summary sentence per cluster. Cluster2Sent outperforms its purely abstractive counterpart by 8 ROUGE-1 points, and produces significantly more factual and coherent sentences as assessed by expert human evaluators. For reproducibility, we demonstrate similar benefits on the publicly available AMI dataset. Our results speak to the benefits of structuring summaries into sections and annotating supporting evidence when constructing summarization corpora."
    }
  },
  {
    "id": "abstract-2020--acl-main--460",
    "result": [
      {
        "value": {
          "start": 199,
          "end": 280,
          "text": "encourages the inclusion of key terms from the original document into the summary",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--460:E0"
      },
      {
        "value": {
          "start": 661,
          "end": 671,
          "text": "R-1 points",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--460:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--460:E0",
        "to_id": "abstract-2020--acl-main--460:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This work presents a new approach to unsupervised abstractive summarization based on maximizing a combination of coverage and fluency for a given length constraint. It introduces a novel method that encourages the inclusion of key terms from the original document into the summary: key terms are masked out of the original document and must be filled in by a coverage model using the current generated summary. A novel unsupervised training procedure leverages this coverage model along with a fluency model to generate and score summaries. When tested on popular news summarization datasets, the method outperforms previous unsupervised methods by more than 2 R-1 points, and approaches results of competitive supervised methods. Our model attains higher levels of abstraction with copied passages roughly two times shorter than prior work, and learns to compress and merge sentences without supervision."
    }
  },
  {
    "id": "abstract-2020--acl-main--705",
    "result": [
      {
        "value": {
          "start": 349,
          "end": 373,
          "text": "finetuned BERT (teacher)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--705:E0"
      },
      {
        "value": {
          "start": 484,
          "end": 495,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--705:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--705:E0",
        "to_id": "abstract-2020--acl-main--705:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Large-scale pre-trained language model such as BERT has achieved great success in language understanding tasks. However, it remains an open question how to utilize BERT for language generation. In this paper, we present a novel approach, Conditional Masked Language Modeling (C-MLM), to enable the finetuning of BERT on target generation tasks. The finetuned BERT (teacher) is exploited as extra supervision to improve conventional Seq2Seq models (student) for better text generation performance. By leveraging BERT’s idiosyncratic bidirectional nature, distilling knowledge learned in BERT can encourage auto-regressive Seq2Seq models to plan ahead, imposing global sequence-level supervision for coherent text generation. Experiments show that the proposed approach significantly outperforms strong Transformer baselines on multiple language generation tasks such as machine translation and text summarization. Our proposed model also achieves new state of the art on IWSLT German-English and English-Vietnamese MT datasets."
    }
  },
  {
    "id": "abstract-2021--acl-long--190",
    "result": [
      {
        "value": {
          "start": 564,
          "end": 595,
          "text": "domain-regularized module (DRM)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--190:E0"
      },
      {
        "value": {
          "start": 679,
          "end": 693,
          "text": "generalization",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--190:E1"
      },
      {
        "value": {
          "start": 949,
          "end": 972,
          "text": "BERT and RoBERTa models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--190:E2"
      },
      {
        "value": {
          "start": 356,
          "end": 367,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--190:E3"
      },
      {
        "from_id": "abstract-2021--acl-long--190:E0",
        "to_id": "abstract-2021--acl-long--190:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--190:E2",
        "to_id": "abstract-2021--acl-long--190:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Intent classification is a major task in spoken language understanding (SLU). Since most models are built with pre-collected in-domain (IND) training utterances, their ability to detect unsupported out-of-domain (OOD) utterances has a critical effect in practical use. Recent works have shown that using extra data and labels can improve the OOD detection performance, yet it could be costly to collect such data. This paper proposes to train a model with only IND data while supporting both IND intent classification and OOD detection. Our method designs a novel domain-regularized module (DRM) to reduce the overconfident phenomenon of a vanilla classifier, achieving a better generalization in both cases. Besides, DRM can be used as a drop-in replacement for the last layer in any neural network-based intent classifier, providing a low-cost strategy for a significant improvement. The evaluation on four datasets shows that our method built on BERT and RoBERTa models achieves state-of-the-art performance against existing approaches and the strong baselines we created for the comparisons."
    }
  },
  {
    "id": "abstract-2020--acl-main--172",
    "result": [
      {
        "value": {
          "start": 315,
          "end": 371,
          "text": "augmenting salient ontological terms into the summarizer",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--172:E0"
      },
      {
        "value": {
          "start": 585,
          "end": 590,
          "text": "ROUGE",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--172:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--172:E0",
        "to_id": "abstract-2020--acl-main--172:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Sequence-to-sequence (seq2seq) network is a well-established model for text summarization task. It can learn to produce readable content; however, it falls short in effectively identifying key regions of the source. In this paper, we approach the content selection problem for clinical abstractive summarization by augmenting salient ontological terms into the summarizer. Our experiments on two publicly available clinical data sets (107,372 reports of MIMIC-CXR, and 3,366 reports of OpenI) show that our model statistically significantly boosts state-of-the-art results in terms of ROUGE metrics (with improvements: 2.9% RG-1, 2.5% RG-2, 1.9% RG-L), in the healthcare domain where any range of improvement impacts patients’ welfare."
    }
  },
  {
    "id": "abstract-2021--acl-long--84",
    "result": [
      {
        "value": {
          "start": 564,
          "end": 594,
          "text": "pre-trained transformer models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--84:E0"
      },
      {
        "value": {
          "start": 629,
          "end": 637,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--84:E1"
      },
      {
        "value": {
          "start": 642,
          "end": 677,
          "text": "confidence estimation effectiveness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--84:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--84:E0",
        "to_id": "abstract-2021--acl-long--84:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--84:E0",
        "to_id": "abstract-2021--acl-long--84:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In selective prediction, a classifier is allowed to abstain from making predictions on low-confidence examples. Though this setting is interesting and important, selective prediction has rarely been examined in natural language processing (NLP) tasks. To fill this void in the literature, we study in this paper selective prediction for NLP, comparing different models and confidence estimators. We further propose a simple error regularization trick that improves confidence estimation without substantially increasing the computation budget. We show that recent pre-trained transformer models simultaneously improve both model accuracy and confidence estimation effectiveness. We also find that our proposed regularization improves confidence estimation and can be applied to other relevant scenarios, such as using classifier cascades for accuracy–efficiency trade-offs. Source code for this paper can be found at https://github.com/castorini/transformers-selective."
    }
  },
  {
    "id": "abstract-2021--acl-long--334",
    "result": [
      {
        "value": {
          "start": 200,
          "end": 210,
          "text": "BinaryBERT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--334:E0"
      },
      {
        "value": {
          "start": 592,
          "end": 603,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--334:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--334:E0",
        "to_id": "abstract-2021--acl-long--334:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "The rapid development of large pre-trained language models has greatly increased the demand for model compression techniques, among which quantization is a popular solution. In this paper, we propose BinaryBERT, which pushes BERT quantization to the limit by weight binarization. We find that a binary BERT is hard to be trained directly than a ternary counterpart due to its complex and irregular loss landscape. Therefore, we propose ternary weight splitting, which initializes BinaryBERT by equivalently splitting from a half-sized ternary network. The binary model thus inherits the good performance of the ternary one, and can be further enhanced by fine-tuning the new architecture after splitting. Empirical results show that our BinaryBERT has only a slight performance drop compared with the full-precision model while being 24x smaller, achieving the state-of-the-art compression results on the GLUE and SQuAD benchmarks. Code will be released."
    }
  },
  {
    "id": "abstract-2020--acl-main--19",
    "result": [
      {
        "value": {
          "start": 447,
          "end": 467,
          "text": "SEQ2SEQ NLG approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--19:E0"
      },
      {
        "value": {
          "start": 534,
          "end": 545,
          "text": "correctness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--19:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--19:E0",
        "to_id": "abstract-2020--acl-main--19:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Question answering (QA) is an important aspect of open-domain conversational agents, garnering specific research focus in the conversational QA (ConvQA) subtask. One notable limitation of recent ConvQA efforts is the response being answer span extraction from the target corpus, thus ignoring the natural language generation (NLG) aspect of high-quality conversational agents. In this work, we propose a method for situating QA responses within a SEQ2SEQ NLG approach to generate fluent grammatical answer responses while maintaining correctness. From a technical perspective, we use data augmentation to generate training data for an end-to-end system. Specifically, we develop Syntactic Transformations (STs) to produce question-specific candidate answer responses and rank them using a BERT-based classifier (Devlin et al., 2019). Human evaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the proposed model outperforms baseline CoQA and QuAC models in generating conversational responses. We further show our model’s scalability by conducting tests on the CoQA dataset. The code and data are available at https://github.com/abaheti95/QADialogSystem."
    }
  },
  {
    "id": "P11-1044",
    "result": [
      {
        "value": {
          "start": 13,
          "end": 40,
          "text": "language-independent method",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1044:E0"
      },
      {
        "value": {
          "start": 356,
          "end": 365,
          "text": "F-measure",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1044:E1"
      },
      {
        "value": {
          "start": 638,
          "end": 703,
          "text": "integrate the transliteration module into the GIZA++ word aligner",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1044:E2"
      },
      {
        "value": {
          "start": 779,
          "end": 788,
          "text": "precision",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1044:E3"
      },
      {
        "value": {
          "start": 793,
          "end": 799,
          "text": "recall",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1044:E4"
      },
      {
        "from_id": "P11-1044:E0",
        "to_id": "P11-1044:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P11-1044:E2",
        "to_id": "P11-1044:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P11-1044:E2",
        "to_id": "P11-1044:E4",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We propose a language-independent method for the automatic extraction of transliteration pairs from parallel corpora. In contrast to previous work, our method uses no form of supervision, and does not require linguistically informed preprocessing. We conduct experiments on data sets from the NEWS 2010 shared task on transliteration mining and achieve an F-measure of up to 92%, outperforming most of the semi-supervised systems that were submitted. We also apply our method to English/Hindi and English/Arabic parallel corpora and compare the results with manually built gold standards which mark transliterated word pairs. Finally, we integrate the transliteration module into the GIZA++ word aligner and evaluate it on two word alignment tasks achieving improvements in both precision and recall measured against gold standard word alignments."
    }
  },
  {
    "id": "abstract-2020--acl-main--452",
    "result": [
      {
        "value": {
          "start": 632,
          "end": 646,
          "text": "summary length",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--452:E0"
      },
      {
        "value": {
          "start": 600,
          "end": 608,
          "text": "ROUGE F1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--452:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--452:E0",
        "to_id": "abstract-2020--acl-main--452:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Automatic sentence summarization produces a shorter version of a sentence, while preserving its most important information. A good summary is characterized by language fluency and high information overlap with the source sentence. We model these two aspects in an unsupervised objective function, consisting of language modeling and semantic similarity metrics. We search for a high-scoring summary by discrete optimization. Our proposed method achieves a new state-of-the art for unsupervised sentence summarization according to ROUGE scores. Additionally, we demonstrate that the commonly reported ROUGE F1 metric is sensitive to summary length. Since this is unwillingly exploited in recent work, we emphasize that future evaluation should explicitly group summarization systems by output length brackets."
    }
  },
  {
    "id": "abstract-2020--acl-main--275",
    "result": [
      {
        "value": {
          "start": 52,
          "end": 55,
          "text": "DPE",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--275:E0"
      },
      {
        "value": {
          "start": 850,
          "end": 854,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--275:E1"
      },
      {
        "value": {
          "start": 850,
          "end": 854,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--275:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--275:E0",
        "to_id": "abstract-2020--acl-main--275:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--275:E0",
        "to_id": "abstract-2020--acl-main--275:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This paper introduces Dynamic Programming Encoding (DPE), a new segmentation algorithm for tokenizing sentences into subword units. We view the subword segmentation of output sentences as a latent variable that should be marginalized out for learning and inference. A mixed character-subword transformer is proposed, which enables exact log marginal likelihood estimation and exact MAP inference to find target segmentations with maximum posterior probability. DPE uses a lightweight mixed character-subword transformer as a means of pre-processing parallel data to segment output sentences using dynamic programming. Empirical results on machine translation suggest that DPE is effective for segmenting output sentences and can be combined with BPE dropout for stochastic segmentation of source sentences. DPE achieves an average improvement of 0.9 BLEU over BPE (Sennrich et al., 2016) and an average improvement of 0.55 BLEU over BPE dropout (Provilkov et al., 2019) on several WMT datasets including English <=> (German, Romanian, Estonian, Finnish, Hungarian)."
    }
  },
  {
    "id": "abstract-2020--acl-main--650",
    "result": [
      {
        "value": {
          "start": 645,
          "end": 697,
          "text": "train accurate models with unlabeled historical data",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--650:E0"
      },
      {
        "value": {
          "start": 433,
          "end": 457,
          "text": "manually normalized data",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--650:E1"
      },
      {
        "value": {
          "start": 810,
          "end": 818,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--650:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--650:E0",
        "to_id": "abstract-2020--acl-main--650:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--650:E0",
        "to_id": "abstract-2020--acl-main--650:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Historical text normalization, the task of mapping historical word forms to their modern counterparts, has recently attracted a lot of interest (Bollmann, 2019; Tang et al., 2018; Lusetti et al., 2018; Bollmann et al., 2018;Robertson and Goldwater, 2018; Bollmannet al., 2017; Korchagina, 2017). Yet, virtually all approaches suffer from the two limitations: 1) They consider a fully supervised setup, often with impractically large manually normalized datasets; 2) Normalization happens on words in isolation. By utilizing a simple generative normalization model and obtaining powerful contextualization from the target-side language model, we train accurate models with unlabeled historical data. In realistic training scenarios, our approach often leads to reduction in manually normalized data at the same accuracy levels."
    }
  },
  {
    "id": "abstract-2020--acl-main--681",
    "result": [
      {
        "value": {
          "start": 632,
          "end": 638,
          "text": "BERT-A",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--681:E0"
      },
      {
        "value": {
          "start": 654,
          "end": 663,
          "text": "F1 across",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--681:E1"
      },
      {
        "value": {
          "start": 678,
          "end": 706,
          "text": "using no labeled target data",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--681:E2"
      },
      {
        "value": {
          "start": 787,
          "end": 800,
          "text": "self-training",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--681:E3"
      },
      {
        "value": {
          "start": 554,
          "end": 556,
          "text": "F1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--681:E4"
      },
      {
        "from_id": "abstract-2020--acl-main--681:E0",
        "to_id": "abstract-2020--acl-main--681:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--681:E2",
        "to_id": "abstract-2020--acl-main--681:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--681:E3",
        "to_id": "abstract-2020--acl-main--681:E4",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We tackle the task of building supervised event trigger identification models which can generalize better across domains. Our work leverages the adversarial domain adaptation (ADA) framework to introduce domain-invariance. ADA uses adversarial training to construct representations that are predictive for trigger identification, but not predictive of the example’s domain. It requires no labeled data from the target domain, making it completely unsupervised. Experiments with two domains (English literature and news) show that ADA leads to an average F1 score improvement of 3.9 on out-of-domain data. Our best performing model (BERT-A) reaches 44-49 F1 across both domains, using no labeled target data. Preliminary experiments reveal that finetuning on 1% labeled data, followed by self-training leads to substantial improvement, reaching 51.5 and 67.2 F1 on literature and news respectively."
    }
  },
  {
    "id": "abstract-2022--acl-short--80",
    "result": [
      {
        "value": {
          "start": 752,
          "end": 789,
          "text": "optimizing discrete alignment metrics",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-short--80:E0"
      },
      {
        "value": {
          "start": 929,
          "end": 944,
          "text": "stable training",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-short--80:E1"
      },
      {
        "from_id": "abstract-2022--acl-short--80:E0",
        "to_id": "abstract-2022--acl-short--80:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "A common way to combat exposure bias is by applying scores from evaluation metrics as rewards in reinforcement learning (RL). Metrics leveraging contextualized embeddings appear more flexible than their n-gram matching counterparts and thus ideal as training rewards. However, metrics such as BERTScore greedily align candidate and reference tokens, which can allow system outputs to receive excess credit relative to a reference. Furthermore, past approaches featuring semantic similarity rewards suffer from repetitive outputs and overfitting. We address these issues by proposing metrics that replace the greedy alignments in BERTScore with optimized ones. We compute them on a model’s trained token embeddings to prevent domain mismatch. Our model optimizing discrete alignment metrics consistently outperforms cross-entropy and BLEU reward baselines on AMR-to-text generation. In addition, we find that this approach enjoys stable training compared to a non-RL setting."
    }
  },
  {
    "id": "abstract-2022--acl-long--502",
    "result": [
      {
        "value": {
          "start": 657,
          "end": 668,
          "text": "Transkimmer",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--502:E0"
      },
      {
        "value": {
          "start": 1154,
          "end": 1187,
          "text": "average speedup on GLUE benchmark",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--502:E1"
      },
      {
        "value": {
          "start": 1247,
          "end": 1267,
          "text": "accuracy degradation",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--502:E2"
      },
      {
        "from_id": "abstract-2022--acl-long--502:E0",
        "to_id": "abstract-2022--acl-long--502:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--502:E0",
        "to_id": "abstract-2022--acl-long--502:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Transformer architecture has become the de-facto model for many machine learning tasks from natural language processing and computer vision. As such, improving its computational efficiency becomes paramount. One of the major computational inefficiency of Transformer based models is that they spend the identical amount of computation throughout all layers. Prior works have proposed to augment the Transformer model with the capability of skimming tokens to improve its computational efficiency. However, they suffer from not having effectual and end-to-end optimization of the discrete skimming predictor. To address the above limitations, we propose the Transkimmer architecture, which learns to identify hidden state tokens that are not required by each layer. The skimmed tokens are then forwarded directly to the final output, thus reducing the computation of the successive layers. The key idea in Transkimmer is to add a parameterized predictor before each layer that learns to make the skimming decision. We also propose to adopt reparameterization trick and add skim loss for the end-to-end training of Transkimmer. Transkimmer achieves 10.97x average speedup on GLUE benchmark compared with vanilla BERT-base baseline with less than 1% accuracy degradation."
    }
  },
  {
    "id": "abstract-2022--acl-long--326",
    "result": [
      {
        "value": {
          "start": 264,
          "end": 281,
          "text": "synthetic samples",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--326:E0"
      },
      {
        "value": {
          "start": 294,
          "end": 308,
          "text": "bitext quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--326:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--326:E0",
        "to_id": "abstract-2022--acl-long--326:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Synthetic translations have been used for a wide range of NLP tasks primarily as a means of data augmentation. This work explores, instead, how synthetic translations can be used to revise potentially imperfect reference translations in mined bitext. We find that synthetic samples can improve bitext quality without any additional bilingual supervision when they replace the originals based on a semantic equivalence classifier that helps mitigate NMT noise. The improved quality of the revised bitext is confirmed intrinsically via human evaluation and extrinsically through bilingual induction and MT tasks."
    }
  },
  {
    "id": "abstract-2022--acl-long--91",
    "result": [
      {
        "value": {
          "start": 1107,
          "end": 1181,
          "text": "explicit incorporation of coreference information in the fine-tuning stage",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--91:E0"
      }
    ],
    "data": {
      "text": "Machine reading comprehension is a heavily-studied research and test field for evaluating new pre-trained language models (PrLMs) and fine-tuning strategies, and recent studies have enriched the pre-trained language models with syntactic, semantic and other linguistic information to improve the performance of the models. In this paper, we imitate the human reading process in connecting the anaphoric expressions and explicitly leverage the coreference information of the entities to enhance the word embeddings from the pre-trained language model, in order to highlight the coreference mentions of the entities that must be identified for coreference-intensive question answering in QUOREF, a relatively new dataset that is specifically designed to evaluate the coreference-related performance of a model. We use two strategies to fine-tune a pre-trained language model, namely, placing an additional encoder layer after a pre-trained language model to focus on the coreference mentions or constructing a relational graph convolutional network to model the coreference relations. We demonstrate that the explicit incorporation of coreference information in the fine-tuning stage performs better than the incorporation of the coreference information in pre-training a language model."
    }
  },
  {
    "id": "abstract-2022--acl-long--234",
    "result": [
      {
        "value": {
          "start": 1189,
          "end": 1222,
          "text": "performance on human-written data",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--234:E0"
      },
      {
        "value": {
          "start": 1131,
          "end": 1141,
          "text": "finetuning",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--234:E1"
      }
    ],
    "data": {
      "text": "Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate. Such over-reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language. To help mitigate these issues, we create ToxiGen, a new large-scale and machine-generated dataset of 274k toxic and benign statements about 13 minority groups. We develop a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method to generate subtly toxic and benign text with a massive pretrained language model. Controlling machine generation in this way allows ToxiGen to cover implicitly toxic text at a larger scale, and about more demographic groups, than previous resources of human-written text. We conduct a human evaluation on a challenging subset of ToxiGen and find that annotators struggle to distinguish machine-generated text from human-written language. We also find that 94.5% of toxic examples are labeled as hate speech by human annotators. Using three publicly-available datasets, we show that finetuning a toxicity classifier on our data improves its performance on human-written data substantially. We also demonstrate that ToxiGen can be used to fight machine-generated toxicity as finetuning improves the classifier significantly on our evaluation subset."
    }
  },
  {
    "id": "abstract-2022--acl-long--38",
    "result": [
      {
        "value": {
          "start": 281,
          "end": 301,
          "text": "evaluation framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--38:E0"
      },
      {
        "value": {
          "start": 325,
          "end": 346,
          "text": "evaluation procedures",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--38:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--38:E0",
        "to_id": "abstract-2022--acl-long--38:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The few-shot natural language understanding (NLU) task has attracted much recent attention. However, prior methods have been evaluated under a disparate set of protocols, which hinders fair comparison and measuring the progress of the field. To address this issue, we introduce an evaluation framework that improves previous evaluation procedures in three key aspects, i.e., test performance, dev-test correlation, and stability. Under this new evaluation framework, we re-evaluate several state-of-the-art few-shot methods for NLU tasks. Our framework reveals new insights: (1) both the absolute performance and relative gap of the methods were not accurately estimated in prior literature; (2) no single method dominates most tasks with consistent performance; (3) improvements of some methods diminish with a larger pretrained model; and (4) gains from different methods are often complementary and the best combined model performs close to a strong fully-supervised baseline. We open-source our toolkit, FewNLU, that implements our evaluation framework along with a number of state-of-the-art methods."
    }
  },
  {
    "id": "abstract-2022--acl-long--30",
    "result": [
      {
        "value": {
          "start": 1055,
          "end": 1073,
          "text": "CUC-VAE TTS system",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--30:E0"
      },
      {
        "value": {
          "start": 843,
          "end": 854,
          "text": "naturalness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--30:E1"
      },
      {
        "value": {
          "start": 1099,
          "end": 1116,
          "text": "prosody diversity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--30:E2"
      },
      {
        "from_id": "abstract-2022--acl-long--30:E0",
        "to_id": "abstract-2022--acl-long--30:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--30:E0",
        "to_id": "abstract-2022--acl-long--30:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Modelling prosody variation is critical for synthesizing natural and expressive speech in end-to-end text-to-speech (TTS) systems. In this paper, a cross-utterance conditional VAE (CUC-VAE) is proposed to estimate a posterior probability distribution of the latent prosody features for each phoneme by conditioning on acoustic features, speaker information, and text features obtained from both past and future sentences. At inference time, instead of the standard Gaussian distribution used by VAE, CUC-VAE allows sampling from an utterance-specific prior distribution conditioned on cross-utterance information, which allows the prosody features generated by the TTS system to be related to the context and is more similar to how humans naturally produce prosody. The performance of CUC-VAE is evaluated via a qualitative listening test for naturalness, intelligibility and quantitative measurements, including word error rates and the standard deviation of prosody attributes. Experimental results on LJ-Speech and LibriTTS data show that the proposed CUC-VAE TTS system improves naturalness and prosody diversity with clear margins."
    }
  },
  {
    "id": "abstract-2022--acl-long--57",
    "result": [
      {
        "value": {
          "start": 729,
          "end": 733,
          "text": "DEAM",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--57:E0"
      },
      {
        "value": {
          "start": 1199,
          "end": 1232,
          "text": "correlations with human judgments",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--57:E1"
      },
      {
        "value": {
          "start": 1445,
          "end": 1460,
          "text": "baseline models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--57:E2"
      },
      {
        "value": {
          "start": 1468,
          "end": 1494,
          "text": "detect incoherent examples",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--57:E3"
      },
      {
        "from_id": "abstract-2022--acl-long--57:E0",
        "to_id": "abstract-2022--acl-long--57:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--57:E2",
        "to_id": "abstract-2022--acl-long--57:E3",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Automatic evaluation metrics are essential for the rapid development of open-domain dialogue systems as they facilitate hyper-parameter tuning and comparison between models. Although recently proposed trainable conversation-level metrics have shown encouraging results, the quality of the metrics is strongly dependent on the quality of training data. Prior works mainly resort to heuristic text-level manipulations (e.g. utterances shuffling) to bootstrap incoherent conversations (negative examples) from coherent dialogues (positive examples). Such approaches are insufficient to appropriately reflect the incoherence that occurs in interactions between advanced dialogue models and humans. To tackle this problem, we propose DEAM, a Dialogue coherence Evaluation metric that relies on Abstract Meaning Representation (AMR) to apply semantic-level Manipulations for incoherent (negative) data generation. AMRs naturally facilitate the injection of various types of incoherence sources, such as coreference inconsistency, irrelevancy, contradictions, and decrease engagement, at the semantic level, thus resulting in more natural incoherent samples. Our experiments show that DEAM achieves higher correlations with human judgments compared to baseline methods on several dialog datasets by significant margins. We also show that DEAM can distinguish between coherent and incoherent dialogues generated by baseline manipulations, whereas those baseline models cannot detect incoherent examples generated by DEAM. Our results demonstrate the potential of AMR-based semantic manipulations for natural negative example generation."
    }
  },
  {
    "id": "abstract-2022--acl-short--34",
    "result": [
      {
        "value": {
          "start": 201,
          "end": 210,
          "text": "LM-BFF-MS",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-short--34:E0"
      }
    ],
    "data": {
      "text": "LM-BFF (CITATION) achieves significant few-shot performance by using auto-generated prompts and adding demonstrations similar to an input example. To improve the approach of LM-BFF, this paper proposesLM-BFF-MS—betterfew-shotfine-tuning oflanguagemodels withmultiplesoft demonstrations by making its further extensions, which include 1) prompts withmultiple demonstrationsbased on automatic generation of multiple label words; and 2)soft demonstration memorywhich consists of multiple sequences ofglobally sharedword embeddings for a similar context. Experiments conducted on eight NLP tasks show that LM-BFF-MS leads to improvements over LM-BFF on five tasks, particularly achieving 94.0 and 90.4 on SST-2 and MRPC, respectively."
    }
  },
  {
    "id": "abstract-2022--acl-long--478",
    "result": [
      {
        "value": {
          "start": 502,
          "end": 507,
          "text": "MILIE",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--478:E0"
      }
    ],
    "data": {
      "text": "Open Information Extraction (OpenIE) is the task of extracting (subject, predicate, object) triples from natural language sentences. Current OpenIE systems extract all triple slots independently. In contrast, we explore the hypothesis that it may be beneficial to extract triple slots iteratively: first extract easy slots, followed by the difficult ones by conditioning on the easy slots, and therefore achieve a better overall extraction. Based on this hypothesis, we propose a neural OpenIE system, MILIE, that operates in an iterative fashion. Due to the iterative nature, the system is also modularit is possible to seamlessly integrate rule based extraction systems with a neural end-to-end system, thereby allowing rule based systems to supply extraction slots which MILIE can leverage for extracting the remaining slots. We confirm our hypothesis empirically: MILIE outperforms SOTA systems on multiple languages ranging from Chinese to Arabic. Additionally, we are the first to provide an OpenIE test dataset for Arabic and Galician."
    }
  },
  {
    "id": "abstract-2022--acl-long--124",
    "result": [
      {
        "value": {
          "start": 836,
          "end": 875,
          "text": "cross-modal graph convolutional network",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--124:E0"
      },
      {
        "value": {
          "start": 1055,
          "end": 1116,
          "text": "state-of-the-art performance in multi-modal sarcasm detection",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--124:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--124:E0",
        "to_id": "abstract-2022--acl-long--124:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "With the increasing popularity of posting multimodal messages online, many recent studies have been carried out utilizing both textual and visual information for multi-modal sarcasm detection. In this paper, we investigate multi-modal sarcasm detection from a novel perspective by constructing a cross-modal graph for each instance to explicitly draw the ironic relations between textual and visual modalities. Specifically, we first detect the objects paired with descriptions of the image modality, enabling the learning of important visual information. Then, the descriptions of the objects are served as a bridge to determine the importance of the association between the objects of image modality and the contextual words of text modality, so as to build a cross-modal graph for each multi-modal instance. Furthermore, we devise a cross-modal graph convolutional network to make sense of the incongruity relations between modalities for multi-modal sarcasm detection. Extensive experimental results and in-depth analysis show that our model achieves state-of-the-art performance in multi-modal sarcasm detection."
    }
  },
  {
    "id": "abstract-2022--acl-long--595",
    "result": [
      {
        "value": {
          "start": 441,
          "end": 459,
          "text": "unsupervised model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--595:E0"
      },
      {
        "value": {
          "start": 792,
          "end": 834,
          "text": "state of the art of unsupervised retrieval",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--595:E1"
      },
      {
        "value": {
          "start": 495,
          "end": 523,
          "text": "single-pair supervised model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--595:E2"
      },
      {
        "value": {
          "start": 900,
          "end": 947,
          "text": "performance of multilingually supervised models",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--595:E3"
      },
      {
        "from_id": "abstract-2022--acl-long--595:E0",
        "to_id": "abstract-2022--acl-long--595:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--595:E2",
        "to_id": "abstract-2022--acl-long--595:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This work presents methods for learning cross-lingual sentence representations using paired or unpaired bilingual texts. We hypothesize that the cross-lingual alignment strategy is transferable, and therefore a model trained to align only two languages can encode multilingually more aligned representations. We thus introduce dual-pivot transfer: training on one language pair and evaluating on other pairs. To study this theory, we design unsupervised models trained on unpaired sentences and single-pair supervised models trained on bitexts, both based on the unsupervised language model XLM-R with its parameters frozen. The experiments evaluate the models as universal sentence encoders on the task of unsupervised bitext mining on two datasets, where the unsupervised model reaches the state of the art of unsupervised retrieval, and the alternative single-pair supervised model approaches the performance of multilingually supervised models. The results suggest that bilingual training techniques as proposed can be applied to get sentence representations with multilingual alignment."
    }
  },
  {
    "id": "abstract-2022--acl-long--602",
    "result": [
      {
        "value": {
          "start": 444,
          "end": 456,
          "text": "Pyramid-BERT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--602:E0"
      }
    ],
    "data": {
      "text": "Transformer-based language models such as BERT (CITATION) have achieved the state-of-the-art performance on various NLP tasks, but are computationally prohibitive. A recent line of works use various heuristics to successively shorten sequence length while transforming tokens through encoders, in tasks such as classification and ranking that require a single token embedding for prediction. We present a novel solution to this problem, called Pyramid-BERT where we replace previously used heuristics with acore-setbased token selection method justified by theoretical results. The core-set based token selection technique allows us to avoid expensive pre-training, gives a space-efficient fine tuning, and thus makes it suitable to handle longer sequence lengths. We provide extensive experiments establishing advantages of pyramid BERT over several baselines and existing works on the GLUE benchmarks and Long Range Arena (CITATION) datasets."
    }
  },
  {
    "id": "abstract-2022--acl-long--246",
    "result": [
      {
        "value": {
          "start": 512,
          "end": 523,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--246:E0"
      },
      {
        "value": {
          "start": 706,
          "end": 750,
          "text": "state-of-the-art large-scale language models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--246:E1"
      }
    ],
    "data": {
      "text": "Given the ubiquitous nature of numbers in text, reasoning with numbers to perform simple calculations is an important skill of AI systems. While many datasets and models have been developed to this end, state-of-the-art AI systems are brittle; failing to perform the underlying mathematical reasoning when they appear in a slightly different scenario. Drawing inspiration from GLUE that was proposed in the context of natural language understanding, we propose NumGLUE, a multi-task benchmark that evaluates the performance of AI systems on eight different tasks, that at their core require simple arithmetic understanding. We show that this benchmark is far from being solved with neural models including state-of-the-art large-scale language models performing significantly worse than humans (lower by 46.4 %). Further, NumGLUE promotes sharing knowledge across tasks, especially those with limited training data as evidenced by the superior performance (average gain of 3.4 % on each task) when a model is jointly trained on all the tasks as opposed to task-specific modeling. Finally, we hope that NumGLUE will encourage systems that perform robust and general arithmetic reasoning within language, a first step towards being able to perform more complex mathematical reasoning."
    }
  },
  {
    "id": "abstract-2022--acl-long--340",
    "result": [
      {
        "value": {
          "start": 594,
          "end": 600,
          "text": "KG-FiD",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--340:E0"
      },
      {
        "value": {
          "start": 957,
          "end": 989,
          "text": "performance in answer prediction",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--340:E1"
      },
      {
        "value": {
          "start": 1026,
          "end": 1042,
          "text": "computation cost",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--340:E2"
      },
      {
        "from_id": "abstract-2022--acl-long--340:E0",
        "to_id": "abstract-2022--acl-long--340:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--340:E0",
        "to_id": "abstract-2022--acl-long--340:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Current Open-Domain Question Answering (ODQA) models typically include a retrieving module and a reading module, where the retriever selects potentially relevant passages from open-source documents for a given question, and the reader produces an answer based on the retrieved passages. The recently proposed Fusion-in-Decoder (FiD) framework is a representative example, which is built on top of a dense passage retriever and a generative reader, achieving the state-of-the-art performance. In this paper we further improve the FiD approach by introducing a knowledge-enhanced version, namely KG-FiD. Our new model uses a knowledge graph to establish the structural relationship among the retrieved passages, and a graph neural network (GNN) to re-rank the passages and select only a top few for further processing. Our experiments on common ODQA benchmark datasets (Natural Questions and TriviaQA) demonstrate that KG-FiD can achieve comparable or better performance in answer prediction than FiD, with less than 40% of the computation cost."
    }
  },
  {
    "id": "abstract-2022--acl-long--371",
    "result": [
      {
        "value": {
          "start": 963,
          "end": 970,
          "text": "Prix-LM",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--371:E0"
      }
    ],
    "data": {
      "text": "Knowledge bases (KBs) contain plenty of structured world and commonsense knowledge. As such, they often complement distributional text-based information and facilitate various downstream tasks. Since their manual construction is resource- and time-intensive, recent efforts have tried leveraging large pretrained language models (PLMs) to generate additional monolingual knowledge facts for KBs. However, such methods have not been attempted for building and enriching multilingual KBs. Besides wider application, such multilingual KBs can provide richer combined knowledge than monolingual (e.g., English) KBs. Knowledge expressed in different languages may be complementary and unequally distributed: this implies that the knowledge available in high-resource languages can be transferred to low-resource ones. To achieve this, it is crucial to represent multilingual knowledge in a shared/unified space. To this end, we propose a unified representation model, Prix-LM, for multilingual KB construction and completion. We leverage two types of knowledge, monolingual triples and cross-lingual links, extracted from existing multilingual KBs, and tune a multilingual language encoder XLM-R via a causal language modeling objective. Prix-LM integrates useful multilingual and KB-based factual knowledge into a single model. Experiments on standard entity-related tasks, such as link prediction in multiple languages, cross-lingual entity linking and bilingual lexicon induction, demonstrate its effectiveness, with gains reported over strong task-specialised baselines."
    }
  },
  {
    "id": "abstract-2022--acl-long--146",
    "result": [
      {
        "value": {
          "start": 281,
          "end": 335,
          "text": "injecting non-local features into the training process",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--146:E0"
      },
      {
        "value": {
          "start": 522,
          "end": 528,
          "text": "method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--146:E1"
      },
      {
        "value": {
          "start": 648,
          "end": 688,
          "text": "BERT-based performance on PTB (95.92 F1)",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--146:E2"
      },
      {
        "value": {
          "start": 700,
          "end": 729,
          "text": "performance on CTB (92.31 F1)",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--146:E3"
      },
      {
        "value": {
          "start": 81,
          "end": 87,
          "text": "parser",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--146:E4"
      },
      {
        "from_id": "abstract-2022--acl-long--146:E1",
        "to_id": "abstract-2022--acl-long--146:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--146:E1",
        "to_id": "abstract-2022--acl-long--146:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Thanks to the strong representation power of neural encoders, neural chart-based parsers have achieved highly competitive performance by using local features. Recently, it has been shown that non-local features in CRF structures lead to improvements. In this paper, we investigate injecting non-local features into the training process of a local span-based parser, by predicting constituentn-gram non-local patterns and ensuring consistency between non-local patterns and local constituents. Results show that our simple method gives better results than the self-attentive parser on both PTB and CTB. Besides, our method achieves state-of-the-art BERT-based performance on PTB (95.92 F1) and strong performance on CTB (92.31 F1). Our parser also outperforms the self-attentive parser in multi-lingual and zero-shot cross-domain settings."
    }
  },
  {
    "id": "abstract-2022--acl-long--408",
    "result": [
      {
        "value": {
          "start": 87,
          "end": 117,
          "text": "understanding of math problems",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--408:E0"
      }
    ],
    "data": {
      "text": "In this paper, we study how to continually pre-train language models for improving the understanding of math problems. Specifically, we focus on solving a fundamental challenge in modeling math problems, how to fuse the semantics of textual description and formulas, which are highly different in essence. To address this issue, we propose a new approach calledCOMUStocontinually pre-train language models formath problemunderstanding withsyntax-aware memory network. In this approach, we first construct the math syntax graph to model the structural semantic information, by combining the parsing trees of the text and formulas, and then design the syntax-aware memory networks to deeply fuse the features from the graph and text. With the help of syntax relations, we can model the interaction between the token from the text and its semantic-related nodes within the formulas, which is helpful to capture fine-grained semantic correlations between texts and formulas. Besides, we devise three continual pre-training tasks to further align and fuse the representations of the text and math syntax graph. Experimental results on four tasks in the math domain demonstrate the effectiveness of our approach. Our code and data are publicly available at the link: bluehttps://github.com/RUCAIBox/COMUS."
    }
  },
  {
    "id": "abstract-2022--acl-long--406",
    "result": [
      {
        "value": {
          "start": 353,
          "end": 357,
          "text": "EGT2",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--406:E0"
      },
      {
        "value": {
          "start": 832,
          "end": 864,
          "text": "current state-of-the-art methods",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--406:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--406:E0",
        "to_id": "abstract-2022--acl-long--406:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Typed entailment graphs try to learn the entailment relations between predicates from text and model them as edges between predicate nodes. The construction of entailment graphs usually suffers from severe sparsity and unreliability of distributional similarity. We propose a two-stage method, Entailment Graph with Textual Entailment and Transitivity (EGT2). EGT2 learns the local entailment relations by recognizing the textual entailment between template sentences formed by typed CCG-parsed predicates. Based on the generated local graph, EGT2 then uses three novel soft transitivity constraints to consider the logical transitivity in entailment structures. Experiments on benchmark datasets show that EGT2 can well model the transitivity in entailment graph to alleviate the sparsity, and leads to signifcant improvement over current state-of-the-art methods."
    }
  },
  {
    "id": "abstract-2022--acl-long--453",
    "result": [
      {
        "value": {
          "start": 710,
          "end": 739,
          "text": "small number of AL iterations",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--453:E0"
      },
      {
        "value": {
          "start": 795,
          "end": 804,
          "text": "precision",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--453:E1"
      },
      {
        "value": {
          "start": 806,
          "end": 812,
          "text": "recall",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--453:E2"
      },
      {
        "value": {
          "start": 818,
          "end": 838,
          "text": "diversity of results",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--453:E3"
      },
      {
        "from_id": "abstract-2022--acl-long--453:E0",
        "to_id": "abstract-2022--acl-long--453:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--453:E0",
        "to_id": "abstract-2022--acl-long--453:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--453:E0",
        "to_id": "abstract-2022--acl-long--453:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Detecting disclosures of individuals’ employment status on social media can provide valuable information to match job seekers with suitable vacancies, offer social protection, or measure labor market flows. However, identifying such personal disclosures is a challenging task due to their rarity in a sea of social media content and the variety of linguistic forms used to describe them. Here, we examine three Active Learning (AL) strategies in real-world settings of extreme class imbalance, and identify five types of disclosures about individuals’ employment status (e.g. job loss) in three languages using BERT-based classification models. Our findings show that, even under extreme imbalance settings, a small number of AL iterations is sufficient to obtain large and significant gains in precision, recall, and diversity of results compared to a supervised baseline with the same number of labels. We also find that no AL strategy consistently outperforms the rest. Qualitative analysis suggests that AL helps focus the attention mechanism of BERT on core terms and adjust the boundaries of semantic expansion, highlighting the importance of interpretable models to provide greater control and visibility into this dynamic learning process."
    }
  },
  {
    "id": "abstract-2022--acl-long--536",
    "result": [
      {
        "value": {
          "start": 525,
          "end": 557,
          "text": "novel transfer learning strategy",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--536:E0"
      },
      {
        "value": {
          "start": 1032,
          "end": 1057,
          "text": "generalization capability",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--536:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--536:E0",
        "to_id": "abstract-2022--acl-long--536:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Identifying argument components from unstructured texts and predicting the relationships expressed among them are two primary steps of argument mining. The intrinsic complexity of these tasks demands powerful learning models. While pretrained Transformer-based Language Models (LM) have been shown to provide state-of-the-art results over different NLP tasks, the scarcity of manually annotated data and the highly domain-dependent nature of argumentation restrict the capabilities of such models. In this work, we propose a novel transfer learning strategy to overcome these challenges. We utilize argumentation-rich social discussions from theChangeMyViewsubreddit as a source of unsupervised, argumentative discourse-aware knowledge by finetuning pretrained LMs on a selectively masked language modeling task. Furthermore, we introduce a novel prompt-based strategy for inter-component relation prediction that compliments our proposed finetuning method while leveraging on the discourse context. Exhaustive experiments show the generalization capability of our method on these two tasks over within-domain as well as out-of-domain datasets, outperforming several existing and employed strong baselines."
    }
  },
  {
    "id": "abstract-2022--acl-demo--19",
    "result": [
      {
        "value": {
          "start": 1079,
          "end": 1105,
          "text": "controllable dialog system",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-demo--19:E0"
      },
      {
        "value": {
          "start": 1208,
          "end": 1219,
          "text": "user effort",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-demo--19:E1"
      },
      {
        "from_id": "abstract-2022--acl-demo--19:E0",
        "to_id": "abstract-2022--acl-demo--19:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Intelligent conversational assistants have become an integral part of our lives for performing simple tasks. However, such agents, for example, Google bots, Alexa and others are yet to have any social impact on minority population, for example, for people with neurological disorders and people with speech, language and social communication disorders, sometimes with locked-in states where speaking or typing is a challenge. Language model technologies can be very powerful tools in enabling these users to carry out daily communication and social interactions. In this work, we present a system that users with varied levels of disabilties can use to interact with the world, supported by eye-tracking, mouse controls and an intelligent agent Cue-bot, that can represent the user in a conversation. The agent provides relevant controllable ‘cues’ to generate desirable responses quickly for an ongoing dialog context. In the context of usage of such systems for people with degenerative disorders, we present automatic and human evaluation of our cue/keyword predictor and the controllable dialog system and show that our models perform significantly better than models without control and can also reduce user effort (fewer keystrokes) and speed up communication (typing time) significantly."
    }
  },
  {
    "id": "abstract-2022--acl-long--194",
    "result": [
      {
        "value": {
          "start": 346,
          "end": 353,
          "text": "KGTuner",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--194:E0"
      },
      {
        "value": {
          "start": 689,
          "end": 743,
          "text": "average relative improvement for four embedding models",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--194:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--194:E0",
        "to_id": "abstract-2022--acl-long--194:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "While hyper-parameters (HPs) are important for knowledge graph (KG) learning, existing methods fail to search them efficiently. To solve this problem, we first analyze the properties of different HPs and measure the transfer ability from small subgraph to the full graph. Based on the analysis, we propose an efficient two-stage search algorithm KGTuner, which efficiently explores HP configurations on small subgraph at the first stage and transfers the top-performed configurations for fine-tuning on the large full graph at the second stage. Experiments show that our method can consistently find better HPs than the baseline algorithms within the same time budget, which achieves 9.1% average relative improvement for four embedding models on the large-scale KGs in open graph benchmark. Our code is released inhttps://github.com/AutoML-Research/KGTuner."
    }
  },
  {
    "id": "abstract-2022--acl-long--321",
    "result": [
      {
        "value": {
          "start": 957,
          "end": 981,
          "text": "state-of-the-art results",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--321:E0"
      }
    ],
    "data": {
      "text": "Formality style transfer (FST) is a task that involves paraphrasing an informal sentence into a formal one without altering its meaning. To address the data-scarcity problem of existing parallel datasets, previous studies tend to adopt a cycle-reconstruction scheme to utilize additional unlabeled data, where the FST model mainly benefits from target-side unlabeled sentences. In this work, we propose a simple yet effective semi-supervised framework to better utilize source-side unlabeled sentences based on consistency training. Specifically, our approach augments pseudo-parallel data obtained from a source-side informal sentence by enforcing the model to generate similar outputs for its perturbed version. Moreover, we empirically examined the effects of various data perturbation methods and propose effective data filtering strategies to improve our framework. Experimental results on the GYAFC benchmark demonstrate that our approach can achieve state-of-the-art results, even with less than 40% of the parallel data."
    }
  },
  {
    "id": "abstract-2022--acl-demo--3",
    "result": [
      {
        "value": {
          "start": 292,
          "end": 309,
          "text": "multimodal models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-demo--3:E0"
      },
      {
        "value": {
          "start": 333,
          "end": 344,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-demo--3:E1"
      },
      {
        "value": {
          "start": 346,
          "end": 356,
          "text": "robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-demo--3:E2"
      },
      {
        "value": {
          "start": 396,
          "end": 421,
          "text": "complementary information",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-demo--3:E3"
      },
      {
        "from_id": "abstract-2022--acl-demo--3:E0",
        "to_id": "abstract-2022--acl-demo--3:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-demo--3:E0",
        "to_id": "abstract-2022--acl-demo--3:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-demo--3:E0",
        "to_id": "abstract-2022--acl-demo--3:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "There is a growing need to model interactions between data modalities (e.g., vision, language) — both to improve AI predictions on existing tasks and to enable new applications. In the recent field of multimodal medical AI, integrating multiple modalities has gained widespread popularity as multimodal models have proven to improve performance, robustness, require less training samples and add complementary information. To improve technical reproducibility and transparency for multimodal medical tasks as well as speed up progress across medical AI, we present ViLMedic, a Vision-and-Language medical library. As of 2022, the library contains a dozen reference implementations replicating the state-of-the-art results for problems that range from medical visual question answering and radiology report generation to multimodal representation learning on widely adopted medical datasets. In addition, ViLMedic hosts a model-zoo with more than twenty pretrained models for the above tasks designed to be extensible by researchers but also simple for practitioners. Ultimately, we hope our reproducible pipelines can enable clinical translation and create real impact. The library is available athttps://github.com/jbdel/vilmedic."
    }
  },
  {
    "id": "abstract-2022--acl-long--13",
    "result": [
      {
        "value": {
          "start": 404,
          "end": 416,
          "text": "TopWORDS-Seg",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--13:E0"
      },
      {
        "value": {
          "start": 459,
          "end": 477,
          "text": "robust performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--13:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--13:E0",
        "to_id": "abstract-2022--acl-long--13:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Processing open-domain Chinese texts has been a critical bottleneck in computational linguistics for decades, partially because text segmentation and word discovery often entangle with each other in this challenging scenario. No existing methods yet can achieve effective text segmentation and word discovery simultaneously in open domain. This study fills in this gap by proposing a novel method called TopWORDS-Seg based on Bayesian inference, which enjoys robust performance and transparent interpretation when no training corpus and domain vocabulary are available. Advantages of TopWORDS-Seg are demonstrated by a series of experimental studies."
    }
  },
  {
    "id": "abstract-2022--acl-long--80",
    "result": [
      {
        "value": {
          "start": 286,
          "end": 309,
          "text": "Continual Prompt Tuning",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--80:E0"
      }
    ],
    "data": {
      "text": "A desirable dialog system should be able to continually learn new skills without forgetting old ones, and thereby adapt to new domains or tasks in its life cycle. However, continually training a model often leads to a well-known catastrophic forgetting issue. In this paper, we present Continual Prompt Tuning, a parameter-efficient framework that not only avoids forgetting but also enables knowledge transfer between tasks. To avoid forgetting, we only learn and store a few prompt tokens’ embeddings for each task while freezing the backbone pre-trained model. To achieve bi-directional knowledge transfer among tasks, we propose several techniques (continual prompt initialization, query fusion, and memory replay) to transfer knowledge from preceding tasks and a memory-guided technique to transfer knowledge from subsequent tasks. Extensive experiments demonstrate the effectiveness and efficiency of our proposed method on continual learning for dialog state tracking, compared with state-of-the-art baselines."
    }
  },
  {
    "id": "abstract-2022--acl-long--166",
    "result": [
      {
        "value": {
          "start": 533,
          "end": 561,
          "text": "few-shot prompt-based models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--166:E0"
      }
    ],
    "data": {
      "text": "Finetuning large pre-trained language models with a task-specific head has advanced the state-of-the-art on many natural language understanding benchmarks. However, models with a task-specific head require a lot of training data, making them susceptible to learning and exploiting dataset-specific superficial cues that do not generalize to other datasets. Prompting has reduced the data requirement by reusing the language model head and formatting the task input to match the pre-training objective. Therefore, it is expected that few-shot prompt-based models do not exploit superficial cues. This paper presents an empirical examination of whether few-shot prompt-based models also exploit superficial cues. Analyzing few-shot prompt-based models on MNLI, SNLI, HANS, and COPA has revealed that prompt-based models also exploit superficial cues. While the models perform well on instances with superficial cues, they often underperform or only marginally outperform random accuracy on instances without superficial cues."
    }
  },
  {
    "id": "abstract-2022--acl-long--148",
    "result": [
      {
        "value": {
          "start": 1156,
          "end": 1185,
          "text": "effectiveness and superiority",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--148:E0"
      }
    ],
    "data": {
      "text": "The goal of the cross-lingual summarization (CLS) is to convert a document in one language (e.g., English) to a summary in another one (e.g., Chinese). The CLS task is essentially the combination of machine translation (MT) and monolingual summarization (MS), and thus there exists the hierarchical relationship between MT&MS and CLS. Existing studies on CLS mainly focus on utilizing pipeline methods or jointly training an end-to-end model through an auxiliary MT or MS objective. However, it is very challenging for the model to directly conduct CLS as it requires both the abilities to translate and summarize. To address this issue, we propose a hierarchical model for the CLS task, based on the conditional variational auto-encoder. The hierarchical model contains two kinds of latent variables at the local and global levels, respectively. At the local level, there are two latent variables, one for translation and the other for summarization. As for the global level, there is another latent variable for cross-lingual summarization conditioned on the two local-level variables. Experiments on two language directions (English-Chinese) verify the effectiveness and superiority of the proposed approach. In addition, we show that our model is able to generate better cross-lingual summaries than comparison models in the few-shot setting."
    }
  },
  {
    "id": "abstract-2022--acl-long--96",
    "result": [
      {
        "value": {
          "start": 572,
          "end": 582,
          "text": "perplexity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--96:E0"
      }
    ],
    "data": {
      "text": "Class-based language models (LMs) have been long devised to address context sparsity inn-gram LMs. In this study, we revisit this approach in the context of neural LMs. We hypothesize that class-based prediction leads to an implicit context aggregation for similar words and thus can improve generalization for rare words. We map words that have a common WordNet hypernym to the same class and train large neural LMs by gradually annealing from predicting the class to token prediction during training. Empirically, this curriculum learning strategy consistently improves perplexity over various large, highly-performant state-of-the-art Transformer-based models on two datasets, WikiText-103 and ARXIV. Our analysis shows that the performance improvement is achieved without sacrificing performance on rare words. Finally, we document other attempts that failed to yield empirical gains, and discuss future directions for the adoption of class-based LMs on a larger scale."
    }
  },
  {
    "id": "abstract-2022--acl-long--44",
    "result": [
      {
        "value": {
          "start": 525,
          "end": 571,
          "text": "memory imitation meta-learning (MemIML) method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--44:E0"
      }
    ],
    "data": {
      "text": "Building models of natural language processing (NLP) is challenging in low-resource scenarios where limited data are available. Optimization-based meta-learning algorithms achieve promising results in low-resource scenarios by adapting a well-generalized model initialization to handle new tasks. Nonetheless, these approaches suffer from the memorization overfitting issue, where the model tends to memorize the meta-training tasks while ignoring support sets when adapting to new tasks. To address this issue, we propose a memory imitation meta-learning (MemIML) method that enhances the model’s reliance on support sets for task adaptation. Specifically, we introduce a task-specific memory module to store support set information and construct an imitation module to force query sets to imitate the behaviors of support sets stored in the memory. A theoretical analysis is provided to prove the effectiveness of our method, and empirical results also demonstrate that our method outperforms competitive baselines on both text classification and generation tasks."
    }
  },
  {
    "id": "abstract-2022--acl-long--266",
    "result": [
      {
        "value": {
          "start": 340,
          "end": 353,
          "text": "best ensemble",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--266:E0"
      }
    ],
    "data": {
      "text": "In this paper, we investigate improvements to the GEC sequence tagging architecture with a focus on ensembling of recent cutting-edge Transformer-based encoders in Large configurations. We encourage ensembling models by majority votes on span-level edits because this approach is tolerant to the model architecture and vocabulary size. Our best ensemble achieves a new SOTA result with anF0.5score of 76.05 on BEA-2019 (test), even without pre-training on synthetic datasets. In addition, we perform knowledge distillation with a trained ensemble to generate new synthetic training datasets, “Troy-Blogs” and “Troy-1BW”. Our best single sequence tagging model that is pretrained on the generated Troy- datasets in combination with the publicly available synthetic PIE dataset achieves a near-SOTA result with anF0.5score of 73.21 on BEA-2019 (test). The code, datasets, and trained models are publicly available."
    }
  },
  {
    "id": "abstract-2022--acl-long--579",
    "result": [
      {
        "value": {
          "start": 924,
          "end": 981,
          "text": "search-query based access of the internet in conversation",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--579:E0"
      },
      {
        "value": {
          "start": 1000,
          "end": 1011,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--579:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--579:E0",
        "to_id": "abstract-2022--acl-long--579:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The largest store of continually updating knowledge on our planet can be accessed via internet search. In this work we study giving access to this information to conversational agents. Large language models, even though they store an impressive amount of knowledge within their weights, are known to hallucinate facts when generating dialogue (Shuster et al., 2021); moreover, those facts are frozen in time at the point of model training. In contrast, we propose an approach that learns to generate an internet search query based on the context, and then conditions on the search results to finally generate a response, a method that can employ up-to-the-minute relevant information. We train and evaluate such models on a newly collected dataset of human-human conversations whereby one of the speakers is given access to internet search during knowledgedriven discussions in order to ground their responses. We find that search-query based access of the internet in conversation provides superior performance compared to existing approaches that either use no augmentation or FAISS-based retrieval (Lewis et al., 2020b)."
    }
  },
  {
    "id": "abstract-2022--acl-long--285",
    "result": [
      {
        "value": {
          "start": 421,
          "end": 453,
          "text": "multi-task encoder-decoder model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--285:E0"
      }
    ],
    "data": {
      "text": "Recent work in cross-lingual semantic parsing has successfully applied machine translation to localize parsers to new languages. However, these advances assume access to high-quality machine translation systems and word alignment tools. We remove these assumptions and study cross-lingual semantic parsing as a zero-shot problem, without parallel data (i.e., utterance-logical form pairs) for new languages. We propose a multi-task encoder-decoder model to transfer parsing knowledge to additional languages using only English-logical form paired data and in-domain natural language corpora in each new language. Our model encourages language-agnostic encodings by jointly optimizing for logical-form generation with auxiliary objectives designed for cross-lingual latent representation alignment. Our parser performs significantly above translation-based baselines and, in some cases, competes with the supervised upper-bound."
    }
  },
  {
    "id": "abstract-2022--acl-long--450",
    "result": [
      {
        "value": {
          "start": 13,
          "end": 19,
          "text": "MemSum",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--450:E0"
      },
      {
        "value": {
          "start": 613,
          "end": 658,
          "text": "state-of-the-art test-set performance (ROUGE)",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--450:E1"
      },
      {
        "value": {
          "start": 870,
          "end": 907,
          "text": "redundancy of the generated summaries",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--450:E2"
      },
      {
        "from_id": "abstract-2022--acl-long--450:E0",
        "to_id": "abstract-2022--acl-long--450:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--450:E0",
        "to_id": "abstract-2022--acl-long--450:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "We introduce MemSum (Multi-step Episodic Markov decision process extractive SUMmarizer), a reinforcement-learning-based extractive summarizer enriched at each step with information on the current extraction history. When MemSum iteratively selects sentences into the summary, it considers a broad information set that would intuitively also be used by humans in this task: 1) the text content of the sentence, 2) the global text context of the rest of the document, and 3) the extraction history consisting of the set of sentences that have already been extracted. With a lightweight architecture, MemSum obtains state-of-the-art test-set performance (ROUGE) in summarizing long documents taken from PubMed, arXiv, and GovReport. Ablation studies demonstrate the importance of local, global, and history information. A human evaluation confirms the high quality and low redundancy of the generated summaries, stemming from MemSum’s awareness of extraction history."
    }
  },
  {
    "id": "abstract-2022--acl-long--518",
    "result": [
      {
        "value": {
          "start": 649,
          "end": 689,
          "text": "junction type embedding or heading delta",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--518:E0"
      },
      {
        "value": {
          "start": 590,
          "end": 624,
          "text": "gain in outdoor VLN on unseen data",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--518:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--518:E0",
        "to_id": "abstract-2022--acl-long--518:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Vision and language navigation (VLN) is a challenging visually-grounded language understanding task. Given a natural language navigation instruction, a visual agent interacts with a graph-based environment equipped with panorama images and tries to follow the described route. Most prior work has been conducted in indoor scenarios where best results were obtained for navigation on routes that are similar to the training routes, with sharp drops in performance when testing on unseen environments. We focus on VLN in outdoor scenarios and find that in contrast to indoor VLN, most of the gain in outdoor VLN on unseen data is due to features like junction type embedding or heading delta that are specific to the respective environment graph, while image information plays a very minor role in generalizing VLN to unseen outdoor areas. These findings show a bias to specifics of graph representations of urban environments, demanding that VLN tasks grow in scale and diversity of geographical environments."
    }
  },
  {
    "id": "abstract-2022--acl-short--10",
    "result": [
      {
        "value": {
          "start": 345,
          "end": 374,
          "text": "neural network based detector",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-short--10:E0"
      },
      {
        "value": {
          "start": 886,
          "end": 894,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-short--10:E1"
      },
      {
        "from_id": "abstract-2022--acl-short--10:E0",
        "to_id": "abstract-2022--acl-short--10:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In this work, we focus on the problem of distinguishing a human written news article from a news article that is created by manipulating entities in a human written news article (e.g., replacing entities with factually incorrect entities). Such manipulated articles can mislead the reader by posing as a human written news article. We propose a neural network based detector that detects manipulated news articles by reasoning about the facts mentioned in the article. Our proposed detector exploits factual knowledge via graph convolutional neural network along with the textual information in the news article. We also create challenging datasets for this task by considering various strategies to generate the new replacement entity (e.g., entity generation from GPT-2). In all the settings, our proposed model either matches or outperforms the state-of-the-art detector in terms of accuracy. Our code and data are available athttps://github.com/UBC-NLP/manipulated_entity_detection."
    }
  },
  {
    "id": "abstract-2022--acl-long--559",
    "result": [
      {
        "value": {
          "start": 439,
          "end": 455,
          "text": "program transfer",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--559:E0"
      }
    ],
    "data": {
      "text": "Program induction for answering complex questions over knowledge bases (KBs) aims to decompose a question into a multi-step program, whose execution against the KB produces the final answer. Learning to induce programs relies on a large number of parallel question-program pairs for the given KB. However, for most KBs, the gold program annotations are usually lacking, making learning difficult. In this paper, we propose the approach of program transfer, which aims to leverage the valuable program annotations on the rich-resourced KBs as external supervision signals to aid program induction for the low-resourced KBs that lack program annotations. For program transfer, we design a novel two-stage parsing framework with an efficient ontology-guided pruning strategy. First, a sketch parser translates the question into a high-level program sketch, which is the composition of functions. Second, given the question and sketch, an argument parser searches the detailed arguments from the KB for functions. During the searching, we incorporate the KB ontology to prune the search space. The experiments on ComplexWebQuestions and WebQuestionSP show that our method outperforms SOTA methods significantly, demonstrating the effectiveness of program transfer and our framework. Our codes and datasets can be obtained fromhttps://github.com/THU-KEG/ProgramTransfer."
    }
  },
  {
    "id": "abstract-2022--acl-long--529",
    "result": [
      {
        "value": {
          "start": 521,
          "end": 568,
          "text": "pre-training of both source and target language",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--529:E0"
      },
      {
        "value": {
          "start": 697,
          "end": 722,
          "text": "cross-lingual performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--529:E1"
      },
      {
        "value": {
          "start": 581,
          "end": 607,
          "text": "matching language families",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--529:E2"
      },
      {
        "from_id": "abstract-2022--acl-long--529:E0",
        "to_id": "abstract-2022--acl-long--529:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--529:E2",
        "to_id": "abstract-2022--acl-long--529:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Cross-lingual transfer learning with large multilingual pre-trained models can be an effective approach for low-resource languages with no labeled training data. Existing evaluations of zero-shot cross-lingual generalisability of large pre-trained models use datasets with English training data, and test data in a selection of target languages. We explore a more extensive transfer learning setup with 65 different source languages and 105 target languages for part-of-speech tagging. Through our analysis, we show that pre-training of both source and target language, as well as matching language families, writing systems, word order systems, and lexical-phonetic distance significantly impact cross-lingual performance. The findings described in this paper can be used as indicators of which factors are important for effective zero-shot cross-lingual transfer to zero- and low-resource languages."
    }
  },
  {
    "id": "abstract-2022--acl-long--409",
    "result": [
      {
        "value": {
          "start": 484,
          "end": 495,
          "text": "SimpDefiner",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--409:E0"
      },
      {
        "value": {
          "start": 1074,
          "end": 1084,
          "text": "SARI score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--409:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--409:E0",
        "to_id": "abstract-2022--acl-long--409:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The definition generation task can help language learners by providing explanations for unfamiliar words. This task has attracted much attention in recent years. We propose a novel task of Simple Definition Generation (SDG) to help language learners and low literacy readers. A significant challenge of this task is the lack of learner’s dictionaries in many languages, and therefore the lack of data for supervised training. We explore this task and propose a multitasking framework SimpDefiner that only requires a standard dictionary with complex definitions and a corpus containing arbitrary simple texts. We disentangle the complexity factors from the text by carefully designing a parameter sharing scheme between two decoders. By jointly training these components, the framework can generate both complex and simple definitions simultaneously. We demonstrate that the framework can generate relevant, simple definitions for the target words through automatic and manual evaluations on English and Chinese datasets. Our method outperforms the baseline model by a 1.77 SARI score on the English dataset, and raises the proportion of the low level (HSK level 1-3) words in Chinese definitions by 3.87%."
    }
  },
  {
    "id": "abstract-2022--acl-long--475",
    "result": [
      {
        "value": {
          "start": 485,
          "end": 488,
          "text": "CQG",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--475:E0"
      },
      {
        "value": {
          "start": 188,
          "end": 199,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--475:E1"
      },
      {
        "value": {
          "start": 984,
          "end": 1007,
          "text": "BLEU points on HotpotQA",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--475:E2"
      },
      {
        "from_id": "abstract-2022--acl-long--475:E0",
        "to_id": "abstract-2022--acl-long--475:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--475:E0",
        "to_id": "abstract-2022--acl-long--475:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Multi-hop question generation focuses on generating complex questions that require reasoning over multiple pieces of information of the input passage. Current models with state-of-the-art performance have been able to generate the correct questions corresponding to the answers. However, most models can not ensure the complexity of generated questions, so they may generate shallow questions that can be answered without multi-hop reasoning. To address this challenge, we propose the CQG, which is a simple and effective controlled framework. CQG employs a simple method to generate the multi-hop questions that contain key entities in multi-hop reasoning chains, which ensure the complexity and quality of the questions. In addition, we introduce a novel controlled Transformer-based decoder to guarantee that key entities appear in the questions. Experiment results show that our model greatly improves performance, which also outperforms the state-of-the-art model about 25% by 5 BLEU points on HotpotQA."
    }
  },
  {
    "id": "abstract-2022--acl-short--59",
    "result": [
      {
        "value": {
          "start": 690,
          "end": 729,
          "text": "simple extension of temperature scaling",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-short--59:E0"
      },
      {
        "value": {
          "start": 330,
          "end": 347,
          "text": "calibration error",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-short--59:E1"
      },
      {
        "from_id": "abstract-2022--acl-short--59:E0",
        "to_id": "abstract-2022--acl-short--59:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Certainty calibration is an important goal on the path to interpretability and trustworthy AI. Particularly in the context of human-in-the-loop systems, high-quality low to mid-range certainty estimates are essential. In the presence of a dominant high-certainty class, for instance the non-entity class in NER problems, existing calibration error measures are completely insensitive to potentially large errors in this certainty region of interest. We introduce a region-balanced calibration error metric that weights all certainty regions equally. When low and mid certainty estimates are taken into account, calibration error is typically larger than previously reported. We introduce a simple extension of temperature scaling, requiring no additional computation, that can reduce both traditional and region-balanced notions of calibration error over existing baselines."
    }
  },
  {
    "id": "abstract-2022--acl-long--474",
    "result": [
      {
        "value": {
          "start": 319,
          "end": 358,
          "text": "length-aware attention mechanism (LAAM)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--474:E0"
      },
      {
        "value": {
          "start": 626,
          "end": 669,
          "text": "high-quality summaries with desired lengths",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--474:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--474:E0",
        "to_id": "abstract-2022--acl-long--474:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Previous length-controllable summarization models mostly control lengths at the decoding stage, whereas the encoding or the selection of information from the source document is not sensitive to the designed length. They also tend to generate summaries as long as those in the training data. In this paper, we propose a length-aware attention mechanism (LAAM) to adapt the encoding of the source based on the desired length. Our approach works by training LAAM on a summary length balanced dataset built from the original training data, and then fine-tuning as usual. Results show that this approach is effective in generating high-quality summaries with desired lengths and even those short lengths never seen in the original training set."
    }
  },
  {
    "id": "abstract-2022--acl-long--542",
    "result": [
      {
        "value": {
          "start": 660,
          "end": 706,
          "text": "adaptive segmentation policy for end-to-end ST",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--542:E0"
      },
      {
        "value": {
          "start": 1044,
          "end": 1070,
          "text": "accuracy-latency trade-off",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--542:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--542:E0",
        "to_id": "abstract-2022--acl-long--542:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "End-to-end simultaneous speech-to-text translation aims to directly perform translation from streaming source speech to target text with high translation quality and low latency. A typical simultaneous translation (ST) system consists of a speech translation model and a policy module, which determines when to wait and when to translate. Thus the policy is crucial to balance translation quality and latency. Conventional methods usually adopt fixed policies, e.g. segmenting the source speech with a fixed length and generating translation. However, this method ignores contextual information and suffers from low translation quality. This paper proposes an adaptive segmentation policy for end-to-end ST. Inspired by human interpreters, the policy learns to segment the source streaming speech into meaningful units by considering both acoustic features and translation history, maintaining consistency between the segmentation and translation. Experimental results on English-German and Chinese-English show that our method achieves a good accuracy-latency trade-off over recently proposed state-of-the-art methods."
    }
  },
  {
    "id": "abstract-2022--acl-long--592",
    "result": [
      {
        "value": {
          "start": 524,
          "end": 530,
          "text": "FlipDA",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--592:E0"
      },
      {
        "value": {
          "start": 834,
          "end": 862,
          "text": "effectiveness and robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--592:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--592:E0",
        "to_id": "abstract-2022--acl-long--592:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Most previous methods for text data augmentation are limited to simple tasks and weak baselines. We explore data augmentation on hard tasks (i.e., few-shot natural language understanding) and strong baselines (i.e., pretrained models with over one billion parameters). Under this setting, we reproduced a large number of previous augmentation methods and found that these methods bring marginal gains at best and sometimes degrade the performance much. To address this challenge, we propose a novel data augmentation method FlipDA that jointly uses a generative model and a classifier to generate label-flipped data. Central to the idea of FlipDA is the discovery that generating label-flipped data is more crucial to the performance than generating label-preserved data. Experiments show that FlipDA achieves a good tradeoff between effectiveness and robustness—it substantially improves many tasks while not negatively affecting the others."
    }
  },
  {
    "id": "abstract-2022--acl-short--38",
    "result": [
      {
        "value": {
          "start": 392,
          "end": 423,
          "text": "simple baseline approach (PARE)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-short--38:E0"
      }
    ],
    "data": {
      "text": "Neural models for distantly supervised relation extraction (DS-RE) encode each sentence in an entity-pair bag separately. These are then aggregated for bag-level relation prediction. Since, at encoding time, these approaches do not allow information to flow from other sentences in the bag, we believe that they do not utilize the available bag data to the fullest. In response, we explore a simple baseline approach (PARE) in which all sentences of a bag are concatenated into a passage of sentences, and encoded jointly using BERT. The contextual embeddings of tokens are aggregated using attention with the candidate relation as query – this summary of whole passage predicts the candidate relation. We find that our simple baseline solution outperforms existing state-of-the-art DS-RE models in both monolingual and multilingual DS-RE datasets."
    }
  },
  {
    "id": "abstract-2022--acl-long--402",
    "result": [
      {
        "value": {
          "start": 985,
          "end": 1021,
          "text": "performance of link prediction tasks",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--402:E0"
      }
    ],
    "data": {
      "text": "Temporal factors are tied to the growth of facts in realistic applications, such as the progress of diseases and the development of political situation, therefore, research on Temporal Knowledge Graph (TKG) attracks much attention. In TKG, relation patterns inherent with temporality are required to be studied for representation learning and reasoning across temporal facts. However, existing methods can hardly model temporal relation patterns, nor can capture the intrinsic connections between relations when evolving over time, lacking of interpretability. In this paper, we propose a novel temporal modeling method which represents temporal entities as Rotations in Quaternion Vector Space (RotateQVS) and relations as complex vectors in Hamilton’s quaternion space. We demonstrate our method can model key patterns of relations in TKG, such as symmetry, asymmetry, inverse, and can capture time-evolved relations by theory. And empirically, we show that our method can boost the performance of link prediction tasks over four temporal knowledge graph benchmarks."
    }
  },
  {
    "id": "abstract-2022--acl-long--327",
    "result": [
      {
        "value": {
          "start": 393,
          "end": 437,
          "text": "Unsupervised Dependency Graph Network (UDGN)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--327:E0"
      },
      {
        "value": {
          "start": 588,
          "end": 631,
          "text": "unsupervised dependency parsing performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--327:E1"
      },
      {
        "value": {
          "start": 432,
          "end": 436,
          "text": "UDGN",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--327:E2"
      },
      {
        "from_id": "abstract-2022--acl-long--327:E0",
        "to_id": "abstract-2022--acl-long--327:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recent work has identified properties of pretrained self-attention models that mirror those of dependency parse structures. In particular, some self-attention heads correspond well to individual dependency types. Inspired by these developments, we propose a new competitive mechanism that encourages these attention heads to model different dependency relations. We introduce a new model, the Unsupervised Dependency Graph Network (UDGN), that can induce dependency structures from raw corpora and the masked language modeling task. Experiment results show that UDGN achieves very strong unsupervised dependency parsing performance without gold POS tags and any other external information. The competitive gated heads show a strong correlation with human-annotated dependency types. Furthermore, the UDGN can also achieve competitive performance on masked language modeling and sentence textual similarity tasks."
    }
  },
  {
    "id": "abstract-2022--acl-long--89",
    "result": [
      {
        "value": {
          "start": 27,
          "end": 52,
          "text": "flow-adapter architecture",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--89:E0"
      }
    ],
    "data": {
      "text": "In this work, we propose a flow-adapter architecture for unsupervised NMT. It leverages normalizing flows to explicitly model the distributions of sentence-level latent representations, which are subsequently used in conjunction with the attention mechanism for the translation task. The primary novelties of our model are: (a) capturing language-specific sentence representations separately for each language using normalizing flows and (b) using a simple transformation of these latent representations for translating from one language to another. This architecture allows for unsupervised training of each language independently. While there is prior work on latent variables for supervised MT, to the best of our knowledge, this is the first work that uses latent variables and normalizing flows for unsupervised MT. We obtain competitive results on several unsupervised MT benchmarks."
    }
  },
  {
    "id": "abstract-2022--acl-long--72",
    "result": [
      {
        "value": {
          "start": 11,
          "end": 17,
          "text": "biases",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--72:E0"
      },
      {
        "value": {
          "start": 1114,
          "end": 1154,
          "text": "language models’ understanding abilities",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--72:E1"
      }
    ],
    "data": {
      "text": "Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. Different from previous debiasing work that uses external corpora to fine-tune the pretrained models, we instead directly probe the biases encoded in pretrained models through prompts. Specifically, we propose a variant of the beam search method to automatically search forbiased promptssuch that the cloze-style completions are the most different with respect to different demographic groups. Given the identified biased prompts, we then propose a distribution alignment loss to mitigate the biases. Experiment results on standard datasets and metrics show that our proposedAuto-Debiasapproach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT. Moreover, the improvement in fairness does not decrease the language models’ understanding abilities, as shown using the GLUE benchmark."
    }
  },
  {
    "id": "abstract-2022--acl-short--22",
    "result": [
      {
        "value": {
          "start": 312,
          "end": 350,
          "text": "jointly distill and quantize the model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-short--22:E0"
      },
      {
        "value": {
          "start": 611,
          "end": 628,
          "text": "compression ratio",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-short--22:E1"
      },
      {
        "value": {
          "start": 94,
          "end": 105,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-short--22:E2"
      },
      {
        "value": {
          "start": 821,
          "end": 853,
          "text": "performance-efficiency trade-off",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-short--22:E3"
      },
      {
        "from_id": "abstract-2022--acl-short--22:E0",
        "to_id": "abstract-2022--acl-short--22:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-short--22:E0",
        "to_id": "abstract-2022--acl-short--22:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2022--acl-short--22:E0",
        "to_id": "abstract-2022--acl-short--22:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve state-of-the-art performance on many generative NLP tasks. However, such models pose a great challenge in resource-constrained scenarios owing to their large memory requirements and high latency. To alleviate this issue, we propose to jointly distill and quantize the model, where knowledge is transferred from the full-precision teacher model to the quantized and distilled low-precision student model. Empirical analyses show that, despite the challenging nature of generative tasks, we were able to achieve a 16.5x model footprint compression ratio with little performance drop relative to the full-precision counterparts on multiple summarization and QA datasets. We further pushed the limit of compression ratio to 27.7x and presented the performance-efficiency trade-off for generative tasks using pre-trained models. To the best of our knowledge, this is the first work aiming to effectively distill and quantize sequence-to-sequence pre-trained models for language generation tasks."
    }
  },
  {
    "id": "abstract-2022--acl-long--59",
    "result": [
      {
        "value": {
          "start": 757,
          "end": 812,
          "text": "Intra- and Inter-entity Deconfounding Data Augmentation",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--59:E0"
      },
      {
        "value": {
          "start": 953,
          "end": 992,
          "text": "performance of the generative NER model",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--59:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--59:E0",
        "to_id": "abstract-2022--acl-long--59:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Named entity recognition (NER) is a fundamental task to recognize specific types of entities from a given sentence. Depending on how the entities appear in the sentence, it can be divided into three subtasks, namely, Flat NER, Nested NER, and Discontinuous NER. Among the existing approaches, only the generative model can be uniformly adapted to these three subtasks. However, when the generative model is applied to NER, its optimization objective is not consistent with the task, which makes the model vulnerable to the incorrect biases. In this paper, we analyze the incorrect biases in the generation process from a causality perspective and attribute them to two confounders: pre-context confounder and entity-order confounder. Furthermore, we design Intra- and Inter-entity Deconfounding Data Augmentation methods to eliminate the above confounders according to the theory of backdoor adjustment. Experiments show that our method can improve the performance of the generative NER model in various datasets."
    }
  },
  {
    "id": "abstract-2022--acl-long--413",
    "result": [
      {
        "value": {
          "start": 279,
          "end": 290,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--413:E0"
      }
    ],
    "data": {
      "text": "Recent years have witnessed growing interests in incorporating external knowledge such as pre-trained word embeddings (PWEs) or pre-trained language models (PLMs) into neural topic modeling. However, we found that employing PWEs and PLMs for topic modeling only achieved limited performance improvements but with huge computational overhead. In this paper, we propose a novel strategy to incorporate external knowledge into neural topic modeling where the neural topic model is pre-trained on a large corpus and then fine-tuned on the target dataset. Experiments have been conducted on three datasets and results show that the proposed approach significantly outperforms both current state-of-the-art neural topic models and some topic modeling approaches enhanced with PWEs or PLMs. Moreover, further study shows that the proposed approach greatly reduces the need for the huge size of training data."
    }
  },
  {
    "id": "abstract-2022--acl-long--260",
    "result": [
      {
        "value": {
          "start": 262,
          "end": 269,
          "text": "FormNet",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--260:E0"
      }
    ],
    "data": {
      "text": "Sequence modeling has demonstrated state-of-the-art performance on natural language and document understanding tasks. However, it is challenging to correctly serialize tokens in form-like documents in practice due to their variety of layout patterns. We propose FormNet, a structure-aware sequence model to mitigate the suboptimal serialization of forms. First, we design Rich Attention that leverages the spatial relationship between tokens in a form for more precise attention score calculation. Second, we construct Super-Tokens for each word by embedding representations from their neighboring tokens through graph convolutions. FormNet therefore explicitly recovers local syntactic information that may have been lost during serialization. In experiments, FormNet outperforms existing methods with a more compact model size and less pre-training data, establishing new state-of-the-art performance on CORD, FUNSD and Payment benchmarks."
    }
  },
  {
    "id": "abstract-2022--acl-long--561",
    "result": [
      {
        "value": {
          "start": 483,
          "end": 497,
          "text": "Context-to-Vec",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--561:E0"
      },
      {
        "value": {
          "start": 514,
          "end": 549,
          "text": "post-processing retrofitting method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--561:E1"
      }
    ],
    "data": {
      "text": "Although contextualized embeddings generated from large-scale pre-trained models perform well in many tasks, traditional static embeddings (e.g., Skip-gram, Word2Vec) still play an important role in low-resource and lightweight settings due to their low computational cost, ease of deployment, and stability. In this paper, we aim to improve word embeddings by 1) incorporating more contextual information from existing pre-trained models into the Skip-gram framework, which we call Context-to-Vec; 2) proposing a post-processing retrofitting method for static embeddings independent of training by employing priori synonym knowledge and weighted vector distribution. Through extrinsic and intrinsic tasks, our methods are well proven to outperform the baselines by a large margin."
    }
  },
  {
    "id": "abstract-2022--acl-long--204",
    "result": [
      {
        "value": {
          "start": 790,
          "end": 796,
          "text": "Divter",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--204:E0"
      },
      {
        "value": {
          "start": 1171,
          "end": 1195,
          "text": "state-of-the-art results",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--204:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--204:E0",
        "to_id": "abstract-2022--acl-long--204:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Responsing with image has been recognized as an important capability for an intelligent conversational agent. Yet existing works only focus on exploring the multimodal dialogue models which depend on retrieval-based methods, but neglecting generation methods. To fill in the gaps, we first present a new task: multimodal dialogue response generation (MDRG) - given the dialogue history, one model needs to generate a text sequence or an image as response. Learning such a MDRG model often requires multimodal dialogues containing both texts and images which are difficult to obtain. Motivated by the challenge in practice, we consider MDRG under a natural assumption that only limited training examples are available. In such a low-resource setting, we devise a novel conversational agent, Divter, in order to isolate parameters that depend on multimodal dialogues from the entire generation model. By this means, the major part of the model can be learned from a large number of text-only dialogues and text-image pairs respectively, then the whole parameters can be well fitted using the limited training examples. Extensive experiments demonstrate our method achieves state-of-the-art results in both automatic and human evaluation, and can generate informative text and high-resolution image responses."
    }
  },
  {
    "id": "abstract-2022--acl-short--14",
    "result": [
      {
        "value": {
          "start": 139,
          "end": 181,
          "text": "novel approach for generating explanations",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-short--14:E0"
      },
      {
        "value": {
          "start": 705,
          "end": 721,
          "text": "user performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-short--14:E1"
      },
      {
        "from_id": "abstract-2022--acl-short--14:E0",
        "to_id": "abstract-2022--acl-short--14:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "A key challenge facing natural language interfaces is enabling users to understand the capabilities of the underlying system. We propose a novel approach for generating explanations of a natural language interface based on semantic parsing. We focus on counterfactual explanations, which are post-hoc explanations that describe to the user how they could have minimally modified their utterance to achieve their desired goal. In particular, the user provides an utterance along with a demonstration of their desired goal; then, our algorithm synthesizes a paraphrase of their utterance that is guaranteed to achieve their goal. In two user studies, we demonstrate that our approach substantially improves user performance, and that it generates explanations that more closely match the user’s intent compared to two ablations."
    }
  },
  {
    "id": "abstract-2022--acl-demo--1",
    "result": [
      {
        "value": {
          "start": 11,
          "end": 16,
          "text": "DoTAT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-demo--1:E0"
      },
      {
        "value": {
          "start": 292,
          "end": 311,
          "text": "annotation accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-demo--1:E1"
      },
      {
        "value": {
          "start": 598,
          "end": 619,
          "text": "annotation efficiency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-demo--1:E2"
      },
      {
        "value": {
          "start": 687,
          "end": 708,
          "text": "event annotation time",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-demo--1:E3"
      },
      {
        "from_id": "abstract-2022--acl-demo--1:E0",
        "to_id": "abstract-2022--acl-demo--1:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-demo--1:E0",
        "to_id": "abstract-2022--acl-demo--1:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-demo--1:E0",
        "to_id": "abstract-2022--acl-demo--1:E3",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "We propose DoTAT, a domain-oriented text annotation tool. The tool designs and implements functions heavily in need in domain-oriented information extraction. Firstly, the tool supports a multi-person collaborative process with automatically merging and review, which can greatly improve the annotation accuracy. Secondly, the tool provides annotation of events, nested event and nested entity, which are frequently required in domain-related text structuring tasks. Finally, DoTAT provides visual annotation specification definition, automatic batch annotation and iterative annotation to improve annotation efficiency. Experiments on the ACE2005 dataset show that DoTAT can reduce the event annotation time by 19.7% compared with existing annotation tools. The accuracy without review is 84.09%, 1.35% higher than Brat and 2.59% higher than Webanno. The accuracy of DoTAT even reaches 93.76% with review. The demonstration video can be accessed fromhttps://ecust-nlp-docker.oss-cn-shanghai.aliyuncs.com/dotat_demo.mp4. A live demo website is available athttps://github.com/FXLP/MarkTool."
    }
  },
  {
    "id": "abstract-2022--acl-long--123",
    "result": [
      {
        "value": {
          "start": 495,
          "end": 528,
          "text": "named entity translation accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--123:E0"
      },
      {
        "value": {
          "start": 957,
          "end": 979,
          "text": "entity accuracy points",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--123:E1"
      }
    ],
    "data": {
      "text": "It has been shown that machine translation models usually generate poor translations for named entities that are infrequent in the training corpus. Earlier named entity translation methods mainly focus on phonetic transliteration, which ignores the sentence context for translation and is limited in domain and language coverage. To address this limitation, we propose DEEP, a DEnoising Entity Pre-training method that leverages large amounts of monolingual data and a knowledge base to improve named entity translation accuracy within sentences. Besides, we investigate a multi-task learning strategy that finetunes a pre-trained neural machine translation model on both entity-augmented monolingual data and parallel data to further improve entity translation. Experimental results on three language pairs demonstrate that DEEP results in significant improvements over strong denoising auto-encoding baselines, with a gain of up to 1.3 BLEU and up to 9.2 entity accuracy points for English-Russian translation."
    }
  },
  {
    "id": "abstract-2022--acl-long--484",
    "result": [
      {
        "value": {
          "start": 561,
          "end": 584,
          "text": "spoiler type classifier",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--484:E0"
      },
      {
        "value": {
          "start": 597,
          "end": 605,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--484:E1"
      },
      {
        "value": {
          "start": 624,
          "end": 662,
          "text": "question answering model DeBERTa-large",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--484:E2"
      },
      {
        "value": {
          "start": 689,
          "end": 708,
          "text": "generating spoilers",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--484:E3"
      },
      {
        "from_id": "abstract-2022--acl-long--484:E0",
        "to_id": "abstract-2022--acl-long--484:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--484:E2",
        "to_id": "abstract-2022--acl-long--484:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We introduce and study the task of clickbait spoiling: generating a short text that satisfies the curiosity induced by a clickbait post. Clickbait links to a web page and advertises its contents by arousing curiosity instead of providing an informative summary. Our contributions are approaches to classify the type of spoiler needed (i.e., a phrase or a passage), and to generate appropriate spoilers. A large-scale evaluation and error analysis on a new corpus of 5,000 manually spoiled clickbait posts—the Webis Clickbait Spoiling Corpus 2022—shows that our spoiler type classifier achieves an accuracy of 80%, while the question answering model DeBERTa-large outperforms all others in generating spoilers for both types."
    }
  },
  {
    "id": "abstract-2022--acl-long--360",
    "result": [
      {
        "value": {
          "start": 13,
          "end": 20,
          "text": "PRIMERA",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--360:E0"
      }
    ],
    "data": {
      "text": "We introduce PRIMERA, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data. PRIMERA uses our newly proposed pre-training objective designed to teach the model to connect and aggregate information across documents. It also uses efficient encoder-decoder transformers to simplify the processing of concatenated input documents. With extensive experiments on 6 multi-document summarization datasets from 3 different domains on zero-shot, few-shot and full-supervised settings, PRIMERA outperforms current state-of-the-art dataset-specific and pre-trained models on most of these settings with large margins."
    }
  },
  {
    "id": "abstract-2022--acl-long--197",
    "result": [
      {
        "value": {
          "start": 705,
          "end": 738,
          "text": "FewVLM with prompt-based learning",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--197:E0"
      },
      {
        "value": {
          "start": 639,
          "end": 646,
          "text": "prompts",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--197:E1"
      },
      {
        "value": {
          "start": 932,
          "end": 953,
          "text": "zero-shot performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--197:E2"
      },
      {
        "value": {
          "start": 976,
          "end": 996,
          "text": "few-shot performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--197:E3"
      },
      {
        "value": {
          "start": 583,
          "end": 591,
          "text": "MaskedLM",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--197:E4"
      },
      {
        "value": {
          "start": 1120,
          "end": 1129,
          "text": "VQA tasks",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--197:E5"
      },
      {
        "value": {
          "start": 543,
          "end": 551,
          "text": "PrefixLM",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--197:E6"
      },
      {
        "value": {
          "start": 1152,
          "end": 1174,
          "text": "captioning performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--197:E7"
      },
      {
        "from_id": "abstract-2022--acl-long--197:E1",
        "to_id": "abstract-2022--acl-long--197:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--197:E1",
        "to_id": "abstract-2022--acl-long--197:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--197:E4",
        "to_id": "abstract-2022--acl-long--197:E5",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--197:E6",
        "to_id": "abstract-2022--acl-long--197:E7",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Large pre-trained vision-language (VL) models can learn a new task with a handful of examples and generalize to a new task without fine-tuning. However, these VL models are hard to deploy for real-world applications due to their impractically huge sizes and slow inference speed. To solve this limitation, we study prompt-based low-resource learning of VL tasks with our proposed method, FewVLM, relatively smaller than recent few-shot learners. For FewVLM, we pre-train a sequence-to-sequence transformer model with prefix language modeling (PrefixLM) and masked language modeling (MaskedLM).Furthermore, we analyze the effect of diverse prompts for few-shot tasks. Experimental results on VQA show that FewVLM with prompt-based learning outperforms Frozen which is 31x larger than FewVLM by 18.2% point and achieves comparable results to a 246x larger model, PICa.In our analysis, we observe that (1) prompts significantly affect zero-shot performance but marginally affect few-shot performance, (2) models with noisy prompts learn as quickly as hand-crafted prompts given larger training data, and (3) MaskedLM helps VQA tasks while PrefixLM boosts captioning performance. Our code is publicly available athttps://github.com/woojeongjin/FewVLM"
    }
  },
  {
    "id": "abstract-2022--acl-long--430",
    "result": [
      {
        "value": {
          "start": 567,
          "end": 581,
          "text": "OIE@OIA system",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--430:E0"
      },
      {
        "value": {
          "start": 875,
          "end": 892,
          "text": "SOTA performances",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--430:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--430:E0",
        "to_id": "abstract-2022--acl-long--430:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Different Open Information Extraction (OIE) tasks require different types of information, so the OIE field requires strong adaptability of OIE algorithms to meet different task requirements. This paper discusses the adaptability problem in existing OIE systems and designs a new adaptable and efficient OIE system - OIE@OIA as a solution. OIE@OIA follows the methodology of Open Information eXpression (OIX): parsing a sentence to an Open Information Annotation (OIA) Graph and then adapting the OIA graph to different OIE tasks with simple rules. As the core of our OIE@OIA system, we implement an end-to-end OIA generator by annotating a dataset (we make it open available) and designing an efficient learning algorithm for the complex OIA graph. We easily adapt the OIE@OIA system to accomplish three popular OIE tasks. The experimental show that our OIE@OIA achieves new SOTA performances on these tasks, showing the great adaptability of our OIE@OIA system. Furthermore, compared to other end-to-end OIE baselines that need millions of samples for training, our OIE@OIA needs much fewer training samples (12K), showing a significant advantage in terms of efficiency."
    }
  },
  {
    "id": "abstract-2022--acl-demo--22",
    "result": [
      {
        "value": {
          "start": 659,
          "end": 708,
          "text": "model quantization and parameter-efficient tuning",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-demo--22:E0"
      },
      {
        "value": {
          "start": 713,
          "end": 749,
          "text": "efficient model inference and tuning",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-demo--22:E1"
      },
      {
        "value": {
          "start": 789,
          "end": 863,
          "text": "model offloading, model checkpointing, and CPU-GPU scheduling optimization",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-demo--22:E2"
      },
      {
        "from_id": "abstract-2022--acl-demo--22:E0",
        "to_id": "abstract-2022--acl-demo--22:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In recent years, large-scale pre-trained language models (PLMs) containing billions of parameters have achieved promising results on various NLP tasks. Although we can pre-train these big models by stacking computing clusters at any cost, it is impractical to use such huge computing resources to apply big models for each downstream task. To address the computation bottleneck encountered in deploying big models in real-world scenarios, we introduce an open-source toolkit for big model inference and tuning (BMInf), which can support big model inference and tuning at extremely low computation cost. More specifically, at the algorithm level, we introduce model quantization and parameter-efficient tuning for efficient model inference and tuning. At the implementation level, we apply model offloading, model checkpointing, and CPU-GPU scheduling optimization to further reduce the computation and memory cost of big models. Based on above efforts, we can efficiently perform big model inference and tuning with a single GPU (even a consumer-level GPU like GTX 1060) instead of computing clusters, which is difficult for existing distributed learning toolkits for PLMs. BMInf is publicly released athttps://github.com/OpenBMB/BMInf."
    }
  },
  {
    "id": "abstract-2022--acl-long--400",
    "result": [
      {
        "value": {
          "start": 883,
          "end": 896,
          "text": "full pipeline",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--400:E0"
      },
      {
        "value": {
          "start": 970,
          "end": 978,
          "text": "F1-score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--400:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--400:E0",
        "to_id": "abstract-2022--acl-long--400:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We present a complete pipeline to extract characters in a novel and link them to their direct-speech utterances. Our model is divided into three independent components: extracting direct-speech, compiling a list of characters, and attributing those characters to their utterances. Although we find that existing systems can perform the first two tasks accurately, attributing characters to direct speech is a challenging problem due to the narrator’s lack of explicit character mentions, and the frequent use of nominal and pronominal coreference when such explicit mentions are made. We adapt the progress made on Dialogue State Tracking to tackle a new problem: attributing speakers to dialogues. This is the first application of deep learning to speaker attribution, and it shows that is possible to overcome the need for the hand-crafted features and rules used in the past. Our full pipeline improves the performance of state-of-the-art models by a relative 50% in F1-score."
    }
  },
  {
    "id": "abstract-2022--acl-long--485",
    "result": [
      {
        "value": {
          "start": 54,
          "end": 64,
          "text": "MetaDistil",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--485:E0"
      },
      {
        "value": {
          "start": 345,
          "end": 389,
          "text": "performance of the distilled student network",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--485:E1"
      },
      {
        "value": {
          "start": 445,
          "end": 467,
          "text": "pilot update mechanism",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--485:E2"
      },
      {
        "value": {
          "start": 483,
          "end": 535,
          "text": "alignment between the inner-learner and meta-learner",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--485:E3"
      },
      {
        "from_id": "abstract-2022--acl-long--485:E0",
        "to_id": "abstract-2022--acl-long--485:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--485:E2",
        "to_id": "abstract-2022--acl-long--485:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We present Knowledge Distillation with Meta Learning (MetaDistil), a simple yet effective alternative to traditional knowledge distillation (KD) methods where the teacher model is fixed during training. We show the teacher network can learn to better transfer knowledge to the student network (i.e.,learning to teach) with the feedback from the performance of the distilled student network in a meta learning framework. Moreover, we introduce a pilot update mechanism to improve the alignment between the inner-learner and meta-learner in meta learning algorithms that focus on an improved inner-learner. Experiments on various benchmarks show that MetaDistil can yield significant improvements compared with traditional KD algorithms and is less sensitive to the choice of different student capacity and hyperparameters, facilitating the use of KD on different tasks and models."
    }
  },
  {
    "id": "abstract-2022--acl-long--510",
    "result": [
      {
        "value": {
          "start": 407,
          "end": 423,
          "text": "weak supervision",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--510:E0"
      },
      {
        "value": {
          "start": 667,
          "end": 687,
          "text": "segmentation quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--510:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--510:E0",
        "to_id": "abstract-2022--acl-long--510:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Word and morpheme segmentation are fundamental steps of language documentation as they allow to discover lexical units in a language for which the lexicon is unknown. However, in most language documentation scenarios, linguists do not start from a blank page: they may already have a pre-existing dictionary or have initiated manual segmentation of a small part of their data. This paper studies how such a weak supervision can be taken advantage of in Bayesian non-parametric models of segmentation. Our experiments on two very low resource languages (Mboshi and Japhug), whose documentation is still in progress, show that weak supervision can be beneficial to the segmentation quality. In addition, we investigate an incremental learning scenario where manual segmentations are provided in a sequential manner. This work opens the way for interactive annotation tools for documentary linguists."
    }
  },
  {
    "id": "abstract-2022--acl-long--445",
    "result": [
      {
        "value": {
          "start": 662,
          "end": 678,
          "text": "human evaluation",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--445:E0"
      },
      {
        "value": {
          "start": 1338,
          "end": 1346,
          "text": "personas",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--445:E1"
      },
      {
        "value": {
          "start": 1391,
          "end": 1411,
          "text": "conversation quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--445:E2"
      },
      {
        "from_id": "abstract-2022--acl-long--445:E1",
        "to_id": "abstract-2022--acl-long--445:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Evaluation of open-domain dialogue systems is highly challenging and development of better techniques is highlighted time and again as desperately needed. Despite substantial efforts to carry out reliable live evaluation of systems in recent competitions, annotations have been abandoned and reported as too unreliable to yield sensible results. This is a serious problem since automatic metrics are not known to provide a good indication of what may or may not be a high-quality conversation. Answering the distress call of competitions that have emphasized the urgent need for better evaluation techniques in dialogue, we present the successful development of human evaluation that is highly reliable while still remaining feasible and low cost. Self-replication experiments reveal almost perfectly repeatable results with a correlation ofr=0.969. Furthermore, due to the lack of appropriate methods of statistical significance testing, the likelihood of potential improvements to systems occurring due to chance is rarely taken into account in dialogue evaluation, and the evaluation we propose facilitates application of standard tests. Since we have developed a highly reliable evaluation method, new insights into system performance can be revealed. We therefore include a comparison of state-of-the-art models (i) with and without personas, to measure the contribution of personas to conversation quality, as well as (ii) prescribed versus freely chosen topics. Interestingly with respect to personas, results indicate that personas do not positively contribute to conversation quality as expected."
    }
  },
  {
    "id": "abstract-2022--acl-long--205",
    "result": [
      {
        "value": {
          "start": 969,
          "end": 996,
          "text": "commonsense-aware NS module",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--205:E0"
      },
      {
        "value": {
          "start": 1018,
          "end": 1031,
          "text": "NS techniques",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--205:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--205:E0",
        "to_id": "abstract-2022--acl-long--205:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Knowledge graphs store a large number of factual triples while they are still incomplete, inevitably. The previous knowledge graph completion (KGC) models predict missing links between entities merely relying on fact-view data, ignoring the valuable commonsense knowledge. The previous knowledge graph embedding (KGE) techniques suffer from invalid negative sampling and the uncertainty of fact-view link prediction, limiting KGC’s performance. To address the above challenges, we propose a novel and scalable Commonsense-Aware Knowledge Embedding (CAKE) framework to automatically extract commonsense from factual triples with entity concepts. The generated commonsense augments effective self-supervision to facilitate both high-quality negative sampling (NS) and joint commonsense and fact-view link prediction. Experimental results on the KGC task demonstrate that assembling our framework could enhance the performance of the original KGE models, and the proposed commonsense-aware NS module is superior to other NS techniques. Besides, our proposed framework could be easily adaptive to various KGE models and explain the predicted results."
    }
  },
  {
    "id": "abstract-2022--acl-long--157",
    "result": [
      {
        "value": {
          "start": 906,
          "end": 921,
          "text": "proposed method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--157:E0"
      },
      {
        "value": {
          "start": 944,
          "end": 966,
          "text": "adversarial robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--157:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--157:E0",
        "to_id": "abstract-2022--acl-long--157:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recent works on Lottery Ticket Hypothesis have shown that pre-trained language models (PLMs) contain smaller matching subnetworks(winning tickets) which are capable of reaching accuracy comparable to the original models. However, these tickets are proved to be notrobust to adversarial examples, and even worse than their PLM counterparts. To address this problem, we propose a novel method based on learning binary weight masks to identify robust tickets hidden in the original PLMs. Since the loss is not differentiable for the binary mask, we assign the hard concrete distribution to the masks and encourage their sparsity using a smoothing approximation of L0 regularization. Furthermore, we design an adversarial loss objective to guide the search for robust tickets and ensure that the tickets perform well bothin accuracy and robustness. Experimental results show the significant improvement of the proposed method over previous work on adversarial robustness evaluation."
    }
  },
  {
    "id": "abstract-2022--acl-long--295",
    "result": [
      {
        "value": {
          "start": 731,
          "end": 737,
          "text": "SimKGC",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--295:E0"
      }
    ],
    "data": {
      "text": "Knowledge graph completion (KGC) aims to reason over known facts and infer the missing links. Text-based methods such as KGBERT (Yao et al., 2019) learn entity representations from natural language descriptions, and have the potential for inductive KGC. However, the performance of text-based methods still largely lag behind graph embedding-based methods like TransE (Bordes et al., 2013) and RotatE (Sun et al., 2019b). In this paper, we identify that the key issue is efficient contrastive learning. To improve the learning efficiency, we introduce three types of negatives: in-batch negatives, pre-batch negatives, and self-negatives which act as a simple form of hard negatives. Combined with InfoNCE loss, our proposed model SimKGC can substantially outperform embedding-based methods on several benchmark datasets. In terms of mean reciprocal rank (MRR), we advance the state-of-the-art by +19% on WN18RR, +6.8% on the Wikidata5M transductive setting, and +22% on the Wikidata5M inductive setting. Thorough analyses are conducted to gain insights into each component. Our code is available athttps://github.com/intfloat/SimKGC."
    }
  },
  {
    "id": "abstract-2022--acl-long--69",
    "result": [
      {
        "value": {
          "start": 362,
          "end": 369,
          "text": "PathFid",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--69:E0"
      },
      {
        "value": {
          "start": 935,
          "end": 952,
          "text": "performance gains",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--69:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--69:E0",
        "to_id": "abstract-2022--acl-long--69:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Fusion-in-decoder (Fid) (Izacard and Grave, 2020) is a generative question answering (QA) model that leverages passage retrieval with a pre-trained transformer and pushed the state of the art on single-hop QA. However, the complexity of multi-hop QA hinders the effectiveness of the generative QA approach. In this work, we propose a simple generative approach (PathFid) that extends the task beyond just answer generation by explicitly modeling the reasoning process to resolve the answer for multi-hop questions. By linearizing the hierarchical reasoning path of supporting passages, their key sentences, and finally the factoid answer, we cast the problem as a single sequence prediction task. To facilitate complex reasoning with multiple clues, we further extend the unified flat representation of multiple input documents by encoding cross-passage interactions. Our extensive experiments demonstrate that PathFid leads to strong performance gains on two multi-hop QA datasets: HotpotQA and IIRC. Besides the performance gains, PathFid is more interpretable, which in turn yields answers that are more faithfully grounded to the supporting passages and facts compared to the baseline Fid model."
    }
  },
  {
    "id": "abstract-2022--acl-long--181",
    "result": [
      {
        "value": {
          "start": 327,
          "end": 332,
          "text": "CoSHC",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--181:E0"
      },
      {
        "value": {
          "start": 691,
          "end": 705,
          "text": "retrieval time",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--181:E1"
      },
      {
        "value": {
          "start": 216,
          "end": 234,
          "text": "retrieval accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--181:E2"
      },
      {
        "from_id": "abstract-2022--acl-long--181:E0",
        "to_id": "abstract-2022--acl-long--181:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--181:E0",
        "to_id": "abstract-2022--acl-long--181:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Code search is to search reusable code snippets from source code corpus based on natural languages queries. Deep learning-based methods on code search have shown promising results. However, previous methods focus on retrieval accuracy, but lacked attention to the efficiency of the retrieval process. We propose a novel method CoSHC to accelerate code search with deep hashing and code classification, aiming to perform efficient code search without sacrificing too much accuracy. To evaluate the effectiveness of CoSHC, we apply our methodon five code search models. Extensive experimental results indicate that compared with previous code search baselines, CoSHC can save more than 90% of retrieval time meanwhile preserving at least 99% of retrieval accuracy."
    }
  },
  {
    "id": "abstract-2022--acl-long--405",
    "result": [
      {
        "value": {
          "start": 381,
          "end": 445,
          "text": "EA Decoding Algorithm via Third-order Tensor Isomorphism (DATTI)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--405:E0"
      },
      {
        "value": {
          "start": 852,
          "end": 863,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--405:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--405:E0",
        "to_id": "abstract-2022--acl-long--405:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Entity alignment (EA) aims to discover the equivalent entity pairs between KGs, which is a crucial step for integrating multi-source KGs.For a long time, most researchers have regarded EA as a pure graph representation learning task and focused on improving graph encoders while paying little attention to the decoding process. In this paper, we propose an effective and efficient EA Decoding Algorithm via Third-order Tensor Isomorphism (DATTI).Specifically, we derive two sets of isomorphism equations: (1) Adjacency tensor isomorphism equations and (2) Gramian tensor isomorphism equations. By combining these equations, DATTI could effectively utilize the adjacency and inner correlation isomorphisms of KGs to enhance the decoding process of EA.Extensive experiments on public datasets indicate that our decoding algorithm can deliver significant performance improvements even on the most advanced EA methods, while the extra required time is less than 3 seconds."
    }
  },
  {
    "id": "abstract-2022--acl-long--226",
    "result": [
      {
        "value": {
          "start": 267,
          "end": 272,
          "text": "REINA",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--226:E0"
      },
      {
        "value": {
          "start": 787,
          "end": 802,
          "text": "results on XSum",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--226:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--226:E0",
        "to_id": "abstract-2022--acl-long--226:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Retrieval-based methods have been shown to be effective in NLP tasks via introducing external knowledge. However, the indexing and retrieving of large-scale corpora bring considerable computational cost. Surprisingly, we found that REtrieving from the traINing datA (REINA) only can lead to significant gains on multiple NLG and NLU tasks. We retrieve the labeled training instances most similar to the input text and then concatenate them with the input to feed into the model to generate the output. Experimental results show that this simple method can achieve significantly better performance on a variety of NLU and NLG tasks, including summarization, machine translation, language modeling, and question answering tasks. For instance, our proposed method achieved state-of-the-art results on XSum, BigPatent, and CommonsenseQA. Our code is released,https://github.com/microsoft/REINA."
    }
  },
  {
    "id": "abstract-2022--acl-long--150",
    "result": [
      {
        "value": {
          "start": 968,
          "end": 979,
          "text": "pretraining",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--150:E0"
      },
      {
        "value": {
          "start": 1277,
          "end": 1300,
          "text": "fine-tuning performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--150:E1"
      },
      {
        "value": {
          "start": 1351,
          "end": 1372,
          "text": "crosslingual transfer",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--150:E2"
      },
      {
        "from_id": "abstract-2022--acl-long--150:E0",
        "to_id": "abstract-2022--acl-long--150:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--150:E0",
        "to_id": "abstract-2022--acl-long--150:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "AI technologies for Natural Languages have made tremendous progress recently. However, commensurate progress has not been made on Sign Languages, in particular, in recognizing signs as individual words or as complete sentences. We introduce OpenHands, a library where we take four key ideas from the NLP community for low-resource languages and apply them to sign languages for word-level recognition. First, we propose using pose extracted through pretrained models as the standard modality of data in this work to reduce training time and enable efficient inference, and we release standardized pose datasets for different existing sign language datasets. Second, we train and release checkpoints of 4 pose-based isolated sign language recognition models across 6 languages (American, Argentinian, Chinese, Greek, Indian, and Turkish), providing baselines and ready checkpoints for deployment. Third, to address the lack of labelled data, we propose self-supervised pretraining on unlabelled data. We curate and release the largest pose-based pretraining dataset on Indian Sign Language (Indian-SL). Fourth, we compare different pretraining strategies and for the first time establish that pretraining is effective for sign language recognition by demonstrating (a) improved fine-tuning performance especially in low-resource settings, and (b) high crosslingual transfer from Indian-SL to few other sign languages. We open-source all models and datasets in OpenHands with a hope that it makes research in sign languages reproducible and more accessible."
    }
  },
  {
    "id": "abstract-2022--acl-long--87",
    "result": [
      {
        "value": {
          "start": 428,
          "end": 437,
          "text": "LexSubCon",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--87:E0"
      },
      {
        "value": {
          "start": 1280,
          "end": 1308,
          "text": "lexical substitution metrics",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--87:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--87:E0",
        "to_id": "abstract-2022--acl-long--87:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Lexical substitution is the task of generating meaningful substitutes for a word in a given textual context. Contextual word embedding models have achieved state-of-the-art results in the lexical substitution task by relying on contextual information extracted from the replaced word within the sentence. However, such models do not take into account structured knowledge that exists in external lexical databases. We introduce LexSubCon, an end-to-end lexical substitution framework based on contextual embedding models that can identify highly-accurate substitute candidates. This is achieved by combining contextual information with knowledge from structured lexical resources. Our approach involves: (i) introducing a novel mix-up embedding strategy to the target word’s embedding through linearly interpolating the pair of the target input embedding and the average embedding of its probable synonyms; (ii) considering the similarity of the sentence-definition embeddings of the target word and its proposed candidates; and, (iii) calculating the effect of each substitution on the semantics of the sentence through a fine-tuned sentence similarity model. Our experiments show that LexSubCon outperforms previous state-of-the-art methods by at least 2% over all the official lexical substitution metrics on LS07 and CoInCo benchmark datasets that are widely used for lexical substitution tasks."
    }
  },
  {
    "id": "abstract-2022--acl-long--355",
    "result": [
      {
        "value": {
          "start": 356,
          "end": 375,
          "text": "feedback from users",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--355:E0"
      }
    ],
    "data": {
      "text": "We study learning from user feedback for extractive question answering by simulating feedback using supervised data. We cast the problem as contextual bandit learning, and analyze the characteristics of several learning scenarios with focus on reducing data annotation. We show that systems initially trained on few examples can dramatically improve given feedback from users on model-predicted answers, and that one can use existing datasets to deploy systems in new domains without any annotation effort, but instead improving the system on-the-fly via user feedback."
    }
  },
  {
    "id": "abstract-2022--acl-long--242",
    "result": [
      {
        "value": {
          "start": 655,
          "end": 664,
          "text": "MM-Deacon",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--242:E0"
      },
      {
        "value": {
          "start": 860,
          "end": 870,
          "text": "robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--242:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--242:E0",
        "to_id": "abstract-2022--acl-long--242:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Molecular representation learning plays an essential role in cheminformatics. Recently, language model-based approaches have gained popularity as an alternative to traditional expert-designed features to encode molecules. However, these approaches only utilize a single molecular language for representation learning. Motivated by the fact that a given molecule can be described using different languages such as Simplified Molecular Line Entry System (SMILES), The International Union of Pure and Applied Chemistry (IUPAC), and The IUPAC International Chemical Identifier (InChI), we propose a multilingual molecular embedding generation approach called MM-Deacon (multilingual molecular domain embedding analysis via contrastive learning). MM-Deacon is pre-trained using SMILES and IUPAC as two different languages on large-scale molecules. We evaluated the robustness of our method on seven molecular property prediction tasks from MoleculeNet benchmark, zero-shot cross-lingual retrieval, and a drug-drug interaction prediction task."
    }
  },
  {
    "id": "abstract-2022--acl-long--375",
    "result": [
      {
        "value": {
          "start": 320,
          "end": 328,
          "text": "∞-former",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--375:E0"
      },
      {
        "value": {
          "start": 925,
          "end": 974,
          "text": "ability to retain information from long sequences",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--375:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--375:E0",
        "to_id": "abstract-2022--acl-long--375:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Transformers are unable to model long-term memories effectively, since the amount of computation they need to perform grows with the context length. While variations of efficient transformers have been proposed, they all have a finite memory capacity and are forced to drop old information. In this paper, we propose the∞-former, which extends the vanilla transformer with an unbounded long-term memory. By making use of a continuous-space attention mechanism to attend over the long-term memory, the∞-former’s attention complexity becomes independent of the context length, trading off memory length with precision.In order to control where precision is more important,∞-former maintains “sticky memories,” being able to model arbitrarily long contexts while keeping the computation budget fixed.Experiments on a synthetic sorting task, language modeling, and document grounded dialogue generation demonstrate the∞-former’s ability to retain information from long sequences."
    }
  },
  {
    "id": "abstract-2022--acl-long--566",
    "result": [
      {
        "value": {
          "start": 618,
          "end": 661,
          "text": "two computationally efficient modifications",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--566:E0"
      }
    ],
    "data": {
      "text": "Uncertainty estimation (UE) of model predictions is a crucial step for a variety of tasks such as active learning, misclassification detection, adversarial attack detection, out-of-distribution detection, etc. Most of the works on modeling the uncertainty of deep neural networks evaluate these methods on image classification tasks. Little attention has been paid to UE in natural language processing. To fill this gap, we perform a vast empirical investigation of state-of-the-art UE methods for Transformer models on misclassification detection in named entity recognition and text classification tasks and propose two computationally efficient modifications, one of which approaches or even outperforms computationally intensive methods."
    }
  },
  {
    "id": "abstract-2022--acl-long--145",
    "result": [
      {
        "value": {
          "start": 407,
          "end": 479,
          "text": "aspect-specific and language-agnostic discrete latent opinion tree model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--145:E0"
      },
      {
        "value": {
          "start": 825,
          "end": 848,
          "text": "competitive performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--145:E1"
      },
      {
        "value": {
          "start": 853,
          "end": 869,
          "text": "interpretability",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--145:E2"
      },
      {
        "from_id": "abstract-2022--acl-long--145:E0",
        "to_id": "abstract-2022--acl-long--145:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--145:E0",
        "to_id": "abstract-2022--acl-long--145:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Dependency trees have been intensively used with graph neural networks for aspect-based sentiment classification. Though being effective, such methods rely on external dependency parsers, which can be unavailable for low-resource languages or perform worse in low-resource domains. In addition, dependency trees are also not optimized for aspect-based sentiment classification. In this paper, we propose an aspect-specific and language-agnostic discrete latent opinion tree model as an alternative structure to explicit dependency trees. To ease the learning of complicated structured latent variables, we build a connection between aspect-to-context attention scores and syntactic distances, inducing trees from the attention scores. Results on six English benchmarks and one Chinese dataset show that our model can achieve competitive performance and interpretability."
    }
  },
  {
    "id": "abstract-2022--acl-long--170",
    "result": [
      {
        "value": {
          "start": 546,
          "end": 570,
          "text": "neural clustering method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--170:E0"
      },
      {
        "value": {
          "start": 489,
          "end": 502,
          "text": "effectiveness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--170:E1"
      },
      {
        "value": {
          "start": 64,
          "end": 74,
          "text": "efficiency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--170:E2"
      },
      {
        "from_id": "abstract-2022--acl-long--170:E0",
        "to_id": "abstract-2022--acl-long--170:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--170:E0",
        "to_id": "abstract-2022--acl-long--170:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recently, a lot of research has been carried out to improve the efficiency of Transformer. Among them, the sparse pattern-based method is an important branch of efficient Transformers. However, some existing sparse methods usually use fixed patterns to select words, without considering similarities between words. Other sparse methods use clustering patterns to select words, but the clustering process is separate from the training process of the target task, which causes a decrease in effectiveness. To address these limitations, we design a neural clustering method, which can be seamlessly integrated into the Self-Attention Mechanism in Transformer. The clustering task and the target task are jointly trained and optimized to benefit each other, leading to significant effectiveness improvement. In addition, our method groups the words with strong dependencies into the same cluster and performs the attention mechanism for each cluster independently, which improves the efficiency. We verified our method on machine translation, text classification, natural language inference, and text matching tasks. Experimental results show that our method outperforms two typical sparse attention methods, Reformer and Routing Transformer while having a comparable or even better time and memory efficiency."
    }
  },
  {
    "id": "abstract-2022--acl-long--519",
    "result": [
      {
        "value": {
          "start": 618,
          "end": 657,
          "text": "labeling spans within the same document",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--519:E0"
      }
    ],
    "data": {
      "text": "Neural coreference resolution models trained on one dataset may not transfer to new, low-resource domains. Active learning mitigates this problem by sampling a small subset of data for annotators to label. While active learning is well-defined for classification tasks, its application to coreference resolution is neither well-defined nor fully understood. This paper explores how to actively label coreference, examining sources of model uncertainty and document reading costs. We compare uncertainty sampling strategies and their advantages through thorough error analysis. In both synthetic and human experiments, labeling spans within the same document is more effective than annotating spans across documents. The findings contribute to a more realistic development of coreference resolution models."
    }
  },
  {
    "id": "abstract-2022--acl-srw--16",
    "result": [
      {
        "value": {
          "start": 1166,
          "end": 1200,
          "text": "English-Malay embeddings alignment",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-srw--16:E0"
      },
      {
        "value": {
          "start": 926,
          "end": 976,
          "text": "better seed lexicon and cleaner nearest neighbours",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-srw--16:E1"
      }
    ],
    "data": {
      "text": "As high-quality Malay language resources are still a scarcity, cross lingual word embeddings make it possible for richer English resources to be leveraged for downstream Malay text classification tasks. This paper focuses on creating an English-Malay cross-lingual word embeddings using embedding alignment by exploiting existing language resources. We augmented the training bilingual lexicons using machine translation with the goal to improve the alignment precision of our cross-lingual word embeddings. We investigated the quality of the current state-of-the-art English-Malay bilingual lexicon and worked on improving its quality using Google Translate. We also examined the effect of Malay word coverage on the quality of cross-lingual word embeddings. Experimental results with a precision up till 28.17% show that the alignment precision of the cross-lingual word embeddings would inevitably degrade after 1-NN but a better seed lexicon and cleaner nearest neighbours can reduce the number of word pairs required to achieve satisfactory performance. As the English and Malay monolingual embeddings are pre-trained on informal language corpora, our proposed English-Malay embeddings alignment approach is also able to map non-standard Malay translations in the English nearest neighbours."
    }
  },
  {
    "id": "abstract-2022--acl-long--216",
    "result": [
      {
        "value": {
          "start": 468,
          "end": 501,
          "text": "prototype-based clustering method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--216:E0"
      }
    ],
    "data": {
      "text": "Representations of events described in text are important for various tasks. In this work, we present SWCC: a Simultaneous Weakly supervised Contrastive learning and Clustering framework for event representation learning. SWCC learns event representations by making better use of co-occurrence information of events. Specifically, we introduce a weakly supervised contrastive learning method that allows us to consider multiple positives and multiple negatives, and a prototype-based clustering method that avoids semantically related events being pulled apart. For model training, SWCC learns representations by simultaneously performing weakly supervised contrastive learning and prototype-based clustering. Experimental results show that SWCC outperforms other baselines on Hard Similarity and Transitive Sentence Similarity tasks. In addition, a thorough analysis of the prototype-based clustering method demonstrates that the learned prototype vectors are able to implicitly capture various relations between events."
    }
  },
  {
    "id": "abstract-2022--acl-long--437",
    "result": [
      {
        "value": {
          "start": 744,
          "end": 802,
          "text": "Guided Attention Multimodal Multitask Network (GAME) model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--437:E0"
      }
    ],
    "data": {
      "text": "Most works on financial forecasting use information directly associated with individual companies (e.g., stock prices, news on the company) to predict stock returns for trading. We refer to such company-specific information as local information. Stock returns may also be influenced by global information (e.g., news on the economy in general), and inter-company relationships. Capturing such diverse information is challenging due to the low signal-to-noise ratios, different time-scales, sparsity and distributions of global and local information from different modalities. In this paper, we propose a model that captures both global and local multimodal information for investment and risk management-related forecasting tasks. Our proposed Guided Attention Multimodal Multitask Network (GAME) model addresses these challenges by using novel attention modules to guide learning with global and local information from different modalities and dynamic inter-company relationship networks. Our extensive experiments show that GAME outperforms other state-of-the-art models in several forecasting tasks and important real-world application case studies."
    }
  },
  {
    "id": "abstract-2022--acl-long--313",
    "result": [
      {
        "value": {
          "start": 686,
          "end": 715,
          "text": "new training mechanism for ED",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--313:E0"
      },
      {
        "value": {
          "start": 805,
          "end": 826,
          "text": "promising performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--313:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--313:E0",
        "to_id": "abstract-2022--acl-long--313:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Event detection (ED) is a critical subtask of event extraction that seeks to identify event triggers of certain types in texts. Despite significant advances in ED, existing methods typically follow a “one model fits all types” approach, which sees no differences between event types and often results in a quite skewed performance. Finding the causes of skewed performance is crucial for the robustness of an ED model, but to date there has been little exploration of this problem. This research examines the issue in depth and presents a new concept termed trigger salience attribution, which can explicitly quantify the underlying patterns of events. On this foundation, we develop a new training mechanism for ED, which can distinguish between trigger-dependent and context-dependent types and achieve promising performance on two benchmarks. Finally, by highlighting many distinct characteristics of trigger-dependent and context-dependent types, our work may promote more research into this problem."
    }
  },
  {
    "id": "abstract-2022--acl-long--501",
    "result": [
      {
        "value": {
          "start": 1412,
          "end": 1425,
          "text": "larger models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--501:E0"
      },
      {
        "value": {
          "start": 1362,
          "end": 1386,
          "text": "commonsense capabilities",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--501:E1"
      },
      {
        "value": {
          "start": 1432,
          "end": 1449,
          "text": "math capabilities",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--501:E2"
      },
      {
        "value": {
          "start": 1484,
          "end": 1515,
          "text": "simple decoding hyperparameters",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--501:E3"
      },
      {
        "value": {
          "start": 1555,
          "end": 1588,
          "text": "perceived quality of machine text",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--501:E4"
      },
      {
        "from_id": "abstract-2022--acl-long--501:E0",
        "to_id": "abstract-2022--acl-long--501:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--501:E0",
        "to_id": "abstract-2022--acl-long--501:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--501:E3",
        "to_id": "abstract-2022--acl-long--501:E4",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Modern neural language models can produce remarkably fluent and grammatical text. So much, in fact, that recent work by Clark et al. (2021) has reported that conventional crowdsourcing can no longer reliably distinguish between machine-authored (GPT-3) and human-authored writing. As errors in machine generations become ever subtler and harder to spot, it poses a new challenge to the research community for robust machine text evaluation. We propose a new framework called Scarecrow for scrutinizing machine text via crowd annotation. To support the broad range of real machine errors that can be identified by laypeople, the ten error categories of Scarecrow—such as redundancy, commonsense errors, and incoherence—are identified through several rounds of crowd annotation experiments without a predefined ontology. We then use Scarecrow to collect over 41k error spans in human-written and machine-generated paragraphs of English language news text. We isolate factors for detailed analysis, including parameter count, training data, and various decoding-time configurations. Our approach successfully quantifies measurable gaps between human authored text and generations from models of several sizes, including fourteen configurations of GPT-3. In addition, our analysis unveils new insights, with detailed rationales provided by laypeople, e.g., that the commonsense capabilities have been improving with larger models while math capabilities have not, and that the choices of simple decoding hyperparameters can make remarkable differences on the perceived quality of machine text. We release our training material, annotation toolkit and dataset athttps://yao-dou.github.io/scarecrow/."
    }
  },
  {
    "id": "abstract-2022--acl-long--86",
    "result": [
      {
        "value": {
          "start": 148,
          "end": 176,
          "text": "Semantic Autoencoder (SemAE)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--86:E0"
      },
      {
        "value": {
          "start": 686,
          "end": 726,
          "text": "performance on SPACE and AMAZON datasets",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--86:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--86:E0",
        "to_id": "abstract-2022--acl-long--86:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Opinion summarization is the task of automatically generating summaries that encapsulate information expressed in multiple user reviews. We present Semantic Autoencoder (SemAE) to perform extractive opinion summarization in an unsupervised manner. SemAE uses dictionary learning to implicitly capture semantic information from the review text and learns a latent representation of each sentence over semantic units. Our extractive summarization algorithm leverages the representations to identify representative opinions among hundreds of reviews. SemAE is also able to perform controllable summarization to generate aspect-specific summaries using only a few samples. We report strong performance on SPACE and AMAZON datasets and perform experiments to investigate the functioning of our model."
    }
  },
  {
    "id": "abstract-2022--acl-long--129",
    "result": [
      {
        "value": {
          "start": 1107,
          "end": 1125,
          "text": "proposed framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--129:E0"
      },
      {
        "value": {
          "start": 1247,
          "end": 1255,
          "text": "fastText",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--129:E1"
      }
    ],
    "data": {
      "text": "Pre-trained contextual representations have led to dramatic performance improvements on a range of downstream tasks. Such performance improvements have motivated researchers to quantify and understand the linguistic information encoded in these representations. In general, researchers quantify the amount of linguistic information through probing, an endeavor which consists of training a supervised model to predict a linguistic property directly from the contextual representations. Unfortunately, this definition of probing has been subject to extensive criticism in the literature, and has been observed to lead to paradoxical and counter-intuitive results. In the theoretical portion of this paper, we take the position that the goal of probing ought to be measuring the amount of inductive bias that the representations encode on a specific task. We further describe a Bayesian framework that operationalizes this goal and allows us to quantify the representations’ inductive bias. In the empirical portion of the paper, we apply our framework to a variety of NLP tasks. Our results suggest that our proposed framework alleviates many previous problems found in probing. Moreover, we are able to offer concrete evidence that—for some tasks—fastText can offer a better inductive bias than BERT."
    }
  },
  {
    "id": "abstract-2022--acl-long--401",
    "result": [
      {
        "value": {
          "start": 1170,
          "end": 1217,
          "text": "fairness metric based on prediction sensitivity",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--401:E0"
      }
    ],
    "data": {
      "text": "With the rapid growth in language processing applications, fairness has emerged as an important consideration in data-driven solutions. Although various fairness definitions have been explored in the recent literature, there is lack of consensus on which metrics most accurately reflect the fairness of a system. In this work, we propose a new formulation – accumulated prediction sensitivity, which measures fairness in machine learning models based on the model’s prediction sensitivity to perturbations in input features. The metric attempts to quantify the extent to which a single prediction depends on a protected attribute, where the protected attribute encodes the membership status of an individual in a protected group. We show that the metric can be theoretically linked with a specific notion of group fairness (statistical parity) and individual fairness. It also correlates well with humans’ perception of fairness. We conduct experiments on two text classification datasets – Jigsaw Toxicity, and Bias in Bios, and evaluate the correlations between metrics and manual annotations on whether the model produced a fair outcome. We observe that the proposed fairness metric based on prediction sensitivity is statistically significantly more correlated with human annotation than the existing counterfactual fairness metric."
    }
  },
  {
    "id": "abstract-2022--acl-long--368",
    "result": [
      {
        "value": {
          "start": 463,
          "end": 509,
          "text": "mixup strategy for pre-trained language models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--368:E0"
      },
      {
        "value": {
          "start": 216,
          "end": 233,
          "text": "model calibration",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--368:E1"
      },
      {
        "value": {
          "start": 1095,
          "end": 1121,
          "text": "expected calibration error",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--368:E2"
      },
      {
        "from_id": "abstract-2022--acl-long--368:E0",
        "to_id": "abstract-2022--acl-long--368:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "A well-calibrated neural model produces confidence (probability outputs) closely approximated by the expected accuracy. While prior studies have shown that mixup training as a data augmentation technique can improve model calibration on image classification tasks, little is known about using mixup for model calibration on natural language understanding (NLU) tasks. In this paper, we explore mixup for model calibration on several NLU tasks and propose a novel mixup strategy for pre-trained language models that improves model calibration further. Our proposed mixup is guided by both the Area Under the Margin (AUM) statistic (Pleiss et al., 2020) and the saliency map of each sample (Simonyan et al., 2013). Moreover, we combine our mixup strategy with model miscalibration correction techniques (i.e., label smoothing and temperature scaling) and provide detailed analyses of their impact on our proposed mixup. We focus on systematically designing experiments on three NLU tasks: natural language inference, paraphrase detection, and commonsense reasoning. Our method achieves the lowest expected calibration error compared to strong baselines on both in-domain and out-of-domain test samples while maintaining competitive accuracy."
    }
  },
  {
    "id": "abstract-2022--acl-long--50",
    "result": [
      {
        "value": {
          "start": 301,
          "end": 306,
          "text": "MoSST",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--50:E0"
      },
      {
        "value": {
          "start": 784,
          "end": 810,
          "text": "translation quality (BLEU)",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--50:E1"
      },
      {
        "value": {
          "start": 815,
          "end": 822,
          "text": "latency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--50:E2"
      },
      {
        "from_id": "abstract-2022--acl-long--50:E0",
        "to_id": "abstract-2022--acl-long--50:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--50:E0",
        "to_id": "abstract-2022--acl-long--50:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "How to find proper moments to generate partial sentence translation given a streaming speech input? Existing approaches waiting-and-translating for a fixed duration often break the acoustic units in speech, since the boundaries between acoustic units in speech are not even. In this paper, we propose MoSST, a simple yet effective method for translating streaming speech content. Given a usually long speech sequence, we develop an efficient monotonic segmentation module inside an encoder-decoder model to accumulate acoustic information incrementally and detect proper speech unit boundaries for the input in speech translation task. Experiments on multiple translation directions of the MuST-C dataset show that outperforms existing methods and achieves the best trade-off between translation quality (BLEU) and latency. Our code is available athttps://github.com/dqqcasia/mosst."
    }
  },
  {
    "id": "abstract-2022--acl-long--12",
    "result": [
      {
        "value": {
          "start": 313,
          "end": 318,
          "text": "SixT+",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--12:E0"
      },
      {
        "value": {
          "start": 977,
          "end": 997,
          "text": "SixT+ initialization",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--12:E1"
      }
    ],
    "data": {
      "text": "This paper demonstrates that multilingual pretraining and multilingual fine-tuning are both critical for facilitating cross-lingual transfer in zero-shot translation, where the neural machine translation (NMT) model is tested on source languages unseen during supervised training. Following this idea, we present SixT+, a strong many-to-English NMT model that supports 100 source languages but is trained with a parallel dataset in only six source languages. SixT+ initializes the decoder embedding and the full encoder with XLM-R large and then trains the encoder and decoder layers with a simple two-stage training strategy. SixT+ achieves impressive performance on many-to-English translation. It significantly outperforms CRISS and m2m-100, two strong multilingual NMT systems, with an average gain of 7.2 and 5.0 BLEU respectively. Additionally, SixT+ offers a set of model parameters that can be further fine-tuned to other unsupervised tasks. We demonstrate that adding SixT+ initialization outperforms state-of-the-art explicitly designed unsupervised NMT models on Si<->En and Ne<->En by over 1.2 average BLEU. When applied to zero-shot cross-lingual abstractive summarization, it produces an average performance gain of 12.3 ROUGE-L over mBART-ft. We conduct detailed analyses to understand the key ingredients of SixT+, including multilinguality of the auxiliary parallel data, positional disentangled encoder, and the cross-lingual transferability of its encoder."
    }
  },
  {
    "id": "abstract-2022--acl-long--42",
    "result": [
      {
        "value": {
          "start": 735,
          "end": 741,
          "text": "NLSSum",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--42:E0"
      },
      {
        "value": {
          "start": 1008,
          "end": 1032,
          "text": "state-of-the-art results",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--42:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--42:E0",
        "to_id": "abstract-2022--acl-long--42:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In zero-shot multilingual extractive text summarization, a model is typically trained on English summarization dataset and then applied on summarization datasets of other languages. Given English gold summaries and documents, sentence-level labels for extractive summarization are usually generated using heuristics. However, these monolingual labels created on English datasets may not be optimal on datasets of other languages, for that there is the syntactic or semantic discrepancy between different languages. In this way, it is possible to translate the English dataset to other languages and obtain different sets of labels again using heuristics. To fully leverage the information of these different sets of labels, we propose NLSSum (Neural Label Search for Summarization), which jointly learns hierarchical weights for these different sets of labels together with our summarization model. We conduct multilingual zero-shot summarization experiments on MLSUM and WikiLingua datasets, and we achieve state-of-the-art results using both human and automatic evaluations across these two datasets."
    }
  },
  {
    "id": "abstract-2022--acl-short--35",
    "result": [
      {
        "value": {
          "start": 984,
          "end": 1012,
          "text": "Flexible Goal Accuracy (FGA)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-short--35:E0"
      },
      {
        "value": {
          "start": 1386,
          "end": 1424,
          "text": "discriminator of DST model performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-short--35:E1"
      },
      {
        "from_id": "abstract-2022--acl-short--35:E0",
        "to_id": "abstract-2022--acl-short--35:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Dialogue State Tracking (DST) is primarily evaluated using Joint Goal Accuracy (JGA) defined as the fraction of turns where the ground-truth dialogue state exactly matches the prediction. Generally in DST, the dialogue state or belief state for a given turn contain all the intents shown by the user till that turn. Due to this cumulative nature of the belief state, it is difficult to get a correct prediction once a misprediction has occurred. Thus, although being a useful metric, it can be harsh at times and underestimate the true potential of a DST model. Moreover, an improvement in JGA can sometimes decrease the performance of turn-level or non-cumulative belief state prediction due to inconsistency in annotations. So, using JGA as the only metric for model selection may not be ideal for all scenarios. In this work, we discuss various evaluation metrics used for DST along with their shortcomings. To address the existing issues, we propose a new evaluation metric named Flexible Goal Accuracy (FGA). FGA is a generalized version of JGA. But unlike JGA, it tries to give penalized rewards to mispredictions that are locally correct i.e. the root cause of the error is an earlier turn. By doing so, FGA considers the performance of both cumulative and turn-level prediction flexibly and provides a better insight than the existing metrics. We also show that FGA is a better discriminator of DST model performance."
    }
  },
  {
    "id": "abstract-2022--acl-long--534",
    "result": [
      {
        "value": {
          "start": 418,
          "end": 464,
          "text": "Language-independent Layout Transformer (LiLT)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--534:E0"
      }
    ],
    "data": {
      "text": "Structured document understanding has attracted considerable attention and made significant progress recently, owing to its crucial role in intelligent document processing. However, most existing related models can only deal with the document data of specific language(s) (typically English) included in the pre-training collection, which is extremely limited. To address this issue, we propose a simple yet effective Language-independent Layout Transformer (LiLT) for structured document understanding. LiLT can be pre-trained on the structured documents of a single language and then directly fine-tuned on other languages with the corresponding off-the-shelf monolingual/multilingual pre-trained textual models. Experimental results on eight languages have shown that LiLT can achieve competitive or even superior performance on diverse widely-used downstream benchmarks, which enables language-independent benefit from the pre-training of document layout structure. Code and model are publicly available athttps://github.com/jpWang/LiLT."
    }
  },
  {
    "id": "abstract-2022--acl-long--215",
    "result": [
      {
        "value": {
          "start": 808,
          "end": 859,
          "text": "discretized multi-modal fine-grained representation",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--215:E0"
      },
      {
        "value": {
          "start": 980,
          "end": 1022,
          "text": "performance on cross-modal retrieval tasks",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--215:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--215:E0",
        "to_id": "abstract-2022--acl-long--215:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In contrast to recent advances focusing on high-level representation learning across modalities, in this work we present a self-supervised learning framework that is able to learn a representation that captures finer levels of granularity across different modalities such as concepts or events represented by visual objects or spoken words. Our framework relies on a discretized embedding space created via vector quantization that is shared across different modalities. Beyond the shared embedding space, we propose a Cross-Modal Code Matching objective that forces the representations from different views (modalities) to have a similar distribution over the discrete embedding space such that cross-modal objects/actions localization can be performed without direct supervision. We show that the proposed discretized multi-modal fine-grained representation (e.g., pixel/word/frame) can complement high-level summary representations (e.g., video/sentence/waveform) for improved performance on cross-modal retrieval tasks. We also observe that the discretized representation uses individual clusters to represent the same semantic concept across modalities."
    }
  },
  {
    "id": "abstract-2022--acl-long--36",
    "result": [
      {
        "value": {
          "start": 1314,
          "end": 1327,
          "text": "effectiveness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--36:E0"
      }
    ],
    "data": {
      "text": "Predicting missing facts in a knowledge graph (KG) is crucial as modern KGs are far from complete. Due to labor-intensive human labeling, this phenomenon deteriorates when handling knowledge represented in various languages. In this paper, we explore multilingual KG completion, which leverages limited seed alignment as a bridge, to embrace the collective knowledge from multiple languages. However, language alignment used in prior works is still not fully exploited: (1) alignment pairs are treated equally to maximally push parallel entities to be close, which ignores KG capacity inconsistency; (2) seed alignment is scarce and new alignment identification is usually in a noisily unsupervised manner. To tackle these issues, we propose a novel self-supervised adaptive graph alignment (SS-AGA) method. Specifically, SS-AGA fuses all KGs as a whole graph by regarding alignment as a new edge type. As such, information propagation and noise influence across KGs can be adaptively controlled via relation-aware attention weights. Meanwhile, SS-AGA features a new pair generator that dynamically captures potential alignment pairs in a self-supervised paradigm. Extensive experiments on both the public multilingual DBPedia KG and newly-created industrial multilingual E-commerce KG empirically demonstrate the effectiveness of SS-AGA"
    }
  },
  {
    "id": "abstract-2022--acl-long--128",
    "result": [
      {
        "value": {
          "start": 930,
          "end": 958,
          "text": "recall-then-verify framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--128:E0"
      },
      {
        "value": {
          "start": 1158,
          "end": 1182,
          "text": "state-of-the-art results",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--128:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--128:E0",
        "to_id": "abstract-2022--acl-long--128:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Open-domain questions are likely to be open-ended and ambiguous, leading to multiple valid answers. Existing approaches typically adopt the rerank-then-read framework, where a reader reads top-ranking evidence to predict answers. According to our empirical analysis, this framework faces three problems: first, to leverage a large reader under a memory constraint, the reranker should select only a few relevant passages to cover diverse answers, while balancing relevance and diversity is non-trivial; second, the small reading budget prevents the reader from accessing valuable retrieved evidence filtered out by the reranker; third, when using a generative reader to predict answers all at once based on all selected evidence, whether a valid answer will be predicted also pathologically depends on evidence of some other valid answer(s). To address these issues, we propose to answer open-domain multi-answer questions with a recall-then-verify framework, which separates the reasoning process of each answer so that we can make better use of retrieved evidence while also leveraging large models under the same memory constraint. Our framework achieves state-of-the-art results on two multi-answer datasets, and predicts significantly more gold answers than a rerank-then-read system that uses an oracle reranker."
    }
  },
  {
    "id": "abstract-2022--acl-long--2",
    "result": [
      {
        "value": {
          "start": 578,
          "end": 588,
          "text": "QRA method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--2:E0"
      },
      {
        "value": {
          "start": 598,
          "end": 630,
          "text": "degree-of-reproducibility scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--2:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--2:E0",
        "to_id": "abstract-2022--acl-long--2:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This paper describes and tests a method for carrying out quantified reproducibility assessment (QRA) that is based on concepts and definitions from metrology. QRA produces a single score estimating the degree of reproducibility of a given system and evaluation measure, on the basis of the scores from, and differences between, different reproductions. We test QRA on 18 different system and evaluation measure combinations (involving diverse NLP tasks and types of evaluation), for each of which we have the original results and one to seven reproduction results. The proposed QRA method produces degree-of-reproducibility scores that are comparable across multiple reproductions not only of the same, but also of different, original studies. We find that the proposed method facilitates insights into causes of variation between reproductions, and as a result, allows conclusions to be drawn about what aspects of system and/or evaluation design need to be changed in order to improve reproducibility."
    }
  },
  {
    "id": "abstract-2022--acl-long--548",
    "result": [
      {
        "value": {
          "start": 28,
          "end": 65,
          "text": "mixture model-based end-to-end method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--548:E0"
      },
      {
        "value": {
          "start": 821,
          "end": 883,
          "text": "performance in predicting short distance semantic dependencies",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--548:E1"
      },
      {
        "value": {
          "start": 1036,
          "end": 1105,
          "text": "small but statistically significant improvement over baseline methods",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--548:E2"
      },
      {
        "value": {
          "start": 1150,
          "end": 1203,
          "text": "competitive performance with state-of-the-art methods",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--548:E3"
      },
      {
        "from_id": "abstract-2022--acl-long--548:E0",
        "to_id": "abstract-2022--acl-long--548:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--548:E0",
        "to_id": "abstract-2022--acl-long--548:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--548:E0",
        "to_id": "abstract-2022--acl-long--548:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In this paper, we propose a mixture model-based end-to-end method to model the syntactic-semantic dependency correlation in Semantic Role Labeling (SRL). Semantic dependencies in SRL are modeled as a distribution over semantic dependency labels conditioned on a predicate and an argument word. The semantic label distribution varies depending on Shortest Syntactic Dependency Path (SSDP) hop patterns. We target the variation of semantic label distributions using a mixture model, separately estimating semantic label distributions for different hop patterns and probabilistically clustering hop patterns with similar semantic label distributions. Experiments show that the proposed method successfully learns a cluster assignment reflecting the variation of semantic label distributions. Modeling the variation improves performance in predicting short distance semantic dependencies, in addition to the improvement on long distance semantic dependencies that previous syntax-aware methods have achieved. The proposed method achieves a small but statistically significant improvement over baseline methods in English, German, and Spanish and obtains competitive performance with state-of-the-art methods in English."
    }
  },
  {
    "id": "abstract-2022--acl-long--526",
    "result": [
      {
        "value": {
          "start": 342,
          "end": 387,
          "text": "intermediate unsupervised classification task",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--526:E0"
      },
      {
        "value": {
          "start": 256,
          "end": 267,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--526:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--526:E0",
        "to_id": "abstract-2022--acl-long--526:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In real-world scenarios, a text classification task often begins with a cold start, when labeled data is scarce. In such cases, the common practice of fine-tuning pre-trained models, such as BERT, for a target classification task, is prone to produce poor performance. We suggest a method to boost the performance of such models by adding an intermediate unsupervised classification task, between the pre-training and fine-tuning phases. As such an intermediate task, we perform clustering and train the pre-trained model on predicting the cluster labels. We test this hypothesis on various data sets, and show that this additional classification phase can significantly improve performance, mainly for topical classification tasks, when the number of labeled instances available for fine-tuning is only a couple of dozen to a few hundred."
    }
  },
  {
    "id": "abstract-2022--acl-long--185",
    "result": [
      {
        "value": {
          "start": 263,
          "end": 282,
          "text": "Seq2Seq pretraining",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--185:E0"
      },
      {
        "value": {
          "start": 509,
          "end": 529,
          "text": "diverse translations",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--185:E1"
      },
      {
        "value": {
          "start": 541,
          "end": 576,
          "text": "adequacy-related translation errors",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--185:E2"
      },
      {
        "value": {
          "start": 672,
          "end": 691,
          "text": "translation quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--185:E3"
      },
      {
        "value": {
          "start": 734,
          "end": 755,
          "text": "over-estimation issue",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--185:E4"
      },
      {
        "value": {
          "start": 874,
          "end": 916,
          "text": "in-domain pretraining and input adaptation",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--185:E5"
      },
      {
        "value": {
          "start": 1081,
          "end": 1104,
          "text": "translation performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--185:E6"
      },
      {
        "value": {
          "start": 1109,
          "end": 1125,
          "text": "model robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--185:E7"
      },
      {
        "from_id": "abstract-2022--acl-long--185:E0",
        "to_id": "abstract-2022--acl-long--185:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--185:E0",
        "to_id": "abstract-2022--acl-long--185:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--185:E0",
        "to_id": "abstract-2022--acl-long--185:E3",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--185:E0",
        "to_id": "abstract-2022--acl-long--185:E4",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--185:E5",
        "to_id": "abstract-2022--acl-long--185:E6",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--185:E5",
        "to_id": "abstract-2022--acl-long--185:E7",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In this paper, we present a substantial step in better understanding the SOTA sequence-to-sequence (Seq2Seq) pretraining for neural machine translation (NMT). We focus on studying the impact of the jointly pretrained decoder, which is the main difference between Seq2Seq pretraining and previous encoder-based pretraining approaches for NMT. By carefully designing experiments on three language pairs, we find that Seq2Seq pretraining is a double-edged sword: On one hand, it helps NMT models to produce more diverse translations and reduce adequacy-related translation errors. On the other hand, the discrepancies between Seq2Seq pretraining and NMT finetuning limit the translation quality (i.e., domain discrepancy) and induce the over-estimation issue (i.e., objective discrepancy). Based on these observations, we further propose simple and effective strategies, named in-domain pretraining and input adaptation to remedy the domain and objective discrepancies, respectively. Experimental results on several language pairs show that our approach can consistently improve both translation performance and model robustness upon Seq2Seq pretraining."
    }
  },
  {
    "id": "abstract-2022--acl-long--480",
    "result": [
      {
        "value": {
          "start": 508,
          "end": 540,
          "text": "leveraging the streaming history",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--480:E0"
      }
    ],
    "data": {
      "text": "Simultaneous Machine Translation is the task of incrementally translating an input sentence before it is fully available. Currently, simultaneous translation is carried out by translating each sentence independently of the previously translated text. More generally, Streaming MT can be understood as an extension of Simultaneous MT to the incremental translation of a continuous input text stream. In this work, a state-of-the-art simultaneous sentence-level MT system is extended to the streaming setup by leveraging the streaming history. Extensive empirical results are reported on IWSLT Translation Tasks, showing that leveraging the streaming history leads to significant quality gains. In particular, the proposed system proves to compare favorably to the best performing systems."
    }
  },
  {
    "id": "abstract-2022--acl-srw--3",
    "result": [
      {
        "value": {
          "start": 863,
          "end": 882,
          "text": "multi-task learning",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-srw--3:E0"
      }
    ],
    "data": {
      "text": "In communication, a human would recognize the emotion of an interlocutor and respond with an appropriate emotion, such as empathy and comfort. Toward developing a dialogue system with such a human-like ability, we propose a method to build a dialogue corpus annotated with two kinds of emotions. We collect dialogues from Twitter and annotate each utterance with the emotion that a speaker put into the utterance (expressed emotion) and the emotion that a listener felt after listening to the utterance (experienced emotion). We built a dialogue corpus in Japanese using this method, and its statistical analysis revealed the differences between expressed and experienced emotions. We conducted experiments on recognition of the two kinds of emotions. The experimental results indicated the difficulty in recognizing experienced emotions and the effectiveness of multi-task learning of the two kinds of emotions. We hope that the constructed corpus will facilitate the study on emotion recognition in a dialogue and emotion-aware dialogue response generation."
    }
  },
  {
    "id": "abstract-2022--acl-short--44",
    "result": [
      {
        "value": {
          "start": 26,
          "end": 56,
          "text": "Self-Contrastive Decorrelation",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-short--44:E0"
      }
    ],
    "data": {
      "text": "In this paper, we propose Self-Contrastive Decorrelation (SCD), a self-supervised approach. Given an input sentence, it optimizes a joint self-contrastive and decorrelation objective. Learning a representation is facilitated by leveraging the contrast arising from the instantiation of standard dropout at different rates. The proposed method is conceptually simple yet empirically powerful. It achieves comparable results with state-of-the-art methods on multiple benchmarks without using contrastive pairs. This study opens up avenues for efficient self-supervised learning methods that are more robust than current contrastive methods."
    }
  },
  {
    "id": "abstract-2022--acl-long--490",
    "result": [
      {
        "value": {
          "start": 251,
          "end": 269,
          "text": "boundary smoothing",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--490:E0"
      },
      {
        "value": {
          "start": 691,
          "end": 708,
          "text": "model calibration",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--490:E1"
      },
      {
        "value": {
          "start": 729,
          "end": 742,
          "text": "neural minima",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--490:E2"
      },
      {
        "value": {
          "start": 752,
          "end": 776,
          "text": "smoothed loss landscapes",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--490:E3"
      },
      {
        "from_id": "abstract-2022--acl-long--490:E0",
        "to_id": "abstract-2022--acl-long--490:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--490:E0",
        "to_id": "abstract-2022--acl-long--490:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--490:E0",
        "to_id": "abstract-2022--acl-long--490:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Neural named entity recognition (NER) models may easily encounter the over-confidence issue, which degrades the performance and calibration. Inspired by label smoothing and driven by the ambiguity of boundary annotation in NER engineering, we propose boundary smoothing as a regularization technique for span-based neural NER models. It re-assigns entity probabilities from annotated spans to the surrounding ones. Built on a simple but strong baseline, our model achieves results better than or competitive with previous state-of-the-art systems on eight well-known NER benchmarks. Further empirical analysis suggests that boundary smoothing effectively mitigates over-confidence, improves model calibration, and brings flatter neural minima and more smoothed loss landscapes."
    }
  },
  {
    "id": "abstract-2022--acl-long--154",
    "result": [
      {
        "value": {
          "start": 785,
          "end": 805,
          "text": "retrieval efficiency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--154:E0"
      },
      {
        "value": {
          "start": 838,
          "end": 867,
          "text": "cluster-based Compact Network",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--154:E1"
      },
      {
        "value": {
          "start": 1007,
          "end": 1028,
          "text": "cluster-based pruning",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--154:E2"
      },
      {
        "value": {
          "start": 1137,
          "end": 1153,
          "text": "proposed methods",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--154:E3"
      },
      {
        "value": {
          "start": 852,
          "end": 867,
          "text": "Compact Network",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--154:E4"
      },
      {
        "value": {
          "start": 1480,
          "end": 1512,
          "text": "generalization on unseen domains",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--154:E5"
      },
      {
        "from_id": "abstract-2022--acl-long--154:E4",
        "to_id": "abstract-2022--acl-long--154:E5",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "k-Nearest-Neighbor Machine Translation (kNN-MT) has been recently proposed as a non-parametric solution for domain adaptation in neural machine translation (NMT). It aims to alleviate the performance degradation of advanced MT systems in translating out-of-domain sentences by coordinating with an additional token-level feature-based retrieval module constructed from in-domain data. Previous studies (Khandelwal et al., 2021; Zheng et al., 2021) have already demonstrated that non-parametric NMT is even superior to models fine-tuned on out-of-domain data. In spite of this success,kNN retrieval is at the expense of high latency, in particular for large datastores. To make it practical, in this paper, we explore a more efficientkNN-MT and propose to use clustering to improve the retrieval efficiency. Concretely, we first propose a cluster-based Compact Network for feature reduction in a contrastive learning manner to compress context features into 90+% lower dimensional vectors. We then suggest a cluster-based pruning solution to filter out 10% 40% redundant nodes in large datastores while retaining translation quality. Our proposed methods achieve better or comparable performance while reducing up to 57% inference latency against the advanced non-parametric MT model on several machine translation benchmarks. Experimental results indicate that the proposed methods maintain the most useful information of the original datastore and the Compact Network shows good generalization on unseen domains. Codes are available athttps://github.com/tjunlp-lab/PCKMT."
    }
  },
  {
    "id": "abstract-2022--acl-long--280",
    "result": [
      {
        "value": {
          "start": 13,
          "end": 21,
          "text": "ParaBLEU",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--280:E0"
      },
      {
        "value": {
          "start": 330,
          "end": 390,
          "text": "state-of-the-art results on the 2017 WMT Metrics Shared Task",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--280:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--280:E0",
        "to_id": "abstract-2022--acl-long--280:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We introduce ParaBLEU, a paraphrase representation learning model and evaluation metric for text generation. Unlike previous approaches, ParaBLEU learns to understand paraphrasis using generative conditioning as a pretraining objective. ParaBLEU correlates more strongly with human judgements than existing metrics, obtaining new state-of-the-art results on the 2017 WMT Metrics Shared Task. We show that our model is robust to data scarcity, exceeding previous state-of-the-art performance using only 50% of the available training data and surpassing BLEU, ROUGE and METEOR with only 40 labelled examples. Finally, we demonstrate that ParaBLEU can be used to conditionally generate novel paraphrases from a single demonstration, which we use to confirm our hypothesis that it learns abstract, generalized paraphrase representations."
    }
  },
  {
    "id": "abstract-2022--acl-short--86",
    "result": [
      {
        "value": {
          "start": 562,
          "end": 597,
          "text": "Expectation-Adjusted Distinct (EAD)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-short--86:E0"
      }
    ],
    "data": {
      "text": "Distinct is a widely used automatic metric for evaluating diversity in language generation tasks. However, we observed that the original approach to calculating distinct scores has evident biases that tend to assign higher penalties to longer sequences. We refine the calculation of distinct scores by scaling the number of distinct tokens based on their expectations. We provide both empirical and theoretical evidence to show that our method effectively removes the biases existing in the original distinct score. Our experiments show that our proposed metric,Expectation-Adjusted Distinct (EAD), correlates better with human judgment in evaluating response diversity.To assist future research, we provide an example implementation athttps://github.com/lsy641/Expectation-Adjusted-Distinct."
    }
  },
  {
    "id": "abstract-2022--acl-long--33",
    "result": [
      {
        "value": {
          "start": 284,
          "end": 307,
          "text": "explanation information",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--33:E0"
      },
      {
        "value": {
          "start": 834,
          "end": 870,
          "text": "stability of causal reasoning models",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--33:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--33:E0",
        "to_id": "abstract-2022--acl-long--33:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Understanding causality has vital importance for various Natural Language Processing (NLP) applications. Beyond the labeled instances, conceptual explanations of the causality can provide deep understanding of the causal fact to facilitate the causal reasoning process. However, such explanation information still remains absent in existing causal reasoning resources. In this paper, we fill this gap by presenting a human-annotated explainable CAusal REasoning dataset (e-CARE), which contains over 20K causal reasoning questions, together with natural language formed explanations of the causal questions. Experimental results show that generating valid explanations for causal facts still remains especially challenging for the state-of-the-art models, and the explanation information can be helpful for promoting the accuracy and stability of causal reasoning models."
    }
  },
  {
    "id": "abstract-2022--acl-long--389",
    "result": [
      {
        "value": {
          "start": 416,
          "end": 442,
          "text": "fully hyperbolic framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--389:E0"
      }
    ],
    "data": {
      "text": "Hyperbolic neural networks have shown great potential for modeling complex data. However, existing hyperbolic networks are not completely hyperbolic, as they encode features in the hyperbolic space yet formalize most of their operations in the tangent space (a Euclidean subspace) at the origin of the hyperbolic model. This hybrid method greatly limits the modeling ability of networks. In this paper, we propose a fully hyperbolic framework to build hyperbolic networks based on the Lorentz model by adapting the Lorentz transformations (including boost and rotation) to formalize essential operations of neural networks. Moreover, we also prove that linear transformation in tangent spaces used by existing hyperbolic networks is a relaxation of the Lorentz rotation and does not include the boost, implicitly limiting the capabilities of existing hyperbolic networks. The experimental results on four NLP tasks show that our method has better performance for building both shallow and deep networks. Our code will be released to facilitate follow-up research."
    }
  },
  {
    "id": "abstract-2022--acl-long--107",
    "result": [
      {
        "value": {
          "start": 548,
          "end": 587,
          "text": "CoFi (Coarse- and Fine-grained Pruning)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--107:E0"
      },
      {
        "value": {
          "start": 357,
          "end": 365,
          "text": "speedups",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--107:E1"
      },
      {
        "value": {
          "start": 1191,
          "end": 1204,
          "text": "accuracy drop",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--107:E2"
      },
      {
        "from_id": "abstract-2022--acl-long--107:E0",
        "to_id": "abstract-2022--acl-long--107:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--107:E0",
        "to_id": "abstract-2022--acl-long--107:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "The growing size of neural language models has led to increased attention in model compression. The two predominant approaches are pruning, which gradually removes weights from a pre-trained model, and distillation, which trains a smaller compact model to match a larger one. Pruning methods can significantly reduce the model size but hardly achieve large speedups as distillation. However, distillation methods require large amounts of unlabeled data and are expensive to train. In this work, we propose a task-specific structured pruning method CoFi (Coarse- and Fine-grained Pruning), which delivers highly parallelizable subnetworks and matches the distillation methods in both accuracy and latency, without resorting to any unlabeled data. Our key insight is to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads and hidden units) modules, which controls the pruning decision of each parameter with masks of different granularity. We also devise a layerwise distillation strategy to transfer knowledge from unpruned to pruned models during optimization. Our experiments on GLUE and SQuAD datasets show that CoFi yields models with over 10X speedups with a small accuracy drop, showing its effectiveness and efficiency compared to previous pruning and distillation approaches."
    }
  },
  {
    "id": "abstract-2022--acl-long--117",
    "result": [
      {
        "value": {
          "start": 736,
          "end": 778,
          "text": "Data augmentation with RGF counterfactuals",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--117:E0"
      },
      {
        "value": {
          "start": 788,
          "end": 848,
          "text": "performance on out-of-domain and challenging evaluation sets",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--117:E1"
      },
      {
        "value": {
          "start": 968,
          "end": 976,
          "text": "RGF data",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--117:E2"
      },
      {
        "value": {
          "start": 1016,
          "end": 1057,
          "text": "model’s robustness to local perturbations",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--117:E3"
      },
      {
        "from_id": "abstract-2022--acl-long--117:E0",
        "to_id": "abstract-2022--acl-long--117:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--117:E2",
        "to_id": "abstract-2022--acl-long--117:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Deep NLP models have been shown to be brittle to input perturbations. Recent work has shown that data augmentation using counterfactuals — i.e. minimally perturbed inputs — can help ameliorate this weakness. We focus on the task of creating counterfactuals for question answering, which presents unique challenges related to world knowledge, semantic diversity, and answerability. To address these challenges, we develop a Retrieve-Generate-Filter(RGF) technique to create counterfactual evaluation and training data with minimal human supervision. Using an open-domain QA framework and question generation model trained on original task data, we create counterfactuals that are fluent, semantically diverse, and automatically labeled. Data augmentation with RGF counterfactuals improves performance on out-of-domain and challenging evaluation sets over and above existing methods, in both the reading comprehension and open-domain QA settings. Moreover, we find that RGF data leads to significant improvements in a model’s robustness to local perturbations."
    }
  },
  {
    "id": "abstract-2022--acl-long--523",
    "result": [
      {
        "value": {
          "start": 843,
          "end": 852,
          "text": "CPC model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--523:E0"
      },
      {
        "value": {
          "start": 867,
          "end": 889,
          "text": "native language effect",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--523:E1"
      },
      {
        "value": {
          "start": 900,
          "end": 918,
          "text": "wav2vec and HuBERT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--523:E2"
      },
      {
        "value": {
          "start": 937,
          "end": 970,
          "text": "universal speech perception space",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--523:E3"
      },
      {
        "value": {
          "start": 449,
          "end": 471,
          "text": "self-supervised models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--523:E4"
      },
      {
        "value": {
          "start": 1138,
          "end": 1171,
          "text": "fine-grained perceptual phenomena",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--523:E5"
      },
      {
        "value": {
          "start": 5,
          "end": 22,
          "text": "supervised models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--523:E6"
      },
      {
        "from_id": "abstract-2022--acl-long--523:E0",
        "to_id": "abstract-2022--acl-long--523:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--523:E2",
        "to_id": "abstract-2022--acl-long--523:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--523:E4",
        "to_id": "abstract-2022--acl-long--523:E5",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Self-supervised models for speech processing form representational spaces without using any external labels. Increasingly, they appear to be a feasible way of at least partially eliminating costly manual annotations, a problem of particular concern for low-resource languages. But what kind of representational spaces do these models construct?Human perception specializes to the sounds of listeners’ native languages. Does the same thing happen in self-supervised models? We examine the representational spaces of three kinds of state of the art self-supervised models: wav2vec, HuBERT and contrastive predictive coding (CPC), and compare them with the perceptual spaces of French-speaking and English-speaking human listeners, both globally and taking account of the behavioural differences between the two language groups. We show that the CPC model shows a small native language effect, but that wav2vec and HuBERT seem to develop a universal speech perception space which is not language specific. A comparison against the predictions of supervised phone recognisers suggests that all three self-supervised models capture relatively fine-grained perceptual phenomena, while supervised models are better at capturing coarser, phone-level effects, and effects of listeners’ native language, on perception."
    }
  },
  {
    "id": "abstract-2022--acl-long--339",
    "result": [
      {
        "value": {
          "start": 574,
          "end": 611,
          "text": "time-segmented evaluation methodology",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--339:E0"
      },
      {
        "value": {
          "start": 1174,
          "end": 1192,
          "text": "evaluation results",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--339:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--339:E0",
        "to_id": "abstract-2022--acl-long--339:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "There has been a growing interest in developing machine learning (ML) models for code summarization tasks, e.g., comment generation and method naming. Despite substantial increase in the effectiveness of ML models, the evaluation methodologies, i.e., the way people split datasets into training, validation, and test sets, were not well studied. Specifically, no prior work on code summarization considered the timestamps of code and comments during evaluation. This may lead to evaluations that are inconsistent with the intended use cases. In this paper, we introduce the time-segmented evaluation methodology, which is novel to the code summarization research community, and compare it with the mixed-project and cross-project methodologies that have been commonly used. Each methodology can be mapped to some use cases, and the time-segmented methodology should be adopted in the evaluation of ML models for code summarization. To assess the impact of methodologies, we collect a dataset of (code, comment) pairs with timestamps to train and evaluate several recent ML models for code summarization. Our experiments show that different methodologies lead to conflicting evaluation results. We invite the community to expand the set of methodologies used in evaluations."
    }
  },
  {
    "id": "abstract-2022--acl-long--393",
    "result": [
      {
        "value": {
          "start": 145,
          "end": 163,
          "text": "SpeechT5 framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--393:E0"
      }
    ],
    "data": {
      "text": "Motivated by the success of T5 (Text-To-Text Transfer Transformer) in pre-trained natural language processing models, we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. After preprocessing the input speech/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech/text modality based on the output of the decoder. Leveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text. To align the textual and speech information into this unified semantic space, we propose a cross-modal vector quantization approach that randomly mixes up speech/text states with latent units as the interface between encoder and decoder. Extensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification."
    }
  },
  {
    "id": "abstract-2022--acl-long--110",
    "result": [
      {
        "value": {
          "start": 438,
          "end": 455,
          "text": "latency reduction",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--110:E0"
      },
      {
        "value": {
          "start": 810,
          "end": 825,
          "text": "parsing quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--110:E1"
      }
    ],
    "data": {
      "text": "Standard conversational semantic parsing maps a complete user utterance into an executable program, after which the program is executed to respond to the user. This could be slow when the program contains expensive function calls. We investigate the opportunity to reduce latency by predicting and executing function calls while the user is still speaking. We introduce the task of online semantic parsing for this purpose, with a formal latency reduction metric inspired by simultaneous machine translation. We propose a general framework with first a learned prefix-to-program prediction module, and then a simple yet effective thresholding heuristic for subprogram selection for early execution. Experiments on the SMCalFlow and TreeDST datasets show our approach achieves large latency reduction with good parsing quality, with a 30%–65% latency reduction depending on function execution time and allowed cost."
    }
  },
  {
    "id": "abstract-2022--acl-long--31",
    "result": [
      {
        "value": {
          "start": 268,
          "end": 284,
          "text": "Mix and Match LM",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--31:E0"
      }
    ],
    "data": {
      "text": "Recent work on controlled text generation has either required attribute-based fine-tuning of the base language model (LM), or has restricted the parameterization of the attribute discriminator to be compatible with the base autoregressive LM. In this work, we propose Mix and Match LM, a global score-based alternative for controllable text generation that combines arbitrary pre-trained black-box models for achieving the desired attributes in the generated text without involving any fine-tuning or structural assumptions about the black-box models. We interpret the task of controllable generation as drawing samples from an energy-based model whose energy values are a linear combination of scores from black-box models that are separately responsible for fluency, the control attribute, and faithfulness to any conditioning context. We use a Metropolis-Hastings sampling scheme to sample from this energy-based model using bidirectional context and global attribute features. We validate the effectiveness of our approach on various controlled generation and style-based text revision tasks by outperforming recently proposed methods that involve extra training, fine-tuning, or restrictive assumptions over the form of models."
    }
  },
  {
    "id": "abstract-2022--acl-long--138",
    "result": [
      {
        "value": {
          "start": 355,
          "end": 377,
          "text": "translation efficiency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--138:E0"
      }
    ],
    "data": {
      "text": "Interactive neural machine translation (INMT) is able to guarantee high-quality translations by taking human interactions into account. Existing IMT systems relying on lexical constrained decoding (LCD) enable humans to translate in a flexible translation order beyond the left-to-right. However, they typically suffer from two significant limitations in translation efficiency and quality due to the reliance on LCD. In this work, we propose a novel BiTIIMT system, Bilingual Text-Infilling for Interactive Neural Machine Translation. The key idea to BiTIIMT is Bilingual Text-infilling (BiTI) which aims to fill missing segments in a manually revised translation for a given source sentence. We propose a simple yet effective solution by casting this task as a sequence-to-sequence task. In this way, our system performs decoding without explicit constraints and makes full use of revised words for better translation prediction. Experiment results show that BiTiIMT performs significantly better and faster than state-of-the-art LCD-based IMT on three translation tasks."
    }
  },
  {
    "id": "abstract-2022--acl-short--57",
    "result": [
      {
        "value": {
          "start": 213,
          "end": 249,
          "text": "Pluggable Entity Lookup Table (PELT)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-short--57:E0"
      },
      {
        "value": {
          "start": 737,
          "end": 762,
          "text": "transfer entity knowledge",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-short--57:E1"
      },
      {
        "from_id": "abstract-2022--acl-short--57:E0",
        "to_id": "abstract-2022--acl-short--57:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Pre-trained language models (PLMs) cannot well recall rich factual knowledge of entities exhibited in large-scale corpora, especially those rare entities. In this paper, we propose to build a simple but effective Pluggable Entity Lookup Table (PELT) on demand by aggregating the entity’s output representations of multiple occurrences in the corpora. PELT can be compatibly plugged as inputs to infuse supplemental entity knowledge into PLMs. Compared to previous knowledge-enhanced PLMs, PELT only requires 0.2%-5% pre-computation with capability of acquiring knowledge from out-of-domain corpora for domain adaptation scenario. The experiments on knowledge-related tasks demonstrate that our method, PELT, can flexibly and effectively transfer entity knowledge from related corpora into PLMs with different architectures. Our code and models are publicly available athttps://github.com/thunlp/PELT"
    }
  },
  {
    "id": "abstract-2022--acl-long--179",
    "result": [
      {
        "value": {
          "start": 1096,
          "end": 1122,
          "text": "Gen2OIE with AACTrans data",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--179:E0"
      },
      {
        "value": {
          "start": 1173,
          "end": 1175,
          "text": "F1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--179:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--179:E0",
        "to_id": "abstract-2022--acl-long--179:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Progress with supervised Open Information Extraction (OpenIE) has been primarily limited to English due to the scarcity of training data in other languages. In this paper, we explore techniques to automatically convert English text for training OpenIE systems in other languages. We introduce the Alignment-Augmented Constrained Translation (AACTrans) model to translate English sentences and their corresponding extractions consistently with each other — with no changes to vocabulary or semantic meaning which may result from independent translations. Using the data generated with AACTrans, we train a novel two-stage generative OpenIE model, which we call Gen2OIE, that outputs for each sentence: 1) relations in the first stage and 2) all extractions containing the relation in the second stage. Gen2OIE increases relation coverage using a training data transformation technique that is generalizable to multiple languages, in contrast to existing models that use an English-specific training loss. Evaluations on 5 languages — Spanish, Portuguese, Chinese, Hindi and Telugu — show that the Gen2OIE with AACTrans data outperforms prior systems by a margin of 6-25% in F1."
    }
  },
  {
    "id": "abstract-2022--acl-long--512",
    "result": [
      {
        "value": {
          "start": 636,
          "end": 663,
          "text": "sentence sorting experiment",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--512:E0"
      },
      {
        "value": {
          "start": 678,
          "end": 747,
          "text": "sentences sharing the same construction are closer in embedding space",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--512:E1"
      },
      {
        "value": {
          "start": 853,
          "end": 868,
          "text": "more input data",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--512:E2"
      },
      {
        "value": {
          "start": 992,
          "end": 1023,
          "text": "LMs associate ASCs with meaning",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--512:E3"
      },
      {
        "from_id": "abstract-2022--acl-long--512:E0",
        "to_id": "abstract-2022--acl-long--512:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In lexicalist linguistic theories, argument structure is assumed to be predictable from the meaning of verbs. As a result, the verb is the primary determinant of the meaning of a clause. In contrast, construction grammarians propose that argument structure is encoded in constructions (or form-meaning pairs) that are distinct from verbs. Two decades of psycholinguistic research have produced substantial empirical evidence in favor of the construction view. Here we adapt several psycholinguistic studies to probe for the existence of argument structure constructions (ASCs) in Transformer-based language models (LMs). First, using a sentence sorting experiment, we find that sentences sharing the same construction are closer in embedding space than sentences sharing the same verb. Furthermore, LMs increasingly prefer grouping by construction with more input data, mirroring the behavior of non-native language learners. Second, in a “Jabberwocky” priming-based experiment, we find that LMs associate ASCs with meaning, even in semantically nonsensical sentences. Our work offers the first evidence for ASCs in LMs and highlights the potential to devise novel probing methods grounded in psycholinguistic research."
    }
  },
  {
    "id": "abstract-2022--acl-short--58",
    "result": [
      {
        "value": {
          "start": 398,
          "end": 407,
          "text": "S4-Tuning",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-short--58:E0"
      },
      {
        "value": {
          "start": 919,
          "end": 985,
          "text": "improvements over vanilla fine-tuning on three multi-lingual tasks",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-short--58:E1"
      },
      {
        "from_id": "abstract-2022--acl-short--58:E0",
        "to_id": "abstract-2022--acl-short--58:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The emergence of multilingual pre-trained language models makes it possible to adapt to target languages with only few labeled examples. However, vanilla fine-tuning tends to achieve degenerated and unstable results, owing to the Language Interference among different languages, and Parameter Overload under the few-sample transfer learning scenarios. To address two problems elegantly, we propose S4-Tuning, a Simple Cross-lingual Sub-network Tuning method. S4-Tuning first detects the most essential sub-network for each target language, and only updates it during fine-tuning.In this way, the language sub-networks lower the scale of trainable parameters, and hence better suit the low-resource scenarios.Meanwhile, the commonality and characteristics across languages are modeled by the overlapping and non-overlapping parts to ease the interference among languages.Simple but effective, S4-Tuning gains consistent improvements over vanilla fine-tuning on three multi-lingual tasks involving 37 different languages in total (XNLI, PAWS-X, and Tatoeba)."
    }
  },
  {
    "id": "abstract-2022--acl-long--448",
    "result": [
      {
        "value": {
          "start": 844,
          "end": 874,
          "text": "increasing compound divergence",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--448:E0"
      },
      {
        "value": {
          "start": 884,
          "end": 914,
          "text": "dependency parsing performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--448:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--448:E0",
        "to_id": "abstract-2022--acl-long--448:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Compositionality— the ability to combine familiar units like words into novel phrases and sentences— has been the focus of intense interest in artificial intelligence in recent years. To test compositional generalization in semantic parsing, Keysers et al. (2020) introduced Compositional Freebase Queries (CFQ). This dataset maximizes the similarity between the test and train distributions over primitive units, like words, while maximizing the compound divergence: the dissimilarity between test and train distributions over larger structures, like phrases. Dependency parsing, however, lacks a compositional generalization benchmark. In this work, we introduce a gold-standard set of dependency parses for CFQ, and use this to analyze the behaviour of a state-of-the art dependency parser (Qi et al., 2020) on the CFQ dataset. We find that increasing compound divergence degrades dependency parsing performance, although not as dramatically as semantic parsing performance. Additionally, we find the performance of the dependency parser does not uniformly degrade relative to compound divergence, and the parser performs differently on different splits with the same compound divergence. We explore a number of hypotheses for what causes the non-uniform degradation in dependency parsing performance, and identify a number of syntactic structures that drive the dependency parser’s lower performance on the most challenging splits."
    }
  },
  {
    "id": "abstract-2022--acl-long--281",
    "result": [
      {
        "value": {
          "start": 944,
          "end": 981,
          "text": "combination of multiple input signals",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--281:E0"
      },
      {
        "value": {
          "start": 999,
          "end": 1028,
          "text": "cross-target stance detection",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--281:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--281:E0",
        "to_id": "abstract-2022--acl-long--281:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Research in stance detection has so far focused on models which leverage purely textual input. In this paper, we investigate the integration of textual and financial signals for stance detection in the financial domain. Specifically, we propose a robust multi-task neural architecture that combines textual input with high-frequency intra-day time series from stock market prices. Moreover, we extend wt–wt, an existing stance detection dataset which collects tweets discussing Mergers and Acquisitions operations, with the relevant financial signal. Importantly, the obtained dataset aligns with Stander, an existing news stance detection dataset, thus resulting in a unique multimodal, multi-genre stance detection resource. We show experimentally and through detailed result analysis that our stance detection system benefits from financial information, and achieves state-of-the-art results on the wt–wt dataset: this demonstrates that the combination of multiple input signals is effective for cross-target stance detection, and opens interesting research directions for future work."
    }
  },
  {
    "id": "abstract-2022--acl-long--202",
    "result": [
      {
        "value": {
          "start": 384,
          "end": 416,
          "text": "Pragmatic Rational Speaker (PRS)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--202:E0"
      },
      {
        "value": {
          "start": 1079,
          "end": 1105,
          "text": "collaborative task outcome",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--202:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--202:E0",
        "to_id": "abstract-2022--acl-long--202:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Human communication is a collaborative process. Speakers, on top of conveying their own intent, adjust the content and language expressions by taking the listeners into account, including their knowledge background, personalities, and physical capabilities. Towards building AI agents with similar abilities in language communication, we propose a novel rational reasoning framework, Pragmatic Rational Speaker (PRS), where the speaker attempts to learn the speaker-listener disparity and adjust the speech accordingly, by adding a light-weighted disparity adjustment layer into working memory on top of speaker’s long-term memory system. By fixing the long-term memory, the PRS only needs to update its working memory to learn and adapt to different types of listeners. To validate our framework, we create a dataset that simulates different types of speaker-listener disparities in the context of referential games. Our empirical results demonstrate that the PRS is able to shift its output towards the language that listeners are able to understand, significantly improve the collaborative task outcome, and learn the disparity more efficiently than joint training."
    }
  },
  {
    "id": "abstract-2022--acl-long--358",
    "result": [
      {
        "value": {
          "start": 378,
          "end": 462,
          "text": "generative template-based event extraction method with dynamic prefix (GTEE-DynPref)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--358:E0"
      },
      {
        "value": {
          "start": 629,
          "end": 719,
          "text": "competitive results with the state-of-the-art classification-based model OneIE on ACE 2005",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--358:E1"
      },
      {
        "value": {
          "start": 737,
          "end": 761,
          "text": "best performances on ERE",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--358:E2"
      },
      {
        "from_id": "abstract-2022--acl-long--358:E0",
        "to_id": "abstract-2022--acl-long--358:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--358:E0",
        "to_id": "abstract-2022--acl-long--358:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We consider event extraction in a generative manner with template-based conditional generation. Although there is a rising trend of casting the task of event extraction as a sequence generation problem with prompts, these generation-based methods have two significant challenges, including using suboptimal prompts and static event type information. In this paper, we propose a generative template-based event extraction method with dynamic prefix (GTEE-DynPref) by integrating context information with type-specific prefixes to learn a context-specific prefix for each context. Experimental results show that our model achieves competitive results with the state-of-the-art classification-based model OneIE on ACE 2005 and achieves the best performances on ERE.Additionally, our model is proven to be portable to new types of events effectively."
    }
  },
  {
    "id": "abstract-2022--acl-long--93",
    "result": [
      {
        "value": {
          "start": 319,
          "end": 330,
          "text": "FrugalScore",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--93:E0"
      },
      {
        "value": {
          "start": 445,
          "end": 456,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--93:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--93:E0",
        "to_id": "abstract-2022--acl-long--93:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Fast and reliable evaluation metrics are key to R&D progress. While traditional natural language generation metrics are fast, they are not very reliable. Conversely, new metrics based on large pretrained language models are much more reliable, but require significant computational resources. In this paper, we propose FrugalScore, an approach to learn a fixed, low cost version of any expensive NLG metric, while retaining most of its original performance. Experiments with BERTScore and MoverScore on summarization and translation show that FrugalScore is on par with the original metrics (and sometimes better), while having several orders of magnitude less parameters and running several times faster. On average over all learned metrics, tasks, and variants, FrugalScore retains 96.8% of the performance, runs 24 times faster, and has 35 times less parameters than the original metrics. We make our trained metrics publicly available, to benefit the entire NLP community and in particular researchers and practitioners with limited resources."
    }
  },
  {
    "id": "abstract-2022--acl-long--362",
    "result": [
      {
        "value": {
          "start": 200,
          "end": 233,
          "text": "transformer-based language models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--362:E0"
      }
    ],
    "data": {
      "text": "There is a growing interest in the combined use of NLP and machine learning methods to predict gaze patterns during naturalistic reading. While promising results have been obtained through the use of transformer-based language models, little work has been undertaken to relate the performance of such models to general text characteristics. In this paper we report on experiments with two eye-tracking corpora of naturalistic reading and two language models (BERT and GPT-2). In all experiments, we test effects of a broad spectrum of features for predicting human reading behavior that fall into five categories (syntactic complexity, lexical richness, register-based multiword combinations, readability and psycholinguistic word properties). Our experiments show that both the features included and the architecture of the transformer-based language models play a role in predicting multiple eye-tracking measures during naturalistic reading. We also report the results of experiments aimed at determining the relative importance of features from different groups using SP-LIME."
    }
  },
  {
    "id": "abstract-2022--acl-long--294",
    "result": [
      {
        "value": {
          "start": 345,
          "end": 376,
          "text": "EDU-level pre-training approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--294:E0"
      },
      {
        "value": {
          "start": 929,
          "end": 937,
          "text": "F1-score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--294:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--294:E0",
        "to_id": "abstract-2022--acl-long--294:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Pre-trained language models (PLMs) have shown great potentials in natural language processing (NLP) including rhetorical structure theory (RST) discourse parsing. Current PLMs are obtained by sentence-level pre-training, which is different from the basic processing unit, i.e. element discourse unit (EDU).To this end, we propose a second-stage EDU-level pre-training approach in this work, which presents two novel tasks to learn effective EDU representations continually based on well pre-trained language models. Concretely, the two tasks are (1) next EDU prediction (NEP) and (2) discourse marker prediction (DMP).We take a state-of-the-art transition-based neural parser as baseline, and adopt it with a light bi-gram EDU modification to effectively explore the EDU-level pre-trained EDU representation. Experimental results on a benckmark dataset show that our method is highly effective,leading a 2.1-point improvement in F1-score. All codes and pre-trained models will be released publicly to facilitate future studies."
    }
  },
  {
    "id": "abstract-2022--acl-short--21",
    "result": [
      {
        "value": {
          "start": 1152,
          "end": 1194,
          "text": "coverage, consistency and interpretability",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-short--21:E0"
      }
    ],
    "data": {
      "text": "In recent years, a flurry of morphological datasets had emerged, most notably UniMorph, aa multi-lingual repository of inflection tables. However, the flat structure of the current morphological annotation makes the treatment of some languages quirky, if not impossible, specifically in cases of polypersonal agreement. In this paper we propose a general solution for such cases and expand the UniMorph annotation schema to naturally address this phenomenon, in which verbs agree with multiple arguments using true affixes. We apply this extended schema to one such language, Georgian, and provide a human-verified, accurate and balanced morphological dataset for Georgian verbs. The dataset has 4 times more tables and 6 times more verb forms compared to the existing UniMorph dataset, covering all possible variants of argument marking, demonstrating the adequacy of our proposed scheme. Experiments on a reinflection task show that generalization is easy when the data is split at the form level, but extremely hard when splitting along lemma lines. Expanding the other languages in UniMorph according to this schema is expected to improve both the coverage, consistency and interpretability of this benchmark."
    }
  },
  {
    "id": "abstract-2022--acl-long--487",
    "result": [
      {
        "value": {
          "start": 1024,
          "end": 1066,
          "text": "integrating vectorized lexical constraints",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--487:E0"
      }
    ],
    "data": {
      "text": "Lexically constrained neural machine translation (NMT), which controls the generation of NMT models with pre-specified constraints, is important in many practical scenarios. Due to the representation gap between discrete constraints and continuous vectors in NMT models, most existing works choose to construct synthetic data or modify the decoding algorithm to impose lexical constraints, treating the NMT model as a black box. In this work, we propose to open this black box by directly integrating the constraints into NMT models. Specifically, we vectorize source and target constraints into continuous keys and values, which can be utilized by the attention modules of NMT models. The proposed integration method is based on the assumption that the correspondence between keys and values in attention modules is naturally suitable for modeling constraint pairs. Experimental results show that our method consistently outperforms several representative baselines on four language pairs, demonstrating the superiority of integrating vectorized lexical constraints."
    }
  },
  {
    "id": "abstract-2022--acl-long--213",
    "result": [
      {
        "value": {
          "start": 11,
          "end": 19,
          "text": "ProtoTEx",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--213:E0"
      },
      {
        "value": {
          "start": 623,
          "end": 631,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--213:E1"
      },
      {
        "value": {
          "start": 761,
          "end": 789,
          "text": "prototype-based explanations",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--213:E2"
      },
      {
        "value": {
          "start": 817,
          "end": 837,
          "text": "recognize propaganda",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--213:E3"
      },
      {
        "from_id": "abstract-2022--acl-long--213:E0",
        "to_id": "abstract-2022--acl-long--213:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--213:E2",
        "to_id": "abstract-2022--acl-long--213:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We present ProtoTEx, a novel white-box NLP classification architecture based on prototype networks (Li et al., 2018). ProtoTEx faithfully explains model decisions based on prototype tensors that encode latent clusters of training examples. At inference time, classification decisions are based on the distances between the input text and the prototype tensors, explained via the training examples most similar to the most influential prototypes. We also describe a novel interleaved training algorithm that effectively handles classes characterized by ProtoTEx indicative features. On a propaganda detection task, ProtoTEx accuracy matches BART-large and exceeds BERTlarge with the added benefit of providing faithful explanations. A user study also shows that prototype-based explanations help non-experts to better recognize propaganda in online news."
    }
  },
  {
    "id": "abstract-2022--acl-long--497",
    "result": [
      {
        "value": {
          "start": 589,
          "end": 632,
          "text": "adaptive and weighted sampling distribution",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--497:E0"
      },
      {
        "value": {
          "start": 863,
          "end": 879,
          "text": "loss convergence",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--497:E1"
      },
      {
        "value": {
          "start": 902,
          "end": 928,
          "text": "improved negative sampling",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--497:E2"
      },
      {
        "value": {
          "start": 947,
          "end": 971,
          "text": "state-of-the-art results",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--497:E3"
      },
      {
        "from_id": "abstract-2022--acl-long--497:E0",
        "to_id": "abstract-2022--acl-long--497:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--497:E2",
        "to_id": "abstract-2022--acl-long--497:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Negative sampling is highly effective in handling missing annotations for named entity recognition (NER). One of our contributions is an analysis on how it makes sense through introducing two insightful concepts: missampling and uncertainty. Empirical studies show low missampling rate and high uncertainty are both essential for achieving promising performances with negative sampling. Based on the sparsity of named entities, we also theoretically derive a lower bound for the probability of zero missampling rate, which is only relevant to sentence length. The other contribution is an adaptive and weighted sampling distribution that further improves negative sampling via our former analysis. Experiments on synthetic datasets and well-annotated datasets (e.g., CoNLL-2003) show that our proposed approach benefits negative sampling in terms of F1 score and loss convergence. Besides, models with improved negative sampling have achieved new state-of-the-art results on real-world datasets (e.g., EC)."
    }
  },
  {
    "id": "abstract-2022--acl-long--331",
    "result": [
      {
        "value": {
          "start": 547,
          "end": 583,
          "text": "token-level contrastive distillation",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--331:E0"
      },
      {
        "value": {
          "start": 632,
          "end": 659,
          "text": "module-wise dynamic scaling",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--331:E1"
      },
      {
        "value": {
          "start": 759,
          "end": 774,
          "text": "proposed method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--331:E2"
      },
      {
        "value": {
          "start": 953,
          "end": 978,
          "text": "compression rate on GPT-2",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--331:E3"
      },
      {
        "from_id": "abstract-2022--acl-long--331:E2",
        "to_id": "abstract-2022--acl-long--331:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The increasing size of generative Pre-trained Language Models (PLMs) have greatly increased the demand for model compression. Despite various methods to compress BERT or its variants, there are few attempts to compress generative PLMs, and the underlying difficulty remains unclear. In this paper, we compress generative PLMs by quantization. We find that previous quantization methods fail on generative tasks due to the homogeneous word embeddings caused by reduced capacity and the varied distribution of weights. Correspondingly, we propose a token-level contrastive distillation to learn distinguishable word embeddings, and a module-wise dynamic scaling to make quantizers adaptive to different modules. Empirical results on various tasks show that our proposed method outperforms the state-of-the-art compression methods on generative PLMs by a clear margin. With comparable performance with the full-precision models, we achieve 14.4x and 13.4x compression rate on GPT-2 and BART, respectively."
    }
  },
  {
    "id": "abstract-2022--acl-short--37",
    "result": [
      {
        "value": {
          "start": 416,
          "end": 454,
          "text": "Hierarchical Curriculum Learning (HCL)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-short--37:E0"
      },
      {
        "value": {
          "start": 747,
          "end": 774,
          "text": "learning complex structures",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-short--37:E1"
      },
      {
        "from_id": "abstract-2022--acl-short--37:E0",
        "to_id": "abstract-2022--acl-short--37:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Abstract Meaning Representation (AMR) parsing aims to translate sentences to semantic representation with a hierarchical structure, and is recently empowered by pretrained sequence-to-sequence models. However, there exists a gap between their flat training objective (i.e., equally treats all output tokens) and the hierarchical AMR structure, which limits the model generalization. To bridge this gap, we propose a Hierarchical Curriculum Learning (HCL) framework with Structure-level (SC) and Instance-level Curricula (IC). SC switches progressively from core to detail AMR semantic elements while IC transits from structure-simple to -complex AMR instances during training. Through these two warming-up processes, HCL reduces the difficulty of learning complex structures, thus the flat model can better adapt to the AMR hierarchy. Extensive experiments on AMR2.0, AMR3.0, structure-complex and out-of-distribution situations verify the effectiveness of HCL."
    }
  },
  {
    "id": "abstract-2022--acl-short--96",
    "result": [
      {
        "value": {
          "start": 584,
          "end": 605,
          "text": "split-by-lemma method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-short--96:E0"
      },
      {
        "value": {
          "start": 623,
          "end": 657,
          "text": "performance on existing benchmarks",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-short--96:E1"
      },
      {
        "value": {
          "start": 824,
          "end": 858,
          "text": "macro-average for the 90 languages",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-short--96:E2"
      },
      {
        "from_id": "abstract-2022--acl-short--96:E0",
        "to_id": "abstract-2022--acl-short--96:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2022--acl-short--96:E0",
        "to_id": "abstract-2022--acl-short--96:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "In the domain of Morphology, Inflection is a fundamental and important task that gained a lot of traction in recent years, mostly via SIGMORPHON’s shared-tasks. With average accuracy above 0.9 over the scores of all languages, the task is considered mostly solved using relatively generic neural seq2seq models, even with little data provided. In this work, we propose to re-evaluate morphological inflection models by employing harder train-test splits that will challenge the generalization capacity of the models. In particular, as opposed to the naïve split-by-form, we propose a split-by-lemma method to challenge the performance on existing benchmarks. Our experiments with the three top-ranked systems on the SIGMORPHON’s 2020 shared-task show that the lemma-split presents an average drop of 30 percentage points in macro-average for the 90 languages included. The effect is most significant for low-resourced languages with a drop as high as 95 points, but even high-resourced languages lose about 10 points on average. Our results clearly show that generalizing inflection to unseen lemmas is far from being solved, presenting a simple yet effective means to promote more sophisticated models."
    }
  },
  {
    "id": "abstract-2022--acl-long--203",
    "result": [
      {
        "value": {
          "start": 614,
          "end": 625,
          "text": "coCondenser",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--203:E0"
      },
      {
        "value": {
          "start": 962,
          "end": 973,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--203:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--203:E0",
        "to_id": "abstract-2022--acl-long--203:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recent research demonstrates the effectiveness of using fine-tuned language models (LM) for dense retrieval. However, dense retrievers are hard to train, typically requiring heavily engineered fine-tuning pipelines to realize their full potential. In this paper, we identify and address two underlying problems of dense retrievers: i) fragility to training data noise and ii) requiring large batches to robustly learn the embedding space. We use the recently proposed Condenser pre-training architecture, which learns to condense information into the dense vector through LM pre-training. On top of it, we propose coCondenser, which adds an unsupervised corpus-level contrastive loss to warm up the passage embedding space. Experiments on MS-MARCO, Natural Question, and Trivia QA datasets show that coCondenser removes the need for heavy data engineering such as augmentation, synthesis, or filtering, and the need for large batch training. It shows comparable performance to RocketQA, a state-of-the-art, heavily engineered system, using simple small batch fine-tuning."
    }
  },
  {
    "id": "abstract-2022--acl-long--431",
    "result": [
      {
        "value": {
          "start": 615,
          "end": 660,
          "text": "retrieval-augmented code completion framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--431:E0"
      },
      {
        "value": {
          "start": 1019,
          "end": 1053,
          "text": "performance on CodeXGLUE benchmark",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--431:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--431:E0",
        "to_id": "abstract-2022--acl-long--431:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Code completion, which aims to predict the following code token(s) according to the code context, can improve the productivity of software development. Recent work has proved that statistical language modeling with transformers can greatly improve the performance in the code completion task via learning from large-scale source code datasets. However, current approaches focus only on code context within the file or project, i.e. internal context. Our distinction is utilizing ”external” context, inspired by human behaviors of copying from the related code snippets when writing code. Specifically, we propose a retrieval-augmented code completion framework, leveraging both lexical copying and referring to code with similar semantics by retrieval. We adopt a stage-wise training approach that combines a source code retriever and an auto-regressive language model for programming language. We evaluate our approach in the code completion task in Python and Java programming languages, achieving a state-of-the-art performance on CodeXGLUE benchmark."
    }
  },
  {
    "id": "abstract-2022--acl-long--116",
    "result": [
      {
        "value": {
          "start": 1149,
          "end": 1165,
          "text": "DoKTra framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--116:E0"
      },
      {
        "value": {
          "start": 125,
          "end": 156,
          "text": "performance on downstream tasks",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--116:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--116:E0",
        "to_id": "abstract-2022--acl-long--116:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Since the development and wide use of pretrained language models (PLMs), several approaches have been applied to boost their performance on downstream tasks in specific domains, such as biomedical or scientific domains. Additional pre-training with in-domain texts is the most common approach for providing domain-specific knowledge to PLMs. However, these pre-training methods require considerable in-domain data and training resources and a longer training time. Moreover, the training must be re-performed whenever a new PLM emerges. In this study, we propose a domain knowledge transferring (DoKTra) framework for PLMs without additional in-domain pretraining. Specifically, we extract the domain knowledge from an existing in-domain pretrained language model and transfer it to other PLMs by applying knowledge distillation. In particular, we employ activation boundary distillation, which focuses on the activation of hidden neurons. We also apply an entropy regularization term in both teacher training and distillation to encourage the model to generate reliable output probabilities, and thus aid the distillation. By applying the proposed DoKTra framework to downstream tasks in the biomedical, clinical, and financial domains, our student models can retain a high percentage of teacher performance and even outperform the teachers in certain tasks. Our code is available athttps://github.com/DMCB-GIST/DoKTra."
    }
  },
  {
    "id": "abstract-2022--acl-long--575",
    "result": [
      {
        "value": {
          "start": 383,
          "end": 387,
          "text": "GLAT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--575:E0"
      },
      {
        "value": {
          "start": 607,
          "end": 635,
          "text": "outperforms strong baselines",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--575:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--575:E0",
        "to_id": "abstract-2022--acl-long--575:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recently, parallel text generation has received widespread attention due to its success in generation efficiency. Although many advanced techniques are proposed to improve its generation quality, they still need the help of an autoregressive model for training to overcome the one-to-many multi-modal phenomenon in the dataset, limiting their applications. In this paper, we propose GLAT, which employs the discrete latent variables to capture word categorical information and invoke an advanced curriculum learning technique, alleviating the multi-modality problem. Experiment results show that our method outperforms strong baselines without the help of an autoregressive model, which further broadens the application scenarios of the parallel decoding paradigm."
    }
  },
  {
    "id": "abstract-2022--acl-long--351",
    "result": [
      {
        "value": {
          "start": 1126,
          "end": 1137,
          "text": "ROUGE score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--351:E0"
      },
      {
        "value": {
          "start": 1151,
          "end": 1163,
          "text": "METEOR score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--351:E1"
      }
    ],
    "data": {
      "text": "Multi-document summarization (MDS) has made significant progress in recent years, in part facilitated by the availability of new, dedicated datasets and capacious language models. However, a standing limitation of these models is that they are trained against limited references and with plain maximum-likelihood objectives. As for many other generative tasks, reinforcement learning (RL) offers the potential to improve the training of MDS models; yet, it requires a carefully-designed reward that can ensure appropriate leverage of both the reference summaries and the input documents. For this reason, in this paper we propose fine-tuning an MDS baseline with a reward that balances a reference-based metric such as ROUGE with coverage of the input documents. To implement the approach, we utilize RELAX (Grathwohl et al., 2018), a contemporary gradient estimator which is both low-variance and unbiased, and we fine-tune the baseline in a few-shot style for both stability and computational efficiency. Experimental results over the Multi-News and WCEP MDS datasets show significant improvements of up to +0.95 pp average ROUGE score and +3.17 pp METEOR score over the baseline, and competitive results with the literature. In addition, they show that the coverage of the input documents is increased, and evenly across all documents."
    }
  },
  {
    "id": "abstract-2022--acl-long--78",
    "result": [
      {
        "value": {
          "start": 1044,
          "end": 1072,
          "text": "hierarchy-aware logical form",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--78:E0"
      },
      {
        "value": {
          "start": 1126,
          "end": 1139,
          "text": "effectiveness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--78:E1"
      },
      {
        "value": {
          "start": 1180,
          "end": 1209,
          "text": "entity and quantity alignment",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--78:E2"
      },
      {
        "value": {
          "start": 1304,
          "end": 1337,
          "text": "reduce spurious predictions in QA",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--78:E3"
      },
      {
        "value": {
          "start": 1350,
          "end": 1376,
          "text": "better descriptions in NLG",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--78:E4"
      },
      {
        "from_id": "abstract-2022--acl-long--78:E0",
        "to_id": "abstract-2022--acl-long--78:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--78:E2",
        "to_id": "abstract-2022--acl-long--78:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--78:E2",
        "to_id": "abstract-2022--acl-long--78:E4",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Tables are often created with hierarchies, but existing works on table reasoning mainly focus on flat tables and neglect hierarchical tables. Hierarchical tables challenge numerical reasoning by complex hierarchical indexing, as well as implicit relationships of calculation and semantics. We present a new dataset, HiTab, to study question answering (QA) and natural language generation (NLG) over hierarchical tables. HiTab is a cross-domain dataset constructed from a wealth of statistical reports and Wikipedia pages, and has unique characteristics: (1) nearly all tables are hierarchical, and (2) QA pairs are not proposed by annotators from scratch, but are revised from real and meaningful sentences authored by analysts. (3) to reveal complex numerical reasoning in statistical reports, we provide fine-grained annotations of quantity and entity alignment. Experiments suggest that this HiTab presents a strong challenge for existing baselines and a valuable benchmark for future research. Targeting hierarchical structure, we devise a hierarchy-aware logical form for symbolic reasoning over tables, which shows high effectiveness. Targeting table reasoning, we leverage entity and quantity alignment to explore partially supervised training in QA and conditional generation in NLG, and largely reduce spurious predictions in QA and produce better descriptions in NLG."
    }
  },
  {
    "id": "abstract-2022--acl-long--140",
    "result": [
      {
        "value": {
          "start": 249,
          "end": 286,
          "text": "Heterogeneous Linguistics Graph (HLG)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--140:E0"
      },
      {
        "value": {
          "start": 716,
          "end": 727,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--140:E1"
      },
      {
        "value": {
          "start": 1034,
          "end": 1044,
          "text": "parameters",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--140:E2"
      },
      {
        "from_id": "abstract-2022--acl-long--140:E0",
        "to_id": "abstract-2022--acl-long--140:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2022--acl-long--140:E0",
        "to_id": "abstract-2022--acl-long--140:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Chinese pre-trained language models usually exploit contextual character information to learn representations, while ignoring the linguistics knowledge, e.g., word and sentence information. Hence, we propose a task-free enhancement module termed as Heterogeneous Linguistics Graph (HLG) to enhance Chinese pre-trained language models by integrating linguistics knowledge. Specifically, we construct a hierarchical heterogeneous graph to model the characteristics linguistics structure of Chinese language, and conduct a graph-based method to summarize and concretize information on different granularities of Chinese linguistics hierarchies. Experimental results demonstrate our model has the ability to improve the performance of vanilla BERT, BERTwwm and ERNIE 1.0 on 6 natural language processing tasks with 10 benchmark datasets. Further, the detailed experimental analyses have proven that this kind of modelization achieves more improvements compared with previous strong baseline MWA. Meanwhile, our model introduces far fewer parameters (about half of MWA) and the training/inference speed is about 7x faster than MWA."
    }
  },
  {
    "id": "abstract-2022--acl-long--297",
    "result": [
      {
        "value": {
          "start": 860,
          "end": 881,
          "text": "legal-oriented models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--297:E0"
      }
    ],
    "data": {
      "text": "Laws and their interpretations, legal arguments and agreements are typically expressed in writing, leading to the production of vast corpora of legal text. Their analysis, which is at the center of legal practice, becomes increasingly elaborate as these collections grow in size. Natural language understanding (NLU) technologies can be a valuable tool to support legal practitioners in these endeavors. Their usefulness, however, largely depends on whether current state-of-the-art models can generalize across various tasks in the legal domain. To answer this currently open question, we introduce the Legal General Language Understanding Evaluation (LexGLUE) benchmark, a collection of datasets for evaluating model performance across a diverse set of legal NLU tasks in a standardized way. We also provide an evaluation and analysis of several generic and legal-oriented models demonstrating that the latter consistently offer performance improvements across multiple tasks."
    }
  },
  {
    "id": "abstract-2022--acl-long--391",
    "result": [
      {
        "value": {
          "start": 879,
          "end": 923,
          "text": "state-of-the-art methods on the M3ED dataset",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--391:E0"
      },
      {
        "value": {
          "start": 938,
          "end": 973,
          "text": "validity and quality of the dataset",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--391:E1"
      },
      {
        "value": {
          "start": 1136,
          "end": 1147,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--391:E2"
      },
      {
        "from_id": "abstract-2022--acl-long--391:E0",
        "to_id": "abstract-2022--acl-long--391:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "The emotional state of a speaker can be influenced by many different factors in dialogues, such as dialogue scene, dialogue topic, and interlocutor stimulus. The currently available data resources to support such multimodal affective analysis in dialogues are however limited in scale and diversity. In this work, we propose a Multi-modal Multi-scene Multi-label Emotional Dialogue dataset, M3ED, which contains 990 dyadic emotional dialogues from 56 different TV series, a total of 9,082 turns and 24,449 utterances. M3ED is annotated with 7 emotion categories (happy, surprise, sad, disgust, anger, fear, and neutral) at utterance level, and encompasses acoustic, visual, and textual modalities. To the best of our knowledge, M3ED is the first multimodal emotional dialogue dataset in Chinese.It is valuable for cross-culture emotion analysis and recognition. We apply several state-of-the-art methods on the M3ED dataset to verify the validity and quality of the dataset. We also propose a general Multimodal Dialogue-aware Interaction framework, MDI, to model the dialogue context for emotion recognition, which achieves comparable performance to the state-of-the-art methods on the M3ED. The full dataset and codes are available."
    }
  },
  {
    "id": "abstract-2022--acl-long--364",
    "result": [
      {
        "value": {
          "start": 599,
          "end": 631,
          "text": "models pre-trained on phone data",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2022--acl-long--364:E0"
      },
      {
        "value": {
          "start": 667,
          "end": 675,
          "text": "F1-score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2022--acl-long--364:E1"
      },
      {
        "from_id": "abstract-2022--acl-long--364:E0",
        "to_id": "abstract-2022--acl-long--364:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Multi-modal techniques offer significant untapped potential to unlock improved NLP technology for local languages. However, many advances in language model pre-training are focused on text, a fact that only increases systematic inequalities in the performance of NLP tasks across the world’s languages. In this work, we propose a multi-modal approach to train language models using whatever text and/or audio data might be available in a language. Initial experiments using Swahili and Kinyarwanda data suggest the viability of the approach for downstream Named Entity Recognition (NER) tasks, with models pre-trained on phone data showing an improvement of up to 6% F1-score above models that are trained from scratch. Preprocessing and training code will be uploaded tohttps://github.com/sil-ai/phone-it-in."
    }
  },
  {
    "id": "abstract-2023--acl-demo--46",
    "result": [
      {
        "value": {
          "start": 928,
          "end": 944,
          "text": "query throughput",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-demo--46:E0"
      }
    ],
    "data": {
      "text": "Alfred is the first system for programmatic weak supervision (PWS) that creates training data for machine learning by prompting. In contrast to typical PWS systems where weak supervision sources are programs coded by experts, Alfred enables users to encode their subject matter expertise via natural language prompts for language and vision-language models. Alfred provides a simple Python interface for the key steps of this emerging paradigm, with a high-throughput backend for large-scale data labeling. Users can quickly create, evaluate, and refine their prompt-based weak supervision sources; map the results to weak labels; and resolve their disagreements with a label model. Alfred enables a seamless local development experience backed by models served from self-managed computing clusters. It automatically optimizes the execution of prompts with optimized batching mechanisms. We find that this optimization improves query throughput by 2.9x versus a naive approach. We present two example use cases demonstrating Alfred on YouTube comment spam detection and pet breeds classification. Alfred is open source, available athttps://github.com/BatsResearch/alfred."
    }
  },
  {
    "id": "abstract-2023--acl-long--436",
    "result": [
      {
        "value": {
          "start": 401,
          "end": 447,
          "text": "Cross-modal Mixup via Optimal Transport (CMOT)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--436:E0"
      },
      {
        "value": {
          "start": 723,
          "end": 735,
          "text": "average BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--436:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--436:E0",
        "to_id": "abstract-2023--acl-long--436:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "End-to-end speech translation (ST) is the task of translating speech signals in the source language into text in the target language. As a cross-modal task, end-to-end ST is difficult to train with limited data. Existing methods often try to transfer knowledge from machine translation (MT), but their performances are restricted by the modality gap between speech and text. In this paper, we propose Cross-modal Mixup via Optimal Transport (CMOT) to overcome the modality gap. We find the alignment between speech and text sequences via optimal transport and then mix up the sequences from different modalities at a token level using the alignment. Experiments on the MuST-C ST benchmark demonstrate that CMOT achieves an average BLEU of 30.0 in 8 translation directions, outperforming previous methods. Further analysis shows CMOT can adaptively find the alignment between modalities, which helps alleviate the modality gap between speech and text."
    }
  },
  {
    "id": "abstract-2023--acl-long--348",
    "result": [
      {
        "value": {
          "start": 645,
          "end": 655,
          "text": "MultiTabQA",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--348:E0"
      }
    ],
    "data": {
      "text": "Recent advances in tabular question answering (QA) with large language models are constrained in their coverage and only answer questions over a single table. However, real-world queries are complex in nature, often over multiple tables in a relational database or web page. Single table questions do not involve common table operations such as set operations, Cartesian products (joins), or nested queries. Furthermore, multi-table operations often result in a tabular output, which necessitates table generation capabilities of tabular QA models. To fill this gap, we propose a new task of answering questions over multiple tables. Our model, MultiTabQA, not only answers questions over multiple tables, but also generalizes to generate tabular answers. To enable effective training, we build a pre-training dataset comprising of 132,645 SQL queries and tabular answers. Further, we evaluate the generated tables by introducing table-specific metrics of varying strictness assessing various levels of granularity of the table structure. MultiTabQA outperforms state-of-the-art single table QA models adapted to a multi-table QA setting by finetuning on three datasets: Spider, Atis and GeoQuery."
    }
  },
  {
    "id": "abstract-2023--acl-long--542",
    "result": [
      {
        "value": {
          "start": 502,
          "end": 550,
          "text": "Multi-View Enhanced Distillation (MVD) framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--542:E0"
      }
    ],
    "data": {
      "text": "Dense retrieval is widely used for entity linking to retrieve entities from large-scale knowledge bases. Mainstream techniques are based on a dual-encoder framework, which encodes mentions and entities independently and calculates their relevances via rough interaction metrics, resulting in difficulty in explicitly modeling multiple mention-relevant parts within entities to match divergent mentions. Aiming at learning entity representations that can match divergent mentions, this paper proposes a Multi-View Enhanced Distillation (MVD) framework, which can effectively transfer knowledge of multiple fine-grained and mention-relevant parts within entities from cross-encoders to dual-encoders. Each entity is split into multiple views to avoid irrelevant information being over-squashed into the mention-relevant view. We further design cross-alignment and self-alignment mechanisms for this framework to facilitate fine-grained knowledge distillation from the teacher model to the student model. Meanwhile, we reserve a global-view that embeds the entity as a whole to prevent dispersal of uniform information. Experiments show our method achieves state-of-the-art performance on several entity linking benchmarks."
    }
  },
  {
    "id": "abstract-2023--acl-short--88",
    "result": [
      {
        "value": {
          "start": 670,
          "end": 717,
          "text": "group-specific layers to multi-annotator models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--88:E0"
      },
      {
        "value": {
          "start": 885,
          "end": 902,
          "text": "model performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--88:E1"
      },
      {
        "from_id": "abstract-2023--acl-short--88:E0",
        "to_id": "abstract-2023--acl-short--88:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Many NLP tasks exhibit human label variation, where different annotators give different labels to the same texts. This variation is known to depend, at least in part, on the sociodemographics of annotators. Recent research aims to model individual annotator behaviour rather than predicting aggregated labels, and we would expect that sociodemographic information is useful for these models. On the other hand, the ecological fallacy states that aggregate group behaviour, such as the behaviour of the average female annotator, does not necessarily explain individual behaviour. To account for sociodemographics in models of individual annotator behaviour, we introduce group-specific layers to multi-annotator models. In a series of experiments for toxic content detection, we find that explicitly accounting for sociodemographic attributes in this way does not significantly improve model performance. This result shows that individual annotation behaviour depends on much more than just sociodemographics."
    }
  },
  {
    "id": "abstract-2023--acl-long--664",
    "result": [
      {
        "value": {
          "start": 460,
          "end": 472,
          "text": "MultiCapCLIP",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--664:E0"
      },
      {
        "value": {
          "start": 1398,
          "end": 1410,
          "text": "CIDEr metric",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--664:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--664:E0",
        "to_id": "abstract-2023--acl-long--664:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Supervised visual captioning models typically require a large scale of images or videos paired with descriptions in a specific language (i.e., the vision-caption pairs) for training. However, collecting and labeling large-scale datasets is time-consuming and expensive for many scenarios and languages. Therefore, sufficient labeled pairs are usually not available. To deal with the label shortage problem, we present a simple yet effective zero-shot approach MultiCapCLIP that can generate visual captions for different scenarios and languages without any labeled vision-caption pairs of downstream datasets. In the training stage, MultiCapCLIP only requires text data for input. Then it conducts two main steps: 1) retrieving concept prompts that preserve the corresponding domain knowledge of new scenarios; 2) auto-encoding the prompts to learn writing styles to output captions in a desired language. In the testing stage, MultiCapCLIP instead takes visual data as input directly to retrieve the concept prompts to generate the final visual descriptions. The extensive experiments on image and video captioning across four benchmarks and four languages (i.e., English, Chinese, German, and French) confirm the effectiveness of our approach. Compared with state-of-the-art zero-shot and weakly-supervised methods, our method achieves 4.8% and 21.5% absolute improvements in terms of BLEU@4 and CIDEr metrics. Our code is available athttps://github.com/yangbang18/MultiCapCLIP."
    }
  },
  {
    "id": "abstract-2023--acl-long--811",
    "result": [
      {
        "value": {
          "start": 382,
          "end": 394,
          "text": "ManagerTower",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--811:E0"
      },
      {
        "value": {
          "start": 916,
          "end": 942,
          "text": "accuracy on VQAv2 Test-Std",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--811:E1"
      },
      {
        "value": {
          "start": 967,
          "end": 984,
          "text": "TR@1 on Flickr30K",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--811:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--811:E0",
        "to_id": "abstract-2023--acl-long--811:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--811:E0",
        "to_id": "abstract-2023--acl-long--811:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Two-Tower Vision-Language (VL) models have shown promising improvements on various downstream VL tasks. Although the most advanced work improves performance by building bridges between encoders, it suffers from ineffective layer-by-layer utilization of uni-modal representations and cannot flexibly exploit different levels of uni-modal semantic knowledge. In this work, we propose ManagerTower, a novel VL model architecture that gathers and combines the insights of pre-trained uni-modal experts at different levels. The managers introduced in each cross-modal layer can adaptively aggregate uni-modal semantic knowledge to facilitate more comprehensive cross-modal alignment and fusion. ManagerTower outperforms previous strong baselines both with and without Vision-Language Pre-training (VLP). With only 4M VLP data, ManagerTower achieves superior performances on various downstream VL tasks, especially 79.15% accuracy on VQAv2 Test-Std, 86.56% IR@1 and 95.64% TR@1 on Flickr30K. Code and checkpoints are available athttps://github.com/LooperXX/ManagerTower."
    }
  },
  {
    "id": "abstract-2023--acl-long--467",
    "result": [
      {
        "value": {
          "start": 314,
          "end": 395,
          "text": "training paradigm for GenQA using supervision from automatic QA evaluation models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--467:E0"
      },
      {
        "value": {
          "start": 914,
          "end": 932,
          "text": "answering accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--467:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--467:E0",
        "to_id": "abstract-2023--acl-long--467:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recent studies show that sentence-level extractive QA, i.e., based on Answer Sentence Selection (AS2), is outperformed by Generation-based QA (GenQA) models, which generate answers using the top-k answer sentences ranked by AS2 models (a la retrieval-augmented generation style). In this paper, we propose a novel training paradigm for GenQA using supervision from automatic QA evaluation models (GAVA). Specifically, we propose three strategies to transfer knowledge from these QA evaluation models to a GenQA model: (i) augmenting training data with answers generated by the GenQA model and labelled by GAVA (either statically, before training, or (ii) dynamically, at every training epoch); and (iii) using the GAVA score for weighting the generator loss during the learning of the GenQA model. We evaluate our proposed methods on two academic and one industrial dataset, obtaining a significant improvement in answering accuracy over the previous state of the art."
    }
  },
  {
    "id": "abstract-2023--acl-long--905",
    "result": [
      {
        "value": {
          "start": 678,
          "end": 682,
          "text": "LENS",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--905:E0"
      }
    ],
    "data": {
      "text": "Training learnable metrics using modern language models has recently emerged as a promising method for the automatic evaluation of machine translation. However, existing human evaluation datasets for text simplification have limited annotations that are based on unitary or outdated models, making them unsuitable for this approach. To address these issues, we introduce the SimpEval corpus that contains: SimpEval_past, comprising 12K human ratings on 2.4K simplifications of 24 past systems, and SimpEval_2022, a challenging simplification benchmark consisting of over 1K human ratings of 360 simplifications including GPT-3.5 generated text. Training on SimpEval, we present LENS, a Learnable Evaluation Metric for Text Simplification. Extensive empirical results show that LENS correlates much better with human judgment than existing metrics, paving the way for future progress in the evaluation of text simplification. We also introduce Rank & Rate, a human evaluation framework that rates simplifications from several models in a list-wise manner using an interactive interface, which ensures both consistency and accuracy in the evaluation process and is used to create the SimpEval datasets."
    }
  },
  {
    "id": "abstract-2023--acl-srw--19",
    "result": [
      {
        "value": {
          "start": 693,
          "end": 709,
          "text": "correlation bias",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-srw--19:E0"
      },
      {
        "value": {
          "start": 891,
          "end": 910,
          "text": "overall performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-srw--19:E1"
      },
      {
        "from_id": "abstract-2023--acl-srw--19:E0",
        "to_id": "abstract-2023--acl-srw--19:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "The Ninth Revision of the International Classification of Diseases (ICD-9) is a standardized coding system used to classify health conditions. It is used for billing, tracking individual patient conditions, and for epidemiology. The highly detailed and technical nature of the codes and their associated medical conditions make it difficult for humans to accurately record them. Researchers have explored the use of neural networks, particularly language models, for automated ICD-9 code assignment. However, the imbalanced distribution of ICD-9 codes leads to poor performance. One solution is to use domain knowledge to incorporate a useful prior. This paper evaluates the usefulness of the correlation bias: we hypothesize that correlations between ICD-9 codes and other medical codes could help improve language models’ performance. We showed that while the correlation bias worsens the overall performance, the effect on individual class can be negative or positive. Performance on classes that are more imbalanced and less correlated with other codes is more sensitive to incorporating the correlation bias. This suggests that while the correlation bias has potential to improve ICD-9 code assignment in certain cases, the applicability criteria need to be more carefully studied."
    }
  },
  {
    "id": "abstract-2023--acl-long--76",
    "result": [
      {
        "value": {
          "start": 358,
          "end": 366,
          "text": "DIONYSUS",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--76:E0"
      },
      {
        "value": {
          "start": 1057,
          "end": 1069,
          "text": "ROUGE scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--76:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--76:E0",
        "to_id": "abstract-2023--acl-long--76:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Dialogue summarization has recently garnered significant attention due to its wide range of applications. However, existing methods for summarizing dialogues have limitations because they do not take into account the inherent structure of dialogue and rely heavily on labeled data, which can lead to poor performance in new domains. In this work, we propose DIONYSUS (dynamic input optimization in pre-training for dialogue summarization), a pre-trained encoder-decoder model for summarizing dialogues in any new domain. To pre-train DIONYSUS, we create two pseudo summaries for each dialogue example: one from a fine-tuned summarization model and the other from important dialogue turns. We then choose one of these pseudo summaries based on information distribution differences in different types of dialogues. This selected pseudo summary serves as the objective for pre-training DIONYSUS using a self-supervised approach on a large dialogue corpus. Our experiments show that DIONYSUS outperforms existing methods on six datasets, as demonstrated by its ROUGE scores in zero-shot and few-shot settings"
    }
  },
  {
    "id": "abstract-2023--acl-long--514",
    "result": [
      {
        "value": {
          "start": 656,
          "end": 679,
          "text": "knowledge-guided prompt",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--514:E0"
      },
      {
        "value": {
          "start": 1056,
          "end": 1090,
          "text": "performance of PLM-based CE models",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--514:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--514:E0",
        "to_id": "abstract-2023--acl-long--514:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Concepts benefit natural language understanding but are far from complete in existing knowledge graphs (KGs). Recently, pre-trained language models (PLMs) have been widely used in text-based concept extraction (CE). However, PLMs tend to mine the co-occurrence associations from massive corpus as pre-trained knowledge rather than the real causal effect between tokens. As a result, the pre-trained knowledge confounds PLMs to extract biased concepts based on spurious co-occurrence correlations, inevitably resulting in low precision. In this paper, through the lens of a Structural Causal Model (SCM), we propose equipping the PLM-based extractor with a knowledge-guided prompt as an intervention to alleviate concept bias. The prompt adopts the topic of the given entity from the existing knowledge in KGs to mitigate the spurious co-occurrence correlations between entities and biased concepts. Our extensive experiments on representative multilingual KG datasets justify that our proposed prompt can effectively alleviate concept bias and improve the performance of PLM-based CE models."
    }
  },
  {
    "id": "abstract-2023--acl-long--740",
    "result": [
      {
        "value": {
          "start": 531,
          "end": 534,
          "text": "KGA",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--740:E0"
      },
      {
        "value": {
          "start": 1055,
          "end": 1082,
          "text": "improvements over baselines",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--740:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--740:E0",
        "to_id": "abstract-2023--acl-long--740:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recent legislation of the “right to be forgotten” has led to the interest in machine unlearning, where the learned models are endowed with the function to forget information about specific training instances as if they have never existed in the training set. Previous work mainly focuses on computer vision scenarios and largely ignores the essentials of unlearning in NLP field, where text data contains more explicit and sensitive personal information than images. In this paper, we propose a general unlearning framework called KGA to induce forgetfulness. Different from previous work that tries to recover gradients or forces models to perform close to one specific distribution, KGA maintains distribution differences (i.e., knowledge gap). This relaxes the distribution assumption. Furthermore, we first apply the unlearning method to various NLP tasks (i.e., classification, translation, response generation) and propose several unlearning evaluation metrics with pertinence. Experiments on large-scale datasets show that KGA yields comprehensive improvements over baselines, where extensive analyses further validate the effectiveness of KGA and provide insight into unlearning for NLP tasks."
    }
  },
  {
    "id": "abstract-2023--acl-long--878",
    "result": [
      {
        "value": {
          "start": 898,
          "end": 920,
          "text": "compression strategies",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--878:E0"
      },
      {
        "value": {
          "start": 951,
          "end": 968,
          "text": "fairness measures",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--878:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--878:E0",
        "to_id": "abstract-2023--acl-long--878:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Compression techniques for deep learning have become increasingly popular, particularly in settings where latency and memory constraints are imposed. Several methods, such as pruning, distillation, and quantization, have been adopted for compressing models, each providing distinct advantages. However, existing literature demonstrates that compressing deep learning models could affect their fairness. Our analysis involves a comprehensive evaluation of pruned, distilled, and quantized language models, which we benchmark across a range of intrinsic and extrinsic metrics for measuring bias in text classification. We also investigate the impact of using multilingual models and evaluation measures. Our findings highlight the significance of considering both the pre-trained model and the chosen compression strategy in developing equitable language technologies. The results also indicate that compression strategies can have an adverse effect on fairness measures."
    }
  },
  {
    "id": "abstract-2023--acl-long--142",
    "result": [
      {
        "value": {
          "start": 873,
          "end": 897,
          "text": "hidden covariance metric",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--142:E0"
      },
      {
        "value": {
          "start": 1089,
          "end": 1116,
          "text": "simple qualitative analysis",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--142:E1"
      }
    ],
    "data": {
      "text": "Neural architecture search (NAS) has allowed for the automatic creation of new and effective neural network architectures, offering an alternative to the laborious process of manually designing complex architectures. However, traditional NAS algorithms are slow and require immense amounts of computing power. Recent research has investigated training-free NAS metrics for image classification architectures, drastically speeding up search algorithms. In this paper, we investigate training-free NAS metrics for recurrent neural network (RNN) and BERT-based transformer architectures, targeted towards language modeling tasks. First, we develop a new training-free metric, named hidden covariance, that predicts the trained performance of an RNN architecture and significantly outperforms existing training-free metrics. We experimentally evaluate the effectiveness of the hidden covariance metric on the NAS-Bench-NLP benchmark. Second, we find that the current search space paradigm for transformer architectures is not optimized for training-free neural architecture search. Instead, a simple qualitative analysis can effectively shrink the search space to the best performing architectures. This conclusion is based on our investigation of existing training-free metrics and new metrics developed from recent transformer pruning literature, evaluated on our own benchmark of trained BERT architectures. Ultimately, our analysis shows that the architecture search space and the training-free metric must be developed together in order to achieve effective results. Our source code is available athttps://github.com/aaronserianni/training-free-nas."
    }
  },
  {
    "id": "abstract-2023--acl-long--525",
    "result": [
      {
        "value": {
          "start": 472,
          "end": 501,
          "text": "unknown-aware training method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--525:E0"
      },
      {
        "value": {
          "start": 940,
          "end": 966,
          "text": "unknown relation detection",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--525:E1"
      },
      {
        "value": {
          "start": 992,
          "end": 1025,
          "text": "classification of known relations",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--525:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--525:E0",
        "to_id": "abstract-2023--acl-long--525:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--525:E0",
        "to_id": "abstract-2023--acl-long--525:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "The existing supervised relation extraction methods have achieved impressive performance in a closed-set setting, in which the relations remain the same during both training and testing. In a more realistic open-set setting, unknown relations may appear in the test set. Due to the lack of supervision signals from unknown relations, a well-performing closed-set relation extractor can still confidently misclassify them into known relations. In this paper, we propose an unknown-aware training method, regularizing the model by dynamically synthesizing negative instances that can provide the missing supervision signals. Inspired by text adversarial attack, We adaptively apply small but critical perturbations to original training data,synthesizingdifficult enoughnegative instances that are mistaken by the model as known relations, thus facilitating a compact decision boundary. Experimental results show that our method achieves SOTA unknown relation detection without compromising the classification of known relations."
    }
  },
  {
    "id": "abstract-2023--acl-long--143",
    "result": [
      {
        "value": {
          "start": 832,
          "end": 846,
          "text": "proposed model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--143:E0"
      },
      {
        "value": {
          "start": 676,
          "end": 681,
          "text": "ROUGE",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--143:E1"
      },
      {
        "value": {
          "start": 555,
          "end": 559,
          "text": "LaSE",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--143:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--143:E0",
        "to_id": "abstract-2023--acl-long--143:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--143:E0",
        "to_id": "abstract-2023--acl-long--143:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We present CrossSum, a large-scale cross-lingual summarization dataset comprising 1.68 million article-summary samples in 1,500+ language pairs. We create CrossSum by aligning parallel articles written in different languages via cross-lingual retrieval from a multilingual abstractive summarization dataset and perform a controlled human evaluation to validate its quality. We propose a multistage data sampling algorithm to effectively train a cross-lingual summarization model capable of summarizing an article in any target language. We also introduce LaSE, an embedding-based metric for automatically evaluating model-generated summaries. LaSE is strongly correlated with ROUGE and, unlike ROUGE, can be reliably measured even in the absence of references in the target language. Performance on ROUGE and LaSE indicate that our proposed model consistently outperforms baseline models. To the best of our knowledge, CrossSum is the largest cross-lingual summarization dataset and the first ever that is not centered around English. We are releasing the dataset, training and evaluation scripts, and models to spur future research on cross-lingual summarization. The resources can be found athttps://github.com/csebuetnlp/CrossSum"
    }
  },
  {
    "id": "abstract-2023--acl-long--619",
    "result": [
      {
        "value": {
          "start": 626,
          "end": 634,
          "text": "StyleVSG",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--619:E0"
      },
      {
        "value": {
          "start": 1259,
          "end": 1278,
          "text": "overall performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--619:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--619:E0",
        "to_id": "abstract-2023--acl-long--619:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Most research on stylized image captioning aims to generate style-specific captions using unpaired text, and has achieved impressive performance for simple styles like positive and negative. However, unlike previous single-sentence captions whose style is mostly embodied in distinctive words or phrases, real-world styles are likely to be implied at the syntactic and discourse levels. In this work, we introduce a new task of Stylized Visual Storytelling (SVST), which aims to describe a photo stream with stylized stories that are more expressive and attractive. We propose a multitasking memory-augmented framework called StyleVSG, which is jointly trained on factual visual storytelling data and unpaired style corpus, achieving a trade-off between style accuracy and visual relevance. Particularly for unpaired stylized text, StyleVSG learns to reconstruct the stylistic story from roughly parallel visual inputs mined with the CLIP model, avoiding problems caused by random mapping in previous methods. Furthermore, a memory module is designed to preserve the consistency and coherence of generated stories. Experiments show that our method can generate attractive and coherent stories with different styles such as fairy tale, romance, and humor. The overall performance of our StyleVSG surpasses state-of-the-art methods on both automatic and human evaluation metrics."
    }
  },
  {
    "id": "abstract-2023--acl-long--724",
    "result": [
      {
        "value": {
          "start": 41,
          "end": 64,
          "text": "model-generated signals",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--724:E0"
      },
      {
        "value": {
          "start": 78,
          "end": 102,
          "text": "zero-shot generalization",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--724:E1"
      },
      {
        "value": {
          "start": 445,
          "end": 453,
          "text": "METRO-T0",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--724:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--724:E0",
        "to_id": "abstract-2023--acl-long--724:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This paper explores the effectiveness of model-generated signals in improving zero-shot generalization of text-to-text Transformers such as T5. We study various designs to pretrain T5 using an auxiliary model to construct more challenging token replacements for the main model to denoise. Key aspects under study include the decoding target, the location of the RTD head, and the masking pattern. Based on these studies, we develop a new model, METRO-T0, which is pretrained using the redesigned ELECTRA-Style pretraining strategies and then prompt-finetuned on a mixture of NLP tasks. METRO-T0 outperforms all similar-sized baselines on prompted NLP benchmarks, such as _T0 Eval_ and MMLU, and rivals the state-of-the-art T0-11B model with only **8%** of its parameters. Our analysis on model’s neural activation and parameter sensitivity reveals that the effectiveness of METRO-T0 stems from more balanced contribution of parameters and better utilization of their capacity. The code and model checkpoints are available at [https://github.com/gonglinyuan/metro_t0](https://github.com/gonglinyuan/metro_t0)."
    }
  },
  {
    "id": "abstract-2023--acl-short--79",
    "result": [
      {
        "value": {
          "start": 786,
          "end": 850,
          "text": "task-adaptive data augmentation with robust inference procedures",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--79:E0"
      }
    ],
    "data": {
      "text": "Conditional language models still generate unfaithful output that is not supported by their input. These unfaithful generations jeopardize trust in real-world applications such as summarization or human-machine interaction, motivating a need for automatic faithfulness metrics. To implement such metrics, NLI models seem attractive, since they solve a strongly related task that comes with a wealth of prior research and data. But recent research suggests that NLI models require costly additional machinery to perform reliably across datasets, e.g., by running inference on a cartesian product of input and generated sentences, or supporting them with a question-generation/answering step. In this work we show that pure NLI models _can_ outperform more complex metrics when combining task-adaptive data augmentation with robust inference procedures. We propose: (1) Augmenting NLI training data toadapt NL inferences to the specificities of faithfulness prediction in dialogue;(2) Making use of both entailment and contradiction probabilities in NLI, and(3) Using Monte-Carlo dropout during inference. Applied to the TRUE benchmark, which combines faithfulness datasets across diverse domains and tasks, our approach strongly improves a vanilla NLI model and significantly outperforms previous work, while showing favourable computational cost."
    }
  },
  {
    "id": "abstract-2023--acl-long--104",
    "result": [
      {
        "value": {
          "start": 64,
          "end": 93,
          "text": "curriculum learning framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--104:E0"
      }
    ],
    "data": {
      "text": "We introduce the problem of curriculum discovery and describe a curriculum learning framework capable of discovering effective curricula in a curriculum space based on prior knowledge about sample difficulty. Using annotation entropy and loss as measures of difficulty, we show that (i): the top-performing discovered curricula for a given model and dataset are often non-monotonic as apposed to monotonic curricula in existing literature, (ii): the prevailing easy-to-hard or hard-to-easy transition curricula are often at the risk of underperforming, and (iii): the curricula discovered for smaller datasets and models perform well on larger datasets and models respectively. The proposed framework encompasses some of the existing curriculum learning approaches and can discover curricula that outperform them across several NLP tasks."
    }
  },
  {
    "id": "abstract-2023--acl-long--388",
    "result": [
      {
        "value": {
          "start": 297,
          "end": 302,
          "text": "XLM-P",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--388:E0"
      },
      {
        "value": {
          "start": 858,
          "end": 881,
          "text": "performance improvement",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--388:E1"
      },
      {
        "value": {
          "start": 920,
          "end": 957,
          "text": "advantages for low-resource languages",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--388:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--388:E0",
        "to_id": "abstract-2023--acl-long--388:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--388:E0",
        "to_id": "abstract-2023--acl-long--388:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Multilingual pre-trained language models have demonstrated impressive (zero-shot) cross-lingual transfer abilities, however, their performance is hindered when the target language has distant typologyfrom the source language or when pre-training data is limited in size. In this paper, we propose XLM-P, a method that contextually retrieves prompts as flexible guidance for encoding instances conditionally. Our space-efficient and model-agnostic XLM-P approach enables (1) lightweight modeling of language-invariant and language-specific knowledge across languages, and (2) easy integration with other multilingual pre-training methods. On the tasks of XTREME, which include text classification, sequence labeling, question answering, and sentence retrieval, both base- and large-size language models pre-trained with our proposed method exhibit consistent performance improvement. Furthermore, it provides substantial advantages for low-resource languages in unsupervised sentence retrieval and for target languages that differ greatly from the source language in cross-lingual transfer."
    }
  },
  {
    "id": "abstract-2023--acl-long--432",
    "result": [
      {
        "value": {
          "start": 544,
          "end": 592,
          "text": "Hierarchy-aware Tree Isomorphism Network (HiTIN)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--432:E0"
      },
      {
        "value": {
          "start": 1197,
          "end": 1213,
          "text": "test performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--432:E1"
      },
      {
        "value": {
          "start": 1223,
          "end": 1241,
          "text": "memory consumption",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--432:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--432:E0",
        "to_id": "abstract-2023--acl-long--432:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--432:E0",
        "to_id": "abstract-2023--acl-long--432:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Hierarchical text classification (HTC) is a challenging subtask of multi-label classification as the labels form a complex hierarchical structure. Existing dual-encoder methods in HTC achieve weak performance gains with huge memory overheads and their structure encoders heavily rely on domain knowledge. Under such observation, we tend to investigate the feasibility of a memory-friendly model with strong generalization capability that could boost the performance of HTC without prior statistics or label semantics. In this paper, we propose Hierarchy-aware Tree Isomorphism Network (HiTIN) to enhance the text representations with only syntactic information of the label hierarchy. Specifically, we convert the label hierarchy into an unweighted tree structure, termed coding tree, with the guidance of structural entropy. Then we design a structure encoder to incorporate hierarchy-aware information in the coding tree into text representations. Besides the text encoder, HiTIN only contains a few multi-layer perceptions and linear transformations, which greatly saves memory. We conduct experiments on three commonly used datasets and the results demonstrate that HiTIN could achieve better test performance and less memory consumption than state-of-the-art (SOTA) methods."
    }
  },
  {
    "id": "abstract-2023--acl-long--530",
    "result": [
      {
        "value": {
          "start": 13,
          "end": 47,
          "text": "novel supervised learning approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--530:E0"
      },
      {
        "value": {
          "start": 1044,
          "end": 1061,
          "text": "context filtering",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--530:E1"
      },
      {
        "value": {
          "start": 1073,
          "end": 1098,
          "text": "ideological concentration",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--530:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--530:E1",
        "to_id": "abstract-2023--acl-long--530:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We propose a novel supervised learning approach for political ideology prediction (PIP) that is capable of predicting out-of-distribution inputs. This problem is motivated by the fact that manual data-labeling is expensive, while self-reported labels are often scarce and exhibit significant selection bias. We propose a novel statistical model that decomposes the document embeddings into a linear superposition of two vectors; a latent neutralcontextvector independent of ideology, and a latentpositionvector aligned with ideology. We train an end-to-end model that has intermediate contextual and positional vectors as outputs. At deployment time, our model predicts labels for input documents by exclusively leveraging the predicted positional vectors. On two benchmark datasets we show that our model is capable of outputting predictions even when trained with as little as 5% biased data, and is significantly more accurate than the state-of-the-art. Through crowd-sourcing we validate the neutrality of contextual vectors, and show that context filtering results in ideological concentration, allowing for prediction on out-of-distribution examples."
    }
  },
  {
    "id": "abstract-2023--acl-long--170",
    "result": [
      {
        "value": {
          "start": 883,
          "end": 891,
          "text": "F1-score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--170:E0"
      },
      {
        "value": {
          "start": 990,
          "end": 1009,
          "text": "proposed heuristics",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--170:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--170:E1",
        "to_id": "abstract-2023--acl-long--170:E0",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The ability to infer pre- and postconditions of an action is vital for comprehending complex instructions, and is essential for applications such as autonomous instruction-guided agents and assistive AI that supports humans to perform physical tasks. In this work, we propose a task dubbed action condition inference, which extracts mentions of preconditions and postconditions of actions in instructional manuals. We propose a weakly supervised approach utilizing automatically constructed large-scale training instances from online instructions, and curate a densely human-annotated and validated dataset to study how well the current NLP models do on the proposed task. We design two types of models differ by whether contextualized and global information is leveraged, as well as various combinations of heuristics to construct the weak supervisions.Our experiments show a > 20% F1-score improvement with considering the entire instruction contexts and a > 6% F1-score benefit with the proposed heuristics. However, the best performing model is still well-behind human performance."
    }
  },
  {
    "id": "abstract-2023--acl-long--48",
    "result": [
      {
        "value": {
          "start": 159,
          "end": 173,
          "text": "Multi-CLS BERT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--48:E0"
      },
      {
        "value": {
          "start": 872,
          "end": 888,
          "text": "overall accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--48:E1"
      },
      {
        "value": {
          "start": 893,
          "end": 914,
          "text": "confidence estimation",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--48:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--48:E0",
        "to_id": "abstract-2023--acl-long--48:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--48:E0",
        "to_id": "abstract-2023--acl-long--48:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Ensembling BERT models often significantly improves accuracy, but at the cost of significantly more computation and memory footprint. In this work, we propose Multi-CLS BERT, a novel ensembling method for CLS-based prediction tasks that is almost as efficient as a single BERT model. Multi-CLS BERT uses multiple CLS tokens with a parameterization and objective that encourages their diversity. Thus instead of fine-tuning each BERT model in an ensemble (and running them all at test time), we need only fine-tune our single Multi-CLS BERT model (and run the one model at test time, ensembling just the multiple final CLS embeddings). To test its effectiveness, we build Multi-CLS BERT on top of a state-of-the-art pretraining method for BERT (Aroca-Ouellette and Rudzicz, 2020). In experiments on GLUE and SuperGLUE we show that our Multi-CLS BERT reliably improves both overall accuracy and confidence estimation. When only 100 training samples are available in GLUE, the Multi-CLS BERT_Base model can even outperform the corresponding BERT_Large model. We analyze the behavior of our Multi-CLS BERT, showing that it has many of the same characteristics and behavior as a typical BERT 5-way ensemble, but with nearly 4-times less computation and memory."
    }
  },
  {
    "id": "abstract-2023--acl-long--506",
    "result": [
      {
        "value": {
          "start": 832,
          "end": 854,
          "text": "Backpack sense vectors",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--506:E0"
      },
      {
        "value": {
          "start": 787,
          "end": 817,
          "text": "lexical similarity evaluations",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--506:E1"
      },
      {
        "value": {
          "start": 963,
          "end": 989,
          "text": "intervene on sense vectors",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--506:E2"
      },
      {
        "value": {
          "start": 1001,
          "end": 1043,
          "text": "controllable text generation and debiasing",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--506:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--506:E0",
        "to_id": "abstract-2023--acl-long--506:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--506:E2",
        "to_id": "abstract-2023--acl-long--506:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We present Backpacks: a new neural architecture that marries strong modeling performancewith an interface for interpretability and control. Backpacks learn multiple non-contextual sense vectors for each word in a vocabulary, and represent a word in a sequence as a context-dependent, non-negative linear combination ofsense vectors in this sequence. We find that, after training, sense vectors specialize, each encoding a different aspect of a word. We can interpret a sense vector by inspecting its (non-contextual, linear) projection onto the output space, and intervene on these interpretable hooks to change the model’s behavior in predictable ways. We train a 170M-parameter Backpack language model on OpenWebText, matching the loss of a GPT-2 small (124Mparameter) Transformer. On lexical similarity evaluations, we find that Backpack sense vectors outperform even a 6B-parameter Transformer LM’s word embeddings. Finally, we present simple algorithms that intervene on sense vectors to perform controllable text generation and debiasing. For example, we can edit the sense vocabulary to tend more towards a topic, or localize a source of gender bias to a sense vector and globally suppress that sense."
    }
  },
  {
    "id": "abstract-2023--acl-short--126",
    "result": [
      {
        "value": {
          "start": 501,
          "end": 505,
          "text": "RAMP",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--126:E0"
      },
      {
        "value": {
          "start": 624,
          "end": 643,
          "text": "generation accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--126:E1"
      },
      {
        "from_id": "abstract-2023--acl-short--126:E0",
        "to_id": "abstract-2023--acl-short--126:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Attribute-controlled translation (ACT) is a subtask of machine translation that involves controlling stylistic or linguistic attributes (like formality and gender) of translation outputs. While ACT has garnered attention in recent years due to its usefulness in real-world applications, progress in the task is currently limited by dataset availability, since most prior approaches rely on supervised methods. To address this limitation, we propose Retrieval and Attribute-Marking enhanced Prompting (RAMP), which leverages large multilingual language models to perform ACT in few-shot and zero-shot settings. RAMP improves generation accuracy over the standard prompting approach by (1) incorporating a semantic similarity retrieval component for selecting similar in-context examples, and (2) marking in-context examples with attribute annotations. Our comprehensive experiments show that RAMP is a viable approach in both zero-shot and few-shot settings."
    }
  },
  {
    "id": "abstract-2023--acl-long--165",
    "result": [
      {
        "value": {
          "start": 547,
          "end": 579,
          "text": "summary-oriented visual features",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--165:E0"
      },
      {
        "value": {
          "start": 523,
          "end": 538,
          "text": "summary quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--165:E1"
      },
      {
        "value": {
          "start": 1077,
          "end": 1094,
          "text": "proposed approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--165:E2"
      },
      {
        "value": {
          "start": 1111,
          "end": 1139,
          "text": "state-of-the-art performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--165:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--165:E0",
        "to_id": "abstract-2023--acl-long--165:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--165:E2",
        "to_id": "abstract-2023--acl-long--165:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The goal of multimodal abstractive summarization (MAS) is to produce a concise summary given the multimodal data (text and vision). Existing studies on MAS mainly focus on how to effectively use the extracted visual features, having achieved impressive success on the high-resource English dataset. However, less attention has been paid to the quality of the visual features to the summary, which may limit the model performance, especially in the low- and zero-resource scenarios. In this paper, we propose to improve the summary quality through summary-oriented visual features. To this end, we devise two auxiliary tasks including vision to summary task and masked image modeling task. Together with the main summarization task, we optimize the MAS model via the training objectives of all these tasks. By these means, the MAS model can be enhanced by capturing the summary-oriented visual features, thereby yielding more accurate summaries. Experiments on 44 languages, covering mid-high-, low-, and zero-resource scenarios, verify the effectiveness and superiority of the proposed approach, which achieves state-of-the-art performance under all scenarios. Additionally, we will contribute a large-scale multilingual multimodal abstractive summarization (MM-Sum) dataset to the research community."
    }
  },
  {
    "id": "abstract-2023--acl-long--493",
    "result": [
      {
        "value": {
          "start": 668,
          "end": 704,
          "text": "outperform multiple strong baselines",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--493:E0"
      }
    ],
    "data": {
      "text": "Standard language model training employs gold human documents or human-human interaction data, and treats all training data as positive examples. Growing evidence shows that even with very large amounts of positive training data, issues remain that can be alleviated with relatively small amounts of negative data – examples of what the model should not do. In this work, we propose a novel procedure to train with such data called the “CRINGE” loss (ContRastive Iterative Negative GEneration). We show the effectiveness of this approach across three different experiments on the tasks of safe generation, contradiction avoidance, and open-domain dialogue. Our models outperform multiple strong baselines and are conceptually simple, easy to train and implement."
    }
  },
  {
    "id": "abstract-2023--acl-long--404",
    "result": [
      {
        "value": {
          "start": 1297,
          "end": 1323,
          "text": "various evaluation metrics",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--404:E0"
      }
    ],
    "data": {
      "text": "Complaining is an illocutionary act in which the speaker communicates his/her dissatisfaction with a set of circumstances and holds the hearer (the complainee) answerable, directly or indirectly. Considering breakthroughs in machine learning approaches, the complaint detection task has piqued the interest of the natural language processing (NLP) community. Most of the earlier studies failed to justify their findings, necessitating the adoption of interpretable models that can explain the model’s output in real time. We introduce an explainable complaint dataset, X-CI, the first benchmark dataset for explainable complaint detection. Each instance in the X-CI dataset is annotated with five labels: complaint label, emotion label, polarity label, complaint severity level, and rationale (explainability), i.e., the causal span explaining the reason for the complaint/non-complaint label. We address the task of explainable complaint detection and propose a commonsense-aware unified generative framework by reframing the multitask problem as a text-to-text generation task. Our framework can predict the complaint cause, severity level, emotion, and polarity of the text in addition to detecting whether it is a complaint or not. We further establish the advantages of our proposed model on various evaluation metrics over the state-of-the-art models and other baselines when applied to the X-CI dataset in both full and few-shot settings."
    }
  },
  {
    "id": "abstract-2023--acl-long--300",
    "result": [
      {
        "value": {
          "start": 879,
          "end": 923,
          "text": "gradient-based fine-tuning and modifications",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--300:E0"
      },
      {
        "value": {
          "start": 954,
          "end": 987,
          "text": "propagation of injected knowledge",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--300:E1"
      },
      {
        "value": {
          "start": 1128,
          "end": 1176,
          "text": "prepending entity definitions in an LM’s context",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--300:E2"
      },
      {
        "value": {
          "start": 1186,
          "end": 1217,
          "text": "performance across all settings",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--300:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--300:E0",
        "to_id": "abstract-2023--acl-long--300:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--300:E2",
        "to_id": "abstract-2023--acl-long--300:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Pre-trained language models (LMs) are used for knowledge intensive tasks like question answering, but their knowledge gets continuously outdated as the world changes. Prior work has studied targeted updates to LMs, injecting individual facts and evaluating whether the model learns these facts while not changing predictions on other contexts. We take a step forward and study LMs’ abilities to make inferences based on injected facts (or propagate those facts): for example, after learning that something is a TV show, does an LM predict that you can watch it? We study this with two cloze-style tasks: an existing dataset of real-world sentences about novel entities (ECBD) as well as a new controlled benchmark with manually designed templates requiring varying levels of inference about injected knowledge. Surprisingly, we find that existing methods for updating knowledge (gradient-based fine-tuning and modifications of this approach) show little propagation of injected knowledge. These methods improve performance on cloze instances only when there is lexical overlap between injected facts and target inferences. Yet, prepending entity definitions in an LM’s context improves performance across all settings, suggesting that there is substantial headroom for parameter-updating approaches for knowledge injection."
    }
  },
  {
    "id": "abstract-2023--acl-short--87",
    "result": [
      {
        "value": {
          "start": 108,
          "end": 119,
          "text": "ASR quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--87:E0"
      }
    ],
    "data": {
      "text": "Advances in deep neural models for automatic speech recognition (ASR) have yielded dramatic improvements in ASR quality for resource-rich languages, with English ASR now achieving word error rates comparable to that of human transcribers. The vast majority of the world’s languages, however, lack the quantity of data necessary to approach this level of accuracy. In this paper we use four of the most popular ASR toolkits to train ASR models for eleven languages with limited ASR training resources: eleven widely spoken languages of Africa, Asia, and South America, one endangered language of Central America, and three critically endangered languages of North America. We find that no single architecture consistently outperforms any other. These differences in performance so far do not appear to be related to any particular feature of the datasets or characteristics of the languages. These findings have important implications for future research in ASR for under-resourced languages. ASR systems for languages with abundant existing media and available speakers may derive the most benefit simply by collecting large amounts of additional acoustic and textual training data. Communities using ASR to support endangered language documentation efforts, who cannot easily collect more data, might instead focus on exploring multiple architectures and hyperparameterizations to optimize performance within the constraints of their available data and resources."
    }
  },
  {
    "id": "abstract-2023--acl-industry--37",
    "result": [
      {
        "value": {
          "start": 187,
          "end": 231,
          "text": "multi-document hybrid summarization approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-industry--37:E0"
      },
      {
        "value": {
          "start": 898,
          "end": 909,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-industry--37:E1"
      },
      {
        "from_id": "abstract-2023--acl-industry--37:E0",
        "to_id": "abstract-2023--acl-industry--37:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Multi-document summarization is gaining more and more attention recently and serves as an invaluable tool to obtain key facts among a large information pool. In this paper, we proposed a multi-document hybrid summarization approach, which simultaneously generates a human-readable summary and extracts corresponding key evidences based on multi-doc inputs. To fulfill that purpose, we crafted a salient representation learning method to induce latent salient features, which are effective for joint evidence extraction and summary generation. In order to train this model, we conducted multi-task learning to optimize a composited loss, constructed over extractive and abstractive sub-components in a hierarchical way. We implemented the system based on a ubiquiotously adopted transformer architecture and conducted experimental studies on multiple datasets across two domains, achieving superior performance over the baselines."
    }
  },
  {
    "id": "abstract-2023--acl-demo--48",
    "result": [
      {
        "value": {
          "start": 560,
          "end": 565,
          "text": "model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-demo--48:E0"
      },
      {
        "value": {
          "start": 350,
          "end": 368,
          "text": "engaging sentences",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-demo--48:E1"
      },
      {
        "from_id": "abstract-2023--acl-demo--48:E0",
        "to_id": "abstract-2023--acl-demo--48:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Teachers often guide students to improve their essays by adding engaging modifiers to polish the sentences. In this work, we present the first study on automatic sentence polishing by adding modifiers. Since there is no available dataset for the new task, we first automatically construct a large number of parallel data by removing modifiers in the engaging sentences collected from public resources. Then we fine-tune LongLM to reconstruct the original sentences from the corrupted ones. Considering that much overlap between inputs and outputs may bias the model to completely copy the inputs, we split each source sentence into sub-sentences and only require the model to generate the modified sub-sentences. Furthermore, we design a retrieval augmentation algorithm to prompt the model to add suitable modifiers. Automatic and manual evaluation on the auto-constructed test set and real human texts show that our model can generate more engaging sentences with suitable modifiers than strong baselines while keeping fluency. We deploy the model athttp://coai.cs.tsinghua.edu.cn/static/polishSent/. A demo video is available athttps://youtu.be/Y6gFHOgSv8Y."
    }
  },
  {
    "id": "abstract-2023--acl-long--616",
    "result": [
      {
        "value": {
          "start": 379,
          "end": 418,
          "text": "identifying inappropriate communication",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--616:E0"
      },
      {
        "value": {
          "start": 671,
          "end": 695,
          "text": "identify appropriateness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--616:E1"
      },
      {
        "value": {
          "start": 991,
          "end": 1027,
          "text": "contextual-appropriateness judgments",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--616:E2"
      }
    ],
    "data": {
      "text": "Understanding interpersonal communication requires, in part, understanding the social context and norms in which a message is said. However, current methods for identifying offensive content in such communication largely operate independent of context, with only a few approaches considering community norms or prior conversation as context. Here, we introduce a new approach to identifying inappropriate communication by explicitly modeling the social relationship between the individuals. We introduce a new dataset of contextually-situated judgments of appropriateness and show that large language models can readily incorporate relationship information to accurately identify appropriateness in a given context. Using data from online conversations and movie dialogues, we provide insight into how the relationships themselves function as implicit norms and quantify the degree to which context-sensitivity is needed in different conversation settings. Further, we also demonstrate that contextual-appropriateness judgments are predictive of other social factors expressed in language such as condescension and politeness."
    }
  },
  {
    "id": "abstract-2023--acl-long--742",
    "result": [
      {
        "value": {
          "start": 606,
          "end": 615,
          "text": "Rel-CSKGC",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--742:E0"
      },
      {
        "value": {
          "start": 417,
          "end": 429,
          "text": "Dense-ATOMIC",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--742:E1"
      },
      {
        "value": {
          "start": 343,
          "end": 361,
          "text": "knowledge coverage",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--742:E2"
      },
      {
        "value": {
          "start": 366,
          "end": 381,
          "text": "multi-hop paths",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--742:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--742:E1",
        "to_id": "abstract-2023--acl-long--742:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--742:E1",
        "to_id": "abstract-2023--acl-long--742:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "ATOMIC is a large-scale commonsense knowledge graph (CSKG) containing everyday if-then knowledge triplets, i.e., head event, relation, tail event. The one-hop annotation manner made ATOMIC a set of independent bipartite graphs, which ignored the numerous links between events in different bipartite graphs and consequently caused shortages in knowledge coverage and multi-hop paths. In this work, we aim to construct Dense-ATOMIC with high knowledge coverage and massive multi-hop paths. The events in ATOMIC are normalized to a consistent pattern at first. We then propose a CSKG completion method called Rel-CSKGC to predict the relation given the head event and the tail event of a triplet, and train a CSKG completion model based on existing triplets in ATOMIC. We finally utilize the model to complete the missing links in ATOMIC and accordingly construct Dense-ATOMIC. Both automatic and human evaluation on an annotated subgraph of ATOMIC demonstrate the advantage of Rel-CSKGC over strong baselines. We further conduct extensive evaluations on Dense-ATOMIC in terms of statistics, human evaluation, and simple downstream tasks, all proving Dense-ATOMIC’s advantages in Knowledge Coverage and Multi-hop Paths. Both the source code of Rel-CSKGC and Dense-ATOMIC are publicly available onhttps://github.com/NUSTM/Dense-ATOMIC."
    }
  },
  {
    "id": "abstract-2023--acl-long--885",
    "result": [
      {
        "value": {
          "start": 190,
          "end": 200,
          "text": "PromptRank",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--885:E0"
      },
      {
        "value": {
          "start": 718,
          "end": 727,
          "text": "recall@10",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--885:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--885:E0",
        "to_id": "abstract-2023--acl-long--885:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We study few-shot reranking for multi-hop QA (MQA) with open-domain questions. To alleviate the need for a large number of labeled question-document pairs for retriever training, we propose PromptRank, which relies on language model prompting for multi-hop path reranking. PromptRank first constructs an instruction-based prompt that includes a candidate document path and then computes the relevance score between a given question and the path based on the conditional likelihood of the question given the path prompt according to a language model. PromptRank yields strong retrieval performance on HotpotQA with only 128 training examples compared to state-of-the-art methods trained on thousands of examples — 73.6 recall@10 by PromptRank vs. 77.8 by PathRetriever and 77.5 by multi-hop dense retrieval."
    }
  },
  {
    "id": "abstract-2023--acl-long--634",
    "result": [
      {
        "value": {
          "start": 573,
          "end": 583,
          "text": "AlignScore",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--634:E0"
      }
    ],
    "data": {
      "text": "Many text generation applications require the generated text to be factually consistent with input information. Automatic evaluation of factual consistency is challenging. Previous work has developed various metrics that often depend on specific functions, such as natural language inference (NLI) or question answering (QA), trained on limited data. Those metrics thus can hardly assess diverse factual inconsistencies (e.g., contradictions, hallucinations) that occur in varying inputs/outputs (e.g., sentences, documents) from different tasks. In this paper, we propose AlignScore, a new holistic metric that applies to a variety of factual inconsistency scenarios as above. AlignScore is based on a general function of information alignment between two arbitrary text pieces. Crucially, we develop a unified training framework of the alignment function by integrating a large diversity of data sources, resulting in 4.7M training examples from 7 well-established tasks (NLI, QA, paraphrasing, fact verification, information retrieval, semantic similarity, and summarization). We conduct extensive experiments on large-scale benchmarks including 22 evaluation datasets, where 19 of the datasets were never seen in the alignment training. AlignScore achieves substantial improvement over a wide range of previous metrics. Moreover, AlignScore (355M parameters) matches or even outperforms metrics based on ChatGPT and GPT-4 that are orders of magnitude larger."
    }
  },
  {
    "id": "abstract-2023--acl-long--733",
    "result": [
      {
        "value": {
          "start": 1089,
          "end": 1119,
          "text": "commonsense inference modeling",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--733:E0"
      }
    ],
    "data": {
      "text": "Commonsense reasoning, aiming at endowing machines with a human-like ability to make situational presumptions, is extremely challenging to generalize. For someone who barely knows about “meditation,” while is knowledgeable about “singing,” he can still infer that “meditation makes people relaxed” from the existing knowledge that “singing makes people relaxed” by first conceptualizing “singing” as a “relaxing event” and then instantiating that event to “meditation.”This process, known as conceptual induction and deduction, is fundamental to commonsense reasoning while lacking both labeled data and methodologies to enhance commonsense modeling. To fill such a research gap, we propose CAT (Contextualized ConceptuAlization and InsTantiation),a semi-supervised learning framework that integrates event conceptualization and instantiation to conceptualize commonsense knowledge bases at scale. Extensive experiments show that our framework achieves state-of-the-art performances on two conceptualization tasks, and the acquired abstract commonsense knowledge can significantly improve commonsense inference modeling. Our code, data, and fine-tuned models are publicly available at [https://github.com/HKUST-KnowComp/CAT](https://github.com/HKUST-KnowComp/CAT)."
    }
  },
  {
    "id": "abstract-2023--acl-short--161",
    "result": [
      {
        "value": {
          "start": 628,
          "end": 665,
          "text": "randomized positional encoding scheme",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--161:E0"
      },
      {
        "value": {
          "start": 967,
          "end": 980,
          "text": "test accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--161:E1"
      },
      {
        "from_id": "abstract-2023--acl-short--161:E0",
        "to_id": "abstract-2023--acl-short--161:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Transformers have impressive generalization capabilities on tasks with a fixed context length. However, they fail to generalize to sequences of arbitrary length, even for seemingly simple tasks such as duplicating a string. Moreover, simply training on longer sequences is inefficient due to the quadratic computation complexity of the global attention mechanism. In this work, we demonstrate that this failure mode is linked to positional encodings being out-of-distribution for longer sequences (even for relative encodings) and introduce a novel family of positional encodings that can overcome this problem. Concretely, our randomized positional encoding scheme simulates the positions of longer sequences and randomly selects an ordered subset to fit the sequence’s length. Our large-scale empirical evaluation of 6000 models across 15 algorithmic reasoning tasks shows that our method allows Transformers to generalize to sequences of unseen length (increasing test accuracy by 12.0% on average)."
    }
  },
  {
    "id": "abstract-2023--acl-srw--48",
    "result": [
      {
        "value": {
          "start": 418,
          "end": 463,
          "text": "novel method for evaluating semantic accuracy",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-srw--48:E0"
      },
      {
        "value": {
          "start": 275,
          "end": 292,
          "text": "semantic accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-srw--48:E1"
      },
      {
        "value": {
          "start": 576,
          "end": 603,
          "text": "interpretability approaches",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-srw--48:E2"
      },
      {
        "value": {
          "start": 637,
          "end": 660,
          "text": "sources of inaccuracies",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-srw--48:E3"
      },
      {
        "from_id": "abstract-2023--acl-srw--48:E0",
        "to_id": "abstract-2023--acl-srw--48:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2023--acl-srw--48:E2",
        "to_id": "abstract-2023--acl-srw--48:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "With the fast-growing popularity of current large pre-trained language models (LLMs), it is necessary to dedicate efforts to making them more reliable. In this thesis proposal, we aim to improve the reliability of natural language generation systems (NLG) by researching the semantic accuracy of their outputs. We look at this problem from the outside (evaluation) and from the inside (interpretability). We propose a novel method for evaluating semantic accuracy and discuss the importance of working towards a unified and objective benchmark for NLG metrics. We also review interpretability approaches which could help us pinpoint the sources of inaccuracies within the models and explore potential mitigation strategies."
    }
  },
  {
    "id": "abstract-2023--acl-long--266",
    "result": [
      {
        "value": {
          "start": 51,
          "end": 61,
          "text": "CoCo-CroLa",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--266:E0"
      },
      {
        "value": {
          "start": 158,
          "end": 177,
          "text": "multilingual parity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--266:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--266:E0",
        "to_id": "abstract-2023--acl-long--266:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We propose “Conceptual Coverage Across Languages” (CoCo-CroLa), a technique for benchmarking the degree to which any generative text-to-image system provides multilingual parity to its training language in terms of tangible nouns. For each model we can assess “conceptual coverage” of a given target language relative to a source language by comparing the population of images generated for a series of tangible nouns in the source language to the population of images generated for each noun under translation in the target language. This technique allows us to estimate how well-suited a model is to a target language as well as identify model-specific weaknesses, spurious correlations, and biases without a-priori assumptions. We demonstrate how it can be used to benchmark T2I models in terms of multilinguality, and how despite its simplicity it is a good proxy for impressive generalization."
    }
  },
  {
    "id": "abstract-2023--acl-long--471",
    "result": [
      {
        "value": {
          "start": 454,
          "end": 504,
          "text": "attribution-driven knowledge distillation approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--471:E0"
      },
      {
        "value": {
          "start": 959,
          "end": 970,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--471:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--471:E0",
        "to_id": "abstract-2023--acl-long--471:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Knowledge distillation has attracted a great deal of interest recently to compress large language models. However, existing knowledge distillation methods suffer from two limitations. First, the student model simply imitates the teacher’s behavior while ignoring the reasoning behind it. Second, these methods usually focus on the transfer of sophisticated model-specific knowledge but overlook data-specific knowledge. In this paper, we present a novel attribution-driven knowledge distillation approach, which explores the token-level rationale behind the teacher model based on Integrated Gradients (IG) and transfers attribution knowledge to the student model. To enhance the knowledge transfer of model reasoning and generalization, we further explore multi-view attribution distillation on all potential decisions of the teacher. Comprehensive experiments are conducted with BERT on the GLUE benchmark. The experimental results demonstrate the superior performance of our approach to several state-of-the-art methods."
    }
  },
  {
    "id": "abstract-2023--acl-short--38",
    "result": [
      {
        "value": {
          "start": 962,
          "end": 979,
          "text": "extended training",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--38:E0"
      },
      {
        "value": {
          "start": 625,
          "end": 650,
          "text": "intermediate-depth models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--38:E1"
      }
    ],
    "data": {
      "text": "For humans, language production and comprehension is sensitive to the hierarchical structure of sentences. In natural language processing, past work has questioned how effectively neural sequence models like transformers capture this hierarchical structure when generalizing to structurally novel inputs. We show that transformer language models can learn to generalize hierarchically after training for extremely long periods—far beyond the point when in-domain accuracy has saturated. We call this phenomenon structural grokking. On multiple datasets, structural grokking exhibits inverted U-shaped scaling in model depth: intermediate-depth models generalize better than both very deep and very shallow transformers. When analyzing the relationship between model-internal properties and grokking, we find that optimal depth for grokking can be identified using the tree-structuredness metric of CITATION. Overall, our work provides strong evidence that, with extended training, vanilla transformers discover and use hierarchical structure."
    }
  },
  {
    "id": "abstract-2023--acl-short--127",
    "result": [
      {
        "value": {
          "start": 325,
          "end": 357,
          "text": "conditional generation framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--127:E0"
      },
      {
        "value": {
          "start": 951,
          "end": 979,
          "text": "state-of-the-art performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--127:E1"
      },
      {
        "from_id": "abstract-2023--acl-short--127:E0",
        "to_id": "abstract-2023--acl-short--127:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Zero-shot and few-shot stance detection identify the polarity of text with regard to a certain target when we have only limited or no training resources for the target. Previous work generally formulates the problem into a classification setting, ignoring the potential use of label text. In this paper, we instead utilize a conditional generation framework and formulate the problem as denoising from partially-filled templates, which can better utilize the semantics among input, label, and target texts. We further propose to jointly train an auxiliary task, target prediction, and to incorporate manually constructed incorrect samples with unlikelihood training to improve the representations for both target and label texts. We also verify the effectiveness of target-related Wikipedia knowledge with the generation framework. Experiments show that our proposed method significantly outperforms several strong baselines on VAST, and achieves new state-of-the-art performance."
    }
  },
  {
    "id": "abstract-2023--acl-industry--11",
    "result": [
      {
        "value": {
          "start": 860,
          "end": 881,
          "text": "intent classification",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-industry--11:E0"
      },
      {
        "value": {
          "start": 1296,
          "end": 1313,
          "text": "proposed approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-industry--11:E1"
      }
    ],
    "data": {
      "text": "To provide a convenient shopping experience and to answer user queries at scale, conversational platforms are essential for e-commerce. The user queries can be pre-purchase questions, such as product specifications and delivery time related, or post-purchase queries, such as exchange and return. A chatbot should be able to understand and answer a variety of such queries to help users with relevant information. One of the important modules in the chatbot is automated intent identification, i.e., understanding the user’s intention from the query text. Due to non-English speaking users interacting with the chatbot, we often get a significant percentage of code mix queries and queries with grammatical errors, which makes the problem more challenging. This paper proposes a simple yet competent Semi-Supervised Learning (SSL) approach for label-efficient intent classification. We use a small labeled corpus and relatively larger unlabeled query data to train a transformer model. For training the model with labeled data, we explore supervised MixUp data augmentation. To train with unlabeled data, we explore label consistency with dropout noise. We experiment with different pre-trained transformer architectures, such as BERT and sentence-BERT. Experimental results demonstrate that the proposed approach significantly improves over the supervised baseline, even with a limited labeled set. A variant of the model is currently deployed in production."
    }
  },
  {
    "id": "abstract-2023--acl-long--564",
    "result": [
      {
        "value": {
          "start": 569,
          "end": 580,
          "text": "Dialog-Post",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--564:E0"
      },
      {
        "value": {
          "start": 1107,
          "end": 1129,
          "text": "representation ability",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--564:E1"
      },
      {
        "value": {
          "start": 1210,
          "end": 1240,
          "text": "4 dialogue understanding tasks",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--564:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--564:E0",
        "to_id": "abstract-2023--acl-long--564:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--564:E0",
        "to_id": "abstract-2023--acl-long--564:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Dialogue representation and understanding aim to convert conversational inputs into embeddings and fulfill discriminative tasks. Compared with free-form text, dialogue has two important characteristics, hierarchical semantic structure and multi-facet attributes. Therefore, directly applying the pretrained language models (PLMs) might result in unsatisfactory performance. Recently, several work focused on the dialogue-adaptive post-training (DialPost) that further trains PLMs to fit dialogues. To model dialogues more comprehensively, we propose a DialPost method, Dialog-Post, with multi-level self-supervised objectives and a hierarchical model. These objectives leverage dialogue-specific attributes and use self-supervised signals to fully facilitate the representation and understanding of dialogues. The novel model is a hierarchical segment-wise self-attention network, which contains inner-segment and inter-segment self-attention sub-layers followed by an aggregation and updating module. To evaluate the effectiveness of our methods, we first apply two public datasets for the verification of representation ability. Then we conduct experiments on a newly-labelled dataset that is annotated with 4 dialogue understanding tasks. Experimental results show that our method outperforms existing SOTA models and achieves a 3.3% improvement on average."
    }
  },
  {
    "id": "abstract-2023--acl-industry--56",
    "result": [
      {
        "value": {
          "start": 223,
          "end": 232,
          "text": "ToxicTrap",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-industry--56:E0"
      },
      {
        "value": {
          "start": 818,
          "end": 838,
          "text": "adversarial training",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-industry--56:E1"
      }
    ],
    "data": {
      "text": "Recent NLP literature pays little attention to the robustness of toxicity language predictors, while these systems are most likely to be used in adversarial contexts. This paper presents a novel adversarial attack, \\texttt{ToxicTrap}, introducing small word-level perturbations to fool SOTA text classifiers to predict toxic text samples as benign. \\texttt{ToxicTrap} exploits greedy based search strategies to enable fast and effective generation of toxic adversarial examples. Two novel goal function designs allow \\texttt{ToxicTrap} to identify weaknesses in both multiclass and multilabel toxic language detectors. Our empirical results show that SOTA toxicity text classifiers are indeed vulnerable to the proposed attacks, attaining over 98\\% attack success rates in multilabel cases. We also show how a vanilla adversarial training and its improved version can help increase robustness of a toxicity detector even against unseen attacks."
    }
  },
  {
    "id": "abstract-2023--acl-industry--59",
    "result": [
      {
        "value": {
          "start": 680,
          "end": 715,
          "text": "context-aware query rewriting model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-industry--59:E0"
      }
    ],
    "data": {
      "text": "E-commerce queries are often short and ambiguous. Consequently, query understanding often uses query rewriting to disambiguate user-input queries. While using e-commerce search tools, users tend to enter multiple searches, which we call context, before purchasing. These history searches contain contextual insights about users’ true shopping intents. Therefore, modeling such contextual information is critical to a better query rewriting model. However, existing query rewriting models ignore users’ history behaviors and consider only the instant search query, which is often a short string offering limited information about the true shopping intent. We propose an end-to-end context-aware query rewriting model to bridge this gap, which takes the search context into account. Specifically, our model builds a session graph using the history search queries and their contained words. We then employ a graph attention mechanism that models cross-query relations and computes contextual information of the session. The model subsequently calculates session representations by combining the contextual information with the instant search query using an aggregation network. The session representations are then decoded to generate rewritten queries. Empirically, we demonstrate the superiority of our method to state-of-the-art approaches under various metrics."
    }
  },
  {
    "id": "abstract-2023--acl-long--176",
    "result": [
      {
        "value": {
          "start": 887,
          "end": 913,
          "text": "contextualised definitions",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--176:E0"
      },
      {
        "value": {
          "start": 968,
          "end": 1014,
          "text": "word-in-context semantic similarity judgements",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--176:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--176:E0",
        "to_id": "abstract-2023--acl-long--176:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We propose using automatically generated natural language definitions of contextualised word usages as interpretable word and word sense representations. Given a collection of usage examples for a target word, and the corresponding data-driven usage clusters (i.e., word senses), a definition is generated for each usage with a specialised Flan-T5 language model, and the most prototypical definition in a usage cluster is chosen as the sense label. We demonstrate how the resulting sense labels can make existing approaches to semantic change analysis more interpretable, and how they can allow users — historical linguists, lexicographers, or social scientists — to explore and intuitively explain diachronic trajectories of word meaning. Semantic change analysis is only one of many possible applications of the ‘definitions as representations’ paradigm. Beyond being human-readable, contextualised definitions also outperform token or usage sentence embeddings in word-in-context semantic similarity judgements, making them a new promising type of lexical representation for NLP."
    }
  },
  {
    "id": "abstract-2023--acl-long--847",
    "result": [
      {
        "value": {
          "start": 518,
          "end": 528,
          "text": "LayoutMask",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--847:E0"
      }
    ],
    "data": {
      "text": "Visually-rich Document Understanding (VrDU) has attracted much research attention over the past years. Pre-trained models on a large number of document images with transformer-based backbones have led to significant performance gains in this field. The major challenge is how to fusion the different modalities (text, layout, and image) of the documents in a unified model with different pre-training tasks. This paper focuses on improving text-layout interactions and proposes a novel multi-modal pre-training model, LayoutMask. LayoutMask uses local 1D position, instead of global 1D position, as layout input and has two pre-training objectives: (1) Masked Language Modeling: predicting masked tokens with two novel masking strategies; (2) Masked Position Modeling: predicting masked 2D positions to improve layout representation learning. LayoutMask can enhance the interactions between text and layout modalities in a unified model and produce adaptive and robust multi-modal representations for downstream tasks. Experimental results show that our proposed method can achieve state-of-the-art results on a wide variety of VrDU problems, including form understanding, receipt understanding, and document image classification."
    }
  },
  {
    "id": "abstract-2023--acl-long--120",
    "result": [
      {
        "value": {
          "start": 987,
          "end": 994,
          "text": "ExtEval",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--120:E0"
      }
    ],
    "data": {
      "text": "The problems of unfaithful summaries have been widely discussed under the context of abstractive summarization. Though extractive summarization is less prone to the common unfaithfulness issues of abstractive summaries, does that mean extractive is equal to faithful? Turns out that the answer is no. In this work, we define a typology with five types of broad unfaithfulness problems (including and beyond not-entailment) that can appear in extractive summaries, including incorrect coreference, incomplete coreference, incorrect discourse, incomplete discourse, as well as other misleading information. We ask humans to label these problems out of 1600 English summaries produced by 16 diverse extractive systems. We find that 30% of the summaries have at least one of the five issues. To automatically detect these problems, we find that 5 existing faithfulness evaluation metrics for summarization have poor correlations with human judgment. To remedy this, we propose a new metric, ExtEval, that is designed for detecting unfaithful extractive summaries and is shown to have the best performance. We hope our work can increase the awareness of unfaithfulness problems in extractive summarization and help future work to evaluate and resolve these issues."
    }
  },
  {
    "id": "abstract-2023--acl-long--177",
    "result": [
      {
        "value": {
          "start": 658,
          "end": 676,
          "text": "feedback simulator",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--177:E0"
      },
      {
        "value": {
          "start": 818,
          "end": 842,
          "text": "error correction ability",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--177:E1"
      },
      {
        "value": {
          "start": 938,
          "end": 966,
          "text": "error correction performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--177:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--177:E0",
        "to_id": "abstract-2023--acl-long--177:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--177:E0",
        "to_id": "abstract-2023--acl-long--177:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Interactive semantic parsing based on natural language (NL) feedback, where users provide feedback to correct the parser mistakes, has emerged as a more practical scenario than the traditional one-shot semantic parsing. However, prior work has heavily relied on human-annotated feedback data to train the interactive semantic parser, which is prohibitively expensive and not scalable. In this work, we propose a new task of simulating NL feedback for interactive semantic parsing. We accompany the task with a novel feedback evaluator. The evaluator is specifically designed to assess the quality of the simulated feedback, based on which we decide the best feedback simulator from our proposed variants. On a text-to-SQL dataset, we show that our feedback simulator can generate high-quality NL feedback to boost the error correction ability of a specific parser. In low-data settings, our feedback simulator can help achieve comparable error correction performance as trained using the costly, full set of human annotations."
    }
  },
  {
    "id": "abstract-2023--acl-short--7",
    "result": [
      {
        "value": {
          "start": 325,
          "end": 345,
          "text": "task alignment score",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--7:E0"
      },
      {
        "value": {
          "start": 549,
          "end": 575,
          "text": "classification performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--7:E1"
      },
      {
        "from_id": "abstract-2023--acl-short--7:E0",
        "to_id": "abstract-2023--acl-short--7:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Textual representations based on pre-trained language models are key, especially in few-shot learning scenarios. What makes a representation good for text classification? Is it due to the geometric properties of the space or because it is well aligned with the task? We hypothesize the second claim. To test it, we develop a task alignment score based on hierarchical clustering that measures alignment at different levels of granularity. Our experiments on text classification validate our hypothesis by showing that task alignment can explain the classification performance of a given representation."
    }
  },
  {
    "id": "abstract-2023--acl-industry--46",
    "result": [
      {
        "value": {
          "start": 315,
          "end": 352,
          "text": "two-stage taxonomy-agnostic framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-industry--46:E0"
      },
      {
        "value": {
          "start": 962,
          "end": 987,
          "text": "seasonal purchase revenue",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-industry--46:E1"
      },
      {
        "from_id": "abstract-2023--acl-industry--46:E0",
        "to_id": "abstract-2023--acl-industry--46:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "As e-commerce platforms develop different business lines, a special but challenging product categorization scenario emerges, where there are multiple domain-specific category taxonomies and each of them evolves dynamically over time. In order to unify the categorization process and ensure efficiency, we propose a two-stage taxonomy-agnostic framework that relies solely on calculating the semantic relatedness between product titles and category names in the vector space. To further enhance domain transferability and better exploit cross-domain data, we design two plug-in modules: a heuristic mapping scorer and a pretrained contrastive ranking module with the help of meta concepts, which represent keyword knowledge shared across domains. Comprehensive offline experiments show that our method outperforms strong baselineson three dynamic multi-domain product categorization (DMPC) tasks,and online experiments reconfirm its efficacy with a5% increase on seasonal purchase revenue. Related datasets will be released."
    }
  },
  {
    "id": "abstract-2023--acl-long--825",
    "result": [
      {
        "value": {
          "start": 527,
          "end": 570,
          "text": "Language-Specific Transformer Layers (LSLs)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--825:E0"
      },
      {
        "value": {
          "start": 984,
          "end": 988,
          "text": "chrF",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--825:E1"
      },
      {
        "value": {
          "start": 994,
          "end": 1000,
          "text": "spBLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--825:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--825:E0",
        "to_id": "abstract-2023--acl-long--825:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--825:E0",
        "to_id": "abstract-2023--acl-long--825:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Multilingual Machine Translation promises to improve translation quality between non-English languages. This is advantageous for several reasons, namely lower latency (no need to translate twice), and reduced error cascades (e.g., avoiding losing gender and formality information when translating through English).On the downside, adding more languages reduces model capacity per language, which is usually countered by increasing the overall model size, making training harder and inference slower. In this work, we introduce Language-Specific Transformer Layers (LSLs), which allow us to increase model capacity, while keeping the amount of computation and the number of parameters used in the forward pass constant. The key idea is to have some layers of the encoder be source or target language-specific, while keeping the remaining layers shared. We study the best way to place these layers using a neural architecture search inspired approach, and achieve an improvement of 1.3 chrF (1.5 spBLEU) points over not using LSLs on a separate decoder architecture, and 1.9 chrF (2.2 spBLEU) on a shared decoder one."
    }
  },
  {
    "id": "abstract-2023--acl-long--159",
    "result": [
      {
        "value": {
          "start": 66,
          "end": 85,
          "text": "joint goal accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--159:E0"
      }
    ],
    "data": {
      "text": "Despite the recent advances in dialogue state tracking (DST), the joint goal accuracy (JGA) of the existing methods on MultiWOZ 2.1 still remains merely 60%. In our preliminary error analysis, we find that beam search produces a pool of candidates that is likely to include the correct dialogue state. Motivated by this observation, we introduce a novel framework, called BREAK (Beam search and RE-rAnKing), that achieves outstanding performance on DST. BREAK performs DST in two stages: (i) generating k-best dialogue state candidates with beam search and (ii) re-ranking the candidates to select the correct dialogue state. This simple yet powerful framework shows state-of-the-art performance on all versions of MultiWOZ and M2M datasets. Most notably, we push the joint goal accuracy to 80-90% on MultiWOZ 2.1-2.4, which is an improvement of 23.6%, 26.3%, 21.7%, and 10.8% over the previous best-performing models, respectively. The data and code will be available athttps://github.com/tony-won/DST-BREAK"
    }
  },
  {
    "id": "abstract-2023--acl-long--782",
    "result": [
      {
        "value": {
          "start": 142,
          "end": 146,
          "text": "ULRA",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--782:E0"
      },
      {
        "value": {
          "start": 896,
          "end": 924,
          "text": "state-of-the-art performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--782:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--782:E0",
        "to_id": "abstract-2023--acl-long--782:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Automated Essay Scoring (AES) aims to evaluate the quality score for input essays. In this work, we propose a novel unsupervised AES approach ULRA, which does not require groundtruth scores of essays for training. The core idea of our ULRA is to use multiple heuristic quality signals as the pseudo-groundtruth, and then train a neural AES model by learning from the aggregation of these quality signals. To aggregate these inconsistent quality signals into a unified supervision, we view the AES task as a ranking problem, and design a special Deep Pairwise Rank Aggregation (DPRA) loss for training. In the DPRA loss, we set a learnable confidence weight for each signal to address the conflicts among signals, and train the neural AES model in a pairwise way to disentangle the cascade effect among partial-order pairs. Experiments on eight prompts of ASPA dataset show that ULRA achieves the state-of-the-art performance compared with previous unsupervised methods in terms of both transductive and inductive settings. Further, our approach achieves comparable performance with many existing domain-adapted supervised models, showing the effectiveness of ULRA. The code is available athttps://github.com/tenvence/ulra."
    }
  },
  {
    "id": "abstract-2023--acl-long--258",
    "result": [
      {
        "value": {
          "start": 611,
          "end": 626,
          "text": "HyCxG framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--258:E0"
      }
    ],
    "data": {
      "text": "Natural language understanding (NLU) is an essential branch of natural language processing, which relies on representations generated by pre-trained language models (PLMs). However, PLMs primarily focus on acquiring lexico-semantic information, while they may be unable to adequately handle the meaning of constructions. To address this issue, we introduce construction grammar (CxG), which highlights the pairings of form and meaning, to enrich language representation. We adopt usage-based construction grammar as the basis of our work, which is highly compatible with statistical models such as PLMs. Then a HyCxG framework is proposed to enhance language representation through a three-stage solution. First, all constructions are extracted from sentences via a slot-constraints approach. As constructions can overlap with each other, bringing redundancy and imbalance, we formulate the conditional max coverage problem for selecting the discriminative constructions. Finally, we propose a relational hypergraph attention network to acquire representation from constructional information by capturing high-order word interactions among constructions. Extensive experiments demonstrate the superiority of the proposed model on a variety of NLU tasks."
    }
  },
  {
    "id": "abstract-2023--acl-long--346",
    "result": [
      {
        "value": {
          "start": 920,
          "end": 978,
          "text": "Cross-lingual In-context Source Target Alignment (X-InSTA)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--346:E0"
      }
    ],
    "data": {
      "text": "In-context learning (ICL) unfolds as large language models become capable of inferring test labels conditioned on a few labeled samples without any gradient update. ICL-enabled large language models provide a promising step forward toward bypassing recurrent annotation costs in a low-resource setting. Yet, only a handful of past studies have explored ICL in a cross-lingual setting, in which the need for transferring label-knowledge from a high-resource language to a low-resource one is immensely crucial. To bridge the gap, we provide the first in-depth analysis of ICL for cross-lingual text classification. We find that the prevalent mode of selecting random input-label pairs to construct the prompt-context is severely limited in the case of cross-lingual ICL, primarily due to the lack of alignment in the input as well as the output spaces. To mitigate this, we propose a novel prompt construction strategy — Cross-lingual In-context Source Target Alignment (X-InSTA). With an injected coherence in the semantics of the input examples and a task-based alignment across the source and target languages, X-InSTA is able to outperform random prompt selection by a large margin across three different tasks using 44 different cross-lingual pairs."
    }
  },
  {
    "id": "abstract-2023--acl-long--531",
    "result": [
      {
        "value": {
          "start": 939,
          "end": 961,
          "text": "Emotion-Aware Pagerank",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--531:E0"
      }
    ],
    "data": {
      "text": "Understanding what leads to emotions during large-scale crises is important as it can provide groundings for expressed emotions and subsequently improve the understanding of ongoing disasters. Recent approaches trained supervised models to both detect emotions and explain emotion triggers (events and appraisals) via abstractive summarization. However, obtaining timely and qualitative abstractive summaries is expensive and extremely time-consuming, requiring highly-trained expert annotators. In time-sensitive, high-stake contexts, this can block necessary responses. We instead pursue unsupervised systems that extract triggers from text. First, we introduce CovidET-EXT, augmenting (Zhan et al., 2022)’s abstractive dataset (in the context of the COVID-19 crisis) with extractive triggers. Second, we develop new unsupervised learning models that can jointly detect emotions and summarize their triggers. Our best approach, entitled Emotion-Aware Pagerank, incorporates emotion information from external sources combined with a language understanding module, and outperforms strong baselines. We release our data and code athttps://github.com/tsosea2/CovidET-EXT."
    }
  },
  {
    "id": "abstract-2023--acl-short--48",
    "result": [
      {
        "value": {
          "start": 1108,
          "end": 1135,
          "text": "included persona attributes",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--48:E0"
      },
      {
        "value": {
          "start": 1156,
          "end": 1194,
          "text": "performance of all response dimensions",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--48:E1"
      },
      {
        "from_id": "abstract-2023--acl-short--48:E0",
        "to_id": "abstract-2023--acl-short--48:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Predicting how a user responds to news events enables important applications such as allowing intelligent agents or content producers to estimate the effect on different communities and revise unreleased messages to prevent unexpected bad outcomes such as social conflict and moral injury. We present a new task, Response Forecasting on Personas for News Media, to estimate the response a persona (characterizing an individual or a group) might have upon seeing a news message. Compared to the previous efforts which only predict generic comments to news, the proposed task not only introduces personalization in the modeling but also predicts the sentiment polarity and intensity of each response. This enables more accurate and comprehensive inference on the mental state of the persona. Meanwhile, the generated sentiment dimensions make the evaluation and application more reliable. We create the first benchmark dataset, which consists of 13,357 responses to 3,847 news headlines from Twitter. We further evaluate the SOTA neural language models with our dataset. The empirical results suggest that the included persona attributes are helpful for the performance of all response dimensions. Our analysis shows that the best-performing models are capable of predicting responses that are consistent with the personas, and as a byproduct, the task formulation also enables many interesting applications in the analysis of social network groups and their opinions, such as the discovery of extreme opinion groups."
    }
  },
  {
    "id": "abstract-2023--acl-long--384",
    "result": [
      {
        "value": {
          "start": 436,
          "end": 445,
          "text": "CompoundE",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--384:E0"
      }
    ],
    "data": {
      "text": "Geometric transformations including translation, rotation, and scaling are commonly used operations in image processing. Besides, some of them are successfully used in developing effective knowledge graph embedding (KGE). Inspired by the synergy, we propose a new KGE model by leveraging all three operations in this work. Since translation, rotation, and scaling operations are cascaded to form a composite one, the new model is named CompoundE. By casting CompoundE in the framework of group theory, we show that quite a few distanced-based KGE models are special cases of CompoundE. CompoundE extends the simple distance-based scoring functions to relation-dependent compound operations on head and/or tail entities. To demonstrate the effectiveness of CompoundE, we perform three prevalent KG prediction tasks including link prediction, path query answering, and entity typing, on a range of datasets. CompoundE outperforms extant models consistently, demonstrating its effectiveness and flexibility."
    }
  },
  {
    "id": "abstract-2023--acl-short--108",
    "result": [
      {
        "value": {
          "start": 626,
          "end": 655,
          "text": "GEnder Equality Prompt (GEEP)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--108:E0"
      },
      {
        "value": {
          "start": 283,
          "end": 293,
          "text": "forgetting",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--108:E1"
      },
      {
        "from_id": "abstract-2023--acl-short--108:E0",
        "to_id": "abstract-2023--acl-short--108:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Existing studies addressing gender bias of pre-trained language models, usually build a small gender-neutral data set and conduct a second phase pre-training on the model with such data. However, given the limited size and concentrated focus of the gender-neutral data, catastrophic forgetting would occur during second-phase pre-training. Forgetting information in the original training data may damage the model’s downstream performance by a large margin. In this work, we empirically show that catastrophic forgetting occurs in such methods by evaluating them with general NLP tasks in GLUE. Then, we propose a new method, GEnder Equality Prompt (GEEP), to improve gender fairness of pre-trained models with less forgetting. GEEP freezes the pre-trained model and learns gender-related prompts with gender-neutral data. Empirical results show that GEEP not only achieves SOTA performances on gender fairness tasks, but also forgets less and performs better on GLUE by a large margin."
    }
  },
  {
    "id": "abstract-2023--acl-long--184",
    "result": [
      {
        "value": {
          "start": 1068,
          "end": 1081,
          "text": "WSE framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--184:E0"
      },
      {
        "value": {
          "start": 974,
          "end": 1036,
          "text": "predicting plausible novel senses for over 7,500 English words",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--184:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--184:E0",
        "to_id": "abstract-2023--acl-long--184:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Humans often make creative use of words to expressnovel senses. A long-standing effort in natural language processing hasbeen focusing on word sense disambiguation (WSD), but little has been explored about how the sense inventory of a word may be extended toward novel meanings. We present a paradigm of word sense extension (WSE) thatenables words to spawn new senses toward novel context. We develop a framework that simulates novel word sense extension by first partitioning a polysemous word type into two pseudo-tokens that mark its different senses, and then inferring whether the meaning of a pseudo-token can be extended to convey the sense denoted by the token partitioned from the same word type. Our framework combines cognitivemodels of chaining with a learning scheme that transforms a language model embedding space to supportvarious types of word sense extension. We evaluate our frameworkagainst several competitive baselines and show that it is superior in predicting plausible novel senses for over 7,500 English words. Furthermore, we show that our WSE framework improves performance over a range of transformer-based WSD models in predicting rare word senses with few or zero mentions in the training data."
    }
  },
  {
    "id": "abstract-2023--acl-long--536",
    "result": [
      {
        "value": {
          "start": 576,
          "end": 628,
          "text": "unified event temporal relation extraction framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--536:E0"
      },
      {
        "value": {
          "start": 843,
          "end": 878,
          "text": "improvements over a strong baseline",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--536:E1"
      },
      {
        "value": {
          "start": 1117,
          "end": 1150,
          "text": "improvement in low-data scenarios",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--536:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--536:E0",
        "to_id": "abstract-2023--acl-long--536:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--536:E0",
        "to_id": "abstract-2023--acl-long--536:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Event temporal relation extraction (ETRE) is usually formulated as a multi-label classification task, where each type of relation is simply treated as a one-hot label. This formulation ignores the meaning of relations and wipes out their intrinsic dependency. After examining the relation definitions in various ETRE tasks, we observe that all relations can be interpreted using the start and end time points of events. For example, relationIncludescould be interpreted as event 1 starting no later than event 2 and ending no earlier than event 2. In this paper, we propose a unified event temporal relation extraction framework, which transforms temporal relations into logical expressions of time points and completes the ETRE by predicting the relations between certain time point pairs. Experiments on TB-Dense and MATRES show significant improvements over a strong baseline and outperform the state-of-the-art model by 0.3% on both datasets. By representing all relations in a unified framework, we can leverage the relations with sufficient data to assist the learning of other relations, thus achieving stable improvement in low-data scenarios. When the relation definitions are changed, our method can quickly adapt to the new ones by simply modifying the logic expressions that map time points to new event relations. The code is released athttps://github.com/AndrewZhe/A-Unified-Framework-for-ETRE"
    }
  },
  {
    "id": "abstract-2023--acl-long--31",
    "result": [
      {
        "value": {
          "start": 857,
          "end": 903,
          "text": "grounding ability acquired during pre-training",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--31:E0"
      }
    ],
    "data": {
      "text": "The ability to connect language units to their referents in the physical world, referred to as grounding, is crucial to learning and understanding grounded meanings of words. While humans demonstrate fast mapping in new word learning, it remains unclear whether modern vision-language models can truly represent language with their grounded meanings, and how grounding may further bootstrap new word learning. To this end, we introduce Grounded Open Vocabulary Acquisition (GOVA) to examine grounding and bootstrapping in open-world language learning. As an initial attempt, we propose World-to-Words (W2W), a novel visually-grounded language model by pre-training on image-text pairs highlighting grounding as an objective. Through extensive experiments and analysis, we demonstrate that W2W is a more coherent and fast grounded word learner, and that the grounding ability acquired during pre-training helps the model to learn unseen words more rapidly and robustly."
    }
  },
  {
    "id": "abstract-2023--acl-long--538",
    "result": [
      {
        "value": {
          "start": 516,
          "end": 560,
          "text": "decoupled prototype learning framework (DPL)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--538:E0"
      }
    ],
    "data": {
      "text": "Generalized intent discovery aims to extend a closed-set in-domain intent classifier to an open-world intent set including in-domain and out-of-domain intents. The key challenges lie in pseudo label disambiguation and representation learning. Previous methods suffer from a coupling of pseudo label disambiguation and representation learning, that is, the reliability of pseudo labels relies on representation learning, and representation learning is restricted by pseudo labels in turn. In this paper, we propose a decoupled prototype learning framework (DPL) to decouple pseudo label disambiguation and representation learning. Specifically, we firstly introduce prototypical contrastive representation learning (PCL) to get discriminative representations. And then we adopt a prototype-based label disambiguation method (PLD) to obtain pseudo labels. We theoretically prove that PCL and PLD work in a collaborative fashion and facilitate pseudo label disambiguation. Experiments and analysis on three benchmark datasets show the effectiveness of our method."
    }
  },
  {
    "id": "abstract-2023--acl-short--106",
    "result": [
      {
        "value": {
          "start": 776,
          "end": 805,
          "text": "LMs’ math reasoning abilities",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--106:E0"
      }
    ],
    "data": {
      "text": "Mathematical reasoning is regarded as a necessary ability for Language Models (LMs). Recent works demonstrate large LMs’ impressive performance in solving math problems. The success is attributed to their Chain-of-Thought (CoT) reasoning abilities, i.e., the ability to decompose complex questions into step-by-step reasoning chains, but such ability seems only to emerge from models with abundant parameters. This work investigates how to incorporate relatively small LMs with the capabilities of multi-step reasoning. We propose to inject such abilities by continually pre-training LMs on a synthetic dataset MsAT which is composed of Multi-step Arithmetic Tasks. Our experiments on four math word problem datasets show the effectiveness of the proposed method in enhancing LMs’ math reasoning abilities."
    }
  },
  {
    "id": "abstract-2023--acl-long--813",
    "result": [
      {
        "value": {
          "start": 433,
          "end": 442,
          "text": "Jointprop",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--813:E0"
      }
    ],
    "data": {
      "text": "Semi-supervised learning has been an important approach to address challenges in extracting entities and relations from limited data. However, current semi-supervised works handle the two tasks (i.e., Named Entity Recognition and Relation Extraction) separately and ignore the cross-correlation of entity and relation instances as well as the existence of similar instances across unlabeled data. To alleviate the issues, we propose Jointprop, a Heterogeneous Graph-based Propagation framework for joint semi-supervised entity and relation extraction, which captures the global structure information between individual tasks and exploits interactions within unlabeled data. Specifically, we construct a unified span-based heterogeneous graph from entity and relation candidates and propagate class labels based on confidence scores. We then employ a propagation learning scheme to leverage the affinities between labelled and unlabeled samples. Experiments on benchmark datasets show that our framework outperforms the state-of-the-art semi-supervised approaches on NER and RE tasks. We show that the joint semi-supervised learning of the two tasks benefits from their codependency and validates the importance of utilizing the shared information between unlabeled data."
    }
  },
  {
    "id": "abstract-2023--acl-long--167",
    "result": [
      {
        "value": {
          "start": 662,
          "end": 666,
          "text": "TREA",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--167:E0"
      },
      {
        "value": {
          "start": 1001,
          "end": 1014,
          "text": "effectiveness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--167:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--167:E0",
        "to_id": "abstract-2023--acl-long--167:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Conversational recommender systems (CRS) aim to timely trace the dynamic interests of users through dialogues and generate relevant responses for item recommendations. Recently, various external knowledge bases (especially knowledge graphs) are incorporated into CRS to enhance the understanding of conversation contexts. However, recent reasoning-based models heavily rely on simplified structures such as linear structures or fixed-hierarchical structures for causality reasoning, hence they cannot fully figure out sophisticated relationships among utterances with external knowledge. To address this, we propose a novel Tree structure Reasoning schEmA named TREA. TREA constructs a multi-hierarchical scalable tree as the reasoning structure to clarify the causal relationships between mentioned entities, and fully utilizes historical conversations to generate more reasonable and suitable responses for recommended results. Extensive experiments on two public CRS datasets have demonstrated the effectiveness of our approach."
    }
  },
  {
    "id": "abstract-2023--acl-long--485",
    "result": [
      {
        "value": {
          "start": 724,
          "end": 743,
          "text": "Multimodal BERT-ViT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--485:E0"
      },
      {
        "value": {
          "start": 1109,
          "end": 1147,
          "text": "multi-label movie genre classification",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--485:E1"
      },
      {
        "value": {
          "start": 813,
          "end": 831,
          "text": "dynamic adjustment",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--485:E2"
      },
      {
        "value": {
          "start": 849,
          "end": 898,
          "text": "balance between specialization and generalization",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--485:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--485:E0",
        "to_id": "abstract-2023--acl-long--485:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--485:E2",
        "to_id": "abstract-2023--acl-long--485:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Multimodal machine learning is a cutting-edge field that explores ways to incorporate information from multiple sources into models. As more multimodal data becomes available, this field has become increasingly relevant. This work focuses on two key challenges in multimodal machine learning. The first is finding efficient ways to combine information from different data types. The second is that often, one modality (e.g., text) is stronger and more relevant, making it difficult to identify meaningful patterns in the weaker modality (e.g., image). Our approach focuses on more effectively exploiting the weaker modality while dynamically regularizing the loss function. First, we introduce a new two-stream model called Multimodal BERT-ViT, which features a novel intra-CLS token fusion. Second, we utilize a dynamic adjustment that maintains a balance between specialization and generalization during the training to avoid overfitting, which we devised. We add this dynamic adjustment to the Unsupervised Data Augmentation (UDA) framework. We evaluate the effectiveness of these proposals on the task of multi-label movie genre classification using the Moviescope and MM-IMDb datasets. The evaluation revealed that our proposal offers substantial benefits, while simultaneously enabling us to harness the weaker modality without compromising the information provided by the stronger."
    }
  },
  {
    "id": "abstract-2023--acl-industry--27",
    "result": [
      {
        "value": {
          "start": 10,
          "end": 23,
          "text": "neural models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-industry--27:E0"
      },
      {
        "value": {
          "start": 73,
          "end": 110,
          "text": "performance of information extraction",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-industry--27:E1"
      },
      {
        "from_id": "abstract-2023--acl-industry--27:E0",
        "to_id": "abstract-2023--acl-industry--27:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recently, neural models have been leveraged to significantly improve the performance of information extraction from semi-structured websites. However, a barrier for continued progress is the small number of datasets large enough to train these models. In this work, we introduce the PLAtE (Pages of Lists Attribute Extraction) benchmark dataset as a challenging new web extraction task. PLAtE focuses on shopping data, specifically extractions from product review pages with multiple items encompassing the tasks of: (1) finding product list segmentation boundaries and (2) extracting attributes for each product. PLAtE is composed of 52,898 items collected from 6,694 pages and 156,014 attributes, making it the first large-scale list page web extraction dataset. We use a multi-stage approach to collect and annotate the dataset and adapt three state-of-the-art web extraction models to the two tasks comparing their strengths and weaknesses both quantitatively and qualitatively."
    }
  },
  {
    "id": "abstract-2023--acl-long--146",
    "result": [
      {
        "value": {
          "start": 906,
          "end": 938,
          "text": "captioning relevancy and fluency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--146:E0"
      }
    ],
    "data": {
      "text": "Unpaired cross-lingual image captioning has long suffered from irrelevancy and disfluency issues, due to the inconsistencies of the semantic scene and syntax attributes during transfer. In this work, we propose to address the above problems by incorporating the scene graph (SG) structures and the syntactic constituency (SC) trees. Our captioner contains the semantic structure-guided image-to-pivot captioning and the syntactic structure-guided pivot-to-target translation, two of which are joined via pivot language. We then take the SG and SC structures as pivoting, performing cross-modal semantic structure alignment and cross-lingual syntactic structure alignment learning. We further introduce cross-lingual&cross-modal back-translation training to fully align the captioning and translation stages. Experiments on English-Chinese transfers show that our model shows great superiority in improving captioning relevancy and fluency."
    }
  },
  {
    "id": "abstract-2023--acl-long--320",
    "result": [
      {
        "value": {
          "start": 573,
          "end": 616,
          "text": "Verify-and-Edit framework for CoT prompting",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--320:E0"
      },
      {
        "value": {
          "start": 778,
          "end": 786,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--320:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--320:E0",
        "to_id": "abstract-2023--acl-long--320:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "As large language models (LLMs) have become the norm in NLP, demonstrating good performance in generation and reasoning tasks, one of its most fatal disadvantages is the lack of factual correctness. Generating unfactual texts not only leads to lower performances but also degrades the trust and validity of their applications. Chain-of-Thought (CoT) prompting improves trust and model performance on complex reasoning tasks by generating interpretable reasoning chains, but still suffers from factuality concerns in knowledge-intensive tasks. In this paper, we propose the Verify-and-Edit framework for CoT prompting, which seeks to increase prediction factuality by post-editing reasoning chains according to external knowledge. Building on top of GPT-3, our framework lead to accuracy improvements in multiple open-domain question-answering tasks."
    }
  },
  {
    "id": "abstract-2023--acl-industry--22",
    "result": [
      {
        "value": {
          "start": 747,
          "end": 774,
          "text": "multi-span extraction model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-industry--22:E0"
      },
      {
        "value": {
          "start": 876,
          "end": 893,
          "text": "model performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-industry--22:E1"
      },
      {
        "value": {
          "start": 802,
          "end": 856,
          "text": "continual pre-training and multi-task learning schemes",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-industry--22:E2"
      },
      {
        "from_id": "abstract-2023--acl-industry--22:E0",
        "to_id": "abstract-2023--acl-industry--22:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-industry--22:E2",
        "to_id": "abstract-2023--acl-industry--22:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Through an online customer service application, we have collected many conversations between customer service agents and customers. Building a knowledge production system can help reduce the labor cost of maintaining the FAQ database for the customer service chatbot, whose core module is question answering (QA) on these conversations. However, most existing researches focus on document-based QA tasks, and there is a lack of researches on conversation-based QA and related datasets, especially in Chinese language. The challenges of conversation-based QA include: 1) answers may be scattered among multiple dialogue turns; 2) understanding complex dialogue contexts is more complicated than documents. To address these challenges, we propose a multi-span extraction model on this task and introduce continual pre-training and multi-task learning schemes to further improve model performance. To validate our approach, we construct two Chinese datasets using dialogues as the knowledge source, namely cs-qaconv and kd-qaconv, respectively. Experimental results demonstrate that the proposed model outperforms the baseline on both datasets. The online application also verifies the effectiveness of our method. The dataset kd-qaconv will be released publicly for research purposes."
    }
  },
  {
    "id": "abstract-2023--acl-long--37",
    "result": [
      {
        "value": {
          "start": 581,
          "end": 661,
          "text": "Causal intervention and Counterfactual reasoning based Debiasing framework (CCD)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--37:E0"
      },
      {
        "value": {
          "start": 133,
          "end": 164,
          "text": "multi-modal fake news detection",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--37:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--37:E0",
        "to_id": "abstract-2023--acl-long--37:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Due to the rapid upgrade of social platforms, most of today’s fake news is published and spread in a multi-modal form. Most existing multi-modal fake news detection methods neglect the fact that some label-specific features learned from the training set cannot generalize well to the testing set, thus inevitably suffering from the harm caused by the latent data bias. In this paper, we analyze and identify the psycholinguistic bias in the text and the bias of inferring news label based on only image features. We mitigate these biases from a causality perspective and propose a Causal intervention and Counterfactual reasoning based Debiasing framework (CCD) for multi-modal fake news detection. To achieve our goal, we first utilize causal intervention to remove the psycholinguistic bias which introduces the spurious correlations between text features and news label. And then, we apply counterfactual reasoning by imagining a counterfactual world where each news has only image features for estimating the direct effect of the image. Therefore we can eliminate the image-only bias by deducting the direct effect of the image from the total effect on labels. Extensive experiments on two real-world benchmark datasets demonstrate the effectiveness of our framework for improving multi-modal fake news detection."
    }
  },
  {
    "id": "abstract-2023--acl-long--275",
    "result": [
      {
        "value": {
          "start": 158,
          "end": 205,
          "text": "Knowledge Injection into Language Models (KILM)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--275:E0"
      },
      {
        "value": {
          "start": 734,
          "end": 755,
          "text": "zero-shot performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--275:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--275:E0",
        "to_id": "abstract-2023--acl-long--275:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Large pre-trained language models (PLMs) have been shown to retain implicit knowledge within their parameters. To enhance this implicit knowledge, we propose Knowledge Injection into Language Models (KILM), a novel approach that injects entity-related knowledge into encoder-decoder PLMs, via a generative knowledge infilling objective through continued pre-training. This is done without architectural modifications to the PLMs or adding additional parameters. Experimental results over a suite of knowledge-intensive tasks spanning numerous datasets show that KILM enables models to retain more knowledge and hallucinate less while preserving their original performance on general NLU and NLG tasks. KILM also demonstrates improved zero-shot performances on tasks such as entity disambiguation, outperforming state-of-the-art models having 30x more parameters."
    }
  },
  {
    "id": "abstract-2023--acl-long--289",
    "result": [
      {
        "value": {
          "start": 792,
          "end": 824,
          "text": "MIME (MultImodal Meme Explainer)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--289:E0"
      },
      {
        "value": {
          "start": 1120,
          "end": 1128,
          "text": "F1-score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--289:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--289:E0",
        "to_id": "abstract-2023--acl-long--289:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Memes are a powerful tool for communication over social media. Their affinity for evolving across politics, history, and sociocultural phenomena renders them an ideal vehicle for communication. To comprehend the subtle message conveyed within a meme, one must understand the relevant background that facilitates its holistic assimilation. Besides digital archiving of memes and their metadata by a few websites like knowyourmeme.com, currently, there is no efficient way to deduce a meme’s context dynamically. In this work, we propose a novel task, MEMEX - given a meme and a related document, the aim is to mine the context that succinctly explains the background of the meme. At first, we develop MCC (Meme Context Corpus), a novel dataset for MEMEX. Further, to benchmark MCC, we propose MIME (MultImodal Meme Explainer), a multimodal neural framework that uses external knowledge-enriched meme representation and a multi-level approach to capture the cross-modal semantic dependencies between the meme and the context. MIME surpasses several unimodal and multimodal systems and yields an absolute improvement of 4% F1-score over the best baseline. Lastly, we conduct detailed analyses of MIME’s performance, highlighting the aspects that could lead to optimal modeling of cross-modal contextual associations."
    }
  },
  {
    "id": "abstract-2023--acl-long--818",
    "result": [
      {
        "value": {
          "start": 986,
          "end": 1025,
          "text": "Pseudo-Target (PT) augmentation methods",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--818:E0"
      },
      {
        "value": {
          "start": 1065,
          "end": 1082,
          "text": "sequence-level KD",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--818:E1"
      },
      {
        "value": {
          "start": 1099,
          "end": 1120,
          "text": "Joint-Teaching method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--818:E2"
      },
      {
        "value": {
          "start": 1136,
          "end": 1149,
          "text": "word-level KD",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--818:E3"
      },
      {
        "value": {
          "start": 1413,
          "end": 1424,
          "text": "PT training",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--818:E4"
      },
      {
        "value": {
          "start": 1429,
          "end": 1452,
          "text": "task-specific KD in NLG",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--818:E5"
      },
      {
        "from_id": "abstract-2023--acl-long--818:E0",
        "to_id": "abstract-2023--acl-long--818:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--818:E2",
        "to_id": "abstract-2023--acl-long--818:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--818:E4",
        "to_id": "abstract-2023--acl-long--818:E5",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Modern Natural Language Generation (NLG) models come with massive computational and storage requirements. In this work, we study the potential of compressing them, which is crucial for real-world applications serving millions of users. We focus on Knowledge Distillation (KD) techniques, in which a small student model learns to imitate a large teacher model, allowing to transfer knowledge from the teacher to the student. In contrast to much of the previous work, our goal is to optimize the model for a specific NLG task and a specific dataset. Typically in real-world applications, in addition to labeled data there is abundant unlabeled task-specific data, which is crucial for attaining high compression rates via KD. In this work, we conduct a systematic study of task-specific KD techniques for various NLG tasks under realistic assumptions. We discuss the special characteristics of NLG distillation and particularly the exposure bias problem. Following, we derive a family of Pseudo-Target (PT) augmentation methods, substantially extending prior work on sequence-level KD. We propose the Joint-Teaching method, which applies word-level KD to multiple PTs generated by both the teacher and the student. Finally, we validate our findings in an extreme setup with no labeled examples using GPT-4 as the teacher. Our study provides practical model design observations and demonstrates the effectiveness of PT training for task-specific KD in NLG."
    }
  },
  {
    "id": "abstract-2023--acl-long--40",
    "result": [
      {
        "value": {
          "start": 533,
          "end": 562,
          "text": "score-based generative method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--40:E0"
      },
      {
        "value": {
          "start": 1209,
          "end": 1217,
          "text": "F1 score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--40:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--40:E0",
        "to_id": "abstract-2023--acl-long--40:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Adversarial detection aims to detect adversarial samples that threaten the security of deep neural networks, which is an essential step toward building robust AI systems. Density-based estimation is widely considered as an effective technique by explicitly modeling the distribution of normal data and identifying adversarial ones as outliers. However, these methods suffer from significant performance degradation when the adversarial samples lie close to the non-adversarial data manifold. To address this limitation, we propose a score-based generative method to implicitly model the data distribution. Our approach utilizes the gradient of the log-density data distribution and calculates the distribution gap between adversarial and normal samples through multi-step iterations using Langevin dynamics. In addition, we use supervised contrastive learning to guide the gradient estimation using label information, which avoids collapsing to a single data manifold and better preserves the anisotropy of the different labeled data distributions. Experimental results on three text classification tasks upon four advanced attack algorithms show that our approach is a significant improvement (average +15.2 F1 score against previous SOTA) over previous detection methods."
    }
  },
  {
    "id": "abstract-2023--acl-long--775",
    "result": [
      {
        "value": {
          "start": 948,
          "end": 1010,
          "text": "Static-Dynamic graph-based Dialogue Summarization model (SDDS)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--775:E0"
      },
      {
        "value": {
          "start": 1152,
          "end": 1165,
          "text": "effectiveness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--775:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--775:E0",
        "to_id": "abstract-2023--acl-long--775:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Dialogue, the most fundamental and specially privileged arena of language, gains increasing ubiquity across the Web in recent years. Quickly going through the long dialogue context and capturing salient information scattered over the whole dialogue session benefit users in many real-world Web applications such as email thread summarization and meeting minutes draft. Dialogue summarization is a challenging task in that dialogue has dynamic interaction nature and presumably inconsistent information flow among various speakers. Many researchers address this task by modeling dialogue with pre-computed static graph structure using external linguistic toolkits. However, such methods heavily depend on the reliability of external tools and the static graph construction is disjoint with the graph representation learning phase, which makes the graph can’t be dynamically adapted for the downstream summarization task. In this paper, we propose a Static-Dynamic graph-based Dialogue Summarization model (SDDS), which fuses prior knowledge from human expertise and adaptively learns the graph structure in an end-to-end learning fashion. To verify the effectiveness of SDDS, we conduct experiments on three benchmark datasets (SAMSum, MediaSum, and DialogSum) and the results verify the superiority of SDDS."
    }
  },
  {
    "id": "abstract-2023--acl-long--889",
    "result": [
      {
        "value": {
          "start": 1241,
          "end": 1243,
          "text": "F1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--889:E0"
      },
      {
        "value": {
          "start": 1253,
          "end": 1259,
          "text": "Ign F1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--889:E1"
      }
    ],
    "data": {
      "text": "Document-level relation extraction (DocRE) aims to infer complex semantic relations among entities in a document. Distant supervision (DS) is able to generate massive auto-labeled data, which can improve DocRE performance. Recent works leverage pseudo labels generated by the pre-denoising model to reduce noise in DS data. However, unreliable pseudo labels bring new noise, e.g., adding false pseudo labels and losing correct DS labels. Therefore, how to select effective pseudo labels to denoise DS data is still a challenge in document-level distant relation extraction. To tackle this issue, we introduce uncertainty estimation technology to determine whether pseudo labels can be trusted. In this work, we propose a Document-level distant Relation Extraction framework with Uncertainty Guided label denoising, UGDRE. Specifically, we propose a novel instance-level uncertainty estimation method, which measures the reliability of the pseudo labels with overlapping relations. By further considering the long-tail problem, we design dynamic uncertainty thresholds for different types of relations to filter high-uncertainty pseudo labels. We conduct experiments on two public datasets. Our framework outperforms strong baselines by 1.91 F1 and 2.28 Ign F1 on the RE-DocRED dataset."
    }
  },
  {
    "id": "abstract-2023--acl-long--586",
    "result": [
      {
        "value": {
          "start": 581,
          "end": 653,
          "text": "multilingual KGC framework with language-sensitive multi-graph attention",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--586:E0"
      },
      {
        "value": {
          "start": 1108,
          "end": 1153,
          "text": "improvements on the DBP-5L and E-PKG datasets",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--586:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--586:E0",
        "to_id": "abstract-2023--acl-long--586:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Multilingual Knowledge Graph Completion (KGC) aims to predict missing links with multilingual knowledge graphs. However, existing approaches suffer from two main drawbacks: (a) alignment dependency: the multilingual KGC is always realized with joint entity or relation alignment, which introduces additional alignment models and increases the complexity of the whole framework; (b) training inefficiency: the trained model will only be used for the completion of one target KG, although the data from all KGs are used simultaneously. To address these drawbacks, we propose a novel multilingual KGC framework with language-sensitive multi-graph attention such that the missing links on all given KGs can be inferred by a universal knowledge completion model. Specifically, we first build a relational graph neural network by sharing the embeddings of aligned nodes to transfer language-independent knowledge. Meanwhile, a language-sensitive multi-graph attention (LSMGA) is proposed to deal with the information inconsistency among different KGs. Experimental results show that our model achieves significant improvements on the DBP-5L and E-PKG datasets."
    }
  },
  {
    "id": "abstract-2023--acl-long--683",
    "result": [
      {
        "value": {
          "start": 897,
          "end": 935,
          "text": "expanding attributes of existing types",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--683:E0"
      }
    ],
    "data": {
      "text": "We present a new task setting for attribute mining on e-commerce products, serving as a practical solution to extract open-world attributes without extensive human intervention. Our supervision comes from a high-quality seed attribute set bootstrapped from existing resources, and we aim to expand the attribute vocabulary of existing seed types, and also to discover any new attribute types automatically. A new dataset is created to support our setting, and our approach Amacer is proposed specifically to tackle the limited supervision. Especially, given that no direct supervision is available for those unseen new attributes, our novel formulation exploits self-supervised heuristic and unsupervised latent attributes, which attains implicit semantic signals as additional supervision by leveraging product context. Experiments suggest that our approach surpasses various baselines by 12 F1, expanding attributes of existing types significantly by up to 12 times, and discovering values from 39% new types."
    }
  },
  {
    "id": "abstract-2023--acl-short--30",
    "result": [
      {
        "value": {
          "start": 543,
          "end": 571,
          "text": "data intervention strategies",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--30:E0"
      },
      {
        "value": {
          "start": 617,
          "end": 628,
          "text": "gender bias",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--30:E1"
      },
      {
        "from_id": "abstract-2023--acl-short--30:E0",
        "to_id": "abstract-2023--acl-short--30:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability."
    }
  },
  {
    "id": "abstract-2023--acl-short--160",
    "result": [
      {
        "value": {
          "start": 403,
          "end": 446,
          "text": "linear classifiers on bag-of-words features",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--160:E0"
      },
      {
        "value": {
          "start": 523,
          "end": 546,
          "text": "competitive performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--160:E1"
      },
      {
        "value": {
          "start": 553,
          "end": 563,
          "text": "efficiency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--160:E2"
      },
      {
        "value": {
          "start": 569,
          "end": 579,
          "text": "robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--160:E3"
      },
      {
        "from_id": "abstract-2023--acl-short--160:E0",
        "to_id": "abstract-2023--acl-short--160:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-short--160:E0",
        "to_id": "abstract-2023--acl-short--160:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-short--160:E0",
        "to_id": "abstract-2023--acl-short--160:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Large-scale pre-trained language models such as BERT are popular solutions for text classification. Due to the superior performance of these advanced methods, nowadays, people often directly train them for a few epochs and deploy the obtained model. In this opinion paper, we point out that this way may only sometimes get satisfactory results. We argue the importance of running a simple baseline like linear classifiers on bag-of-words features along with advanced methods. First, for many text data, linear methods show competitive performance, high efficiency, and robustness. Second, advanced models such as BERT may only achieve the best results if properly applied. Simple baselines help to confirm whether the results of advanced models are acceptable. Our experimental results fully support these points."
    }
  },
  {
    "id": "abstract-2023--acl-long--210",
    "result": [
      {
        "value": {
          "start": 1041,
          "end": 1121,
          "text": "multilingual models trained on a combination of English and target language data",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--210:E0"
      },
      {
        "value": {
          "start": 1207,
          "end": 1251,
          "text": "translation and annotation projection errors",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--210:E1"
      },
      {
        "value": {
          "start": 1396,
          "end": 1436,
          "text": "dataset quality and RE model performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--210:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--210:E1",
        "to_id": "abstract-2023--acl-long--210:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Relation extraction (RE) is a fundamental task in information extraction, whose extension to multilingual settings has been hindered by the lack of supervised resources comparable in size to large English datasets such as TACRED (Zhang et al., 2017). To address this gap, we introduce the MultiTACRED dataset, covering 12 typologically diverse languages from 9 language families, which is created by machine-translating TACRED instances and automatically projecting their entity annotations. We analyze translation and annotation projection quality, identify error categories, and experimentally evaluate fine-tuned pretrained mono- and multilingual language models in common transfer learning scenarios. Our analyses show that machine translation is a viable strategy to transfer RE instances, with native speakers judging more than 83% of the translated instances to be linguistically and semantically acceptable. We find monolingual RE model performance to be comparable to the English original for many of the target languages, and that multilingual models trained on a combination of English and target language data can outperform their monolingual counterparts. However, we also observe a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages, such as pronoun-dropping, compounding and inflection, that degrade dataset quality and RE model performance."
    }
  },
  {
    "id": "abstract-2023--acl-long--303",
    "result": [
      {
        "value": {
          "start": 905,
          "end": 925,
          "text": "multimodal framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--303:E0"
      },
      {
        "value": {
          "start": 1368,
          "end": 1397,
          "text": "F1@3 for next step prediction",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--303:E1"
      },
      {
        "value": {
          "start": 1426,
          "end": 1463,
          "text": "Acc@1 for partial sequence completion",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--303:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--303:E0",
        "to_id": "abstract-2023--acl-long--303:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--303:E0",
        "to_id": "abstract-2023--acl-long--303:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Online resources such as WikiHow compile a wide range of scripts for performing everyday tasks, which can assist models in learning to reason about procedures. However, the scripts are always presented in a linear manner, which does not reflect the flexibility displayed by people executing tasks in real life. For example, in the CrossTask Dataset, 64.5% of consecutive step pairs are also observed in the reverse order, suggesting their ordering is not fixed. In addition, each step has an average of 2.56 frequent next steps, demonstrating “branching”. In this paper, we propose the new challenging task of non-sequential graph script induction, aiming to capture optional and interchangeable steps in procedural planning. To automate the induction of such graph scripts for given tasks, we propose to take advantage of loosely aligned videos of people performing the tasks. In particular, we design a multimodal framework to ground procedural videos to WikiHow textual steps and thus transform each video into an observed step path on the latent ground truth graph script. This key transformation enables us to train a script knowledge model capable of both generating explicit graph scripts for learnt tasks and predicting future steps given a partial step sequence. Our best model outperforms the strongest pure text/vision baselines by 17.52% absolute gains on F1@3 for next step prediction and 13.8% absolute gains on Acc@1 for partial sequence completion. Human evaluation shows our model outperforming the WikiHow linear baseline by 48.76% absolute gains in capturing sequential and non-sequential step relationships."
    }
  },
  {
    "id": "abstract-2023--acl-short--132",
    "result": [
      {
        "value": {
          "start": 1074,
          "end": 1105,
          "text": "correlation coefficient metrics",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--132:E0"
      },
      {
        "value": {
          "start": 1221,
          "end": 1237,
          "text": "ranking accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--132:E1"
      }
    ],
    "data": {
      "text": "As a subjective metric to evaluate the quality of synthesized speech, Mean opinion score(MOS) usually requires multiple annotators to score the same speech. Such an annotation approach requires a lot of manpower and is also time-consuming. MOS prediction model for automatic evaluation can significantly reduce labor cost. In previous works, it is difficult to accurately rank the quality of speech when the MOS scores are close. However, in practical applications, it is more important to correctly rank the quality of synthesis systems or sentences than simply predicting MOS scores. Meanwhile, as each annotator scores multiple audios during annotation, the score is probably a relative value based on the first or the first few speech scores given by the annotator. Motivated by the above two points, we propose a general framework for MOS prediction based on pair comparison (MOSPC), and we utilize C-Mixup algorithm to enhance the generalization performance of MOSPC.The experiments on BVCC and VCC2018 show that our framework outperforms the baselines on most of the correlation coefficient metrics, especially on the metric KTAU related to quality ranking. And our framework also surpasses the strong baseline in ranking accuracy on each fine-grained segment. These results indicate that our framework contributes to improving the ranking accuracy of speech quality."
    }
  },
  {
    "id": "abstract-2023--acl-long--161",
    "result": [
      {
        "value": {
          "start": 659,
          "end": 684,
          "text": "debiasing framework IEGDB",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--161:E0"
      },
      {
        "value": {
          "start": 915,
          "end": 955,
          "text": "stability of performance on OOD datasets",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--161:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--161:E0",
        "to_id": "abstract-2023--acl-long--161:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Although achieving promising performance, current Natural Language Understanding models tend to utilize dataset biases instead of learning the intended task, which always leads to performance degradation on out-of-distribution (OOD) samples. Toincrease the performance stability, previous debiasing methodsempiricallycapture bias features from data to prevent the model from corresponding biases. However, our analyses show that the empirical debiasing methods may fail to capture part of the potential dataset biases and mistake semantic information of input text as biases, which limits the effectiveness of debiasing. To address these issues, we propose a debiasing framework IEGDB that comprehensively detects the dataset biases to induce a set of biased features, and then purifies the biased features with the guidance of information entropy. Experimental results show that IEGDB can consistently improve the stability of performance on OOD datasets for a set of widely adopted NLU models."
    }
  },
  {
    "id": "abstract-2023--acl-long--95",
    "result": [
      {
        "value": {
          "start": 759,
          "end": 776,
          "text": "model performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--95:E0"
      }
    ],
    "data": {
      "text": "Pre-trained language models (PLMs) are known to be overly parameterized and have significant redundancy, indicating a small degree of freedom of the PLMs. Motivated by the observation, in this paper, we study the problem of re-parameterizing and fine-tuning PLMs from a new perspective: Discovery of intrinsic task-specific subspace. Specifically, by exploiting the dynamics of the fine-tuning process for a given task, the parameter optimization trajectory is learned to uncover its intrinsic task-specific subspace. A key finding is that PLMs can be effectively fine-tuned in the subspace with a small number of free parameters. Beyond, we observe some outlier dimensions emerging during fine-tuning in the subspace. Disabling these dimensions degrades the model performance significantly. This suggests that these dimensions are crucial to induce task-specific knowledge to downstream tasks."
    }
  },
  {
    "id": "abstract-2023--acl-long--495",
    "result": [
      {
        "value": {
          "start": 776,
          "end": 844,
          "text": "orthogonal constraints on textual and visual feature representations",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--495:E0"
      },
      {
        "value": {
          "start": 961,
          "end": 1007,
          "text": "prediction of fine-grained depression symptoms",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--495:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--495:E0",
        "to_id": "abstract-2023--acl-long--495:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The past decade has observed significant attention toward developing computational methods for classifying social media data based on the presence or absence of mental health conditions. In the context of mental health, for clinicians to make an accurate diagnosis or provide personalized intervention, it is crucial to identify fine-grained mental health symptoms. To this end, we conduct a focused study on depression disorder and introduce a new task of identifying fine-grained depressive symptoms from memes. Toward this, we create a high-quality dataset (RESTORE) annotated with 8 fine-grained depression symptoms based on the clinically adopted PHQ-9 questionnaire. We benchmark RESTORE on 20 strong monomodal and multimodal methods. Additionally, we show how imposing orthogonal constraints on textual and visual feature representations in a multimodal setting can enforce the model to learn non-redundant and de-correlated features leading to a better prediction of fine-grained depression symptoms. Further, we conduct an extensive human analysis and elaborate on the limitations of existing multimodal models that often overlook the implicit connection between visual and textual elements of a meme."
    }
  },
  {
    "id": "abstract-2023--acl-long--269",
    "result": [
      {
        "value": {
          "start": 743,
          "end": 764,
          "text": "multilingual training",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--269:E0"
      },
      {
        "value": {
          "start": 917,
          "end": 951,
          "text": "few-shot prompting with BLOOM-176b",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--269:E1"
      }
    ],
    "data": {
      "text": "Recent advancements in high-quality, large-scale English resources have pushed the frontier of English Automatic Text Simplification (ATS) research. However, less work has been done on multilingual text simplification due to the lack of a diverse evaluation benchmark that covers complex-simple sentence pairs in many languages. This paper introduces the MultiSim benchmark, a collection of 27 resources in 12 distinct languages containing over 1.7 million complex-simple sentence pairs. This benchmark will encourage research in developing more effective multilingual text simplification models and evaluation metrics. Our experiments using MultiSim with pre-trained multilingual language models reveal exciting performance improvements from multilingual training in non-English settings. We observe strong performance from Russian in zero-shot cross-lingual transfer to low-resource languages. We further show that few-shot prompting with BLOOM-176b achieves comparable quality to reference simplifications outperforming fine-tuned models in most languages. We validate these findings through human evaluation."
    }
  },
  {
    "id": "abstract-2023--acl-long--608",
    "result": [
      {
        "value": {
          "start": 177,
          "end": 220,
          "text": "synthetic data generation framework (SynDG)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--608:E0"
      },
      {
        "value": {
          "start": 1012,
          "end": 1029,
          "text": "model performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--608:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--608:E0",
        "to_id": "abstract-2023--acl-long--608:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Training grounded response generation models often requires a large collection of grounded dialogues. However, it is costly to build such dialogues. In this paper, we present a synthetic data generation framework (SynDG) for grounded dialogues. The generation process utilizes large pre-trained language models and freely available knowledge data (e.g., Wikipedia pages, persona profiles, etc.). The key idea of designing SynDG is to consider dialogue flow and coherence in the generation process. Specifically, given knowledge data, we first heuristically determine a dialogue flow, which is a series of knowledge pieces. Then, we employ T5 to incrementally turn the dialogue flow into a dialogue. To ensure coherence of both the dialogue flow and the synthetic dialogue, we design a two-level filtering strategy, at the flow-level and the utterance-level respectively. Experiments on two public benchmarks show that the synthetic grounded dialogue data produced by our framework is able to significantly boost model performance in both full training data and low-resource scenarios."
    }
  },
  {
    "id": "abstract-2023--acl-industry--35",
    "result": [
      {
        "value": {
          "start": 580,
          "end": 626,
          "text": "curating and leveraging high-precision samples",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-industry--35:E0"
      },
      {
        "value": {
          "start": 304,
          "end": 323,
          "text": "policy improvements",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-industry--35:E1"
      },
      {
        "from_id": "abstract-2023--acl-industry--35:E0",
        "to_id": "abstract-2023--acl-industry--35:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Off-Policy reinforcement learning has been the driving force for the state-of-the-art conversational AIs leading to more natural human-agent interactions and improving the user satisfaction for goal-oriented agents. However, in large-scale commercial settings, it is often challenging to balance between policy improvements and experience continuity on the broad spectrum of applications handled by such system. In the literature, off-policy evaluation and guard-railing on aggregate statistics has been commonly used to address this problem. In this paper, we propose method for curating and leveraging high-precision samples sourced from historical regression incident reports to validate, safe-guard, and improve policies prior to the online deployment. We conducted extensive experiments using data from a real-world conversational system and actual regression incidents. The proposed method is currently deployed in our production system to protect customers against broken experiences and enable long-term policy improvements."
    }
  },
  {
    "id": "abstract-2023--acl-long--252",
    "result": [
      {
        "value": {
          "start": 389,
          "end": 397,
          "text": "Prompter",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--252:E0"
      }
    ],
    "data": {
      "text": "A challenge in the Dialogue State Tracking (DST) field is adapting models to new domains without using any supervised data — zero-shot domain adaptation. Parameter-Efficient Transfer Learning (PETL) has the potential to address this problem due to its robustness. However, it has yet to be applied to the zero-shot scenarios, as it is not clear how to apply it unsupervisedly. Our method, Prompter, uses descriptions of target domain slots to generate dynamic prefixes that are concatenated to the key and values at each layer’s self-attention mechanism. This allows for the use of prefix-tuning in zero-shot. Prompter outperforms previous methods on both the MultiWOZ and SGD benchmarks. In generating prefixes, our analyses find that Prompter not only utilizes the semantics of slot descriptions but also how often the slots appear together in conversation. Moreover, Prompter’s gains are due to its improved ability to distinguish ”none”-valued dialogue slots, compared against baselines."
    }
  },
  {
    "id": "abstract-2023--acl-long--394",
    "result": [
      {
        "value": {
          "start": 517,
          "end": 524,
          "text": "HighGEN",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--394:E0"
      },
      {
        "value": {
          "start": 1092,
          "end": 1100,
          "text": "F1 score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--394:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--394:E0",
        "to_id": "abstract-2023--acl-long--394:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Most weakly supervised named entity recognition (NER) models rely on domain-specific dictionaries provided by experts. This approach is infeasible in many domains where dictionaries do not exist. While a phrase retrieval model was used to construct pseudo-dictionaries with entities retrieved from Wikipedia automatically in a recent study, these dictionaries often have limited coverage because the retriever is likely to retrieve popular entities rather than rare ones. In this study, we present a novel framework, HighGEN, that generates NER datasets with high-coverage pseudo-dictionaries. Specifically, we create entity-rich dictionaries with a novel search method, called phrase embedding search, which encourages the retriever to search a space densely populated with various entities. In addition, we use a new verification process based on the embedding distance between candidate entity mentions and entity types to reduce the false-positive noise in weak labels generated by high-coverage dictionaries. We demonstrate that HighGEN outperforms the previous best model by an average F1 score of 4.7 across five NER benchmark datasets."
    }
  },
  {
    "id": "abstract-2023--acl-long--109",
    "result": [
      {
        "value": {
          "start": 587,
          "end": 627,
          "text": "large pre-trained language models (PLMs)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--109:E0"
      },
      {
        "value": {
          "start": 838,
          "end": 868,
          "text": "zero-shot relational reasoning",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--109:E1"
      },
      {
        "value": {
          "start": 1042,
          "end": 1067,
          "text": "higher-level abstractions",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--109:E2"
      },
      {
        "value": {
          "start": 1087,
          "end": 1113,
          "text": "PLMs’ analogical reasoning",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--109:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--109:E0",
        "to_id": "abstract-2023--acl-long--109:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--109:E2",
        "to_id": "abstract-2023--acl-long--109:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Analogical reasoning is a fundamental capacity of human cognition that allows us to reason abstractly about novel situations by relating them to past experiences. While it is thought to be essential for robust reasoning in AI systems, conventional approaches require significant training and/or hard-coding of domain knowledge to be applied to benchmark tasks. Inspired by cognitive science research that has found connections between human language and analogy-making, we explore the use of intuitive language-based abstractions to support analogy in AI systems. Specifically, we apply large pre-trained language models (PLMs) to visual Raven’s Progressive Matrices (RPM), a common relational reasoning test. By simply encoding the perceptual features of the problem into language form, we find that PLMs exhibit a striking capacity for zero-shot relational reasoning, exceeding human performance and nearing supervised vision-based methods. We explore different encodings that vary the level of abstraction over task features, finding that higher-level abstractions further strengthen PLMs’ analogical reasoning. Our detailed analysis reveals insights on the role of model complexity, in-context learning, and prior knowledge in solving RPM tasks."
    }
  },
  {
    "id": "abstract-2023--acl-long--339",
    "result": [
      {
        "value": {
          "start": 20,
          "end": 25,
          "text": "miCSE",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--339:E0"
      }
    ],
    "data": {
      "text": "This paper presents miCSE, a mutual information-based contrastive learning framework that significantly advances the state-of-the-art in few-shot sentence embedding. The proposed approach imposes alignment between the attention pattern of different views during contrastive learning. Learning sentence embeddings with miCSE entails enforcing the structural consistency across augmented views for every sentence, making contrastive self-supervised learning more sample efficient. As a result, the proposed approach shows strong performance in the few-shot learning domain. While it achieves superior results compared to state-of-the-art methods on multiple benchmarks in few-shot learning, it is comparable in the full-shot scenario. This study opens up avenues for efficient self-supervised learning methods that are more robust than current contrastive methods for sentence embedding."
    }
  },
  {
    "id": "abstract-2023--acl-long--444",
    "result": [
      {
        "value": {
          "start": 555,
          "end": 579,
          "text": "query refinement prompts",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--444:E0"
      }
    ],
    "data": {
      "text": "Large language models (LLMs) have been shown to perform well in answering questions and in producing long-form texts, both in few-shot closed-book settings. While the former can be validated using well-known evaluation metrics, the latter is difficult to evaluate. We resolve the difficulties to evaluate long-form output by doing both tasks at once – to do question answering that requires long-form answers. Such questions tend to be multifaceted, i.e., they may have ambiguities and/or require information from multiple sources. To this end, we define query refinement prompts that encourage LLMs to explicitly express the multifacetedness in questions and generate long-form answers covering multiple facets of the question. Our experiments on two long-form question answering datasets, ASQA and AQuAMuSe, show that using our prompts allows us to outperform fully finetuned models in the closed book setting, as well as achieve results comparable to retrieve-then-generate open-book models."
    }
  },
  {
    "id": "abstract-2023--acl-long--27",
    "result": [
      {
        "value": {
          "start": 730,
          "end": 739,
          "text": "our model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--27:E0"
      },
      {
        "value": {
          "start": 641,
          "end": 656,
          "text": "length accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--27:E1"
      },
      {
        "value": {
          "start": 658,
          "end": 672,
          "text": "rhyme accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--27:E2"
      },
      {
        "value": {
          "start": 678,
          "end": 698,
          "text": "word boundary recall",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--27:E3"
      },
      {
        "value": {
          "start": 776,
          "end": 791,
          "text": "overall quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--27:E4"
      },
      {
        "from_id": "abstract-2023--acl-long--27:E0",
        "to_id": "abstract-2023--acl-long--27:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--27:E0",
        "to_id": "abstract-2023--acl-long--27:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--27:E0",
        "to_id": "abstract-2023--acl-long--27:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--27:E0",
        "to_id": "abstract-2023--acl-long--27:E4",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The development of general-domain neural machine translation (NMT) methods has advanced significantly in recent years, but the lack of naturalness and musical constraints in the outputs makes them unable to produce singable lyric translations. This paper bridges the singability quality gap by formalizing lyric translation into a constrained translation problem, converting theoretical guidance and practical techniques from translatology literature to prompt-driven NMT approaches, exploring better adaptation methods, and instantiating them to an English-Chinese lyric translation system. Our model achieves 99.85%, 99.00%, and 95.52% on length accuracy, rhyme accuracy, and word boundary recall. In our subjective evaluation, our model shows a 75% relative enhancement on overall quality, compared against naive fine-tuning (Code available athttps://github.com/Sonata165/ControllableLyricTranslation)."
    }
  },
  {
    "id": "abstract-2023--acl-long--125",
    "result": [
      {
        "value": {
          "start": 26,
          "end": 31,
          "text": "SimLM",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--125:E0"
      },
      {
        "value": {
          "start": 747,
          "end": 781,
          "text": "improvements over strong baselines",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--125:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--125:E0",
        "to_id": "abstract-2023--acl-long--125:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In this paper, we propose SimLM (Similarity matching with Language Model pre-training), a simple yet effective pre-training method for dense passage retrieval. It employs a simple bottleneck architecture that learns to compress the passage information into a dense vector through self-supervised pre-training. We use a replaced language modeling objective, which is inspired by ELECTRA (Clark et al., 2020), to improve the sample efficiency and reduce the mismatch of the input distribution between pre-training and fine-tuning. SimLM only requires access to an unlabeled corpus and is more broadly applicable when there are no labeled data or queries. We conduct experiments on several large-scale passage retrieval datasets and show substantial improvements over strong baselines under various settings. Remarkably, SimLM even outperforms multi-vector approaches such as ColBERTv2 (Santhanam et al., 2021) which incurs significantly more storage cost. Our code and model checkpoints are available athttps://github.com/microsoft/unilm/tree/master/simlm."
    }
  },
  {
    "id": "abstract-2023--acl-long--434",
    "result": [
      {
        "value": {
          "start": 812,
          "end": 848,
          "text": "Prefix-Suffix Guided Decoding (PSGD)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--434:E0"
      },
      {
        "value": {
          "start": 987,
          "end": 1006,
          "text": "translation quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--434:E1"
      },
      {
        "value": {
          "start": 1046,
          "end": 1059,
          "text": "time overhead",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--434:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--434:E0",
        "to_id": "abstract-2023--acl-long--434:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--434:E0",
        "to_id": "abstract-2023--acl-long--434:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Machine translation technology has made great progress in recent years, but it cannot guarantee error-free results. Human translators perform post-editing on machine translations to correct errors in the scene of computer aided translation. In favor of expediting the post-editing process, many works have investigated machine translation in interactive modes, in which machines can automatically refine the rest of translations constrained by human’s edits. Translation Suggestion (TS), as an interactive mode to assist human translators, requires machines to generate alternatives for specific incorrect words or phrases selected by human translators. In this paper, we utilize the parameterized objective function of neural machine translation (NMT) and propose a novel constrained decoding algorithm, namely Prefix-Suffix Guided Decoding (PSGD), to deal with the TS problem without additional training. Compared to state-of-the-art lexical-constrained decoding method, PSGD improves translation quality by an average of 10.6 BLEU and reduces time overhead by an average of 63.4% on benchmark datasets. Furthermore, on both the WeTS and the WMT 2022 Translation Suggestion datasets, it is superior over other supervised learning systems trained with TS annotated data."
    }
  },
  {
    "id": "abstract-2023--acl-long--38",
    "result": [
      {
        "value": {
          "start": 788,
          "end": 794,
          "text": "LexSym",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--38:E0"
      }
    ],
    "data": {
      "text": "In tasks like semantic parsing, instruction following, and question answering, standard deep networks fail to generalize compositionally from small datasets. Many existing approaches overcome this limitation with model architectures that enforce a compositional process of sentence interpretation. In this paper, we present a domain-general and model-agnostic formulation of compositionality as a constraint on symmetries of data distributions rather than models. Informally, we prove that whenever a task can be solved by a compositional model, there is a corresponding data augmentation scheme — a procedure for transforming examples into other well-formed examples — that imparts compositional inductive bias on any model trained to solve the same task. We describe a procedure called LexSym that discovers these transformations automatically, then applies them to training data for ordinary neural sequence models. Unlike existing compositional data augmentation procedures, LexSym can be deployed agnostically across text, structured data, and even images. It matches or surpasses state-of-the-art, task-specific models on COGS semantic parsing, SCAN and Alchemy instruction following, and CLEVR-CoGenT visual question answering datasets."
    }
  },
  {
    "id": "abstract-2023--acl-long--400",
    "result": [
      {
        "value": {
          "start": 669,
          "end": 697,
          "text": "scheduled masking approaches",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--400:E0"
      },
      {
        "value": {
          "start": 805,
          "end": 828,
          "text": "pre-training efficiency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--400:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--400:E0",
        "to_id": "abstract-2023--acl-long--400:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Masked Language Modeling (MLM) has been widely used as the denoising objective in pre-training language models (PrLMs). Existing PrLMs commonly adopt a Random-Token Masking strategy where a fixed masking ratio is applied and different contents are masked by an equal probability throughout the entire training. However, the model may receive complicated impact from pre-training status, which changes accordingly as training time goes on. In this paper, we show that such time-invariant MLM settings on masking ratio and masked content are unlikely to deliver an optimal outcome, which motivates us to explore the influence of time-variant MLM settings. We propose two scheduled masking approaches that adaptively tune the masking ratio and masked content in different training stages, which improves the pre-training efficiency and effectiveness verified on the downstream tasks. Our work is a pioneer study on time-variant masking strategy on ratio and content and gives a better understanding of how masking ratio and masked content influence the MLM pre-training."
    }
  },
  {
    "id": "abstract-2023--acl-long--13",
    "result": [
      {
        "value": {
          "start": 169,
          "end": 189,
          "text": "inference efficiency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--13:E0"
      },
      {
        "value": {
          "start": 844,
          "end": 875,
          "text": "long-distance collapse problems",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--13:E1"
      },
      {
        "value": {
          "start": 527,
          "end": 543,
          "text": "pre-trained MLMs",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--13:E2"
      },
      {
        "value": {
          "start": 945,
          "end": 978,
          "text": "performance and inference speedup",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--13:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--13:E2",
        "to_id": "abstract-2023--acl-long--13:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Pre-trained autoregressive (AR) language models such as BART and GPTs have dominated OPen-ended Long Text Generation (Open-LTG).However, the AR nature will decrease the inference efficiency along with the increase of generation length, which hinder their application in Open-LTG.To improve inference efficiency, we alternatively explore the potential of the pre-trained masked language models (MLMs) along with a representative iterative non-autoregressive (NAR) decoding strategy for Open-LTG.Our preliminary study shows that pre-trained MLMs can merely generate short text and will collapse for long text modeling. To enhance the long text generation capability of MLMs, we introduce two simple yet effective strategies for the iterative NAR model: dynamic sliding window attention (DSWA) and linear temperature decay (LTD). It can alleviate long-distance collapse problems and achieve longer text generation with a flexible trade-off between performance and inference speedup. Experiments on the storytelling and multi-paragraph opinionated article writing tasks show that pre-trained MLMs can achieve more than 3×→13×speedup with better performance than strong AR models."
    }
  },
  {
    "id": "abstract-2023--acl-long--550",
    "result": [
      {
        "value": {
          "start": 385,
          "end": 438,
          "text": "constrained keywords-to-sentence generation task (CG)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--550:E0"
      },
      {
        "value": {
          "start": 445,
          "end": 481,
          "text": "Boolean question answering task (QA)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--550:E1"
      }
    ],
    "data": {
      "text": "Large language models (LLMs) have been widely studied for their ability to store and utilize positive knowledge. However, negative knowledge, such as “lions don’t live in the ocean”, is also ubiquitous in the world but rarely mentioned explicitly in text. What do LLMs know about negative knowledge?This work examines the ability of LLMs on negative commonsense knowledge. We design a constrained keywords-to-sentence generation task (CG) and a Boolean question answering task (QA) to probe LLMs.Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions. We term this phenomenon the belief conflict of LLMs.Our further analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict."
    }
  },
  {
    "id": "abstract-2023--acl-industry--40",
    "result": [
      {
        "value": {
          "start": 660,
          "end": 713,
          "text": "billion-scale Korean-language seq2seq language models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-industry--40:E0"
      },
      {
        "value": {
          "start": 885,
          "end": 914,
          "text": "heavy pre-finetuning strategy",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-industry--40:E1"
      },
      {
        "value": {
          "start": 1097,
          "end": 1149,
          "text": "fine-tuning data efficiency in low-resource settings",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-industry--40:E2"
      },
      {
        "from_id": "abstract-2023--acl-industry--40:E1",
        "to_id": "abstract-2023--acl-industry--40:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Pretraining and fine-tuning language models have become the standard practice in industrial natural language processing (NLP), but developing and deploying general-purpose language models without the abundant computation or data resources is a real-world issue faced by smaller organizations or communities whose main focus is languages with less accessible resources (e.g., non-English). This paper explores the sequence-to-sequence (seq2seq) language model architecture as a more practical and compute-efficient alternative to the decoder-oriented approach (e.g., GPT-3), accompanied by novel findings in compute-optimality analyses. We successfully trained billion-scale Korean-language seq2seq language models that strongly outperform other competitive models in Korean benchmarks. Moreover, we demonstrate that such language models can be more efficiently utilized by employing a heavy pre-finetuning strategy, by showcasing a case study on dialog-task adaptation. Our case study shows that adopting language models with more readily available domain-specific unlabeled data greatly improves fine-tuning data efficiency in low-resource settings."
    }
  },
  {
    "id": "abstract-2023--acl-long--35",
    "result": [
      {
        "value": {
          "start": 639,
          "end": 665,
          "text": "Static Model Pruning (SMP)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--35:E0"
      }
    ],
    "data": {
      "text": "To overcome the overparameterized problem in Pre-trained Language Models (PLMs), pruning is widely used as a simple and straightforward compression method by directly removing unimportant weights. Previous first-order methods successfully compress PLMs to extremely high sparsity with little performance drop. These methods, such as movement pruning, use first-order information to prune PLMs while fine-tuning the remaining weights. In this work, we argue fine-tuning is redundant for first-order pruning, since first-order pruning is sufficient to converge PLMs to downstream tasks without fine-tuning. Under this motivation, we propose Static Model Pruning (SMP), which only uses first-order pruning to adapt PLMs to downstream tasks while achieving the target sparsity level. In addition, we also design a new masking function and training objective to further improve SMP. Extensive experiments at various sparsity levels show SMP has significant improvements over first-order and zero-order methods. Unlike previous first-order methods, SMP is also applicable to low sparsity and outperforms zero-order methods. Meanwhile, SMP is more parameter efficient than other methods due to it does not require fine-tuning."
    }
  },
  {
    "id": "abstract-2023--acl-long--706",
    "result": [
      {
        "value": {
          "start": 727,
          "end": 736,
          "text": "DT-Solver",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--706:E0"
      },
      {
        "value": {
          "start": 1290,
          "end": 1302,
          "text": "success rate",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--706:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--706:E0",
        "to_id": "abstract-2023--acl-long--706:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recent advances in neural theorem-proving resort to large language models and tree searches. When proving a theorem, a language model advises single-step actions based on the current proving state and the tree search finds a sequence of correct steps using actions given by the language model. However, prior works often conduct constant computation efforts for each proving state while ignoring that the hard states often need more exploration than easy states. Moreover, they evaluate and guide the proof search solely depending on the current proof state instead of considering the whole proof trajectory as human reasoning does. Here, to accommodate general theorems, we propose a novel Dynamic-Tree Driven Theorem Solver (DT-Solver) by guiding the search procedure with state confidence and proof-level values. Specifically, DT-Solver introduces a dynamic-tree Monte-Carlo search algorithm, which dynamically allocates computing budgets for different state confidences, guided by a new proof-level value function to discover proof states that require substantial exploration. Experiments on two popular theorem-proving datasets, PISA and Mathlib, show significant performance gains by our DT-Solver over the state-of-the-art approaches, with a 6.65% improvement on average in terms of success rate. And especially under low computing resource settings (11.03% improvement on average)."
    }
  },
  {
    "id": "abstract-2023--acl-short--145",
    "result": [
      {
        "value": {
          "start": 412,
          "end": 442,
          "text": "diversity-aware coherence loss",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--145:E0"
      },
      {
        "value": {
          "start": 647,
          "end": 681,
          "text": "performance of neural topic models",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--145:E1"
      },
      {
        "from_id": "abstract-2023--acl-short--145:E0",
        "to_id": "abstract-2023--acl-short--145:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The standard approach for neural topic modeling uses a variational autoencoder (VAE) framework that jointly minimizes the KL divergence between the estimated posterior and prior, in addition to the reconstruction loss. Since neural topic models are trained by recreating individual input documents, they do not explicitly capture the coherence between words on the corpus level. In this work, we propose a novel diversity-aware coherence loss that encourages the model to learn corpus-level coherence scores while maintaining high diversity between topics. Experimental results on multiple datasets show that our method significantly improves the performance of neural topic models without requiring any pretraining or additional parameters."
    }
  },
  {
    "id": "abstract-2023--acl-long--457",
    "result": [
      {
        "value": {
          "start": 383,
          "end": 393,
          "text": "CASE model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--457:E0"
      },
      {
        "value": {
          "start": 766,
          "end": 802,
          "text": "empathetic and informative responses",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--457:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--457:E0",
        "to_id": "abstract-2023--acl-long--457:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Empathetic conversation is psychologically supposed to be the result of conscious alignment and interaction between the cognition and affection of empathy. However, existing empathetic dialogue models usually consider only the affective aspect or treat cognition and affection in isolation, which limits the capability of empathetic response generation. In this work, we propose the CASE model for empathetic dialogue generation. It first builds upon a commonsense cognition graph and an emotional concept graph and then aligns the user’s cognition and affection at both the coarse-grained and fine-grained levels. Through automatic and manual evaluation, we demonstrate that CASE outperforms state-of-the-art baselines of empathetic dialogues and can generate more empathetic and informative responses."
    }
  },
  {
    "id": "abstract-2023--acl-long--590",
    "result": [
      {
        "value": {
          "start": 749,
          "end": 789,
          "text": "abstraction as an optimization objective",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--590:E0"
      },
      {
        "value": {
          "start": 805,
          "end": 834,
          "text": "inducing syntactic categories",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--590:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--590:E0",
        "to_id": "abstract-2023--acl-long--590:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This paper presents an information-theoretical model of syntactic generalization. We study syntactic generalization from the perspective of the capacity to disentangle semantic and structural information, emulating the human capacity to assign a grammaticality judgment to semantically nonsensical sentences. In order to isolate the structure, we propose to represent the probability distribution behind a corpus as the product of the probability of a semantic context and the probability of a structure, the latter being independent of the former. We further elaborate the notion of abstraction as a relaxation of the property of independence. It is based on the measure of structural and contextual information for a given representation. We test abstraction as an optimization objective on the task of inducing syntactic categories from natural language data and show that it significantly outperforms alternative methods. Furthermore, we find that when syntax-unaware optimization objectives succeed in the task, their success is mainly due to an implicit disentanglement process rather than to the model structure. On the other hand, syntactic categories can be deduced in a principled way from the independence between structure and context."
    }
  },
  {
    "id": "abstract-2023--acl-demo--37",
    "result": [
      {
        "value": {
          "start": 305,
          "end": 323,
          "text": "encoder-only model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-demo--37:E0"
      },
      {
        "value": {
          "start": 396,
          "end": 403,
          "text": "quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-demo--37:E1"
      },
      {
        "value": {
          "start": 419,
          "end": 435,
          "text": "easy-to-use tool",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-demo--37:E2"
      },
      {
        "value": {
          "start": 605,
          "end": 613,
          "text": "F1-score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-demo--37:E3"
      },
      {
        "from_id": "abstract-2023--acl-demo--37:E0",
        "to_id": "abstract-2023--acl-demo--37:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-demo--37:E2",
        "to_id": "abstract-2023--acl-demo--37:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The goal of whitespace correction is to fix space errors in arbitrary given text. For example, given the text “whi te space correctio nwithTransf or mers”, produce “whitespace correction with Transformers”. We compare two Transformer-based models, a character-level encoder-decoder model and a byte-level encoder-only model. We find that the encoder-only model is both faster and achieves higher quality. We provide an easy-to-use tool that is over 900 times faster than the previous best tool, with the same high quality. Our tool repairs text at a rate of over 200 kB/s on GPU, with a sequence-averaged F1-score ranging from 87.5% for hard-to-correct text up to 99% for text without any spaces."
    }
  },
  {
    "id": "abstract-2023--acl-long--144",
    "result": [
      {
        "value": {
          "start": 629,
          "end": 635,
          "text": "GetMTL",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--144:E0"
      },
      {
        "value": {
          "start": 234,
          "end": 258,
          "text": "performance of each task",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--144:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--144:E0",
        "to_id": "abstract-2023--acl-long--144:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Multi-task learning (MTL) has emerged as a promising approach for sharing inductive bias across multiple tasks to enable more efficient learning in text classification. However, training all tasks simultaneously often yields degraded performance of each task than learning them independently, since different tasks might conflict with each other. Existing MTL methods for alleviating this issue is to leverage heuristics or gradient-based algorithm to achieve an arbitrary Pareto optimal trade-off among different tasks. In this paper, we present a novel gradient trade-off approach to mitigate the task conflict problem, dubbed GetMTL, which can achieve a specific trade-off among different tasks nearby the main objective of multi-task text classification (MTC), so as to improve the performance of each task simultaneously. The results of extensive experiments on two benchmark datasets back up our theoretical analysis and validate the superiority of our proposed GetMTL."
    }
  },
  {
    "id": "abstract-2023--acl-long--662",
    "result": [
      {
        "value": {
          "start": 506,
          "end": 515,
          "text": "ESCOXLM-R",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--662:E0"
      },
      {
        "value": {
          "start": 1005,
          "end": 1052,
          "text": "state-of-the-art results on 6 out of 9 datasets",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--662:E1"
      },
      {
        "value": {
          "start": 1152,
          "end": 1190,
          "text": "entity-level and surface-level span-F1",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--662:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--662:E0",
        "to_id": "abstract-2023--acl-long--662:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--662:E0",
        "to_id": "abstract-2023--acl-long--662:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The increasing number of benchmarks for Natural Language Processing (NLP) tasks in the computational job market domain highlights the demand for methods that can handle job-related tasks such as skill extraction, skill classification, job title classification, and de-identification. While some approaches have been developed that are specific to the job market domain, there is a lack of generalized, multilingual models and benchmarks for these tasks. In this study, we introduce a language model called ESCOXLM-R, based on XLM-R-large, which uses domain-adaptive pre-training on the European Skills, Competences, Qualifications and Occupations (ESCO) taxonomy, covering 27 languages. The pre-training objectives for ESCOXLM-R include dynamic masked language modeling and a novel additional objective for inducing multilingual taxonomical ESCO relations. We comprehensively evaluate the performance of ESCOXLM-R on 6 sequence labeling and 3 classification tasks in 4 languages and find that it achieves state-of-the-art results on 6 out of 9 datasets. Our analysis reveals that ESCOXLM-R performs better on short spans and outperforms XLM-R-large on entity-level and surface-level span-F1, likely due to ESCO containing short skill and occupation titles, and encoding information on the entity-level."
    }
  },
  {
    "id": "abstract-2023--acl-long--49",
    "result": [
      {
        "value": {
          "start": 326,
          "end": 330,
          "text": "CLPM",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--49:E0"
      },
      {
        "value": {
          "start": 774,
          "end": 800,
          "text": "performance of UNMT models",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--49:E1"
      },
      {
        "value": {
          "start": 902,
          "end": 968,
          "text": "performance of multilingual models on cross-lingual classification",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--49:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--49:E0",
        "to_id": "abstract-2023--acl-long--49:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--49:E0",
        "to_id": "abstract-2023--acl-long--49:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In multilingual pre-training with the objective of MLM (masked language modeling) on multiple monolingual corpora, multilingual models only learn cross-linguality implicitly from isomorphic spaces formed by overlapping different language spaces due to the lack of explicit cross-lingual forward pass. In this work, we present CLPM (Cross-lingual Prototype Masking), a dynamic and token-wise masking scheme, for multilingual pre-training, using a special token[𝒞]xto replace a random tokenxin the input sentence.[𝒞]xis a cross-lingual prototype forxand then forms an explicit cross-lingual forward pass. We instantiate CLPM for the multilingual pre-training phase of UNMT (unsupervised neural machine translation), and experiments show that CLPM can consistently improve the performance of UNMT models on{De, Ro, Ne } ↔ En. Beyond UNMT or bilingual tasks, we show that CLPM can consistently improve the performance of multilingual models on cross-lingual classification."
    }
  },
  {
    "id": "abstract-2023--acl-long--280",
    "result": [
      {
        "value": {
          "start": 815,
          "end": 820,
          "text": "MixDA",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--280:E0"
      },
      {
        "value": {
          "start": 1186,
          "end": 1223,
          "text": "performance on in-domain tasks (GLUE)",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--280:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--280:E0",
        "to_id": "abstract-2023--acl-long--280:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Pre-trained language models (PLMs) demonstrate excellent abilities to understand texts in the generic domain while struggling in a specific domain. Although continued pre-training on a large domain-specific corpus is effective, it is costly to tune all the parameters on the domain. In this paper, we investigate whether we can adapt PLMs both effectively and efficiently by only tuning a few parameters. Specifically, we decouple the feed-forward networks (FFNs) of the Transformer architecture into two parts: the original pre-trained FFNs to maintain the old-domain knowledge and our novel domain-specific adapters to inject domain-specific knowledge in parallel. Then we adopt a mixture-of-adapters gate to fuse the knowledge from different domain adapters dynamically. Our proposed Mixture-of-Domain-Adapters (MixDA) employs a two-stage adapter-tuning strategy that leverages both unlabeled data and labeled data to help the domain adaptation:i) domain-specific adapter on unlabeled data; followed byii) the task-specific adapter on labeled data. MixDA can be seamlessly plugged into the pretraining-finetuning paradigm and our experiments demonstrate that MixDA achieves superior performance on in-domain tasks (GLUE), out-of-domain tasks (ChemProt, RCT, IMDB, Amazon), and knowledge-intensive tasks (KILT).Further analyses demonstrate the reliability, scalability, and efficiency of our method."
    }
  },
  {
    "id": "abstract-2023--acl-long--780",
    "result": [
      {
        "value": {
          "start": 616,
          "end": 627,
          "text": "SymbolicToM",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--780:E0"
      },
      {
        "value": {
          "start": 233,
          "end": 247,
          "text": "theory of mind",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--780:E1"
      },
      {
        "value": {
          "start": 1252,
          "end": 1283,
          "text": "out-of-distribution performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--780:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--780:E0",
        "to_id": "abstract-2023--acl-long--780:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--780:E0",
        "to_id": "abstract-2023--acl-long--780:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Theory of Mind (ToM)—the ability to reason about the mental states of other people—is a key element of our social intelligence. Yet, despite their ever more impressive performance, large-scale neural language models still lack basic theory of mind capabilities out-of-the-box. We posit that simply scaling up models will not imbue them with theory of mind due to the inherently symbolic and implicit nature of the phenomenon, and instead investigate an alternative: can we design a decoding-time algorithm that enhances theory of mind of off-the-shelf neural language models without explicit supervision? We present SymbolicToM, a plug-and-play approach to reason about the belief states of multiple characters in reading comprehension tasks via explicit symbolic representation. More concretely, our approach tracks each entity’s beliefs, their estimation of other entities’ beliefs, and higher-order levels of reasoning, all through graphical representations, allowing for more precise and interpretable reasoning than previous approaches. Empirical results on the well-known ToMi benchmark (Le et al., 2019) demonstrate that SymbolicToM dramatically enhances off-the-shelf neural networks’ theory of mind in a zero-shot setting while showing robust out-of-distribution performance compared to supervised baselines. Our work also reveals spurious patterns in existing theory of mind benchmarks, emphasizing the importance of out-of-distribution evaluation and methods that do not overfit a particular dataset."
    }
  },
  {
    "id": "abstract-2023--acl-industry--26",
    "result": [
      {
        "value": {
          "start": 333,
          "end": 341,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-industry--26:E0"
      },
      {
        "value": {
          "start": 795,
          "end": 809,
          "text": "search latency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-industry--26:E1"
      }
    ],
    "data": {
      "text": "Automatic Speech Recognition (ASR) is essential for any voice-based application. The streaming capability of ASR becomes necessary to provide immediate feedback to the user in applications like Voice Search. LSTM/RNN and CTC based ASR systems are very simple to train and deploy for low latency streaming applications but have lower accuracy when compared to the state-of-the-art models. In this work, we build accurate LSTM, attention and CTC based streaming ASR models for large-scale Hinglish (blend of Hindi and English) Voice Search. We evaluate how various modifications in vanilla LSTM training improve the system’s accuracy while preserving the streaming capabilities. We also discuss a simple integration of end-of-speech (EOS) detection with CTC models, which helps reduce the overall search latency. Our model achieves a word error rate (WER) of 3.69% without EOS and 4.78% with EOS, with ~1300 ms (~46.64%) reduction in latency."
    }
  },
  {
    "id": "abstract-2023--acl-long--827",
    "result": [
      {
        "value": {
          "start": 634,
          "end": 644,
          "text": "StoryTrans",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--827:E0"
      },
      {
        "value": {
          "start": 1291,
          "end": 1328,
          "text": "overall performance of style transfer",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--827:E1"
      },
      {
        "value": {
          "start": 990,
          "end": 1010,
          "text": "content preservation",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--827:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--827:E0",
        "to_id": "abstract-2023--acl-long--827:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--827:E0",
        "to_id": "abstract-2023--acl-long--827:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Non-parallel text style transfer is an important task in natural language generation. However, previous studies concentrate on the token or sentence level, such as sentence sentiment and formality transfer, but neglect long style transfer at the discourse level. Long texts usually involve more complicated author linguistic preferences such as discourse structures than sentences. In this paper, we formulate the task of non-parallel story author-style transfer, which requires transferring an input story into a specified author style while maintaining source semantics. To tackle this problem, we propose a generation model, named StoryTrans, which leverages discourse representations to capture source content information and transfer them to target styles with learnable style embeddings. We use an additional training objective to disentangle stylistic features from the learned discourse representation to prevent the model from degenerating to an auto-encoder. Moreover, to enhance content preservation, we design a mask-and-fill framework to explicitly fuse style-specific keywords of source texts into generation. Furthermore, we constructed new datasets for this task in Chinese and English, respectively. Extensive experiments show that our model outperforms strong baselines in overall performance of style transfer and content preservation."
    }
  },
  {
    "id": "abstract-2023--acl-short--158",
    "result": [
      {
        "value": {
          "start": 554,
          "end": 558,
          "text": "ACTC",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--158:E0"
      },
      {
        "value": {
          "start": 570,
          "end": 593,
          "text": "per-relation thresholds",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--158:E1"
      },
      {
        "value": {
          "start": 1131,
          "end": 1186,
          "text": "average improvement of 4% points over different budgets",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--158:E2"
      },
      {
        "from_id": "abstract-2023--acl-short--158:E0",
        "to_id": "abstract-2023--acl-short--158:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-short--158:E0",
        "to_id": "abstract-2023--acl-short--158:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Self-supervised knowledge-graph completion (KGC) relies on estimating a scoring model over (entity, relation, entity)-tuples, for example, by embedding an initial knowledge graph. Prediction quality can be improved by calibrating the scoring model, typically by adjusting the prediction thresholds using manually annotated examples. In this paper, we attempt for the first time cold-start calibration for KGC, where no annotated examples exist initially for calibration, and only a limited number of tuples can be selected for annotation. Our new method ACTC finds good per-relation thresholds efficiently based on a limited set of annotated tuples. Additionally to a few annotated tuples, ACTC also leverages unlabeled tuples by estimating their correctness with Logistic Regression or Gaussian Process classifiers. We also experiment with different methods for selecting candidate tuples for annotation: density-based and random selection. Experiments with five scoring models and an oracle annotator show an improvement of 7% points when using ACTC in the challenging setting with an annotation budget of only 10 tuples, and an average improvement of 4% points over different budgets."
    }
  },
  {
    "id": "abstract-2023--acl-long--509",
    "result": [
      {
        "value": {
          "start": 937,
          "end": 947,
          "text": "our method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--509:E0"
      }
    ],
    "data": {
      "text": "Fine-tuning has been proven to be a simple and effective technique to transfer the learned knowledge of Pre-trained Language Models (PLMs) to downstream tasks. However, vanilla fine-tuning easily overfits the target data and degrades the generalization ability. Most existing studies attribute it to catastrophic forgetting, and they retain the pre-trained knowledge indiscriminately without identifying what knowledge is transferable. Motivated by this, we frame fine-tuning into a causal graph and discover that the crux of catastrophic forgetting lies in the missing causal effects from the pre-trained data. Based on the causal view, we propose a unified objective for fine-tuning to retrieve the causality back. Intriguingly, the unified objective can be seen as the sum of the vanilla fine-tuning objective, which learns new knowledge from target data, and the causal objective, which preserves old knowledge from PLMs. Therefore, our method is flexible and can mitigate negative transfer while preserving knowledge. Since endowing models with commonsense is a long-standing challenge, we implement our method on commonsense QA with a proposed heuristic estimation to verify its effectiveness. In the experiments, our method outperforms state-of-the-art fine-tuning methods on all six commonsense QA datasets and can be implemented as a plug-in module to inflate the performance of existing QA models."
    }
  },
  {
    "id": "abstract-2023--acl-long--631",
    "result": [
      {
        "value": {
          "start": 802,
          "end": 813,
          "text": "HINT models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--631:E0"
      },
      {
        "value": {
          "start": 789,
          "end": 800,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--631:E1"
      },
      {
        "value": {
          "start": 1067,
          "end": 1080,
          "text": "compute usage",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--631:E2"
      },
      {
        "value": {
          "start": 433,
          "end": 437,
          "text": "HINT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--631:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--631:E0",
        "to_id": "abstract-2023--acl-long--631:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--631:E0",
        "to_id": "abstract-2023--acl-long--631:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recent NLP models have shown the remarkable ability to effectively generalise ‘zero-shot’ to new tasks using only natural language instructions as guidance. However, many of these approaches suffer from high computational costs due to their reliance on concatenating lengthy instructions with every input example, resulting in costly reprocessing of the instruction. To avoid this, we introduce Hypernetworks for INstruction Tuning (HINT), which convert task instructions and examples into parameter-efficient modules inserted into an underlying model using a pretrained text encoder, eliminating the need to include instructions in the model input. The hypernetwork in HINT also produces an encoded instruction, which we concatenate with encoded inputs during decoding to further improve performance. HINT models outperform strong state-of-the-art baselines by over 10% when controlling for compute (measured in FLOPs). By converting instructions into modules, HINT models can effectively disregard the length of instructions and few-shot example inputs in terms of compute usage. As a result, HINT can enhance its performance by up to 25% by incorporating additional few-shot data, while utilizing only up to 5% more compute. This combines the strengths of parameter-efficient fine-tuning and in-context learning."
    }
  },
  {
    "id": "abstract-2023--acl-long--59",
    "result": [
      {
        "value": {
          "start": 984,
          "end": 994,
          "text": "model size",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--59:E0"
      },
      {
        "value": {
          "start": 1103,
          "end": 1122,
          "text": "rationale agreement",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--59:E1"
      },
      {
        "value": {
          "start": 1200,
          "end": 1208,
          "text": "fairness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--59:E2"
      },
      {
        "value": {
          "start": 999,
          "end": 1017,
          "text": "model distillation",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--59:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--59:E0",
        "to_id": "abstract-2023--acl-long--59:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--59:E0",
        "to_id": "abstract-2023--acl-long--59:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--59:E3",
        "to_id": "abstract-2023--acl-long--59:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Explainability methods are used to benchmark the extent to which model predictions align with human rationales i.e., are ‘right for the right reasons’. Previous work has failed to acknowledge, however, that what counts as a rationale is sometimes subjective. This paper presents what we think is a first of its kind, a collection of human rationale annotations augmented with the annotators demographic information. We cover three datasets spanning sentiment analysis and common-sense reasoning, and six demographic groups (balanced across age and ethnicity). Such data enables us to ask both what demographics our predictions align with and whose reasoning patterns our models’ rationales align with. We find systematic inter-group annotator disagreement and show how 16 Transformer-based models align better with rationales provided by certain demographic groups: We find that models are biased towards aligning best with older and/or white annotators. We zoom in on the effects of model size and model distillation, finding –contrary to our expectations– negative correlations between model size and rationale agreement as well as no evidence that either model size or model distillation improves fairness."
    }
  },
  {
    "id": "abstract-2023--acl-long--641",
    "result": [
      {
        "value": {
          "start": 1170,
          "end": 1234,
          "text": "fine-tuning the model on a diverse set of tasks and instructions",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--641:E0"
      },
      {
        "value": {
          "start": 1254,
          "end": 1295,
          "text": "sensitivity to variations in instructions",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--641:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--641:E0",
        "to_id": "abstract-2023--acl-long--641:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Instruction tuning, a new learning paradigm that fine-tunes pre-trained language models on tasks specified through instructions, has shown promising zero-shot performance on various natural language processing tasks. However, it has yet to be explored for vision and multimodal tasks. In this work, we introduce MultiInstruct, the first multimodal instruction tuning benchmark dataset that consists of 62 diverse multimodal tasks in a unified seq-to-seq format covering 10 broad categories. The tasks are derived from 21 existing open-source datasets and each task is equipped with 5 expert-written instructions. We take OFA as the base pre-trained model for multimodal instruction tuning, and to further improve its zero-shot performance, we explore multiple transfer learning strategies to leverage the large-scale Natural Instructions dataset. Experimental results demonstrate strong zero-shot performance on various unseen multimodal tasks and the benefit of transfer learning from a text-only instruction dataset. We also design a new evaluation metric – Sensitivity, to evaluate how sensitive the model is to the variety of instructions. Our results indicate that fine-tuning the model on a diverse set of tasks and instructions leads to a reduced sensitivity to variations in instructions for each task."
    }
  },
  {
    "id": "abstract-2023--acl-long--419",
    "result": [
      {
        "value": {
          "start": 1014,
          "end": 1037,
          "text": "large-scale pretraining",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--419:E0"
      },
      {
        "value": {
          "start": 1055,
          "end": 1083,
          "text": "crosstalk generation quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--419:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--419:E0",
        "to_id": "abstract-2023--acl-long--419:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Language is the principal tool for human communication, in which humor is one of the most attractive parts. Producing natural language like humans using computers, a.k.a, Natural Language Generation (NLG), has been widely used for dialogue systems, chatbots, machine translation, as well as computer-aid creation e.g., idea generations, scriptwriting. However, the humor aspect of natural language is relatively under-investigated, especially in the age of pre-trained language models. In this work, we aim to preliminarily test *whether NLG can generate humor as humans do*. We build a largest dataset consisting of numerous **C**hinese **C**omical **C**rosstalk scripts (called **C**3 in short), which is for a popular Chinese performing art called ‘Xiangsheng’ or ‘相声’ since 1800s.We benchmark various generation approaches including training-from-scratch Seq2seq, fine-tuned middle-scale PLMs, and large-scale PLMs (with and without fine-tuning). Moreover, we also conduct a human assessment, showing that 1) *large-scale pretraining largely improves crosstalk generation quality*; and 2) *even the scripts generated from the best PLM is far from what we expect*. We conclude humor generation could be largely improved using large-scaled PLMs, but it is still in its infancy. The data and benchmarking code are publicly available in [https://github.com/anonNo2/crosstalk-generation](https://github.com/anonNo2/crosstalk-generation)."
    }
  },
  {
    "id": "abstract-2023--acl-long--364",
    "result": [
      {
        "value": {
          "start": 388,
          "end": 392,
          "text": "KiDG",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--364:E0"
      },
      {
        "value": {
          "start": 990,
          "end": 1041,
          "text": "performance compared to retrieval-augmented methods",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--364:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--364:E0",
        "to_id": "abstract-2023--acl-long--364:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Dialogue models are often enriched with extensive external knowledge to provide informative responses through a retrieval-augmented pipeline. Nevertheless, retrieval-augmented approaches rely on finely annotated retrieval training data and knowledge-grounded response generation data, making it costly to transfer. To tackle this challenge, this paper proposed a retrieval-free approach, KiDG, by automatically turning knowledge documents into simulated multi-turn dialogues through a Multi-Document Traversal algorithm. The simulated knowledge-intensive dialogues constructed by KiDG in one domain can be easily used to train and enhance pre-trained dialogue models’ knowledge w.r.t. this domain without costly annotation. We conduct extensive experiments comparing retrieval-augmented models and a variety of retrieval-free models. We found that dialogue models enhanced with data simulated with KiDG largely outperform state-of-the-art retrieval-free methods, and it achieves comparable performance compared to retrieval-augmented methods while being better, and cheaper at domain transfer."
    }
  },
  {
    "id": "abstract-2023--acl-industry--17",
    "result": [
      {
        "value": {
          "start": 387,
          "end": 414,
          "text": "entity contrastive learning",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-industry--17:E0"
      },
      {
        "value": {
          "start": 879,
          "end": 898,
          "text": "overall performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-industry--17:E1"
      },
      {
        "value": {
          "start": 1204,
          "end": 1221,
          "text": "alignment metrics",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-industry--17:E2"
      },
      {
        "value": {
          "start": 1247,
          "end": 1265,
          "text": "embedding clusters",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-industry--17:E3"
      },
      {
        "from_id": "abstract-2023--acl-industry--17:E0",
        "to_id": "abstract-2023--acl-industry--17:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-industry--17:E0",
        "to_id": "abstract-2023--acl-industry--17:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-industry--17:E0",
        "to_id": "abstract-2023--acl-industry--17:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Conversational agents are typically made up of domain (DC) and intent classifiers (IC) that identify the general subject an utterance belongs to and the specific action a user wishes to achieve. In addition, named entity recognition (NER) performs per token labeling to identify specific entities of interest in a spoken utterance. We investigate improving joint IC and NER models using entity contrastive learning that attempts to cluster similar entities together in a learned representation space. We compare a full virtual assistant system trained using entity contrastive learning to a production baseline system that does not use contrastive learning. We present both offline results, using retrospective test sets, as well as live online results from an A/B test that compared the two systems. In both the offline and online settings, entity contrastive training improved overall performance against production baselines. Furthermore, we provide a detailed analysis of learned entity embeddings, including both qualitative analysis via dimensionality-reduced visualizations and quantitative analysis by computing alignment and uniformity metrics. We show that entity contrastive learning improves alignment metrics and produces well-formed embedding clusters in representation space."
    }
  },
  {
    "id": "abstract-2023--acl-long--648",
    "result": [
      {
        "value": {
          "start": 1273,
          "end": 1283,
          "text": "our method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--648:E0"
      },
      {
        "value": {
          "start": 1292,
          "end": 1316,
          "text": "SOTA performance on UFET",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--648:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--648:E0",
        "to_id": "abstract-2023--acl-long--648:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Ultra-fine entity typing (UFET) predicts extremely free-formed types (e.g.,president, politician) of a given entity mention (e.g.,Joe Biden) in context. State-of-the-art (SOTA) methods use the cross-encoder (CE) based architecture. CE concatenates a mention (and its context) with each type and feeds the pair into a pretrained language model (PLM) to score their relevance. It brings deeper interaction between the mention and the type to reach better performance but has to performN(the type set size) forward passes to infer all the types of a single mention. CE is therefore very slow in inference when the type set is large (e.g.,N=10kfor UFET). % Cross-encoder also ignores the correlation between different types.To this end, we propose to perform entity typing in a recall-expand-filter manner. The recall and expansion stages prune the large type set and generateK(typically much smaller thanN) most relevant type candidates for each mention. At the filter stage, we use a novel model called {pasted macro ‘NAME’} to concurrently encode and score all theseKcandidates in only one forward pass to obtain the final type prediction. We investigate different model options for each stage and conduct extensive experiments to compare each option, experiments show that our method reaches SOTA performance on UFET and is thousands of times faster than the CE-based architecture. We also found our method is very effective in fine-grained (130 types) and coarse-grained (9 types) entity typing. Our code is available at {pasted macro ‘CODE’}."
    }
  },
  {
    "id": "abstract-2023--acl-long--534",
    "result": [
      {
        "value": {
          "start": 778,
          "end": 808,
          "text": "weight concatenation operation",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--534:E0"
      }
    ],
    "data": {
      "text": "The attention mechanism is a powerful and effective method utilized in natural language processing. However, it has been observed that this method is insensitive to positional information. Although several studies have attempted to improve positional encoding and investigate the influence of word order perturbation, it remains unclear how positional encoding impacts NLP models from the perspective of word order. In this paper, we aim to shed light on this problem by analyzing the working mechanism of the attention module and investigating the root cause of its inability to encode positional information. Our hypothesis is that the insensitivity can be attributed to the weight sum operation utilized in the attention module. To verify this hypothesis, we propose a novel weight concatenation operation and evaluate its efficacy in neural machine translation tasks. Our enhanced experimental results not only reveal that the proposed operation can effectively encode positional information but also confirm our hypothesis."
    }
  },
  {
    "id": "abstract-2023--acl-short--154",
    "result": [
      {
        "value": {
          "start": 612,
          "end": 627,
          "text": "solve ScoNe-NLI",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--154:E0"
      },
      {
        "value": {
          "start": 734,
          "end": 756,
          "text": "most prompt strategies",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--154:E1"
      },
      {
        "value": {
          "start": 932,
          "end": 950,
          "text": "negation reasoning",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--154:E2"
      }
    ],
    "data": {
      "text": "A number of recent benchmarks seek to assess how well models handle natural language negation. However, these benchmarks lack the controlled example paradigms that would allow us to infer whether a model had truly learned how negation morphemes semantically scope. To fill these analytical gaps, we present the Scoped Negation NLI (ScoNe-NLI) benchmark, which contains contrast sets of six examples with up to two negations where either zero, one, or both negative morphemes affect the NLI label. We use ScoNe-NLI to assess fine-tuning and in-context learning strategies. We find that RoBERTa and DeBERTa models solve ScoNe-NLI after many shot fine-tuning. For in-context learning, we test the latest InstructGPT models and find that most prompt strategies are not successful, including those using step-by-step reasoning. To better understand this result, we extend ScoNe with ScoNe-NLG, a sentence completion test set that embeds negation reasoning in short narratives. Here, InstructGPT is successful, which reveals the model can correctly reason about negation, but struggles to do so on NLI examples outside of its core pretraining regime."
    }
  },
  {
    "id": "abstract-2023--acl-long--97",
    "result": [
      {
        "value": {
          "start": 112,
          "end": 118,
          "text": "QKConv",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--97:E0"
      },
      {
        "value": {
          "start": 919,
          "end": 961,
          "text": "performance compared to supervised methods",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--97:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--97:E0",
        "to_id": "abstract-2023--acl-long--97:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "In this paper, we propose an unsupervised query enhanced approach for knowledge-intensive conversations, namely QKConv. There are three modules in QKConv: a query generator, an off-the-shelf knowledge selector, and a response generator. QKConv is optimized through joint training, which produces the response by exploring multiple candidate queries and leveraging corresponding selected knowledge. The joint training solely relies on the dialogue context and target response, getting exempt from extra query annotations or knowledge provenances. To evaluate the effectiveness of the proposed QKConv, we conduct experiments on three representative knowledge-intensive conversation datasets: conversational question-answering, task-oriented dialogue, and knowledge-grounded conversation. Experimental results reveal that QKConv performs better than all unsupervised methods across three datasets and achieves competitive performance compared to supervised methods."
    }
  },
  {
    "id": "abstract-2023--acl-long--157",
    "result": [
      {
        "value": {
          "start": 902,
          "end": 946,
          "text": "Learning to Substitute Span (L2S2) framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--157:E0"
      }
    ],
    "data": {
      "text": "Despite the rising prevalence of neural sequence models, recent empirical evidences suggest their deficiency in compositional generalization. One of the current de-facto solutions to this problem is compositional data augmentation, aiming to incur additional compositional inductive bias. Nonetheless, the improvement offered by existing handcrafted augmentation strategies is limited when successful systematic generalization of neural sequence models requires multi-grained compositional bias (i.e., not limited to either lexical or structural biases only) or differentiation of training sequences in an imbalanced difficulty distribution. To address the two challenges, we first propose a novel compositional augmentation strategy dubbed Span Substitution (SpanSub) that enables multi-grained composition of substantial substructures in the whole training set. Over and above that, we introduce the Learning to Substitute Span (L2S2) framework which empowers the learning of span substitution probabilities in SpanSub in an end-to-end manner by maximizing the loss of neural sequence models, so as to outweigh those challenging compositions with elusive concepts and novel surroundings. Our empirical results on three standard compositional generalization benchmarks, including SCAN, COGS and GeoQuery (with an improvement of at most 66.5%, 10.3%, 1.2%, respectively), demonstrate the superiority of SpanSub, L2S2 and their combination."
    }
  },
  {
    "id": "abstract-2023--acl-short--125",
    "result": [
      {
        "value": {
          "start": 706,
          "end": 752,
          "text": "simple ways to construct random demonstrations",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--125:E0"
      }
    ],
    "data": {
      "text": "Demonstration-based learning has shown impressive performance in exploiting pretrained language models under few-shot learning settings. It is interesting to see that demonstrations, even those composed of random tokens, can still improve performance. In this paper, we build a Structural Causal Model (SCM) to understand demonstration-based learning from causal perspectives and interpret random demonstrations as interventions on the demonstration variable within the causal model. We investigate the causal effects and find that the concurrence of specific words in the demonstration will induce bias, while randomly sampled tokens in the demonstration do not. Based on this finding, we further propose simple ways to construct random demonstrations, which even outperform hand-crafted, meaningful demonstrations on public sequence labeling benchmarks."
    }
  },
  {
    "id": "abstract-2023--acl-long--312",
    "result": [
      {
        "value": {
          "start": 721,
          "end": 730,
          "text": "IncPrompt",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--312:E0"
      },
      {
        "value": {
          "start": 1014,
          "end": 1050,
          "text": "F1 improvement in temporal relations",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--312:E1"
      },
      {
        "value": {
          "start": 1061,
          "end": 1101,
          "text": "F1 improvement in hierarchical relations",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--312:E2"
      },
      {
        "value": {
          "start": 955,
          "end": 964,
          "text": "IncSchema",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--312:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--312:E0",
        "to_id": "abstract-2023--acl-long--312:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--312:E0",
        "to_id": "abstract-2023--acl-long--312:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Event schemas are a form of world knowledge about the typical progression of events. Recent methods for event schema induction use information extraction systems to construct a large number of event graph instances from documents, and then learn to generalize the schema from such instances. In contrast, we propose to treat event schemas as a form of commonsense knowledge that can be derived from large language models (LLMs). This new paradigm greatly simplifies the schema induction process and allows us to handle both hierarchical relations and temporal relations between events in a straightforward way. Since event schemas have complex graph structures, we design an incremental prompting and verification method IncPrompt to break down the construction of a complex event graph into three stages: event skeleton construction, event expansion, and event-event relation verification. Compared to directly using LLMs to generate a linearized graph, IncSchema can generate large and complex schemas with 7.2% F1 improvement in temporal relations and 31.0% F1 improvement in hierarchical relations. In addition, compared to the previous state-of-the-art closed-domain schema induction model, human assessors were able to cover ~10% more events when translating the schemas into coherent stories and rated our schemas 1.3 points higher (on a 5-point scale) in terms of readability."
    }
  },
  {
    "id": "abstract-2023--acl-long--725",
    "result": [
      {
        "value": {
          "start": 504,
          "end": 508,
          "text": "BITE",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--725:E0"
      },
      {
        "value": {
          "start": 471,
          "end": 490,
          "text": "attack success rate",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--725:E1"
      },
      {
        "value": {
          "start": 1189,
          "end": 1195,
          "text": "DeBITE",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--725:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--725:E0",
        "to_id": "abstract-2023--acl-long--725:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Backdoor attacks have become an emerging threat to NLP systems. By providing poisoned training data, the adversary can embed a “backdoor” into the victim model, which allows input instances satisfying certain textual patterns (e.g., containing a keyword) to be predicted as a target label of the adversary’s choice. In this paper, we demonstrate that it is possible to design a backdoor attack that is both stealthy (i.e., hard to notice) and effective (i.e., has a high attack success rate). We propose BITE, a backdoor attack that poisons the training data to establish strong correlations between the target label and a set of “trigger words”. These trigger words are iteratively identified and injected into the target-label instances through natural word-level perturbations. The poisoned training data instruct the victim model to predict the target label on inputs containing trigger words, forming the backdoor. Experiments on four text classification datasets show that our proposed attack is significantly more effective than baseline methods while maintaining decent stealthiness, raising alarm on the usage of untrusted training data. We further propose a defense method named DeBITE based on potential trigger word removal, which outperforms existing methods in defending against BITE and generalizes well to handling other backdoor attacks."
    }
  },
  {
    "id": "abstract-2023--acl-long--772",
    "result": [
      {
        "value": {
          "start": 358,
          "end": 378,
          "text": "zero-shot adaptation",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--772:E0"
      },
      {
        "value": {
          "start": 437,
          "end": 494,
          "text": "self-training entailment-based models with unlabeled data",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--772:E1"
      },
      {
        "value": {
          "start": 525,
          "end": 567,
          "text": "adaptation performance on downstream tasks",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--772:E2"
      },
      {
        "value": {
          "start": 678,
          "end": 701,
          "text": "pseudo-labeling quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--772:E3"
      },
      {
        "value": {
          "start": 945,
          "end": 973,
          "text": "robust self-training results",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--772:E4"
      },
      {
        "value": {
          "start": 995,
          "end": 1025,
          "text": "self-trained entailment models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--772:E5"
      },
      {
        "from_id": "abstract-2023--acl-long--772:E1",
        "to_id": "abstract-2023--acl-long--772:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Entailment has been recognized as an important metric for evaluating natural language understanding (NLU) models, and recent studies have found that entailment pretraining benefits weakly supervised fine-tuning. In this work, we design a prompting strategy that formulates a number of different NLU tasks as contextual entailment. This approach improves the zero-shot adaptation of pretrained entailment models. Secondly, we notice that self-training entailment-based models with unlabeled data can significantly improve the adaptation performance on downstream tasks. To achieve more stable improvement, we propose the Simple Pseudo-Label Editing (SimPLE) algorithm for better pseudo-labeling quality in self-training. We also found that both pretrained entailment-based models and the self-trained models are robust against adversarial evaluation data. Experiments on binary and multi-class classification tasks show that SimPLE leads to more robust self-training results, indicating that the self-trained entailment models are more efficient and trustworthy than large language models on language understanding tasks."
    }
  },
  {
    "id": "abstract-2023--acl-long--580",
    "result": [
      {
        "value": {
          "start": 627,
          "end": 650,
          "text": "contrast between layers",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--580:E0"
      },
      {
        "value": {
          "start": 804,
          "end": 830,
          "text": "quality of generated texts",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--580:E1"
      },
      {
        "value": {
          "start": 871,
          "end": 921,
          "text": "contrasting between model layers at inference time",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--580:E2"
      },
      {
        "value": {
          "start": 964,
          "end": 1010,
          "text": "aspects of general language model capabilities",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--580:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--580:E0",
        "to_id": "abstract-2023--acl-long--580:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--580:E2",
        "to_id": "abstract-2023--acl-long--580:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Applying language models to natural language processing tasks typically relies on the representations in the final model layer, as intermediate hidden layer representations are presumed to be less informative. In this work, we argue that due to the gradual improvement across model layers, additional information can be gleaned from the contrast between higher and lower layers during inference. Specifically, in choosing between the probable next token predictions of a generative model, the predictions of lower layers can be used to highlight which candidates are best avoided. We propose a novel approach that utilizes the contrast between layers to improve text generation outputs, and show that it mitigates degenerative behaviors of the model in open-ended generation, significantly improving the quality of generated texts. Furthermore, our results indicate that contrasting between model layers at inference time can yield substantial benefits to certain aspects of general language model capabilities, more effectively extracting knowledge during inference from a given set of model parameters."
    }
  },
  {
    "id": "abstract-2023--acl-long--17",
    "result": [
      {
        "value": {
          "start": 106,
          "end": 133,
          "text": "retrieval-augmented methods",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--17:E0"
      },
      {
        "value": {
          "start": 641,
          "end": 659,
          "text": "document-level EAE",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--17:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--17:E0",
        "to_id": "abstract-2023--acl-long--17:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recent studies have shown the effectiveness of retrieval augmentation in many generative NLP tasks. These retrieval-augmented methods allow models to explicitly acquire prior external knowledge in a non-parametric manner and regard the retrieved reference instances as cues to augment text generation. These methods use similarity-based retrieval, which is based on a simple hypothesis: the more the retrieved demonstration resembles the original input, the more likely the demonstration label resembles the input label. However, due to the complexity of event labels and sparsity of event arguments, this hypothesis does not always hold in document-level EAE. This raises an interesting question: How do we design the retrieval strategy for document-level EAE? We investigate various retrieval settings from the input and label distribution views in this paper. We further augment document-level EAE with pseudo demonstrations sampled from event semantic regions that can cover adequate alternatives in the same context and event schema. Through extensive experiments on RAMS and WikiEvents, we demonstrate the validity of our newly introduced retrieval-augmented methods and analyze why they work."
    }
  },
  {
    "id": "abstract-2023--acl-long--510",
    "result": [
      {
        "value": {
          "start": 985,
          "end": 1018,
          "text": "translation-enhanced mTTI systems",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--510:E0"
      },
      {
        "value": {
          "start": 858,
          "end": 874,
          "text": "mTTI performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--510:E1"
      },
      {
        "value": {
          "start": 670,
          "end": 675,
          "text": "EnsAd",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--510:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--510:E0",
        "to_id": "abstract-2023--acl-long--510:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Research on text-to-image generation (TTI) still predominantly focuses on the English language due to the lack of annotated image-caption data in other languages; in the long run, this might widen inequitable access to TTI technology. In this work, we thus investigate multilingual TTI (termed mTTI) and the current potential of neural machine translation (NMT) to bootstrap mTTI systems. We provide two key contributions. 1) Relying on a multilingual multi-modal encoder, we provide a systematic empirical study of standard methods used in cross-lingual NLP when applied to mTTI: Translate Train, Translate Test, and Zero-Shot Transfer. 2) We propose Ensemble Adapter (EnsAd), a novel parameter-efficient approach that learns to weigh and consolidate the multilingual text knowledge within the mTTI framework, mitigating the language gap and thus improving mTTI performance. Our evaluations on standard mTTI datasets COCO-CN, Multi30K Task2, and LAION-5B demonstrate the potential of translation-enhanced mTTI systems and also validate the benefits of the proposed EnsAd which derives consistent gains across all datasets. Further investigations on model variants, ablation studies, and qualitative analyses provide additional insights on the inner workings of the proposed mTTI approaches."
    }
  },
  {
    "id": "abstract-2023--acl-long--677",
    "result": [
      {
        "value": {
          "start": 99,
          "end": 110,
          "text": "WhitenedCSE",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--677:E0"
      },
      {
        "value": {
          "start": 1626,
          "end": 1659,
          "text": "Spearman correlation on STS tasks",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--677:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--677:E0",
        "to_id": "abstract-2023--acl-long--677:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This paper presents a whitening-based contrastive learning method for sentence embedding learning (WhitenedCSE), which combines contrastive learning with a novel shuffled group whitening. Generally, contrastive learning pulls distortions of a single sample (i.e., positive samples) close and push negative samples far away, correspondingly facilitating the alignment and uniformity in the feature space. A popular alternative to the “pushing” operation is whitening the feature space, which scatters all the samples for uniformity. Since the whitening and the contrastive learning have large redundancy w.r.t. the uniformity, they are usually used separately and do not easily work together. For the first time, this paper integrates whitening into the contrastive learning scheme and facilitates two benefits. 1) Better uniformity. We find that these two approaches are not totally redundant but actually have some complementarity due to different uniformity mechanism. 2) Better alignment. We randomly divide the feature into multiple groups along the channel axis and perform whitening independently within each group. By shuffling the group division, we derive multiple distortions of a single sample and thus increase the positive sample diversity. Consequently, using multiple positive samples with enhanced diversity further improves contrastive learning due to better alignment. Extensive experiments on seven semantic textual similarity tasks show our method achieves consistent improvement over the contrastive learning baseline and sets new states of the art, e.g., 78.78% (+2.53% based on BERT{pasted macro ‘BA’}) Spearman correlation on STS tasks."
    }
  },
  {
    "id": "abstract-2023--acl-long--598",
    "result": [
      {
        "value": {
          "start": 139,
          "end": 174,
          "text": "supplementary structural constraint",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--598:E0"
      },
      {
        "value": {
          "start": 61,
          "end": 76,
          "text": "time complexity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--598:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--598:E0",
        "to_id": "abstract-2023--acl-long--598:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Span-based nested named-entity recognition (NER) has a cubic-time complexity using avariant of the CYK algorithm. We show that by adding a supplementary structural constraint on the search space, nested NER has a quadratic-time complexity, that is the same asymptotic complexity than the non-nested case. The proposed algorithm covers a large part of three standard English benchmarks and delivers comparable experimental results."
    }
  },
  {
    "id": "abstract-2023--acl-long--482",
    "result": [
      {
        "value": {
          "start": 903,
          "end": 946,
          "text": "Summary Chain-of-Thought (SumCoT) technique",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--482:E0"
      },
      {
        "value": {
          "start": 1262,
          "end": 1269,
          "text": "ROUGE-L",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--482:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--482:E0",
        "to_id": "abstract-2023--acl-long--482:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Automatic summarization generates concise summaries that contain key ideas of source documents. As the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be noisy, mainly in terms of factual hallucination and information redundancy. To address this challenge, we first annotate new expert-writing Element-aware test sets following the “Lasswell Communication Model” proposed by Lasswell, allowing reference summaries to focus on more fine-grained news elements objectively and comprehensively. Utilizing the new test sets, we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs’ zero-shot summaries in prior work. Further, we propose a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with the human writing mindset. Experimental results show our method outperforms state-of-the-art fine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two datasets, respectively. Dataset and code are publicly available athttps://github.com/Alsace08/SumCoT."
    }
  },
  {
    "id": "abstract-2023--acl-short--148",
    "result": [
      {
        "value": {
          "start": 613,
          "end": 620,
          "text": "ACK-DEF",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--148:E0"
      }
    ],
    "data": {
      "text": "Existing knowledge-grounded open-domain dialogue generation models often face the hallucination problem, i.e. the dialogue generative model will persist in an inappropriate knowledge and generate responses that inconsistent with the facts. We argue that this problem mainly stems from the polarized optimization objectives and weak knowledge generation ability. To mitigate the hallucination, we take inspiration from human communicating that people will replay euphemistic responses for the unclear or unrecognizable knowledge, and propose an Augmentative and Contrastive Knowledge Dialogue Expansion Framework (ACK-DEF). ACK-DEF constructs the augmentative and contrastive knowledge dialogue samples, which consist of the knowledge of different degrees of errors and the response of manual design, to expand the original training set and smooth the polarized optimization objective that enables models to generate ground-truth with or without gold knowledge. Not only the knowledge, ACK-DEF also provides the tactful responses of manual design corresponding to the incomplete correct knowledge. Experimental results on the Wikipedia of Wizard dataset show that employing the ACK-DEF is effective to alleviate the hallucination problem."
    }
  },
  {
    "id": "abstract-2023--acl-long--882",
    "result": [
      {
        "value": {
          "start": 395,
          "end": 477,
          "text": "maximizing alignment between texts and a composition of their phrasal constituents",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--882:E0"
      },
      {
        "value": {
          "start": 612,
          "end": 645,
          "text": "semantic textual similarity tasks",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--882:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--882:E0",
        "to_id": "abstract-2023--acl-long--882:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Vector representations of natural language are ubiquitous in search applications. Recently, various methods based on contrastive learning have been proposed to learn textual representations from unlabelled data; by maximizing alignment between minimally-perturbed embeddings of the same text, and encouraging a uniform distribution of embeddings across a broader corpus. Differently, we propose maximizing alignment between texts and a composition of their phrasal constituents. We consider several realizations of this objective and elaborate the impact on representations in each case. Experimental results on semantic textual similarity tasks show improvements over baselines that are comparable with state-of-the-art approaches. Moreover, this work is the first to do so without incurring costs in auxiliary training objectives or additional network parameters."
    }
  },
  {
    "id": "abstract-2023--acl-long--658",
    "result": [
      {
        "value": {
          "start": 396,
          "end": 401,
          "text": "CoNAL",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--658:E0"
      },
      {
        "value": {
          "start": 1032,
          "end": 1081,
          "text": "accuracy under the accuracy-coverage curve (AUAC)",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--658:E1"
      },
      {
        "value": {
          "start": 1091,
          "end": 1096,
          "text": "AUROC",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--658:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--658:E0",
        "to_id": "abstract-2023--acl-long--658:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--658:E0",
        "to_id": "abstract-2023--acl-long--658:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In many task settings, text classification models are likely to encounter examples from novel classes on which they cannot predict correctly. Selective prediction, in which models abstain on low-confidence examples, provides a possible solution, but existing models are often overly confident on unseen classes. To remedy this overconfidence, we introduce Contrastive Novelty-Augmented Learning (CoNAL), a two-step method that generates OOD examples representative of novel classes, then trains to decrease confidence on them. First, we generate OOD examples by prompting a large language model twice: we prompt it to enumerate relevant novel classes, then generate examples from each novel class matching the task format. Second, we train a classifier with a novel contrastive objective that encourages lower confidence on generated OOD examples than training examples. When trained with CoNAL, classifiers improve in their ability to detect and abstain on novel class examples over prior methods by an average of 2.3% in terms of accuracy under the accuracy-coverage curve (AUAC) and 5.5% AUROC across 4 NLP datasets, with no cost to in-distribution accuracy."
    }
  },
  {
    "id": "abstract-2023--acl-long--513",
    "result": [
      {
        "value": {
          "start": 575,
          "end": 614,
          "text": "hierarchical lyric generation framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--513:E0"
      },
      {
        "value": {
          "start": 1407,
          "end": 1434,
          "text": "overall quality improvement",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--513:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--513:E0",
        "to_id": "abstract-2023--acl-long--513:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Automatic melody-to-lyric generation is a task in which song lyrics are generated to go with a given melody. It is of significant practical interest and more challenging than unconstrained lyric generation as the music imposes additional constraints onto the lyrics. The training data is limited as most songs are copyrighted, resulting in models that underfit the complicated cross-modal relationship between melody and lyrics. In this work, we propose a method for generating high-quality lyrics without training on any aligned melody-lyric data. Specifically, we design a hierarchical lyric generation framework that first generates a song outline and second the complete lyrics. The framework enables disentanglement of training (based purely on text) from inference (melody-guided text generation) to circumvent the shortage of parallel data. We leverage the segmentation and rhythm alignment between melody and lyrics to compile the given melody into decoding constraints as guidance during inference. The two-step hierarchical design also enables content control via the lyric outline, a much-desired feature for democratizing collaborative song creation. Experimental results show that our model can generate high-quality lyrics that are more on-topic, singable, intelligible, and coherent than strong baselines, for example SongMASS, a SOTA model trained on a parallel dataset, with a 24% relative overall quality improvement based on human ratings. Our code is available athttps://github.com/amazon-science/unsupervised-melody-to-lyrics-generation."
    }
  },
  {
    "id": "abstract-2023--acl-long--480",
    "result": [
      {
        "value": {
          "start": 534,
          "end": 574,
          "text": "dual class knowledge propagation network",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--480:E0"
      }
    ],
    "data": {
      "text": "Multi-label intent detection aims to assign multiple labels to utterances and attracts increasing attention as a practical task in task-oriented dialogue systems. As dialogue domains change rapidly and new intents emerge fast, the lack of annotated data motivates multi-label few-shot intent detection. However, previous studies are confused by the identical representation of the utterance with multiple labels and overlook the intrinsic intra-class and inter-class interactions. To address these two limitations, we propose a novel dual class knowledge propagation network in this paper. In order to learn well-separated representations for utterances with multiple intents, we first introduce a label-semantic augmentation module incorporating class name information. For better consideration of the inherent intra-class and inter-class relations, an instance-level and a class-level graph neural network are constructed, which not only propagate label information but also propagate feature structure. And we use a simple yet effective method to predict the intent count of each utterance. Extensive experimental results on two multi-label intent datasets have demonstrated that our proposed method outperforms strong baselines by a large margin."
    }
  },
  {
    "id": "abstract-2023--acl-long--231",
    "result": [
      {
        "value": {
          "start": 325,
          "end": 351,
          "text": "counterfactual multihop QA",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--231:E0"
      },
      {
        "value": {
          "start": 1133,
          "end": 1144,
          "text": "Supps score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--231:E1"
      },
      {
        "value": {
          "start": 1041,
          "end": 1072,
          "text": "reducing disconnected reasoning",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--231:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--231:E0",
        "to_id": "abstract-2023--acl-long--231:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--231:E0",
        "to_id": "abstract-2023--acl-long--231:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Multi-hop QA requires reasoning over multiple supporting facts to answer the question. However, the existing QA models always rely on shortcuts, e.g., providing the true answer by only one fact, rather than multi-hop reasoning, which is referred as disconnected reasoning problem. To alleviate this issue, we propose a novel counterfactual multihop QA, a causal-effect approach that enables to reduce the disconnected reasoning. It builds upon explicitly modeling of causality: 1) the direct causal effects of disconnected reasoning and 2) the causal effect of true multi-hop reasoning from the total causal effect. With the causal graph, a counterfactual inference is proposed to disentangle the disconnected reasoning from the total causal effect, which provides us a new perspective and technology to learn a QA model that exploits the true multi-hop reasoning instead of shortcuts. Extensive experiments have been conducted on the benchmark HotpotQA dataset, which demonstrate that the proposed method can achieve notable improvement on reducing disconnected reasoning. For example, our method achieves 5.8% higher points of its Supps score on HotpotQA through true multihop reasoning. The code is available athttps://github.com/guowzh/CFMQA."
    }
  },
  {
    "id": "abstract-2023--acl-long--360",
    "result": [
      {
        "value": {
          "start": 472,
          "end": 481,
          "text": "FutureTOD",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--360:E0"
      },
      {
        "value": {
          "start": 845,
          "end": 859,
          "text": "generalization",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--360:E1"
      },
      {
        "value": {
          "start": 861,
          "end": 871,
          "text": "robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--360:E2"
      },
      {
        "value": {
          "start": 877,
          "end": 938,
          "text": "learning discriminative dialogue representations capabilities",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--360:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--360:E0",
        "to_id": "abstract-2023--acl-long--360:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--360:E0",
        "to_id": "abstract-2023--acl-long--360:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--360:E0",
        "to_id": "abstract-2023--acl-long--360:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Pre-trained language models based on general text enable huge success in the NLP scenario. But the intrinsical difference of linguistic patterns between general text and task-oriented dialogues makes existing pre-trained language models less useful in practice. Current dialogue pre-training methods rely on a contrastive framework and face the challenges of both selecting true positives and hard negatives. In this paper, we propose a novel dialogue pre-training model, FutureTOD, which distills future knowledge to the representation of the previous dialogue context using a self-training framework. Our intuition is that a good dialogue representation both learns local context information and predicts future information. Extensive experiments on diverse downstream dialogue tasks demonstrate the effectiveness of our model, especially the generalization, robustness, and learning discriminative dialogue representations capabilities."
    }
  },
  {
    "id": "abstract-2023--acl-long--759",
    "result": [
      {
        "value": {
          "start": 934,
          "end": 949,
          "text": "our methodology",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--759:E0"
      },
      {
        "value": {
          "start": 632,
          "end": 652,
          "text": "mitigate social bias",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--759:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--759:E0",
        "to_id": "abstract-2023--acl-long--759:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Large pre-trained language models are acknowledged to carry social bias towards different demographics, which can further amplify existing stereotypes in our society and cause even more harm. Text-to-SQL is an important task, models of which are mainly adopted by administrative industries, where unfair decisions may lead to catastrophic consequences. However, existing Text-to-SQL models are trained on clean, neutral datasets, such as Spider and WikiSQL. This, to some extent, cover up social bias in models under ideal conditions, which nevertheless may emerge in real application scenarios. In this work, we aim to uncover and mitigate social bias in Text-to-SQL models. We summarize the categories of social bias that may occur in structural data for Text-to-SQL models. We build test benchmarks and reveal that models with similar task accuracy can contain social bias at very different rates. We show how to take advantage of our methodology to assess and mitigate social bias in the downstream Text-to-SQL task."
    }
  },
  {
    "id": "abstract-2023--acl-long--539",
    "result": [
      {
        "value": {
          "start": 572,
          "end": 582,
          "text": "DecompEval",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--539:E0"
      },
      {
        "value": {
          "start": 1198,
          "end": 1226,
          "text": "state-of-the-art performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--539:E1"
      },
      {
        "value": {
          "start": 1334,
          "end": 1385,
          "text": "dimension-level / task-level generalization ability",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--539:E2"
      },
      {
        "value": {
          "start": 122,
          "end": 138,
          "text": "interpretability",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--539:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--539:E0",
        "to_id": "abstract-2023--acl-long--539:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--539:E0",
        "to_id": "abstract-2023--acl-long--539:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--539:E0",
        "to_id": "abstract-2023--acl-long--539:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Existing evaluation metrics for natural language generation (NLG) tasks face the challenges on generalization ability and interpretability. Specifically, most of the well-performed metrics are required to train on evaluation datasets of specific NLG tasks and evaluation dimensions, which may cause over-fitting to task-specific datasets. Furthermore, existing metrics only provide an evaluation score for each dimension without revealing the evidence to interpret how this score is obtained. To deal with these challenges, we propose a simple yet effective metric called DecompEval. This metric formulates NLG evaluation as an instruction-style question answering task and utilizes instruction-tuned pre-trained language models (PLMs) without training on evaluation datasets, aiming to enhance the generalization ability. To make the evaluation process more interpretable, we decompose our devised instruction-style question about the quality of generated texts into the subquestions that measure the quality of each sentence. The subquestions with their answers generated by PLMs are then recomposed as evidence to obtain the evaluation result. Experimental results show that DecompEval achieves state-of-the-art performance in untrained metrics for evaluating text summarization and dialogue generation, which also exhibits strong dimension-level / task-level generalization ability and interpretability."
    }
  },
  {
    "id": "abstract-2023--acl-long--565",
    "result": [
      {
        "value": {
          "start": 990,
          "end": 1036,
          "text": "Attribute-Discriminative Language Model (ADLM)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--565:E0"
      },
      {
        "value": {
          "start": 1155,
          "end": 1166,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--565:E1"
      },
      {
        "value": {
          "start": 1171,
          "end": 1181,
          "text": "efficiency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--565:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--565:E0",
        "to_id": "abstract-2023--acl-long--565:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--565:E0",
        "to_id": "abstract-2023--acl-long--565:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Transformer-based Language Models (LMs) have achieved impressive results on natural language understanding tasks, but they can also generate toxic text such as insults, threats, and profanity, limiting their real-world applications. To overcome this issue, a few text generation approaches aim to detoxify toxic texts using additional LMs or perturbations. However, previous methods require excessive memory, computations, and time which are serious bottlenecks in their real-world application. To address such limitations, we propose an effective yet efficient method for language detoxification using an attribute-discriminative latent space. Specifically, we project the latent space of an original Transformer LM onto a discriminative latent space that well-separates texts by their attributes using a projection block and an attribute discriminator. This allows the LM to control the text generation to be non-toxic with minimal memory and computation overhead. We validate our model, Attribute-Discriminative Language Model (ADLM) on detoxified language and dialogue generation tasks, on which our method significantly outperforms baselines both in performance and efficiency."
    }
  },
  {
    "id": "abstract-2023--acl-long--94",
    "result": [
      {
        "value": {
          "start": 885,
          "end": 926,
          "text": "fine-grained attention alignment approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--94:E0"
      },
      {
        "value": {
          "start": 1327,
          "end": 1340,
          "text": "effectiveness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--94:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--94:E0",
        "to_id": "abstract-2023--acl-long--94:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Document ranking aims at sorting a collection of documents with their relevance to a query. Contemporary methods explore more efficient transformers or divide long documents into passages to handle the long input. However, intensive query-irrelevant content may lead to harmful distraction and high query latency. Some recent works further propose cascade document ranking models that extract relevant passages with an efficient selector before ranking, however, their selection and ranking modules are almost independently optimized and deployed, leading to selecting error reinforcement and sub-optimal performance. In fact, the document ranker can provide fine-grained supervision to make the selector more generalizable and compatible, and the selector built upon a different structure can offer a distinct perspective to assist in document ranking. Inspired by this, we propose a fine-grained attention alignment approach to jointly optimize a cascade document ranking model. Specifically, we utilize the attention activations over the passages from the ranker as fine-grained attention feedback to optimize the selector. Meanwhile, we fuse the relevance scores from the passage selector into the ranker to assist in calculating the cooperative matching representation. Experiments on MS MARCO and TREC DL demonstrate the effectiveness of our method."
    }
  },
  {
    "id": "abstract-2023--acl-long--155",
    "result": [
      {
        "value": {
          "start": 346,
          "end": 363,
          "text": "defense framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--155:E0"
      },
      {
        "value": {
          "start": 157,
          "end": 187,
          "text": "robustness of the victim model",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--155:E1"
      },
      {
        "value": {
          "start": 997,
          "end": 1045,
          "text": "defending against word-level adversarial attacks",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--155:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--155:E0",
        "to_id": "abstract-2023--acl-long--155:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--155:E0",
        "to_id": "abstract-2023--acl-long--155:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Adversarial attacks on deep neural networks keep raising security concerns in natural language processing research. Existing defenses focus on improving the robustness of the victim model in the training stage. However, they often neglect to proactively mitigate adversarial attacks during inference. Towards this overlooked aspect, we propose a defense framework that aims to mitigate attacks by confusing attackers and correcting adversarial contexts that are caused by malicious perturbations. Our framework comprises three components: (1) a synonym-based transformation to randomly corrupt adversarial contexts in the word level, (2) a developed BERT defender to correct abnormal contexts in the representation level, and (3) a simple detection method to filter out adversarial examples, any of which can be flexibly combined. Additionally, our framework helps improve the robustness of the victim model during training. Extensive experiments demonstrate the effectiveness of our framework in defending against word-level adversarial attacks."
    }
  },
  {
    "id": "abstract-2023--acl-long--148",
    "result": [
      {
        "value": {
          "start": 262,
          "end": 287,
          "text": "representation capability",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--148:E0"
      },
      {
        "value": {
          "start": 1432,
          "end": 1447,
          "text": "transferability",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--148:E1"
      },
      {
        "value": {
          "start": 1464,
          "end": 1486,
          "text": "retrieval performances",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--148:E2"
      }
    ],
    "data": {
      "text": "To better support information retrieval tasks such as web search and open-domain question answering, growing effort is made to develop retrieval-oriented language models, e.g., RetroMAE and many others. Most of the existing works focus on improving the semantic representation capability for the contextualized embedding of the [CLS] token. However, recent study shows that the ordinary tokens besides [CLS] may provide extra information, which help to produce a better representation effect. As such, it’s necessary to extend the current methods where all contextualized embeddings can be jointly pre-trained for the retrieval tasks. In this work, we propose a novel pre-training method called Duplex Masked Auto-Encoder, a.k.a. DupMAE. It is designed to improve the quality of semantic representation where all contextualized embeddings of the pre-trained model can be leveraged. It takes advantage of two complementary auto-encoding tasks: one reconstructs the input sentence on top of the [CLS] embedding; the other one predicts the bag-of-words feature of the input sentence based on the ordinary tokens’ embeddings. The two tasks are jointly conducted to train a unified encoder, where the whole contextualized embeddings are aggregated in a compact way to produce the final semantic representation. DupMAE is simple but empirically competitive: it substantially improves the pre-trained model’s representation capability and transferability, where superior retrieval performances can be achieved on popular benchmarks, like MS MARCO and BEIR. We make our code publicly available athttps://github.com/staoxiao/RetroMAE."
    }
  },
  {
    "id": "abstract-2023--acl-long--226",
    "result": [
      {
        "value": {
          "start": 719,
          "end": 725,
          "text": "UTC-IE",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--226:E0"
      },
      {
        "value": {
          "start": 1095,
          "end": 1125,
          "text": "results on 2 joint IE datasets",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--226:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--226:E0",
        "to_id": "abstract-2023--acl-long--226:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Information Extraction (IE) spans several tasks with different output structures, such as named entity recognition, relation extraction and event extraction. Previously, those tasks were solved with different models because of diverse task output structures. Through re-examining IE tasks, we find that all of them can be interpreted as extracting spans and span relations. They can further be decomposed into token-pair classification tasks by using the start and end token of a span to pinpoint the span, and using the start-to-start and end-to-end token pairs of two spans to determine the relation. Based on the reformulation, we propose a Unified Token-pair Classification architecture for Information Extraction (UTC-IE), where we introduce Plusformer on top of the token-pair feature matrix. Specifically, it models axis-aware interaction with plus-shaped self-attention and local interaction with Convolutional Neural Network over token pairs. Experiments show that our approach outperforms task-specific and unified models on all tasks in 10 datasets, and achieves better or comparable results on 2 joint IE datasets. Moreover, UTC-IE speeds up over state-of-the-art models on IE tasks significantly in most datasets, which verifies the effectiveness of our architecture."
    }
  },
  {
    "id": "abstract-2023--acl-short--64",
    "result": [
      {
        "value": {
          "start": 671,
          "end": 705,
          "text": "simple sequential prediction model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--64:E0"
      },
      {
        "value": {
          "start": 715,
          "end": 730,
          "text": "accuracy scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--64:E1"
      },
      {
        "from_id": "abstract-2023--acl-short--64:E0",
        "to_id": "abstract-2023--acl-short--64:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Current models for quotation attribution in literary novels assume varying levels of available information in their training and test data, which poses a challenge for in-the-wild inference. Here, we approach quotation attribution as a set of four interconnected sub-tasks: character identification, coreference resolution, quotation identification, and speaker attribution. We benchmark state-of-the-art models on each of these sub-tasks independently, using a large dataset of annotated coreferences and quotations in literary novels (the Project Dialogism Novel Corpus). We also train and evaluate models for the speaker attribution task in particular, showing that a simple sequential prediction model achieves accuracy scores on par with state-of-the-art models."
    }
  },
  {
    "id": "abstract-2023--acl-long--408",
    "result": [
      {
        "value": {
          "start": 272,
          "end": 312,
          "text": "Dual Graph ATtention networks (DualGATs)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--408:E0"
      }
    ],
    "data": {
      "text": "Capturing complex contextual dependencies plays a vital role in Emotion Recognition in Conversations (ERC). Previous studies have predominantly focused on speaker-aware context modeling, overlooking the discourse structure of the conversation. In this paper, we introduce Dual Graph ATtention networks (DualGATs) to concurrently consider the complementary aspects of discourse structure and speaker-aware context, aiming for more precise ERC. Specifically, we devise a Discourse-aware GAT (DisGAT) module to incorporate discourse structural information by analyzing the discourse dependencies between utterances. Additionally, we develop a Speaker-aware GAT (SpkGAT) module to incorporate speaker-aware contextual information by considering the speaker dependencies between utterances. Furthermore, we design an interaction module that facilitates the integration of the DisGAT and SpkGAT modules, enabling the effective interchange of relevant information between the two modules. We extensively evaluate our method on four datasets, and experimental results demonstrate that our proposed DualGATs surpass state-of-the-art baselines on the majority of the datasets."
    }
  },
  {
    "id": "abstract-2023--acl-long--372",
    "result": [
      {
        "value": {
          "start": 294,
          "end": 299,
          "text": "FLamE",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--372:E0"
      },
      {
        "value": {
          "start": 575,
          "end": 583,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--372:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--372:E0",
        "to_id": "abstract-2023--acl-long--372:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Natural language explanations have the potential to provide rich information that in principle guides model reasoning. Yet, recent work by Lampinen et al. has shown limited utility of natural language explanations in improving classification. To effectively learn from explanations, we present FLamE, a two-stage few-shot learning framework that first generates explanations using GPT-3, and then fine-tunes a smaller model (e.g., RoBERTa) with generated explanations. Our experiments on natural language inference demonstrate effectiveness over strong baselines, increasing accuracy by 17.6% over GPT-3 Babbage and 5.7% over GPT-3 Davinci in e-SNLI.Despite improving classification performance, human evaluation surprisingly reveals that the majority of generated explanations does not adequately justify classification decisions. Additional analyses point to the important role of label-specific cues (e.g., “not know” for the neutral label) in generated explanations."
    }
  },
  {
    "id": "abstract-2023--acl-long--305",
    "result": [
      {
        "value": {
          "start": 743,
          "end": 750,
          "text": "TM-HGNN",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--305:E0"
      },
      {
        "value": {
          "start": 1196,
          "end": 1228,
          "text": "in-hospital-mortality prediction",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--305:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--305:E0",
        "to_id": "abstract-2023--acl-long--305:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Leveraging knowledge from electronic health records (EHRs) to predict a patient’s condition is essential to the effective delivery of appropriate care. Clinical notes of patient EHRs contain valuable information from healthcare professionals, but have been underused due to their difficult contents and complex hierarchies. Recently, hypergraph-based methods have been proposed for document classifications. Directly adopting existing hypergraph methods on clinical notes cannot sufficiently utilize the hierarchy information of the patient, which can degrade clinical semantic information by (1) frequent neutral words and (2) hierarchies with imbalanced distribution. Thus, we propose a taxonomy-aware multi-level hypergraph neural network (TM-HGNN), where multi-level hypergraphs assemble useful neutral words with rare keywords via note and taxonomy level hyperedges to retain the clinical semantic information. The constructed patient hypergraphs are fed into hierarchical message passing layers for learning more balanced multi-level knowledge at the note and taxonomy levels. We validate the effectiveness of TM-HGNN by conducting extensive experiments with MIMIC-III dataset on benchmark in-hospital-mortality prediction."
    }
  },
  {
    "id": "abstract-2023--acl-long--788",
    "result": [
      {
        "value": {
          "start": 194,
          "end": 225,
          "text": "syntax-guided generation schema",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--788:E0"
      },
      {
        "value": {
          "start": 863,
          "end": 879,
          "text": "interpretability",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--788:E1"
      },
      {
        "value": {
          "start": 881,
          "end": 896,
          "text": "controllability",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--788:E2"
      },
      {
        "value": {
          "start": 902,
          "end": 911,
          "text": "diversity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--788:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--788:E0",
        "to_id": "abstract-2023--acl-long--788:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--788:E0",
        "to_id": "abstract-2023--acl-long--788:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--788:E0",
        "to_id": "abstract-2023--acl-long--788:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Most existing text generation models follow the sequence-to-sequence paradigm. Generative Grammar suggests that humans generate natural language texts by learning language grammar. We propose a syntax-guided generation schema, which generates the sequence guided by a constituency parse tree in a top-down direction. The decoding process can be decomposed into two parts: (1) predicting the infilling texts for each constituent in the lexicalized syntax context given the source sentence; (2) mapping and expanding each constituent to construct the next-level syntax context. Accordingly, we propose a structural beam search method to find possible syntax structures hierarchically. Experiments on paraphrase generation and machine translation show that the proposed method outperforms autoregressive baselines, while also demonstrating effectiveness in terms of interpretability, controllability, and diversity."
    }
  },
  {
    "id": "abstract-2023--acl-long--318",
    "result": [
      {
        "value": {
          "start": 761,
          "end": 766,
          "text": "QUARC",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--318:E0"
      },
      {
        "value": {
          "start": 1120,
          "end": 1138,
          "text": "evaluation metrics",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--318:E1"
      },
      {
        "value": {
          "start": 1153,
          "end": 1169,
          "text": "human evaluation",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--318:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--318:E0",
        "to_id": "abstract-2023--acl-long--318:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--318:E0",
        "to_id": "abstract-2023--acl-long--318:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Counterspeech has been demonstrated to be an efficacious approach for combating hate speech. While various conventional and controlled approaches have been studied in recent years to generate counterspeech, a counterspeech with a certain intent may not be sufficient in every scenario. Due to the complex and multifaceted nature of hate speech, utilizing multiple forms of counter-narratives with varying intents may be advantageous in different circumstances. In this paper, we explore intent-conditioned counterspeech generation. At first, we develop IntentCONAN, a diversified intent-specific counterspeech dataset with 6831 counterspeeches conditioned on five intents, i.e., informative, denouncing, question, positive, and humour. Subsequently, we propose QUARC, a two-stage framework for intent-conditioned counterspeech generation. QUARC leverages vector-quantized representations learned for each intent category along with PerFuMe, a novel fusion module to incorporate intent-specific information into the model. Our evaluation demonstrates that QUARC outperforms several baselines by an average of ~10% across evaluation metrics. An extensive human evaluation supplements our hypothesis of better and more appropriate responses than comparative systems."
    }
  },
  {
    "id": "abstract-2023--acl-long--173",
    "result": [
      {
        "value": {
          "start": 184,
          "end": 188,
          "text": "PLMs",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--173:E0"
      }
    ],
    "data": {
      "text": "Ontological knowledge, which comprises classes and properties and their relationships, is integral to world knowledge. It is significant to explore whether Pretrained Language Models (PLMs) know and understand such knowledge. However, existing PLM-probing studies focus mainly on factual knowledge, lacking a system- atic probing of ontological knowledge. In this paper, we focus on probing whether PLMs store ontological knowledge and have a semantic un- derstanding of the knowledge rather than rote memorization of the surface form. To probe whether PLMs know ontological knowledge, we investigate how well PLMs memorize: (1) types of entities; (2) hierarchical relationships among classes and properties, e.g., Person is a subclass of Animal and Member of Sports Team is a subproperty of Member of ; (3) domain and range constraints of properties, e.g., the subject of Member of Sports Team should be a Person and the object should be a Sports Team. To further probe whether PLMs truly understand ontological knowledge beyond memorization, we comprehensively study whether they can reliably perform logical reasoning with given knowledge according to ontological entailment rules. Our probing results show that PLMs can memorize certain ontological knowledge and utilize implicit knowledge in reasoning. How- ever, both the memorizing and reasoning per- formances are less than perfect, indicating in- complete knowledge and understanding."
    }
  },
  {
    "id": "abstract-2023--acl-long--149",
    "result": [
      {
        "value": {
          "start": 536,
          "end": 543,
          "text": "DecompX",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--149:E0"
      },
      {
        "value": {
          "start": 1163,
          "end": 1187,
          "text": "faithfulness evaluations",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--149:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--149:E0",
        "to_id": "abstract-2023--acl-long--149:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "An emerging solution for explaining Transformer-based models is to use vector-based analysis on how the representations are formed. However, providing a faithful vector-based explanation for a multi-layer model could be challenging in three aspects: (1) Incorporating all components into the analysis, (2) Aggregating the layer dynamics to determine the information flow and mixture throughout the entire model, and (3) Identifying the connection between the vector-based analysis and the model’s predictions. In this paper, we present DecompX to tackle these challenges. DecompX is based on the construction of decomposed token representations and their successive propagation throughout the model without mixing them in between layers. Additionally, our proposal provides multiple advantages over existing solutions for its inclusion of all encoder components (especially nonlinear feed-forward networks) and the classification head. The former allows acquiring precise vectors while the latter transforms the decomposition into meaningful prediction-based values, eliminating the need for norm- or summation-based vector aggregation. According to the standard faithfulness evaluations, DecompX consistently outperforms existing gradient-based and vector-based approaches on various datasets. Our code is available athttps://github.com/mohsenfayyaz/DecompX."
    }
  },
  {
    "id": "abstract-2023--acl-srw--17",
    "result": [
      {
        "value": {
          "start": 683,
          "end": 715,
          "text": "NPC scripts that can fool judges",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-srw--17:E0"
      }
    ],
    "data": {
      "text": "In this paper, we study the viability of the deployment of language models towards non-playable character (NPC) scripts, by introducing a novel pipeline for the automatic construction of NPC scripts using Transformer-based believable scripts for a variety of game genres and specifications. In addition, we propose a self-diagnosis method inspired by previous work to develop language models, tailored specifically to desirable NPC qualities such as coherency, believability, and degree of repetition. Finally, we propose a new benchmark, called The Turing Quest, which we use to show that the pipeline, when applied to GPT-3, can generate for a variety of game genres and contexts, NPC scripts that can fool judges in thinking they have been written by humans. We believe that these findings can greatly benefit both the gaming industry and its global community of users, since many current games continue to base their NPCs on manually-curated scripts that are resource-demanding and may curb the immersiveness and enjoyment of the user."
    }
  },
  {
    "id": "abstract-2023--acl-long--653",
    "result": [
      {
        "value": {
          "start": 329,
          "end": 348,
          "text": "language adaptation",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--653:E0"
      },
      {
        "value": {
          "start": 534,
          "end": 555,
          "text": "zero-shot performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--653:E1"
      },
      {
        "value": {
          "start": 601,
          "end": 625,
          "text": "adapter-based finetuning",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--653:E2"
      },
      {
        "value": {
          "start": 1037,
          "end": 1098,
          "text": "including a new language in the multitask fine-tuning mixture",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--653:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--653:E0",
        "to_id": "abstract-2023--acl-long--653:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The BLOOM model is a large publicly available multilingual language model, but its pretraining was limited to 46 languages. To extend the benefits of BLOOM to other languages without incurring prohibitively large costs, it is desirable to adapt BLOOM to new languages not seen during pretraining. In this work, we apply existing language adaptation strategies to BLOOM and benchmark its zero-shot prompting performance on eight new languages in a resource-constrained setting. We find language adaptation to be effective at improving zero-shot performance in new languages. Surprisingly, we find that adapter-based finetuning is more effective than continued pretraining for large models. In addition, we discover that prompting performance is not significantly affected by language specifics, such as the writing system. It is primarily determined by the size of the language adaptation data. We also add new languages to BLOOMZ, which is a multitask finetuned version of BLOOM capable of following task instructions zero-shot. We find including a new language in the multitask fine-tuning mixture to be the most effective method to teach BLOOMZ a new language. We conclude that with sufficient training data language adaptation can generalize well to diverse languages. Our code is available athttps://github.com/bigscience-workshop/multilingual-modeling."
    }
  },
  {
    "id": "abstract-2023--acl-long--398",
    "result": [
      {
        "value": {
          "start": 363,
          "end": 425,
          "text": "multilingual punctuation-agnostic sentence segmentation method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--398:E0"
      },
      {
        "value": {
          "start": 878,
          "end": 903,
          "text": "average of 6.1% F1 points",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--398:E1"
      },
      {
        "value": {
          "start": 1238,
          "end": 1276,
          "text": "average improvement of 2.3 BLEU points",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--398:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--398:E0",
        "to_id": "abstract-2023--acl-long--398:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--398:E0",
        "to_id": "abstract-2023--acl-long--398:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Many NLP pipelines split text into sentences as one of the crucial preprocessing steps. Prior sentence segmentation tools either rely on punctuation or require a considerable amount of sentence-segmented training data: both central assumptions might fail when porting sentence segmenters to diverse languages on a massive scale. In this work, we thus introduce a multilingual punctuation-agnostic sentence segmentation method, currently covering 85 languages, trained in a self-supervised fashion on unsegmented text, by making use of newline characters which implicitly perform segmentation into paragraphs. We further propose an approach that adapts our method to the segmentation in a given corpus by using only a small number (64-256) of sentence-segmented examples. The main results indicate that our method outperforms all the prior best sentence-segmentation tools by an average of 6.1% F1 points. Furthermore, we demonstrate that proper sentence segmentation has a point: the use of a (powerful) sentence segmenter makes a considerable difference for a downstream application such as machine translation (MT). By using our method to match sentence segmentation to the segmentation used during training of MT models, we achieve an average improvement of 2.3 BLEU points over the best prior segmentation tool, as well as massive gains over a trivial segmenter that splits text into equally-sized blocks."
    }
  },
  {
    "id": "abstract-2023--acl-short--10",
    "result": [
      {
        "value": {
          "start": 115,
          "end": 121,
          "text": "xsim++",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--10:E0"
      },
      {
        "value": {
          "start": 723,
          "end": 748,
          "text": "bitext mining performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--10:E1"
      },
      {
        "from_id": "abstract-2023--acl-short--10:E0",
        "to_id": "abstract-2023--acl-short--10:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We introduce a new proxy score for evaluating bitext mining based on similarity in a multilingual embedding space: xsim++. In comparison to xsim, this improved proxy leverages rule-based approaches to extend English sentences in any evaluation set with synthetic, hard-to-distinguish examples which more closely mirror the scenarios we encounter during large-scale mining. We validate this proxy by running a significant number of bitext mining experiments for a set of low-resource languages, and subsequently train NMT systems on the mined data. In comparison to xsim, we show that xsim++ is better correlated with the downstream BLEU scores of translation systems trained on mined bitexts, providing a reliable proxy of bitext mining performance without needing to run expensive bitext mining pipelines. xsim++ also reports performance for different error types, offering more fine-grained feedbacks for model development."
    }
  },
  {
    "id": "abstract-2023--acl-short--6",
    "result": [
      {
        "value": {
          "start": 183,
          "end": 190,
          "text": "runtime",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--6:E0"
      }
    ],
    "data": {
      "text": "Multiple algorithms are known for efficiently calculating the prefix probability of a string under a probabilistic context-free grammar (PCFG). Good algorithms for the problem have a runtime cubic in the length of the input string. However, some proposed algorithms are suboptimal with respect to the size of the grammar. This paper proposes a new speed-up of Jelinek and Lafferty’s (1991) algorithm, which runs inO(n3|N|3+ |N|4), where n is the input length and |N| is the number of non-terminals in the grammar. In contrast, our speed-up runs inO(n2|N|3+ n3|N|2)."
    }
  },
  {
    "id": "abstract-2023--acl-long--207",
    "result": [
      {
        "value": {
          "start": 608,
          "end": 622,
          "text": "PeerHTC method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--207:E0"
      },
      {
        "value": {
          "start": 521,
          "end": 547,
          "text": "classification performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--207:E1"
      },
      {
        "value": {
          "start": 1089,
          "end": 1112,
          "text": "classification accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--207:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--207:E0",
        "to_id": "abstract-2023--acl-long--207:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--207:E0",
        "to_id": "abstract-2023--acl-long--207:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Hierarchical text classification (HTC) is a challenging task, in which the labels of texts can be organized into a category hierarchy. To deal with the HTC problem, many existing works focus on utilizing the parent-child relationships that are explicitly shown in the hierarchy. However, texts with a category hierarchy also have some latent relevancy among labels in the same level of the hierarchy. We refer to these labels as peer labels, from which the peer effects are originally utilized in our work to improve the classification performance. To fully explore the peer-label relationship, we develop a PeerHTC method. This method innovatively measures the latent relevancy of peer labels through several metrics and then encodes the relevancy with a Graph Convolutional Neural Network. We also propose a sample importance learning method to ameliorate the side effects raised by modelling the peer label relevancy. Our experiments on several standard datasets demonstrate the evidence of peer labels and the superiority of PeerHTC over other state-of-the-art HTC methods in terms of classification accuracy."
    }
  },
  {
    "id": "abstract-2023--acl-long--219",
    "result": [
      {
        "value": {
          "start": 772,
          "end": 798,
          "text": "OT-based alignment methods",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--219:E0"
      }
    ],
    "data": {
      "text": "Monolingual word alignment is crucial to model semantic interactions between sentences. In particular, null alignment, a phenomenon in which words have no corresponding counterparts, is pervasive and critical in handling semantically divergent sentences. Identification of null alignment is useful on its own to reason about the semantic similarity of sentences by indicating there exists information inequality. To achieve unbalanced word alignment that values both alignment and null alignment, this study shows that the family of optimal transport (OT), i.e., balanced, partial, and unbalanced OT, are natural and powerful approaches even without tailor-made techniques. Our extensive experiments covering unsupervised and supervised settings indicate that our generic OT-based alignment methods are competitive against the state-of-the-arts specially designed for word alignment, remarkably on challenging datasets with high null alignment frequencies."
    }
  },
  {
    "id": "abstract-2023--acl-long--124",
    "result": [
      {
        "value": {
          "start": 546,
          "end": 585,
          "text": "denoising bottleneck fusion (DBF) model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--124:E0"
      },
      {
        "value": {
          "start": 941,
          "end": 992,
          "text": "improvement over current state-of-the-art baselines",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--124:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--124:E0",
        "to_id": "abstract-2023--acl-long--124:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Video multimodal fusion aims to integrate multimodal signals in videos, such as visual, audio and text, to make a complementary prediction with multiple modalities contents. However, unlike other image-text multimodal tasks, video has longer multimodal sequences with more redundancy and noise in both visual and audio modalities. Prior denoising methods like forget gate are coarse in the granularity of noise filtering. They often suppress the redundant and noisy information at the risk of losing critical information. Therefore, we propose a denoising bottleneck fusion (DBF) model for fine-grained video multimodal fusion. On the one hand, we employ a bottleneck mechanism to filter out noise and redundancy with a restrained receptive field. On the other hand, we use a mutual information maximization module to regulate the filter-out module to preserve key information within different modalities. Our DBF model achieves significant improvement over current state-of-the-art baselines on multiple benchmarks covering multimodal sentiment analysis and multimodal summarization tasks. It proves that our model can effectively capture salient features from noisy and redundant video, audio, and text inputs. The code for this paper will be publicly available athttps://github.com/WSXRHFG/DBF"
    }
  },
  {
    "id": "abstract-2023--acl-long--703",
    "result": [
      {
        "value": {
          "start": 659,
          "end": 667,
          "text": "DynaInst",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--703:E0"
      },
      {
        "value": {
          "start": 145,
          "end": 171,
          "text": "generalization performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--703:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--703:E0",
        "to_id": "abstract-2023--acl-long--703:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Jointly fine-tuning a Pre-trained Language Model (PLM) on a pre-defined set of tasks with in-context instructions has been proven to improve its generalization performance, allowing us to build a universal language model that can be deployed across task boundaries. In this work, we explore for the first time whether this attractive property of in-context instruction learning can be extended to a scenario in which tasks are fed to the target PLM in a sequential manner. The primary objective of so-called lifelong in-context instruction learning is to improve the target PLM’s instance- and task-level generalization performance as it observes more tasks. DynaInst, the proposed method to lifelong in-context instruction learning, achieves noticeable improvements in both types of generalization, nearly reaching the upper bound performance obtained through joint training."
    }
  },
  {
    "id": "abstract-2023--acl-srw--5",
    "result": [
      {
        "value": {
          "start": 56,
          "end": 78,
          "text": "downstream performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-srw--5:E0"
      }
    ],
    "data": {
      "text": "This paper investigates the effect of tokenizers on the downstream performance of pretrained language models (PLMs) in scriptio continua languages where no explicit spaces exist between words, using Japanese as a case study. The tokenizer for such languages often consists of a morphological analyzer and a subword tokenizer, requiring us to conduct a comprehensive study of all possible pairs. However, previous studies lack this comprehensiveness. We therefore train extensive sets of tokenizers, build a PLM using each, and measure the downstream performance on a wide range of tasks. Our results demonstrate that each downstream task has a different optimal morphological analyzer, and that it is better to use Byte-Pair-Encoding or Unigram rather than WordPiece as a subword tokenizer, regardless of the type of task."
    }
  },
  {
    "id": "abstract-2023--acl-long--519",
    "result": [
      {
        "value": {
          "start": 750,
          "end": 763,
          "text": "2-Step method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--519:E0"
      }
    ],
    "data": {
      "text": "Most existing cross-lingual summarization (CLS) work constructs CLS corpora by simply and directly translating pre-annotated summaries from one language to another, which can contain errors from both summarization and translation processes. To address this issue, we propose ConvSumX, a cross-lingual conversation summarization benchmark, through a new annotation schema that explicitly considers source input context. ConvSumX consists of 2 sub-tasks under different real-world scenarios, with each covering 3 language directions. We conduct thorough analysis on ConvSumX and 3 widely-used manually annotated CLS corpora and empirically find that ConvSumX is more faithful towards input text. Additionally, based on the same intuition, we propose a 2-Step method, which takes both conversation and summary as input to simulate human annotation process. Experimental results show that 2-Step method surpasses strong baselines on ConvSumX under both automatic and human evaluation. Analysis shows that both source input text and summary are crucial for modeling cross-lingual summaries."
    }
  },
  {
    "id": "abstract-2023--acl-long--901",
    "result": [
      {
        "value": {
          "start": 1062,
          "end": 1102,
          "text": "domain and geographical generalizability",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--901:E0"
      }
    ],
    "data": {
      "text": "Extracting geolocation information from social media data enables effective disaster management, as it helps response authorities; for example, in locating incidents for planning rescue activities, and affected people for evacuation. Nevertheless, geolocation extraction is greatly understudied for the low resource languages such as Arabic. To fill this gap, we introduce IDRISI-RA, the first publicly-available Arabic Location Mention Recognition (LMR) dataset that provides human- and automatically-labeled versions in order of thousands and millions of tweets, respectively. It contains both location mentions and their types (e.g., district, city). Our extensive analysis shows the decent geographical, domain, location granularity, temporal, and dialectical coverage of IDRISI-RA. Furthermore, we establish baselines using the standard Arabic NER models and build two simple, yet effective, LMR models. Our rigorous experiments confirm the need for developing specific models for Arabic LMR in the disaster domain. Moreover, experiments show the promising domain and geographical generalizability of IDRISI-RA under zero-shot learning."
    }
  },
  {
    "id": "abstract-2023--acl-long--718",
    "result": [
      {
        "value": {
          "start": 358,
          "end": 365,
          "text": "UniSumm",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--718:E0"
      }
    ],
    "data": {
      "text": "The high annotation costs and diverse demands of various summarization tasks motivate the development of few-shot summarization. However, despite the emergence of many summarization tasks and datasets, the current training paradigm for few-shot summarization systems ignores potentially shareable knowledge in heterogeneous datasets. To this end, we propose UniSumm, a unified few-shot summarization model pre-trained with multiple summarization tasks and can be prefix-tuned to excel at any few-shot summarization task. Meanwhile, to better evaluate few-shot summarizers, under the principles of diversity and robustness, we assemble and release a new benchmark SummZoo. It consists of 8 summarization tasks with multiple sets of few-shot samples for each task, covering diverse domains. Experimental results and analysis show that UniSumm outperforms strong baselines by a large margin across all sub-tasks in SummZoo under both automatic and human evaluations and achieves comparable results in human evaluation compared with a GPT-3.5 model."
    }
  },
  {
    "id": "abstract-2023--acl-long--88",
    "result": [
      {
        "value": {
          "start": 752,
          "end": 785,
          "text": "Bayesian inference-based approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--88:E0"
      },
      {
        "value": {
          "start": 702,
          "end": 718,
          "text": "VWSD performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--88:E1"
      },
      {
        "value": {
          "start": 619,
          "end": 654,
          "text": "context-aware definition generation",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--88:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--88:E0",
        "to_id": "abstract-2023--acl-long--88:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Visual Word Sense Disambiguation (VWSD) is a task to find the image that most accurately depicts the correct sense of the target word for the given context. Previously, image-text matching models often suffered from recognizing polysemous words. This paper introduces an unsupervised VWSD approach that uses gloss information of an external lexical knowledge-base, especially the sense definitions. Specifically, we suggest employing Bayesian inference to incorporate the sense definitions when sense information of the answer is not provided. In addition, to ameliorate the out-of-dictionary (OOD) issue, we propose a context-aware definition generation with GPT-3. Experimental results show that the VWSD performance significantly increased with our Bayesian inference-based approach. In addition, our context-aware definition generation achieved prominent performance improvement in OOD examples exhibiting better performance than the existing definition generation method."
    }
  },
  {
    "id": "abstract-2023--acl-long--694",
    "result": [
      {
        "value": {
          "start": 69,
          "end": 74,
          "text": "TFSGC",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--694:E0"
      },
      {
        "value": {
          "start": 1140,
          "end": 1153,
          "text": "effectiveness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--694:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--694:E0",
        "to_id": "abstract-2023--acl-long--694:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We propose to TransForm Scene Graphs into more descriptive Captions (TFSGC). In TFSGC, we apply multi-head attention (MHA) to design the Graph Neural Network (GNN) for embedding scene graphs. After embedding, different graph embeddings contain diverse specific knowledge for generating the words with different part-of-speech, e.g., object/attribute embedding is good for generating nouns/adjectives. Motivated by this, we design a Mixture-of-Expert (MOE)-based decoder, where each expert is built on MHA, for discriminating the graph embeddings to generate different kinds of words. Since both the encoder and decoder are built based on the MHA, as a result, we construct a simple and homogeneous encoder-decoder unlike the previous heterogeneous ones which usually apply Fully-Connected-based GNN and LSTM-based decoder. The homogeneous architecture enables us to unify the training configuration of the whole model instead of specifying different training strategies for diverse sub-networks as in the heterogeneous pipeline, which releases the training difficulty. Extensive experiments on the MS-COCO captioning benchmark validate the effectiveness of our TFSGC. The code is in:https://anonymous.4open.science/r/ACL23_TFSGC."
    }
  },
  {
    "id": "abstract-2023--acl-long--397",
    "result": [
      {
        "value": {
          "start": 799,
          "end": 831,
          "text": "Local Byte Fusion (LOBEF) method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--397:E0"
      },
      {
        "value": {
          "start": 975,
          "end": 999,
          "text": "multilingual translation",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--397:E1"
      },
      {
        "value": {
          "start": 1001,
          "end": 1033,
          "text": "zero-shot cross-lingual transfer",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--397:E2"
      },
      {
        "value": {
          "start": 1039,
          "end": 1056,
          "text": "domain adaptation",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--397:E3"
      },
      {
        "value": {
          "start": 1102,
          "end": 1119,
          "text": "byte-based models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--397:E4"
      },
      {
        "from_id": "abstract-2023--acl-long--397:E0",
        "to_id": "abstract-2023--acl-long--397:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--397:E0",
        "to_id": "abstract-2023--acl-long--397:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--397:E0",
        "to_id": "abstract-2023--acl-long--397:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Subword tokenization schemes are the dominant technique used in current NLP models. However, such schemes can be rigid and tokenizers built on one corpus may not adapt well to other parallel corpora. It has also been observed that in multilingual corpora, subword tokenization schemes oversegment low-resource languages, leading to a drop in translation performance. An alternative to subword tokenizers is byte-based tokenization, i.e., tokenization into byte sequences using the UTF-8 encoding scheme. Byte tokens often represent inputs at a sub-character granularity, i.e., one character can be represented by a span of byte tokens. This results in much longer byte sequences that are hard to interpret without aggregating local information from multiple byte tokens. In this paper, we propose a Local Byte Fusion (LOBEF) method for byte-based machine translation—utilizing byte n-gram and word boundaries—to aggregate local semantic information. Extensive experiments on multilingual translation, zero-shot cross-lingual transfer, and domain adaptation reveal a consistent improvement over vanilla byte-based models. Further analysis also indicates that our byte-based models are parameter-efficient and perform competitive to subword models."
    }
  },
  {
    "id": "abstract-2023--acl-demo--15",
    "result": [
      {
        "value": {
          "start": 378,
          "end": 413,
          "text": "hyperparameter optimization toolkit",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-demo--15:E0"
      }
    ],
    "data": {
      "text": "Hyperparameter optimization is an important but often overlooked process in the research of deep learning technologies. To obtain a good model, one must carefully tune hyperparameters that determine the architecture and training algorithm. Insufficient tuning may result in poor results, while inequitable tuning may lead to exaggerated differences between models. We present a hyperparameter optimization toolkit for neural machine translation (NMT) to help researchers focus their time on the creative rather than the mundane. The toolkit is implemented as a wrapper on top of the open-source Sockeye NMT software. Using the Asynchronous Successive Halving Algorithm (ASHA), we demonstrate that it is possible to discover near-optimal models under a computational budget with little effort. Code:https://github.com/kevinduh/sockeye-recipes3Videodemo:https://cs.jhu.edu/kevinduh/j/demo.mp4"
    }
  },
  {
    "id": "abstract-2023--acl-long--387",
    "result": [
      {
        "value": {
          "start": 757,
          "end": 763,
          "text": "Patton",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--387:E0"
      }
    ],
    "data": {
      "text": "A real-world text corpus sometimes comprises not only text documents, but also semantic links between them (e.g., academic papers in a bibliographic network are linked by citations and co-authorships).Text documents and semantic connections form a text-rich network, which empowers a wide range of downstream tasks such as classification and retrieval. However, pretraining methods for such structures are still lacking, making it difficult to build one generic model that can be adapted to various tasks on text-rich networks. Current pretraining objectives, such as masked language modeling, purely model texts and do not take inter-document structure information into consideration. To this end, we propose our PretrAining on TexT-Rich NetwOrk framework Patton.Patton includes two pretraining strategies: network-contextualized masked language modeling and masked node prediction, to capture the inherent dependency between textual attributes and network structure. We conduct experiments on four downstream tasks in five datasets from both academic and e-commerce domains, where Patton outperforms baselines significantly and consistently."
    }
  },
  {
    "id": "abstract-2023--acl-long--248",
    "result": [
      {
        "value": {
          "start": 11,
          "end": 24,
          "text": "DiffusionBERT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--248:E0"
      },
      {
        "value": {
          "start": 1200,
          "end": 1210,
          "text": "perplexity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--248:E1"
      },
      {
        "value": {
          "start": 1215,
          "end": 1225,
          "text": "BLEU score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--248:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--248:E0",
        "to_id": "abstract-2023--acl-long--248:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--248:E0",
        "to_id": "abstract-2023--acl-long--248:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We present DiffusionBERT, a new generative masked language model based on discrete dif- fusion models. Diffusion models and many pre- trained language models have a shared training objective, i.e., denoising, making it possible to combine the two powerful models and enjoy the best of both worlds. On the one hand, dif- fusion models offer a promising training strat- egy that helps improve the generation quality. On the other hand, pre-trained denoising lan- guage models (e.g., BERT) can be used as a good initialization that accelerates convergence. We explore training BERT to learn the reverse process of a discrete diffusion process with an absorbing state and elucidate several designs to improve it. First, we propose a new noise schedule for the forward diffusion process that controls the degree of noise added at each step based on the information of each token. Sec- ond, we investigate several designs of incorpo- rating the time step into BERT. Experiments on unconditional text generation demonstrate that DiffusionBERT achieves significant improve- ment over existing diffusion models for text (e.g., D3PM and Diffusion-LM) and previous generative masked language models in terms of perplexity and BLEU score. Promising re- sults in conditional generation tasks show that DiffusionBERT can generate texts of compa- rable quality and more diverse than a series of established baselines."
    }
  },
  {
    "id": "abstract-2023--acl-long--333",
    "result": [
      {
        "value": {
          "start": 831,
          "end": 848,
          "text": "model performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--333:E0"
      }
    ],
    "data": {
      "text": "Targeted syntactic evaluations of language models ask whether models show stable preferences for syntactically acceptable content over minimal-pair unacceptable inputs. Our best syntactic evaluation datasets, however, provide substantially less linguistic context than models receive during pretraining. This mismatch raises an important question: how robust are models’ syntactic judgements across different contexts? In this paper, we vary the input contexts based on: length, the types of syntactic phenomena it contains, and whether or not there are grammatical violations. We find that model judgements are generally robust when placed in randomly sampled linguistic contexts, but are unstable when contexts match the test stimuli in syntactic structure. Among all tested models (GPT-2 and five variants of OPT), we find that model performance is affected when we provided contexts with matching syntactic structure: performance significantly improves when contexts are acceptable, and it significantly declines when they are unacceptable. This effect is amplified by the length of the context, except for unrelated inputs. We show that these changes in model performance are not explainable by acceptability-preserving syntactic perturbations. This sensitivity to highly specific syntactic features of the context can only be explained by the models’ implicit in-context learning abilities."
    }
  },
  {
    "id": "abstract-2023--acl-long--705",
    "result": [
      {
        "value": {
          "start": 605,
          "end": 610,
          "text": "L2TKG",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--705:E0"
      },
      {
        "value": {
          "start": 1003,
          "end": 1016,
          "text": "effectiveness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--705:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--705:E0",
        "to_id": "abstract-2023--acl-long--705:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Temporal Knowledge Graph (TKG) reasoning aims to predict future facts based on historical data. However, due to the limitations in construction tools and data sources, many important associations between entities may be omitted in TKG. We refer to these missing associations as latent relations. Most existing methods have some drawbacks in explicitly capturing intra-time latent relations between co-occurring entities and inter-time latent relations between entities that appear at different times. To tackle these problems, we propose a novel Latent relations Learning method for TKG reasoning, namely L2TKG. Specifically, we first utilize a Structural Encoder (SE) to obtain representations of entities at each timestamp. We then design a Latent Relations Learning (LRL) module to mine and exploit the intra- and inter-time latent relations. Finally, we extract the temporal representations from the output of SE and LRL for entity prediction. Extensive experiments on four datasets demonstrate the effectiveness of L2TKG."
    }
  },
  {
    "id": "abstract-2023--acl-long--856",
    "result": [
      {
        "value": {
          "start": 457,
          "end": 487,
          "text": "performance across model sizes",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--856:E0"
      },
      {
        "value": {
          "start": 551,
          "end": 558,
          "text": "XY-LENT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--856:E1"
      },
      {
        "value": {
          "start": 765,
          "end": 775,
          "text": "XY-LENT XL",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--856:E2"
      },
      {
        "value": {
          "start": 1015,
          "end": 1037,
          "text": "99.3% GLUE performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--856:E3"
      },
      {
        "value": {
          "start": 1042,
          "end": 1069,
          "text": "98.5% SQuAD 2.0 performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--856:E4"
      },
      {
        "from_id": "abstract-2023--acl-long--856:E2",
        "to_id": "abstract-2023--acl-long--856:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--856:E2",
        "to_id": "abstract-2023--acl-long--856:E4",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In this paper, we elaborate upon recipes for building multilingual representation models that are not only competitive with existing state-of-the-art models but are also more parameter efficient, thereby promoting better adoption in resource-constrained scenarios and practical applications. We show that going beyond English-centric bitexts, coupled with a novel sampling strategy aimed at reducing under-utilization of training data, substantially boosts performance across model sizes for both Electra and MLM pre-training objectives. We introduce XY-LENT: X-Y bitext enhanced Language ENcodings using Transformers which not only achieves state-of-the-art performance over 5 cross-lingual tasks within all model size bands, is also competitive across bands. Our XY-LENT XL variant outperforms XLM-R XXL and exhibits competitive performance with mT5 XXL while being 5x and 6x smaller respectively. We then show that our proposed method helps ameliorate the curse of multilinguality, with the XY-LENT XL achieving 99.3% GLUE performance and 98.5% SQuAD 2.0 performance compared to a SoTA English only model in the same size band. We then analyze our models performance on extremely low resource languages and posit that scaling alone may not be sufficient for improving the performance in this scenario"
    }
  },
  {
    "id": "abstract-2023--acl-industry--69",
    "result": [
      {
        "value": {
          "start": 720,
          "end": 725,
          "text": "CUPID",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-industry--69:E0"
      },
      {
        "value": {
          "start": 1016,
          "end": 1046,
          "text": "302 bps improvement on English",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-industry--69:E1"
      },
      {
        "value": {
          "start": 1051,
          "end": 1094,
          "text": "676 bps improvement for low-resource Arabic",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-industry--69:E2"
      },
      {
        "from_id": "abstract-2023--acl-industry--69:E0",
        "to_id": "abstract-2023--acl-industry--69:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-industry--69:E0",
        "to_id": "abstract-2023--acl-industry--69:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Relevance in E-commerce Product Search is crucial for providing customers with accurate results that match their query intent. With recent advancements in NLP and Deep Learning, Transformers have become the default choice for relevance classification tasks. In such a setting, the relevance model uses query text and product title as input features, and estimates if the product is relevant for the customer query. While cross-attention in Transformers enables a more accurate relevance prediction in such a setting, its high evaluation latency makes it unsuitable for real-time predictions in which thousands of products must be evaluated against a user query within few milliseconds. To address this issue, we propose CUPID: a Curriculum learning based real-time Prediction using Distillation that utilizes knowledge distillation within a curriculum learning setting to learn a simpler architecture that can be evaluated within low latency budgets. In a bi-lingual relevance prediction task, our approach shows an 302 bps improvement on English and 676 bps improvement for low-resource Arabic, while maintaining the low evaluation latency on CPUs."
    }
  },
  {
    "id": "abstract-2023--acl-industry--70",
    "result": [
      {
        "value": {
          "start": 497,
          "end": 541,
          "text": "Semantic Question Reformulation (SURF) model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-industry--70:E0"
      },
      {
        "value": {
          "start": 800,
          "end": 812,
          "text": "answer rates",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-industry--70:E1"
      },
      {
        "from_id": "abstract-2023--acl-industry--70:E0",
        "to_id": "abstract-2023--acl-industry--70:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Spoken Question Answering (QA) is a key feature of voice assistants, usually backed by multiple QA systems. Users ask questions via spontaneous speech that can contain disfluencies, errors, and informal syntax or phrasing. This is a major challenge in QA, causing unanswered questions or irrelevant answers, leading to bad user experiences. We analyze failed QA requests to identify core challenges: lexical gaps, proposition types, complex syntactic structure, and high specificity. We propose a Semantic Question Reformulation (SURF) model offering three linguistically-grounded operations (repair, syntactic reshaping, generalization) to rewrite questions to facilitate answering. Offline evaluation on 1M unanswered questions from a leading voice assistant shows that SURF significantly improves answer rates: up to 24% of previously unanswered questions obtain relevant answers (75%). Live deployment shows positive impact for millions of customers with unanswered questions; explicit relevance feedback shows high user satisfaction."
    }
  },
  {
    "id": "abstract-2023--acl-long--595",
    "result": [
      {
        "value": {
          "start": 452,
          "end": 478,
          "text": "two-birds-one-stone method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--595:E0"
      },
      {
        "value": {
          "start": 959,
          "end": 974,
          "text": "inference speed",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--595:E1"
      },
      {
        "value": {
          "start": 1005,
          "end": 1029,
          "text": "performance improvements",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--595:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--595:E0",
        "to_id": "abstract-2023--acl-long--595:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--595:E0",
        "to_id": "abstract-2023--acl-long--595:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Out-of-domain (OOD) intent classification is an active field of natural language understanding, which is of great practical significance for intelligent devices such as the Task-Oriented Dialogue System. It mainly contains two challenges: it requires the model to know what it knows and what it does not know. This paper investigates “overthinking” in the open-world scenario and its impact on OOD intent classification. Inspired by this, we propose a two-birds-one-stone method, which allows the model to decide whether to make a decision on OOD classification early during inference and can ensure accuracy and accelerate inference. At the same time, to adapt to the behavior of dynamic inference, we also propose a training method based on ensemble methods. In addition to bringing certain theoretical insights, we also conduct detailed experiments on three real-world intent datasets. Compared with the previous baselines, our method can not only improve inference speed, but also achieve significant performance improvements."
    }
  },
  {
    "id": "abstract-2023--acl-long--62",
    "result": [
      {
        "value": {
          "start": 1243,
          "end": 1262,
          "text": "competitive results",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--62:E0"
      },
      {
        "value": {
          "start": 1316,
          "end": 1345,
          "text": "robustness on unbalanced data",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--62:E1"
      }
    ],
    "data": {
      "text": "Emotion-Cause Pair Extraction (ECPE) aims to identify the document’s emotion clauses and corresponding cause clauses. Like other relation extraction tasks, ECPE is closely associated with the relationship between sentences. Recent methods based on Graph Convolutional Networks focus on how to model the multiplex relations between clauses by constructing different edges. However, the data of emotions, causes, and pairs are extremely unbalanced, and current methods get their representation using the same graph structure. In this paper, we propose a **J**oint **C**onstrained Learning framework with **B**oundary-adjusting for Emotion-Cause Pair Extraction (**JCB**). Specifically, through constrained learning, we summarize the prior rules existing in the data and force the model to take them into consideration in optimization, which helps the model learn a better representation from unbalanced data. Furthermore, we adjust the decision boundary of classifiers according to the relations between subtasks, which have always been ignored. No longer working independently as in the previous framework, the classifiers corresponding to three subtasks cooperate under the relation constraints. Experimental results show that **JCB** obtains competitive results compared with state-of-the-art methods and prove its robustness on unbalanced data."
    }
  },
  {
    "id": "abstract-2023--acl-long--589",
    "result": [
      {
        "value": {
          "start": 168,
          "end": 193,
          "text": "discriminative approaches",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--589:E0"
      },
      {
        "value": {
          "start": 684,
          "end": 721,
          "text": "zero-shot results on the T0 benchmark",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--589:E1"
      },
      {
        "value": {
          "start": 875,
          "end": 911,
          "text": "results on a wide range of NLP tasks",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--589:E2"
      },
      {
        "value": {
          "start": 992,
          "end": 1017,
          "text": "minimal prompting efforts",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--589:E3"
      },
      {
        "value": {
          "start": 1042,
          "end": 1052,
          "text": "robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--589:E4"
      },
      {
        "from_id": "abstract-2023--acl-long--589:E0",
        "to_id": "abstract-2023--acl-long--589:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--589:E0",
        "to_id": "abstract-2023--acl-long--589:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--589:E3",
        "to_id": "abstract-2023--acl-long--589:E4",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Generative modeling has been the dominant approach for large-scale pretraining and zero-shot generalization. In this work, we challenge this convention by showing that discriminative approaches perform substantially better than generative ones on a large number of NLP tasks. Technically, we train a single discriminator to predict whether a text sample comes from the true data distribution, similar to GANs. Since many NLP tasks can be formulated as selecting from a few options, we use this discriminator to predict the concatenation of input and which option has the highest probability of coming from the true data distribution. This simple formulation achieves state-of-the-art zero-shot results on the T0 benchmark, outperforming T0 by 16.0%, 7.8%, and 11.5% respectively on different scales. In the finetuning setting, our approach also achieves new state-of-the-art results on a wide range of NLP tasks, with only 1/4 parameters of previous methods. Meanwhile, our approach requires minimal prompting efforts, which largely improves robustness and is essential for real-world applications. Furthermore, we also jointly train a generalized UD in combination with generative tasks, which maintains its advantage on discriminative tasks and simultaneously works on generative tasks."
    }
  },
  {
    "id": "abstract-2023--acl-industry--54",
    "result": [
      {
        "value": {
          "start": 902,
          "end": 917,
          "text": "proposed method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-industry--54:E0"
      },
      {
        "value": {
          "start": 887,
          "end": 893,
          "text": "recall",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-industry--54:E1"
      },
      {
        "from_id": "abstract-2023--acl-industry--54:E0",
        "to_id": "abstract-2023--acl-industry--54:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Content moderation on social media is governed by policies that are intricate and frequently updated with evolving world events. However, automated content moderation systems often restrict easy adaptation to policy changes and are expected to learn policy intricacies from limited amounts of labeled data, which make effective policy compliance challenging. We propose to model content moderation as a binary question answering problem where the questions validate the loosely coupled themes constituting a policy. A decision logic is applied on top to aggregate the theme-specific validations. This way the questions pass theme information to a transformer network as explicit policy prompts, that in turn enables explainability. This setting further allows for faster adaptation to policy updates by leveraging zero-shot capabilities of pre-trained transformers. We showcase improved recall for our proposed method at 95\\% precision on two proprietary datasets of social media posts and comments respectively annotated under curated Hate Speech and Commercial Spam policies."
    }
  },
  {
    "id": "abstract-2023--acl-long--779",
    "result": [
      {
        "value": {
          "start": 486,
          "end": 511,
          "text": "performance of FEC models",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--779:E0"
      }
    ],
    "data": {
      "text": "Factuality is important to dialogue summarization. Factual error correction (FEC) of model-generated summaries is one way to improve factuality. Current FEC evaluation that relies on factuality metrics is not reliable and detailed enough. To address this problem, we are the first to manually annotate a FEC dataset for dialogue summarization containing 4000 items and propose FERRANTI, a fine-grained evaluation framework based on reference correction that automatically evaluates the performance of FEC models on different error categories. Using this evaluation framework, we conduct sufficient experiments with FEC approaches under a variety of settings and find the best training modes and significant differences in the performance of the existing approaches on different factual error categories."
    }
  },
  {
    "id": "abstract-2023--acl-long--717",
    "result": [
      {
        "value": {
          "start": 794,
          "end": 819,
          "text": "OOD detection performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--717:E0"
      }
    ],
    "data": {
      "text": "Out-of-distribution (OOD) detection is a critical task for reliable predictions over text. Fine-tuning with pre-trained language models has been a de facto procedure to derive OOD detectors with respect to in-distribution (ID) data. Despite its common use, the understanding of the role of fine-tuning and its necessity for OOD detection is largely unexplored. In this paper, we raise the question: is fine-tuning necessary for OOD detection? We present a study investigating the efficacy of directly leveraging pre-trained language models for OOD detection, without any model fine-tuning on the ID data. We compare the approach with several competitive fine-tuning objectives, and offer new insights under various types of distributional shifts. Extensive experiments demonstrate near-perfect OOD detection performance (with 0% FPR95 in many cases), strongly outperforming the fine-tuned counterpart."
    }
  },
  {
    "id": "abstract-2023--acl-long--544",
    "result": [
      {
        "value": {
          "start": 547,
          "end": 595,
          "text": "contrastive learning- and generation-based model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--544:E0"
      },
      {
        "value": {
          "start": 809,
          "end": 823,
          "text": "accuracy gains",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--544:E1"
      },
      {
        "value": {
          "start": 603,
          "end": 640,
          "text": "novel hard negative sampling strategy",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--544:E2"
      },
      {
        "value": {
          "start": 223,
          "end": 251,
          "text": "persona attribute extraction",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--544:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--544:E0",
        "to_id": "abstract-2023--acl-long--544:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--544:E2",
        "to_id": "abstract-2023--acl-long--544:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Persona attribute extraction is critical for personalized human-computer interaction. Dialogue is an important medium that communicates and delivers persona information. Although there is a public dataset for triplet-based persona attribute extraction from conversations, its automatically generated labels present many issues, including unspecific relations and inconsistent annotations. We fix such issues by leveraging more reliable text-label matching criteria to generate high-quality data for persona attribute extraction. We also propose a contrastive learning- and generation-based model with a novel hard negative sampling strategy for generalized zero-shot persona attribute extraction. We benchmark our model with state-of-the-art baselines on our dataset and a public dataset, showing outstanding accuracy gains. Our sampling strategy also exceeds others by a large margin in persona attribute extraction."
    }
  },
  {
    "id": "abstract-2023--acl-demo--13",
    "result": [
      {
        "value": {
          "start": 25,
          "end": 33,
          "text": "DIAGRAPH",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-demo--13:E0"
      },
      {
        "value": {
          "start": 1127,
          "end": 1162,
          "text": "System Usability Scale (SUS) scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-demo--13:E1"
      },
      {
        "from_id": "abstract-2023--acl-demo--13:E0",
        "to_id": "abstract-2023--acl-demo--13:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In this work, we present DIAGRAPH, an open-source graphical dialog flow editor built on the ADVISER toolkit. Our goal for this tool is threefold: 1) To support subject-experts to intuitively create complex and flexible dialog systems,2) To support rapid prototyping of dialog system behavior, e.g., for research, and 3) To provide a hands-on test bed for students learning about dialog systems. To facilitate this, DIAGRAPH aims to provide a clean and intuitive graphical interface for creating dialog systems without requiring any coding knowledge. Once a dialog graph has been created, it is automatically turned into a dialog system using state of the art language models. This allows for rapid prototyping and testing. Dialog designers can then distribute a link to their finished dialog system or embed it into a website.Additionally, to support scientific experiments and data collection, dialog designers can access chat logs. Finally, to verify the usability of DIAGRAPH, we performed evaluation with subject-experts who extensively worked with the tool and users testing it for the first time, receiving above average System Usability Scale (SUS) scores from both (82 out 100 and 75 out of 100, respectively).In this way, we hope DIAGRAPH helps reduce the barrier to entry for creating dialog interactions."
    }
  },
  {
    "id": "abstract-2023--acl-short--32",
    "result": [
      {
        "value": {
          "start": 608,
          "end": 663,
          "text": "cross-lingual contextualised token embeddings alignment",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--32:E0"
      }
    ],
    "data": {
      "text": "Vision-Language Pre-training (VLP) has advanced the performance of many vision-language tasks, such as image-text retrieval, visual entailment, and visual reasoning. The pre-training mostly utilizes lexical databases and image queries in English. Previous work has demonstrated that the pre-training in English does not transfer well to other languages in a zero-shot setting. However, multilingual pre-trained language models (MPLM) have excelled at a variety of single-modal language tasks. In this paper, we propose a simple yet efficient approach to adapt VLP to unseen languages using MPLM.We utilize a cross-lingual contextualised token embeddings alignment approach to train text encoders for non-English languages. Our approach does not require image input and primarily uses machine translation, eliminating the need for target language data. Our evaluation across three distinct tasks (image-text retrieval, visual entailment, and natural language visual reasoning) demonstrates that this approach outperforms the state-of-the-art multilingual vision-language models without requiring large parallel corpora. Our code is available athttps://github.com/Yasminekaroui/CliCoTea."
    }
  },
  {
    "id": "abstract-2023--acl-long--627",
    "result": [
      {
        "value": {
          "start": 488,
          "end": 529,
          "text": "multi-grained knowledge retriever (MAKER)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--627:E0"
      }
    ],
    "data": {
      "text": "Retrieving proper domain knowledge from an external database lies at the heart of end-to-end task-oriented dialog systems to generate informative responses. Most existing systems blend knowledge retrieval with response generation and optimize them with direct supervision from reference responses, leading to suboptimal retrieval performance when the knowledge base becomes large-scale. To address this, we propose to decouple knowledge retrieval from response generation and introduce a multi-grained knowledge retriever (MAKER) that includes an entity selector to search for relevant entities and an attribute selector to filter out irrelevant attributes. To train the retriever, we propose a novel distillation objective that derives supervision signals from the response generator. Experiments conducted on three standard benchmarks with both small and large-scale knowledge bases demonstrate that our retriever performs knowledge retrieval more effectively than existing methods. Our code has been made publicly available athttps://github.com/18907305772/MAKER."
    }
  },
  {
    "id": "abstract-2023--acl-long--490",
    "result": [
      {
        "value": {
          "start": 298,
          "end": 303,
          "text": "BLIND",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--490:E0"
      }
    ],
    "data": {
      "text": "Models trained on real-world data tend to imitate and amplify social biases. Common methods to mitigate biases require prior information on the types of biases that should be mitigated (e.g., gender or racial bias) and the social groups associated with each data sample. In this work, we introduce BLIND, a method for bias removal with no prior knowledge of the demographics in the dataset. While training a model on a downstream task, BLIND detects biased samples using an auxiliary model that predicts the main model’s success, and down-weights those samples during the training process. Experiments with racial and gender biases in sentiment classification and occupation classification tasks demonstrate that BLIND mitigates social biases without relying on a costly demographic annotation process. Our method is competitive with other methods that require demographic information and sometimes even surpasses them."
    }
  },
  {
    "id": "abstract-2023--acl-long--380",
    "result": [
      {
        "value": {
          "start": 285,
          "end": 296,
          "text": "TemplateGEC",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--380:E0"
      },
      {
        "value": {
          "start": 753,
          "end": 781,
          "text": "effectiveness and robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--380:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--380:E0",
        "to_id": "abstract-2023--acl-long--380:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Grammatical error correction (GEC) can be divided into sequence-to-edit (Seq2Edit) and sequence-to-sequence (Seq2Seq) frameworks, both of which have their pros and cons. To utilize the strengths and make up for the shortcomings of these frameworks, this paper proposes a novel method, TemplateGEC, which capitalizes on the capabilities of both Seq2Edit and Seq2Seq frameworks in error detection and correction respectively. TemplateGEC utilizes the detection labels from a Seq2Edit model, to construct the template as the input. A Seq2Seq model is employed to enforce consistency between the predictions of different templates by utilizing consistency learning. Experimental results on the Chinese NLPCC18, English BEA19 and CoNLL14 benchmarks show the effectiveness and robustness of TemplateGEC.Further analysis reveals the potential of our method in performing human-in-the-loop GEC. Source code and scripts are available athttps://github.com/li-aolong/TemplateGEC."
    }
  },
  {
    "id": "abstract-2023--acl-long--91",
    "result": [
      {
        "value": {
          "start": 294,
          "end": 307,
          "text": "neural models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--91:E0"
      },
      {
        "value": {
          "start": 717,
          "end": 743,
          "text": "reduction in edit distance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--91:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--91:E0",
        "to_id": "abstract-2023--acl-long--91:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We present a state-of-the-art neural approach to the unsupervised reconstruction of ancient word forms. Previous work in this domain used expectation-maximization to predict simple phonological changes between ancient word forms and their cognates in modern languages. We extend this work with neural models that can capture more complicated phonological and morphological changes. At the same time, we preserve the inductive biases from classical methods by building monotonic alignment constraints into the model and deliberately underfitting during the maximization step. We evaluate our performance on the task of reconstructing Latin from a dataset of cognates across five Romance languages, achieving a notable reduction in edit distance from the target word forms compared to previous methods."
    }
  },
  {
    "id": "abstract-2023--acl-industry--60",
    "result": [
      {
        "value": {
          "start": 673,
          "end": 699,
          "text": "pretraining on public data",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-industry--60:E0"
      },
      {
        "value": {
          "start": 326,
          "end": 333,
          "text": "utility",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-industry--60:E1"
      },
      {
        "from_id": "abstract-2023--acl-industry--60:E0",
        "to_id": "abstract-2023--acl-industry--60:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We train and deploy language models (LMs) with federated learning (FL) and differential privacy (DP) in Google Keyboard (Gboard). The recent DP-Follow the Regularized Leader (DP-FTRL) algorithm is applied to achieve meaningfully formal DP guarantees without requiring uniform sampling of clients. To provide favorable privacy-utility trade-offs, we introduce a new client participation criterion and discuss the implication of its configuration in large scale systems. We show how quantile-based clip estimation can be combined with DP-FTRL to adaptively choose the clip norm during training or reduce the hyperparameter tuning in preparation of training. With the help of pretraining on public data, we trained and deployed more than fifteen Gboard LMs that achieve high utility and $\\rho-$zCDP privacy guarantees with $\\rho \\in (0.3, 2)$, with one model additionally trained with secure aggregation. We summarize our experience and provide concrete suggestions on DP training for practitioners."
    }
  },
  {
    "id": "abstract-2023--acl-short--137",
    "result": [
      {
        "value": {
          "start": 572,
          "end": 625,
          "text": "novel ensemble strategies based on Transformer models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--137:E0"
      },
      {
        "value": {
          "start": 637,
          "end": 673,
          "text": "robustness to structural constraints",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--137:E1"
      },
      {
        "value": {
          "start": 699,
          "end": 717,
          "text": "computational time",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--137:E2"
      },
      {
        "from_id": "abstract-2023--acl-short--137:E0",
        "to_id": "abstract-2023--acl-short--137:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-short--137:E0",
        "to_id": "abstract-2023--acl-short--137:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "In this paper, we examine the current state-of-the-art in AMR parsing, which relies on ensemble strategies by merging multiple graph predictions. Our analysis reveals that the present models often violate AMR structural constraints. To address this issue, we develop a validation method, and show how ensemble models can exploit SMATCH metric weaknesses to obtain higher scores, but sometimes result in corrupted graphs. Additionally, we highlight the demanding need to compute the SMATCH score among all possible predictions. To overcome these challenges, we propose two novel ensemble strategies based on Transformer models, improving robustness to structural constraints, while also reducing the computational time. Our methods provide new insights for enhancing AMR parsers and metrics. Our code is available at [https://www.github.com/babelscape/AMRs-Assemble](https://www.github.com/babelscape/AMRs-Assemble)."
    }
  },
  {
    "id": "abstract-2023--acl-long--789",
    "result": [
      {
        "value": {
          "start": 262,
          "end": 267,
          "text": "Tomea",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--789:E0"
      }
    ],
    "data": {
      "text": "Moral rhetoric influences our judgement. Although social scientists recognize moral expression as domain specific, there are no systematic methods for analyzing whether a text classifier learns the domain-specific expression of moral language or not. We propose Tomea, a method to compare a supervised classifier’s representation of moral rhetoric across domains. Tomea enables quantitative and qualitative comparisons of moral rhetoric via an interpretable exploration of similarities and differences across moral concepts and domains. We apply Tomea on moral narratives in thirty-five thousand tweets from seven domains. We extensively evaluate the method via a crowd study, a series of cross-domain moral classification comparisons, and a qualitative analysis of cross-domain moral expression."
    }
  },
  {
    "id": "abstract-2023--acl-short--111",
    "result": [
      {
        "value": {
          "start": 272,
          "end": 289,
          "text": "event type prompt",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--111:E0"
      },
      {
        "value": {
          "start": 316,
          "end": 343,
          "text": "event detection performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--111:E1"
      },
      {
        "value": {
          "start": 75,
          "end": 92,
          "text": "unified framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--111:E2"
      },
      {
        "value": {
          "start": 547,
          "end": 559,
          "text": "F-score gain",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--111:E3"
      },
      {
        "from_id": "abstract-2023--acl-short--111:E0",
        "to_id": "abstract-2023--acl-short--111:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-short--111:E2",
        "to_id": "abstract-2023--acl-short--111:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We compare various forms of prompts to represent event types and develop a unified framework to incorporate the event type specific prompts for supervised, few-shot, and zero-shot event detection. The experimental results demonstrate that a well-defined and comprehensive event type prompt can significantly improve event detection performance, especially when the annotated data is scarce (few-shot event detection) or not available (zero-shot event detection). By leveraging the semantics of event types, our unified framework shows up to 22.2% F-score gain over the previous state-of-the-art baselines."
    }
  },
  {
    "id": "abstract-2023--acl-long--731",
    "result": [
      {
        "value": {
          "start": 304,
          "end": 314,
          "text": "Multi-DYLE",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--731:E0"
      },
      {
        "value": {
          "start": 741,
          "end": 754,
          "text": "ROUGE-1 score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--731:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--731:E0",
        "to_id": "abstract-2023--acl-long--731:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "To enhance the explainability of meeting summarization, we construct a new dataset called “ExplainMeetSum,” an augmented version of QMSum, by newly annotating evidence sentences that faithfully “explain” a summary. Using ExplainMeetSum, we propose a novel multiple extractor guided summarization, namely Multi-DYLE, which extensively generalizes DYLE to enable using a supervised extractor based on human-aligned extractive oracles. We further present an explainability-aware task, named “Explainable Evidence Extraction” (E3), which aims to automatically detect all evidence sentences that support a given summary. Experimental results on the QMSum dataset show that the proposed Multi-DYLE outperforms DYLE with gains of up to 3.13 in the ROUGE-1 score. We further present the initial results on the E3 task, under the settings using separate and joint evaluation metrics."
    }
  },
  {
    "id": "abstract-2023--acl-long--139",
    "result": [
      {
        "value": {
          "start": 610,
          "end": 657,
          "text": "Dynamic Routing Transformer Network (DynRT-Net)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--139:E0"
      },
      {
        "value": {
          "start": 868,
          "end": 881,
          "text": "effectiveness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--139:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--139:E0",
        "to_id": "abstract-2023--acl-long--139:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Multimodal sarcasm detection is an important research topic in natural language processing and multimedia computing, and benefits a wide range of applications in multiple domains. Most existing studies regard the incongruity between image and text as the indicative clue in identifying multimodal sarcasm. To capture cross-modal incongruity, previous methods rely on fixed architectures in network design, which restricts the model from dynamically adjusting to diverse image-text pairs. Inspired by routing-based dynamic network, we model the dynamic mechanism in multimodal sarcasm detection and propose the Dynamic Routing Transformer Network (DynRT-Net). Our method utilizes dynamic paths to activate different routing transformer modules with hierarchical co-attention adapting to cross-modal incongruity. Experimental results on a public dataset demonstrate the effectiveness of our method compared to the state-of-the-art methods. Our codes are available athttps://github.com/TIAN-viola/DynRT."
    }
  },
  {
    "id": "abstract-2023--acl-long--816",
    "result": [
      {
        "value": {
          "start": 336,
          "end": 363,
          "text": "relative position embedding",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--816:E0"
      },
      {
        "value": {
          "start": 180,
          "end": 200,
          "text": "attention resolution",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--816:E1"
      },
      {
        "value": {
          "start": 426,
          "end": 452,
          "text": "blockwise causal attention",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--816:E2"
      },
      {
        "value": {
          "start": 591,
          "end": 600,
          "text": "our model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--816:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--816:E0",
        "to_id": "abstract-2023--acl-long--816:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--816:E2",
        "to_id": "abstract-2023--acl-long--816:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Position modeling plays a critical role in Transformers. In this paper, we focus on length extrapolation, i.e., training on short texts while evaluating longer sequences. We defineattention resolutionas an indicator of extrapolation. Then we propose two designs to improve the above metric of Transformers. Specifically, we introduce a relative position embedding to explicitly maximize attention resolution. Moreover, we use blockwise causal attention during inference for better resolution. We evaluate different Transformer variants with language modeling. Experimental results show that our model achieves strong performance in both interpolation and extrapolation settings. The code will be available athttps://aka.ms/LeX-Transformer."
    }
  },
  {
    "id": "abstract-2023--acl-long--842",
    "result": [
      {
        "value": {
          "start": 328,
          "end": 333,
          "text": "CREST",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--842:E0"
      },
      {
        "value": {
          "start": 521,
          "end": 543,
          "text": "counterfactual quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--842:E1"
      },
      {
        "value": {
          "start": 545,
          "end": 561,
          "text": "model robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--842:E2"
      },
      {
        "value": {
          "start": 105,
          "end": 121,
          "text": "interpretability",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--842:E3"
      },
      {
        "value": {
          "start": 984,
          "end": 1001,
          "text": "rationale quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--842:E4"
      },
      {
        "from_id": "abstract-2023--acl-long--842:E0",
        "to_id": "abstract-2023--acl-long--842:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--842:E0",
        "to_id": "abstract-2023--acl-long--842:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--842:E0",
        "to_id": "abstract-2023--acl-long--842:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Selective rationales and counterfactual examples have emerged as two effective, complementary classes of interpretability methods for analyzing and training NLP models. However, prior work has not explored how these methods can be integrated to combine their complementary advantages. We overcome this limitation by introducing CREST (ContRastive Edits with Sparse raTionalization), a joint framework for selective rationalization and counterfactual text generation, and show that this framework leads to improvements in counterfactual quality, model robustness, and interpretability. First, CREST generates valid counterfactuals that are more natural than those produced by previous methods, and subsequently can be used for data augmentation at scale, reducing the need for human-generated examples. Second, we introduce a new loss function that leverages CREST counterfactuals to regularize selective rationales and show that this regularization improves both model robustness and rationale quality, compared to methods that do not leverage CREST counterfactuals. Our results demonstrate that CREST successfully bridges the gap between selective rationales and counterfactual examples, addressing the limitations of existing methods and providing a more comprehensive view of a model’s predictions."
    }
  },
  {
    "id": "abstract-2023--acl-short--56",
    "result": [
      {
        "value": {
          "start": 505,
          "end": 526,
          "text": "novel training method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--56:E0"
      },
      {
        "value": {
          "start": 890,
          "end": 899,
          "text": "BERTScore",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--56:E1"
      },
      {
        "from_id": "abstract-2023--acl-short--56:E0",
        "to_id": "abstract-2023--acl-short--56:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "An important problem of the sequence-to-sequence neural models widely used in abstractive summarization is exposure bias. To alleviate this problem, re-ranking systems have been applied in recent years. Despite some performance improvements, this approach remains underexplored. Previous works have mostly specified the rank through the ROUGE score and aligned candidate summaries, but there can be quite a large gap between the lexical overlap metric and semantic similarity. In this paper, we propose a novel training method in which a re-ranker balances the lexical and semantic quality. We further newly define false positives in ranking and present a strategy to reduce their influence. Experiments on the CNN/DailyMail and XSum datasets show that our method can estimate the meaning of summaries without seriously degrading the lexical aspect. More specifically, it achieves an 89.67 BERTScore on the CNN/DailyMail dataset, reaching new state-of-the-art performance. Our code is publicly available athttps://github.com/jeewoo1025/BalSum."
    }
  },
  {
    "id": "abstract-2023--acl-short--27",
    "result": [
      {
        "value": {
          "start": 691,
          "end": 706,
          "text": "proposed method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--27:E0"
      }
    ],
    "data": {
      "text": "Multimodal relation extraction (MRE) is the task of identifying the semantic relationships between two entities based on the context of the sentence image pair. Existing retrieval-augmented approaches mainly focused on modeling the retrieved textual knowledge, but this may not be able to accurately identify complex relations. To improve the prediction, this research proposes to retrieve textual and visual evidence based on the object, sentence, and whole image. We further develop a novel approach to synthesize the object-level, image-level, and sentence-level information for better reasoning between the same and different modalities. Extensive experiments and analyses show that the proposed method is able to effectively select and compare evidence across modalities and significantly outperforms state-of-the-art models."
    }
  },
  {
    "id": "abstract-2023--acl-long--886",
    "result": [
      {
        "value": {
          "start": 259,
          "end": 263,
          "text": "DICE",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--886:E0"
      }
    ],
    "data": {
      "text": "Event extraction for the clinical domain is an under-explored research area. The lack of training data along with the high volume of domain-specific terminologies with vague entity boundaries makes the task especially challenging. In this paper, we introduce DICE, a robust and data-efficient generative model for clinical event extraction. DICE frames event extraction as a conditional generation problem and introduces a contrastive learning objective to accurately decide the boundaries of biomedical mentions. DICE also trains an auxiliary mention identification task jointly with event extraction tasks to better identify entity mention boundaries, and further introduces special markers to incorporate identified entity mentions as trigger and argument candidates for their respective tasks. To benchmark clinical event extraction, we compose MACCROBAT-EE, the first clinical event extraction dataset with argument annotation, based on an existing clinical information extraction dataset MACCROBAT. Our experiments demonstrate state-of-the-art performances of DICE for clinical and news domain event extraction, especially under low data settings."
    }
  },
  {
    "id": "abstract-2023--acl-long--271",
    "result": [
      {
        "value": {
          "start": 534,
          "end": 589,
          "text": "two-stage Differentially Private (DP) generation method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--271:E0"
      },
      {
        "value": {
          "start": 718,
          "end": 723,
          "text": "MAUVE",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--271:E1"
      },
      {
        "value": {
          "start": 736,
          "end": 768,
          "text": "parse tree function-type overlap",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--271:E2"
      },
      {
        "value": {
          "start": 1085,
          "end": 1093,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--271:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--271:E0",
        "to_id": "abstract-2023--acl-long--271:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--271:E0",
        "to_id": "abstract-2023--acl-long--271:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--271:E0",
        "to_id": "abstract-2023--acl-long--271:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Task-oriented dialogue systems often assist users with personal or confidential matters. For this reason, the developers of such a system are generally prohibited from observing actual usage. So how can they know where the system is failing and needs more training data or new functionality? In this work, we study ways in which realistic user utterances can be generated synthetically, to help increase the linguistic and functional coverage of the system, without compromising the privacy of actual users. To this end, we propose a two-stage Differentially Private (DP) generation method which first generates latent semantic parses, and then generates utterances based on the parses. Our proposed approach improves MAUVE by 2.5X and parse tree function-type overlap by 1.3X relative to current approaches for private synthetic data generation, improving both on fluency and semantic coverage. We further validate our approach on a realistic domain adaptation task of adding new functionality from private user data to a semantic parser, and show overall gains of 8.5% points on its accuracy with the new feature."
    }
  },
  {
    "id": "abstract-2023--acl-short--8",
    "result": [
      {
        "value": {
          "start": 465,
          "end": 474,
          "text": "influence",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--8:E0"
      }
    ],
    "data": {
      "text": "Social science and psycholinguistic research have shown that power and status affect how people use language in a range of domains. Here, we investigate a similar question in a large, distributed, consensus-driven community with little traditional power hierarchy – the Internet Engineering Task Force (IETF), a collaborative organisation that designs internet standards. Our analysis based on lexical categories (LIWC) and BERT, shows that participants’ levels of influence can be predicted from their email text, and identify key linguistic differences (e.g., certain LIWC categories, such as “WE” are positively correlated with high-influence). We also identify the differences in language use for the same person before and after becoming influential."
    }
  },
  {
    "id": "abstract-2023--acl-long--749",
    "result": [
      {
        "value": {
          "start": 1204,
          "end": 1267,
          "text": "state-of-the-art results on eight multi-modal dialog benchmarks",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--749:E0"
      }
    ],
    "data": {
      "text": "Perceiving multi-modal information and fulfilling dialogues with humans is a long-term goal of artificial intelligence. Pre-training is commonly regarded as an effective approach for multi-modal dialogue. However, due to the limited availability of multi-modal dialogue data, there is still scarce research on multi-modal dialogue pre-training. Yet another intriguing challenge emerges from the encompassing nature of multi-modal dialogue, which involves various modalities and tasks. Moreover, new forms of tasks may arise at unpredictable points in the future. Hence, it is essential for designed multi-modal dialogue models to possess sufficient flexibility to adapt to such scenarios. This paper proposes PaCE, a unified, structured, compositional multi-modal dialogue pre-training framework. It utilizes a combination of several fundamental experts to accommodate multiple dialogue-related tasks and can be pre-trained using limited dialogue and extensive non-dialogue multi-modal data. Furthermore, we propose a progressive training method where old experts from the past can assist new experts, facilitating the expansion of their capabilities. Experimental results demonstrate that PaCE achieves state-of-the-art results on eight multi-modal dialog benchmarks."
    }
  },
  {
    "id": "abstract-2023--acl-long--256",
    "result": [
      {
        "value": {
          "start": 701,
          "end": 738,
          "text": "Unified Demonstration Retriever (UDR)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--256:E0"
      }
    ],
    "data": {
      "text": "In-context learning is a new learning paradigm where a language model conditions on a few input-output pairs (demonstrations) and a test input, and directly outputs the prediction. It has been shown sensitive to the provided demonstrations and thus promotes the research of demonstration retrieval: given a test input, relevant examples are retrieved from the training set to serve as informative demonstrations for in-context learning. While previous works train task-specific retrievers for several tasks separately, these methods are hard to transfer and scale on various tasks, and separately trained retrievers will cause a lot of parameter storage and deployment cost. In this paper, we propose Unified Demonstration Retriever (UDR), a single model to retrieve demonstrations for a wide range of tasks. To train UDR, we cast various tasks’ training signals into a unified list-wise ranking formulation by language model’s feedback. Then we propose a multi-task list-wise ranking training framework with an iterative mining strategy to find high-quality candidates, which can help UDR fully incorporate various tasks’ signals. Experiments on 30+ tasks across 13 task families and multiple data domains show that UDR significantly outperforms baselines. Further analyses show the effectiveness of each proposed component and UDR’s strong ability in various scenarios including different LMs (1.3B 175B), unseen datasets, varying demonstration quantities, etc. We will release the code and model checkpoint after review."
    }
  },
  {
    "id": "abstract-2023--acl-short--84",
    "result": [
      {
        "value": {
          "start": 431,
          "end": 480,
          "text": "contrastive clustering-based bootstrapping method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--84:E0"
      }
    ],
    "data": {
      "text": "Traditional text classification typically categorizes texts into pre-defined coarse-grained classes, from which the produced models cannot handle the real-world scenario where finer categories emerge periodically for accurate services. In this work, we investigate the setting where fine-grained classification is done only using the annotation of coarse-grained categories and the coarse-to-fine mapping. We propose a lightweight contrastive clustering-based bootstrapping method to iteratively refine the labels of passages. During clustering, it pulls away negative passage-prototype pairs under the guidance of the mapping from both global and local perspectives. Experiments on NYT and 20News show that our method outperforms the state-of-the-art methods by a large margin."
    }
  },
  {
    "id": "abstract-2023--acl-industry--61",
    "result": [
      {
        "value": {
          "start": 574,
          "end": 597,
          "text": "knowledge-aware masking",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-industry--61:E0"
      }
    ],
    "data": {
      "text": "Most natural language tasks in the radiology domain use language models pre-trained on biomedical corpus. There are few pretrained language models trained specifically for radiology, and fewer still that have been trained in a low data setting and gone on to produce comparable results in fine-tuning tasks. We present RadLing, a continuously pretrained language model using ELECTRA-small architecture, trained using over 500K radiology reports that can compete with state-of-the-art results for fine tuning tasks in radiology domain. Our main contribution in this paper is knowledge-aware masking which is an taxonomic knowledge-assisted pre-training task that dynamically masks tokens to inject knowledge during pretraining. In addition, we also introduce an knowledge base-aided vocabulary extension to adapt the general tokenization vocabulary to radiology domain."
    }
  },
  {
    "id": "abstract-2023--acl-short--146",
    "result": [
      {
        "value": {
          "start": 248,
          "end": 258,
          "text": "NarrowBERT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--146:E0"
      },
      {
        "value": {
          "start": 310,
          "end": 358,
          "text": "throughput for masked language model pretraining",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--146:E1"
      },
      {
        "value": {
          "start": 663,
          "end": 691,
          "text": "throughput at inference time",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--146:E2"
      },
      {
        "from_id": "abstract-2023--acl-short--146:E0",
        "to_id": "abstract-2023--acl-short--146:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-short--146:E0",
        "to_id": "abstract-2023--acl-short--146:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Large-scale language model pretraining is a very successful form of self-supervised learning in natural language processing, but it is increasingly expensive to perform as the models and pretraining corpora have become larger over time. We propose NarrowBERT, a modified transformer encoder that increases the throughput for masked language model pretraining by more than 2x. NarrowBERT sparsifies the transformer model such that the self-attention queries and feedforward layers only operate on the masked tokens of each sentence during pretraining, rather than all of the tokens as with the usual transformer encoder. We also show that NarrowBERT increases the throughput at inference time by as much as 3.5x with minimal (or no) performance degradation on sentence encoding tasks like MNLI. Finally, we examine the performance of NarrowBERT on the IMDB and Amazon reviews classification and CoNLL NER tasks and show that it is also comparable to standard BERT performance."
    }
  },
  {
    "id": "abstract-2023--acl-long--130",
    "result": [
      {
        "value": {
          "start": 538,
          "end": 594,
          "text": "constructing the optimal policy online via binary search",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--130:E0"
      },
      {
        "value": {
          "start": 833,
          "end": 885,
          "text": "exceed strong baselines across all latency scenarios",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--130:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--130:E0",
        "to_id": "abstract-2023--acl-long--130:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Simultaneous machine translation (SiMT) starts to output translation while reading the source sentence and needs a precise policy to decide when to output the generated translation. Therefore, the policy determines the number of source tokens read during the translation of each target token. However, it is difficult to learn a precise translation policy to achieve good latency-quality trade-offs, because there is no golden policy corresponding to parallel sentences as explicit supervision. In this paper, we present a new method for constructing the optimal policy online via binary search. By employing explicit supervision, our approach enables the SiMT model to learn the optimal policy, which can guide the model in completing the translation during inference. Experiments on four translation tasks show that our method can exceed strong baselines across all latency scenarios."
    }
  },
  {
    "id": "abstract-2023--acl-industry--12",
    "result": [
      {
        "value": {
          "start": 1021,
          "end": 1049,
          "text": "translation-based approaches",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-industry--12:E0"
      },
      {
        "value": {
          "start": 482,
          "end": 499,
          "text": "candidate ranking",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-industry--12:E1"
      },
      {
        "value": {
          "start": 1097,
          "end": 1120,
          "text": "multilingual finetuning",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-industry--12:E2"
      },
      {
        "value": {
          "start": 607,
          "end": 624,
          "text": "answer generation",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-industry--12:E3"
      },
      {
        "from_id": "abstract-2023--acl-industry--12:E0",
        "to_id": "abstract-2023--acl-industry--12:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-industry--12:E2",
        "to_id": "abstract-2023--acl-industry--12:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Product Question Answering (PQA) systems are key in e-commerce applications as they provide responses to customers’ questions as they shop for products. While existing work on PQA focuses mainly on English, in practice there is need to support multiple customer languages while leveraging product information available in English. To study this practical industrial task, we present xPQA, a large-scale annotated cross-lingual PQA dataset in 12 languages, and report results in (1) candidate ranking, to select the best English candidate containing the information to answer a non-English question; and (2) answer generation, to generate a natural-sounding non-English answer based on the selected English candidate. We evaluate various approaches involving machine translation at runtime or offline, leveraging multilingual pre-trained LMs, and including or excluding xPQA training data. We find that in-domain data is essential as cross-lingual rankers trained on other domains perform poorly on the PQA task, and that translation-based approaches are most effective for candidate ranking while multilingual finetuning works best for answer generation. Still, there remains a significant performance gap between the English and the cross-lingual test sets."
    }
  },
  {
    "id": "abstract-2023--acl-long--739",
    "result": [
      {
        "value": {
          "start": 664,
          "end": 709,
          "text": "consistent objectives of contrastive learning",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--739:E0"
      },
      {
        "value": {
          "start": 1064,
          "end": 1077,
          "text": "effectiveness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--739:E1"
      },
      {
        "value": {
          "start": 864,
          "end": 893,
          "text": "multi-center contrastive loss",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--739:E2"
      },
      {
        "value": {
          "start": 1081,
          "end": 1091,
          "text": "our method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--739:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--739:E0",
        "to_id": "abstract-2023--acl-long--739:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Relation extraction (RE), which has relied on structurally annotated corpora for model training, has been particularly challenging in low-resource scenarios and domains. Recent literature has tackled low-resource RE by self-supervised learning, where the solution involves pretraining the entity pair embedding by RE-based objective and finetuning on labeled data by classification-based objective. However, a critical challenge to this approach is the gap in objectives, which prevents the RE model from fully utilizing the knowledge in pretrained representations. In this paper, we aim at bridging the gap and propose to pretrain and finetune the RE model using consistent objectives of contrastive learning. Since in this kind of representation learning paradigm, one relation may easily form multiple clusters in the representation space, we further propose a multi-center contrastive loss that allows one relation to form multiple clusters to better align with pretraining. Experiments on two document-level RE datasets, BioRED and Re-DocRED, demonstrate the effectiveness of our method. Particularly, when using 1% end-task training data, our method outperforms PLM-based RE classifier by 10.5% and 6.1% on the two datasets, respectively."
    }
  },
  {
    "id": "abstract-2023--acl-long--447",
    "result": [
      {
        "value": {
          "start": 488,
          "end": 495,
          "text": "ParaAMR",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--447:E0"
      },
      {
        "value": {
          "start": 940,
          "end": 959,
          "text": "sentence embeddings",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--447:E1"
      },
      {
        "value": {
          "start": 961,
          "end": 1007,
          "text": "syntactically controlled paraphrase generation",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--447:E2"
      },
      {
        "value": {
          "start": 1013,
          "end": 1052,
          "text": "data augmentation for few-shot learning",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--447:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--447:E0",
        "to_id": "abstract-2023--acl-long--447:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--447:E0",
        "to_id": "abstract-2023--acl-long--447:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--447:E0",
        "to_id": "abstract-2023--acl-long--447:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Paraphrase generation is a long-standing task in natural language processing (NLP). Supervised paraphrase generation models, which rely on human-annotated paraphrase pairs, are cost-inefficient and hard to scale up. On the other hand, automatically annotated paraphrase pairs (e.g., by machine back-translation), usually suffer from the lack of syntactic diversity – the generated paraphrase sentences are very similar to the source sentences in terms of syntax. In this work, we present ParaAMR, a large-scale syntactically diverse paraphrase dataset created by abstract meaning representation back-translation. Our quantitative analysis, qualitative examples, and human evaluation demonstrate that the paraphrases of ParaAMR are syntactically more diverse compared to existing large-scale paraphrase datasets while preserving good semantic similarity. In addition, we show that ParaAMR can be used to improve on three NLP tasks: learning sentence embeddings, syntactically controlled paraphrase generation, and data augmentation for few-shot learning. Our results thus showcase the potential of ParaAMR for improving various NLP applications."
    }
  },
  {
    "id": "abstract-2023--acl-long--690",
    "result": [
      {
        "value": {
          "start": 1103,
          "end": 1117,
          "text": "SST-5 accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--690:E0"
      },
      {
        "value": {
          "start": 1162,
          "end": 1175,
          "text": "QNLI accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--690:E1"
      },
      {
        "value": {
          "start": 1214,
          "end": 1227,
          "text": "NMLI accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--690:E2"
      }
    ],
    "data": {
      "text": "Prompt-tuning based few-shot learning has garnered increasing attention in recent years due to its efficiency and promising capability. To achieve the best performance for NLP tasks with just a few samples, it is vital to include as many informative samples as possible and to avoid misleading ones. However, there is no work in prompt-tuning literature addressing the problem of differentiating informative hard samples from misleading ones in model training, which is challenging due to the lack of supervision signals about the quality of the samples to train a well-performed model. We propose a Hard Sample Aware Prompt-Tuning framework (i.e. HardPT) to solve the non-differentiable problem in hard sample identification with reinforcement learning, and to strengthen the discrimination of the feature space without changing the original data distribution via an adaptive contrastive learning method. An extensive empirical study on a series of NLP tasks demonstrates the capability of HardPT in few-shot scenarios. HardPT obtains new SOTA results on all evaluated NLP tasks, including pushing the SST-5 accuracy to 49.5% (1.1% point absolute improvement), QNLI accuracy to 74.6% (1.9% absolute improvement), NMLI accuracy to 71.5 (0.7% absolute improvement), TACREVF1-score to 28.2 (1.0 absolute improvement), and i2b2/VAF1-score to 41.2 (1.3 absolute improvement)."
    }
  },
  {
    "id": "abstract-2023--acl-long--90",
    "result": [
      {
        "value": {
          "start": 159,
          "end": 170,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--90:E0"
      },
      {
        "value": {
          "start": 550,
          "end": 559,
          "text": "our model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--90:E1"
      },
      {
        "value": {
          "start": 619,
          "end": 633,
          "text": "gap with GPT-3",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--90:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--90:E1",
        "to_id": "abstract-2023--acl-long--90:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In question answering requiring common sense, language models (e.g., GPT-3) have been used to generate text expressing background knowledge that helps improve performance. Yet the cost of working with such models is very high; in this work, we finetune smaller language models to generate useful intermediate context, referred to here as elaborations. Our framework alternates between updating two language models—an elaboration generator and an answer predictor—allowing each to influence the other. Using less than 0.5% of the parameters of GPT-3, our model outperforms alternatives with similar sizes and closes the gap with GPT-3 on four commonsense question answering benchmarks. Human evaluations show that the quality of the generated elaborations is high."
    }
  },
  {
    "id": "abstract-2023--acl-long--374",
    "result": [
      {
        "value": {
          "start": 694,
          "end": 700,
          "text": "CLEVER",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--374:E0"
      },
      {
        "value": {
          "start": 1147,
          "end": 1160,
          "text": "effectiveness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--374:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--374:E0",
        "to_id": "abstract-2023--acl-long--374:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Fact verification aims to automatically judge the veracity of a claim according to several pieces of evidence. Due to the manual construction of datasets, spurious correlations between claim patterns and its veracity (i.e., biases) inevitably exist. Recent studies show that models usually learn such biases instead of understanding the semantic relationship between the claim and evidence. Existing debiasing works can be roughly divided into data-augmentation-based and weight-regularization-based pipeline, where the former is inflexible and the latter relies on the uncertain output on the training stage. Unlike previous works, we propose a novel method from a counterfactual view, namely CLEVER, which is augmentation-free and mitigates biases on the inference stage. Specifically, we train a claim-evidence fusion model and a claim-only model independently. Then, we obtain the final prediction via subtracting output of the claim-only model from output of the claim-evidence fusion model, which counteracts biases in two outputs so that the unbiased part is highlighted. Comprehensive experiments on several datasets have demonstrated the effectiveness of CLEVER."
    }
  },
  {
    "id": "abstract-2023--acl-long--426",
    "result": [
      {
        "value": {
          "start": 984,
          "end": 1029,
          "text": "massively multilingual lexical specialization",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--426:E0"
      },
      {
        "value": {
          "start": 1100,
          "end": 1127,
          "text": "bilingual lexicon induction",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--426:E1"
      },
      {
        "value": {
          "start": 1132,
          "end": 1161,
          "text": "cross-lingual word similarity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--426:E2"
      },
      {
        "value": {
          "start": 1177,
          "end": 1209,
          "text": "cross-lingual sentence retrieval",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--426:E3"
      },
      {
        "value": {
          "start": 994,
          "end": 1029,
          "text": "multilingual lexical specialization",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--426:E4"
      },
      {
        "value": {
          "start": 1339,
          "end": 1394,
          "text": "generalization to languages with no lexical constraints",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--426:E5"
      },
      {
        "from_id": "abstract-2023--acl-long--426:E0",
        "to_id": "abstract-2023--acl-long--426:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--426:E0",
        "to_id": "abstract-2023--acl-long--426:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--426:E0",
        "to_id": "abstract-2023--acl-long--426:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--426:E4",
        "to_id": "abstract-2023--acl-long--426:E5",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "While pretrained language models (PLMs) primarily serve as general-purpose text encoders that can be fine-tuned for a wide variety of downstream tasks, recent work has shown that they can also be rewired to produce high-quality word representations (i.e., static word embeddings) and yield good performance in type-level lexical tasks. While existing work primarily focused on the lexical specialization of monolingual PLMs with immense quantities of monolingual constraints, in this work we expose massively multilingual transformers (MMTs, e.g., mBERT or XLM-R) to multilingual lexical knowledge at scale, leveraging BabelNet as the readily available rich source of multilingual and cross-lingual type-level lexical knowledge. Concretely, we use BabelNet’s multilingual synsets to create synonym pairs (or synonym-gloss pairs) across 50 languages and then subject the MMTs (mBERT and XLM-R) to a lexical specialization procedure guided by a contrastive objective. We show that such massively multilingual lexical specialization brings substantial gains in two standard cross-lingual lexical tasks, bilingual lexicon induction and cross-lingual word similarity, as well as in cross-lingual sentence retrieval. Crucially, we observe gains for languages unseen in specialization, indicating that multilingual lexical specialization enables generalization to languages with no lexical constraints. In a series of subsequent controlled experiments, we show that the number of specialization constraints plays a much greater role than the set of languages from which they originate."
    }
  },
  {
    "id": "abstract-2023--acl-long--794",
    "result": [
      {
        "value": {
          "start": 771,
          "end": 816,
          "text": "Structure-based Pseudo Label generation (SPL)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--794:E0"
      },
      {
        "value": {
          "start": 220,
          "end": 247,
          "text": "video sentence localization",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--794:E1"
      },
      {
        "value": {
          "start": 1031,
          "end": 1063,
          "text": "noise-resistant iterative method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--794:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--794:E0",
        "to_id": "abstract-2023--acl-long--794:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Video sentence localization aims to locate moments in an unstructured video according to a given natural language query. A main challenge is the expensive annotation costs and the annotation bias. In this work, we study video sentence localization in a zero-shot setting, which learns with only video data without any annotation. Existing zero-shot pipelines usually generate event proposals and then generate a pseudo query for each event proposal. However, their event proposals are obtained via visual feature clustering, which is query-independent and inaccurate; and the pseudo-queries are short or less interpretable. Moreover, existing approaches ignores the risk of pseudo-label noise when leveraging them in training. To address the above problems, we propose a Structure-based Pseudo Label generation (SPL), which first generate free-form interpretable pseudo queries before constructing query-dependent event proposals by modeling the event temporal structure. To mitigate the effect of pseudo-label noise, we propose a noise-resistant iterative method that repeatedly re-weight the training sample based on noise estimation to train a grounding model and correct pseudo labels. Experiments on the ActivityNet Captions and Charades-STA datasets demonstrate the advantages of our approach. Code can be found athttps://github.com/minghangz/SPL."
    }
  },
  {
    "id": "abstract-2023--acl-long--22",
    "result": [
      {
        "value": {
          "start": 684,
          "end": 705,
          "text": "Rule By Example (RBE)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--22:E0"
      },
      {
        "value": {
          "start": 1364,
          "end": 1393,
          "text": "explainable model predictions",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--22:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--22:E0",
        "to_id": "abstract-2023--acl-long--22:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Classic approaches to content moderation typically apply a rule-based heuristic approach to flag content. While rules are easily customizable and intuitive for humans to interpret, they are inherently fragile and lack the flexibility or robustness needed to moderate the vast amount of undesirable content found online today. Recent advances in deep learning have demonstrated the promise of using highly effective deep neural models to overcome these challenges. However, despite the improved performance, these data-driven models lack transparency and explainability, often leading to mistrust from everyday users and a lack of adoption by many platforms. In this paper, we present Rule By Example (RBE): a novel exemplar-based contrastive learning approach for learning from logical rules for the task of textual content moderation. RBE is capable of providing rule-grounded predictions, allowing for more explainable and customizable predictions compared to typical deep learning-based approaches. We demonstrate that our approach is capable of learning rich rule embedding representations using only a few data examples. Experimental results on 3 popular hate speech classification datasets show that RBE is able to outperform state-of-the-art deep learning classifiers as well as the use of rules in both supervised and unsupervised settings while providing explainable model predictions via rule-grounding."
    }
  },
  {
    "id": "abstract-2023--acl-long--628",
    "result": [
      {
        "value": {
          "start": 1010,
          "end": 1039,
          "text": "simple yet effective baseline",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--628:E0"
      },
      {
        "value": {
          "start": 1106,
          "end": 1114,
          "text": "F1 gains",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--628:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--628:E0",
        "to_id": "abstract-2023--acl-long--628:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Few-shot event detection (ED) has been widely studied, while this brings noticeable discrepancies, e.g., various motivations, tasks, and experimental settings, that hinder the understanding of models for future progress. This paper presents a thorough empirical study, a unified view of ED models, and a better unified baseline. For fair evaluation, we compare 12 representative methods on three datasets, which are roughly grouped into prompt-based and prototype-based models for detailed analysis. Experiments consistently demonstrate that prompt-based methods, including ChatGPT, still significantly trail prototype-based methods in terms of overall performance. To investigate their superior performance, we break down their design elements along several dimensions and build a unified framework on prototype-based methods. Under such unified view, each prototype-method can be viewed a combination of different modules from these design elements. We further combine all advantageous modules and propose a simple yet effective baseline, which outperforms existing methods by a large margin (e.g., 2.7% F1 gains under low-resource setting)."
    }
  },
  {
    "id": "abstract-2023--acl-long--361",
    "result": [
      {
        "value": {
          "start": 716,
          "end": 723,
          "text": "LAMBADA",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--361:E0"
      },
      {
        "value": {
          "start": 876,
          "end": 884,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--361:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--361:E0",
        "to_id": "abstract-2023--acl-long--361:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Remarkable progress has been made on automated reasoning with natural text, by using Large Language Models (LLMs) and methods such as Chain-of-Thought prompting and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning. The classical automated reasoning literature has shown that reasoning in the backward direction (i.e. from intended conclusion to supporting axioms) is significantly more efficient at proof-finding. Importing this intuition into the LM setting, we develop a Backward Chaining algorithm, called LAMBADA, that decomposes reasoning into four sub-modules, that are simply implemented by few-shot prompted LLM inference. We show that LAMBADA achieves sizable accuracy boosts over state-of-the-art forward reasoning methods on two challenging logical reasoning datasets, particularly when deep and accurate proof chains are required."
    }
  },
  {
    "id": "abstract-2023--acl-short--72",
    "result": [
      {
        "value": {
          "start": 408,
          "end": 433,
          "text": "incorporate contradiction",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--72:E0"
      },
      {
        "value": {
          "start": 525,
          "end": 536,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--72:E1"
      },
      {
        "from_id": "abstract-2023--acl-short--72:E0",
        "to_id": "abstract-2023--acl-short--72:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This work examines the use of contradiction in natural language inference (NLI) for question answering (QA). Typically, NLI systems help answer questions by determining if a potential answer is entailed (supported) by some background context. But is it useful to also determine if an answer contradicts the context? We test this in two settings, multiple choice and extractive QA, and find that systems that incorporate contradiction can do slightly better than entailment-only systems on certain datasets. However, the best performances come from using contradiction, entailment, and QA model confidence scores together. This has implications for the deployment of QA systems in domains such as medicine and science where safety is an issue."
    }
  },
  {
    "id": "abstract-2023--acl-long--385",
    "result": [
      {
        "value": {
          "start": 469,
          "end": 478,
          "text": "KB-BINDER",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--385:E0"
      }
    ],
    "data": {
      "text": "Question answering over knowledge bases is considered a difficult problem due to the challenge of generalizing to a wide variety of possible natural language questions. Additionally, the heterogeneity of knowledge base schema items between different knowledge bases often necessitates specialized training for different knowledge base question-answering (KBQA) datasets. To handle questions over diverse KBQA datasets with a unified training-free framework, we propose KB-BINDER, which for the first time enables few-shot in-context learning over KBQA tasks. Firstly, KB-BINDER leverages large language models like Codex to generate logical forms as the draft for a specific question by imitating a few demonstrations. Secondly, KB-BINDER grounds on the knowledge base to bind the generated draft to an executable one with BM25 score matching. The experimental results on four public heterogeneous KBQA datasets show that KB-BINDER can achieve a strong performance with only a few in-context demonstrations. Especially on GraphQA and 3-hop MetaQA, KB-BINDER can even outperform the state-of-the-art trained models. On GrailQA and WebQSP, our model is also on par with other fully-trained models. We believe KB-BINDER can serve as an important baseline for future research. We plan to release all the code and data. Our code is available athttps://github.com/ltl3A87/KB-BINDER."
    }
  },
  {
    "id": "abstract-2023--acl-long--24",
    "result": [
      {
        "value": {
          "start": 524,
          "end": 533,
          "text": "APE model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--24:E0"
      },
      {
        "value": {
          "start": 1026,
          "end": 1033,
          "text": "F1 gain",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--24:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--24:E0",
        "to_id": "abstract-2023--acl-long--24:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The EAE task extracts a structured event record from an event text. Most existing approaches train the EAE model on each dataset independently and ignore the overlap knowledge across datasets. However, insufficient event records in a single dataset often prevent the existing model from achieving better performance. In this paper, we clearly define the overlap knowledge across datasets and split the knowledge of the EAE task into overlap knowledge across datasets and specific knowledge of the target dataset. We propose APE model to learn the two parts of knowledge in two serial learning phases without causing catastrophic forgetting. In addition, we formulate both learning phases as conditional generation tasks and design Stressing Entity Type Prompt to close the gap between the two phases. The experiments show APE achieves new state-of-the-art with a large margin in the EAE task. When only ten records are available in the target dataset, our model dramatically outperforms the baseline model with average 27.27% F1 gain."
    }
  },
  {
    "id": "abstract-2023--acl-long--108",
    "result": [
      {
        "value": {
          "start": 699,
          "end": 710,
          "text": "InstructGPT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--108:E0"
      },
      {
        "value": {
          "start": 797,
          "end": 808,
          "text": "GPT-3 model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--108:E1"
      }
    ],
    "data": {
      "text": "Large language models are able to perform a task by conditioning on a few input-output demonstrations - a paradigm known as in-context learning. We show that language models can explicitly infer an underlying task from a few demonstrations by prompting them to generate a natural language instruction that fits the examples. To explore this ability, we introduce the instruction induction challenge, compile a dataset consisting of 24 tasks, and define a novel evaluation metric based on executing the generated instruction. We discover that, to a large extent, the ability to generate instructions does indeed emerge when using a model that is both large enough and aligned to follow instructions; InstructGPT achieves 65.7% of human performance in our execution-based metric, while the original GPT-3 model reaches only 9.8% of human performance. This surprising result suggests that instruction induction might be a viable learning paradigm in and of itself, where instead of fitting a set of latent continuous parameters to the data, one searches for the best description in the natural language hypothesis space."
    }
  },
  {
    "id": "abstract-2023--acl-short--129",
    "result": [
      {
        "value": {
          "start": 288,
          "end": 303,
          "text": "extraction rate",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--129:E0"
      },
      {
        "value": {
          "start": 923,
          "end": 933,
          "text": "perplexity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--129:E1"
      }
    ],
    "data": {
      "text": "Large Language Models (LLMs) are known to memorize significant portions of their training data. Parts of this memorized content have been shown to be extractable by simply querying the model, which poses a privacy risk. We present a novel approach which uses prompt-tuning to control the extraction rates of memorized content in LLMs. We present two prompt training strategies to increase and decrease extraction rates, which correspond to an attack and a defense, respectively. We demonstrate the effectiveness of our techniques by using models from the GPT-Neo family on a public benchmark. For the 1.3B parameter GPT-Neo model, our attack yields a 9.3 percentage point increase in extraction rate compared to our baseline. Our defense can be tuned to achieve different privacy-utility trade-offs by a user-specified hyperparameter. We achieve an extraction rate reduction of up to 97.7% relative to our baseline, with a perplexity increase of 16.9%."
    }
  },
  {
    "id": "abstract-2023--acl-long--529",
    "result": [
      {
        "value": {
          "start": 798,
          "end": 823,
          "text": "ideal utterance selection",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--529:E0"
      },
      {
        "value": {
          "start": 912,
          "end": 929,
          "text": "parser accuracies",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--529:E1"
      },
      {
        "value": {
          "start": 386,
          "end": 400,
          "text": "error and bias",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--529:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--529:E0",
        "to_id": "abstract-2023--acl-long--529:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--529:E0",
        "to_id": "abstract-2023--acl-long--529:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Multilingual semantic parsing aims to leverage the knowledge from the high-resource languages to improve low-resource semantic parsing, yet commonly suffers from the data imbalance problem. Prior works propose to utilize the translations by either humans or machines to alleviate such issues. However, human translations are expensive, while machine translations are cheap but prone to error and bias. In this work, we propose an active learning approach that exploits the strengths of both human and machine translations by iteratively adding small batches of human translations into the machine-translated training set. Besides, we propose novel aggregated acquisition criteria that help our active learning method select utterances to be manually translated. Our experiments demonstrate that an ideal utterance selection can significantly reduce the error and bias in the translated data, resulting in higher parser accuracies than the parsers merely trained on the machine-translated data."
    }
  },
  {
    "id": "abstract-2023--acl-long--501",
    "result": [
      {
        "value": {
          "start": 344,
          "end": 353,
          "text": "FormNetV2",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--501:E0"
      },
      {
        "value": {
          "start": 921,
          "end": 994,
          "text": "state-of-the-art performance on FUNSD, CORD, SROIE and Payment benchmarks",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--501:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--501:E0",
        "to_id": "abstract-2023--acl-long--501:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The recent advent of self-supervised pre-training techniques has led to a surge in the use of multimodal learning in form document understanding. However, existing approaches that extend the mask language modeling to other modalities require careful multi-task tuning, complex reconstruction target designs, or additional pre-training data. In FormNetV2, we introduce a centralized multimodal graph contrastive learning strategy to unify self-supervised pre-training for all modalities in one loss. The graph contrastive objective maximizes the agreement of multimodal representations, providing a natural interplay for all modalities without special customization. In addition, we extract image features within the bounding box that joins a pair of tokens connected by a graph edge, capturing more targeted visual cues without loading a sophisticated and separately pre-trained image embedder. FormNetV2 establishes new state-of-the-art performance on FUNSD, CORD, SROIE and Payment benchmarks with a more compact model size."
    }
  },
  {
    "id": "abstract-2023--acl-long--92",
    "result": [
      {
        "value": {
          "start": 747,
          "end": 753,
          "text": "DaMSTF",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--92:E0"
      },
      {
        "value": {
          "start": 1596,
          "end": 1615,
          "text": "performance of BERT",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--92:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--92:E0",
        "to_id": "abstract-2023--acl-long--92:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Self-training emerges as an important research line on domain adaptation. By taking the model’s prediction as the pseudo labels of the unlabeled data, self-training bootstraps the model with pseudo instances in the target domain. However, the prediction errors of pseudo labels (label noise) challenge the performance of self-training. To address this problem, previous approaches only use reliable pseudo instances, i.e., pseudo instances with high prediction confidence, to retrain the model. Although these strategies effectively reduce the label noise, they are prone to miss the hard examples. In this paper, we propose a new self-training framework for domain adaptation, namely Domain adversarial learning enhanced Self-Training Framework (DaMSTF). Firstly, DaMSTF involves meta-learning to estimate the importance of each pseudo instance, so as to simultaneously reduce the label noise and preserve hard examples. Secondly, we design a meta constructor for constructing the meta-validation set, which guarantees the effectiveness of the meta-learning module by improving the quality of the meta-validation set. Thirdly, we find that the meta-learning module suffers from the training guidance vanish- ment and tends to converge to an inferior optimal. To this end, we employ domain adversarial learning as a heuristic neural network initialization method, which can help the meta-learning module converge to a better optimal. Theoretically and experimentally, we demonstrate the effectiveness of the proposed DaMSTF. On the cross-domain sentiment classification task, DaMSTF improves the performance of BERT with an average of nearly 4%."
    }
  },
  {
    "id": "abstract-2023--acl-short--102",
    "result": [
      {
        "value": {
          "start": 720,
          "end": 749,
          "text": "discard positional embeddings",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--102:E0"
      },
      {
        "value": {
          "start": 775,
          "end": 796,
          "text": "efficient pretraining",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--102:E1"
      },
      {
        "from_id": "abstract-2023--acl-short--102:E0",
        "to_id": "abstract-2023--acl-short--102:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The use of positional embeddings in transformer language models is widely accepted. However, recent research has called into question the necessity of such embeddings. We further extend this inquiry by demonstrating that a randomly initialized and frozen transformer language model, devoid of positional embeddings, inherently encodes strong positional information through the shrinkage of self-attention variance. To quantify this variance, we derive the underlying distribution of each step within a transformer layer. Through empirical validation using a fully pretrained model, we show that the variance shrinkage effect still persists after extensive gradient updates. Our findings serve to justify the decision to discard positional embeddings and thus facilitate more efficient pretraining of transformer language models."
    }
  },
  {
    "id": "abstract-2023--acl-long--679",
    "result": [
      {
        "value": {
          "start": 400,
          "end": 456,
          "text": "Causality-guided Multi-memory Interaction Network (CMIN)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--679:E0"
      },
      {
        "value": {
          "start": 180,
          "end": 199,
          "text": "prediction accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--679:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--679:E0",
        "to_id": "abstract-2023--acl-long--679:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Over the past few years, we’ve witnessed an enormous interest in stock price movement prediction using AI techniques. In recent literature, auxiliary data has been used to improve prediction accuracy, such as textual news. When predicting a particular stock, we assume that information from other stocks should also be utilized as auxiliary data to enhance performance. In this paper, we propose the Causality-guided Multi-memory Interaction Network (CMIN), a novel end-to-end deep neural network for stock movement prediction which, for the first time, models the multi-modality between financial text data and causality-enhanced stock correlations to achieve higher prediction accuracy. CMIN transforms the basic attention mechanism into Causal Attention by calculating transfer entropy between multivariate stocks in order to avoid attention on spurious correlations. Furthermore, we introduce a fusion mechanism to model the multi-directional interactions through which CMIN learns not only the self-influence but also the interactive influence in information flows representing the interrelationship between text and stock correlations. The effectiveness of the proposed approach is demonstrated by experiments on three real-world datasets collected from the U.S. and Chinese markets, where CMIN outperforms existing models to establish a new state-of-the-art prediction accuracy."
    }
  },
  {
    "id": "abstract-2023--acl-long--105",
    "result": [
      {
        "value": {
          "start": 496,
          "end": 550,
          "text": "k-Nearest-Neighbor Transfer Learning (kNN-TL) approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--105:E0"
      },
      {
        "value": {
          "start": 827,
          "end": 868,
          "text": "child-aware datastore construction method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--105:E1"
      },
      {
        "value": {
          "start": 883,
          "end": 903,
          "text": "inference efficiency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--105:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--105:E1",
        "to_id": "abstract-2023--acl-long--105:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Transfer learning has been shown to be an effective technique for enhancing the performance of low-resource neural machine translation (NMT). This is typically achieved through either fine-tuning a child model with a pre-trained parent model, or by utilizing the out- put of the parent model during the training of the child model. However, these methods do not make use of the parent knowledge during the child inference, which may limit the translation performance. In this paper, we propose a k-Nearest-Neighbor Transfer Learning (kNN-TL) approach for low-resource NMT, which leverages the parent knowledge throughout the entire developing process of the child model. Our approach includes a parent-child representation alignment method, which ensures consistency in the output representations between the two models, and a child-aware datastore construction method that improves inference efficiency by selectively distilling the parent datastore based on relevance to the child model. Experimental results on four low-resource translation tasks show that kNN-TL outperforms strong baselines. Extensive analyses further demonstrate the effectiveness of our approach. Code and scripts are freely available athttps://github.com/NLP2CT/kNN-TL."
    }
  },
  {
    "id": "abstract-2023--acl-long--222",
    "result": [
      {
        "value": {
          "start": 479,
          "end": 488,
          "text": "ContProto",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--222:E0"
      },
      {
        "value": {
          "start": 896,
          "end": 921,
          "text": "accuracy of pseudo labels",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--222:E1"
      },
      {
        "value": {
          "start": 270,
          "end": 289,
          "text": "overall performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--222:E2"
      },
      {
        "value": {
          "start": 713,
          "end": 742,
          "text": "cross-lingual transferability",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--222:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--222:E0",
        "to_id": "abstract-2023--acl-long--222:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--222:E0",
        "to_id": "abstract-2023--acl-long--222:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--222:E0",
        "to_id": "abstract-2023--acl-long--222:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In cross-lingual named entity recognition (NER), self-training is commonly used to bridge the linguistic gap by training on pseudo-labeled target-language data. However, due to sub-optimal performance on target languages, the pseudo labels are often noisy and limit the overall performance. In this work, we aim to improve self-training for cross-lingual NER by combining representation learning and pseudo label refinement in one coherent framework. Our proposed method, namely ContProto mainly comprises two components: (1) contrastive self-training and (2) prototype-based pseudo-labeling. Our contrastive self-training facilitates span classification by separating clusters of different classes, and enhances cross-lingual transferability by producing closely-aligned representations between the source and target language. Meanwhile, prototype-based pseudo-labeling effectively improves the accuracy of pseudo labels during training. We evaluate ContProto on multiple transfer pairs, and experimental results show our method brings substantial improvements over current state-of-the-art methods."
    }
  },
  {
    "id": "abstract-2023--acl-long--609",
    "result": [
      {
        "value": {
          "start": 516,
          "end": 600,
          "text": "choosing the best transfer language(s) in both single-source and multi-source setups",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--609:E0"
      },
      {
        "value": {
          "start": 622,
          "end": 645,
          "text": "POS tagging performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--609:E1"
      },
      {
        "value": {
          "start": 746,
          "end": 848,
          "text": "transferring knowledge from a language that matches the language family and morphosyntactic properties",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--609:E2"
      },
      {
        "value": {
          "start": 880,
          "end": 911,
          "text": "POS tagging in unseen languages",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--609:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--609:E0",
        "to_id": "abstract-2023--acl-long--609:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--609:E2",
        "to_id": "abstract-2023--acl-long--609:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In this paper, we present AfricaPOS, the largest part-of-speech (POS) dataset for 20 typologically diverse African languages. We discuss the challenges in annotating POS for these languages using the universal dependencies (UD) guidelines. We conducted extensive POS baseline experiments using both conditional random field and several multilingual pre-trained language models. We applied various cross-lingual transfer models trained with data available in the UD. Evaluating on the AfricaPOS dataset, we show that choosing the best transfer language(s) in both single-source and multi-source setups greatly improves the POS tagging performance of the target languages, in particular when combined with parameter-fine-tuning methods. Crucially, transferring knowledge from a language that matches the language family and morphosyntactic properties seems to be more effective for POS tagging in unseen languages."
    }
  },
  {
    "id": "abstract-2023--acl-long--112",
    "result": [
      {
        "value": {
          "start": 573,
          "end": 576,
          "text": "REV",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--112:E0"
      }
    ],
    "data": {
      "text": "Generating free-text rationales is a promising step towards explainable NLP, yet evaluating such rationales remains a challenge. Existing metrics have mostly focused on measuring the association between the rationale and a given label. We argue that an ideal metric should focus on the new information uniquely provided in the rationale that is otherwise not provided in the input or the label. We investigate this research problem from an information-theoretic perspective using conditional V-information (Hewitt et al., 2021). More concretely, we propose a metric called REV (Rationale Evaluation with conditional V-information), to quantify the amount of new, label-relevant information in a rationale beyond the information already available in the input or the label. Experiments across four benchmarks with reasoning tasks, including chain-of-thought, demonstrate the effectiveness of REV in evaluating rationale-label pairs, compared to existing metrics. We further demonstrate REV is consistent with human judgments on rationale evaluations and provides more sensitive measurements of new information in free-text rationales. When used alongside traditional performance metrics, REV provides deeper insights into models’ reasoning and prediction processes."
    }
  },
  {
    "id": "abstract-2023--acl-long--744",
    "result": [
      {
        "value": {
          "start": 978,
          "end": 988,
          "text": "NAST model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--744:E0"
      },
      {
        "value": {
          "start": 1009,
          "end": 1019,
          "text": "BLEU score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--744:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--744:E0",
        "to_id": "abstract-2023--acl-long--744:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Combining end-to-end speech translation (ST) and non-autoregressive (NAR) generation is promising in language and speech processing for their advantages of less error propagation and low latency. In this paper, we investigate the potential of connectionist temporal classification (CTC) for non-autoregressive speech translation (NAST).In particular, we develop a model consisting of two encoders that are guided by CTC to predict the source and target texts, respectively. Introducing CTC into NAST on both language sides has obvious challenges: 1) the conditional independent generation somewhat breaks the interdependency among tokens, and 2) the monotonic alignment assumption in standard CTC does not hold in translation tasks. In response, we develop a prediction-aware encoding approach and a cross-layer attention approach to address these issues. We also use curriculum learning to improve convergence of training. Experiments on the MuST-C ST benchmarks show that our NAST model achieves an average BLEU score of 29.5 with a speed-up of 5.67×, which is comparable to the autoregressive counterpart and even outperforms the previous best result of 0.9 BLEU points."
    }
  },
  {
    "id": "abstract-2023--acl-long--849",
    "result": [
      {
        "value": {
          "start": 704,
          "end": 719,
          "text": "trainable gates",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--849:E0"
      },
      {
        "value": {
          "start": 1201,
          "end": 1220,
          "text": "constraint accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--849:E1"
      },
      {
        "value": {
          "start": 1222,
          "end": 1234,
          "text": "text quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--849:E2"
      },
      {
        "value": {
          "start": 426,
          "end": 439,
          "text": "extensibility",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--849:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--849:E0",
        "to_id": "abstract-2023--acl-long--849:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--849:E0",
        "to_id": "abstract-2023--acl-long--849:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--849:E0",
        "to_id": "abstract-2023--acl-long--849:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recently, multi-aspect controllable text generation that controls the generated text in multiple aspects (e.g., sentiment, topic, and keywords) has attracted increasing attention. Although methods based on parameter efficient tuning like prefix-tuning could achieve multi-aspect controlling in a plug-and-play way, the mutual interference of multiple prefixes leads to significant degeneration of constraints and limits their extensibility to training-time unseen aspect combinations. In this work, we provide a theoretical lower bound for the interference and empirically found that the interference grows with the number of layers where prefixes are inserted. Based on these analyses, we propose using trainable gates to normalize the intervention of prefixes to restrain the growing interference. As a result, controlling training-time unseen combinations of aspects can be realized by simply concatenating corresponding plugins such that new constraints can be extended at a lower cost. In addition, we propose a unified way to process both categorical and free-form constraints. Experiments on text generation and machine translation demonstrate the superiority of our approach over baselines on constraint accuracy, text quality, and extensibility."
    }
  },
  {
    "id": "abstract-2023--acl-short--78",
    "result": [
      {
        "value": {
          "start": 265,
          "end": 296,
          "text": "Effective Factual Summarization",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--78:E0"
      },
      {
        "value": {
          "start": 363,
          "end": 381,
          "text": "summary factuality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--78:E1"
      },
      {
        "value": {
          "start": 550,
          "end": 574,
          "text": "similarity-based metrics",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--78:E2"
      },
      {
        "value": {
          "start": 828,
          "end": 842,
          "text": "FactCC on XSUM",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--78:E3"
      },
      {
        "from_id": "abstract-2023--acl-short--78:E0",
        "to_id": "abstract-2023--acl-short--78:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-short--78:E0",
        "to_id": "abstract-2023--acl-short--78:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-short--78:E0",
        "to_id": "abstract-2023--acl-short--78:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Improving factual consistency of abstractive summarization has been a widely studied topic. However, most of the prior works on training factuality-aware models have ignored the negative effect it has on summary quality. We propose {pasted macro ‘MODEL’}name (i.e. Effective Factual Summarization), a candidate summary generation and ranking technique to improve summary factuality without sacrificing quality. We show that using a contrastive learning framework with our refined candidate summaries leads to significant gains on both factuality and similarity-based metrics. Specifically, we propose a ranking strategy in which we effectively combine two metrics, thereby preventing any conflict during training. Models trained using our approach show up to 6 points of absolute improvement over the base model with respect to FactCC on XSUM and 11 points on CNN/DM, without negatively affecting either similarity-based metrics or absractiveness."
    }
  },
  {
    "id": "abstract-2023--acl-long--15",
    "result": [
      {
        "value": {
          "start": 309,
          "end": 331,
          "text": "holographic embeddings",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--15:E0"
      },
      {
        "value": {
          "start": 541,
          "end": 562,
          "text": "supertagging accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--15:E1"
      },
      {
        "value": {
          "start": 649,
          "end": 707,
          "text": "span-based parsing algorithm using holographic composition",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--15:E2"
      },
      {
        "value": {
          "start": 717,
          "end": 792,
          "text": "performance comparable to state-of-the-art neural parsing with Transformers",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--15:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--15:E0",
        "to_id": "abstract-2023--acl-long--15:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--15:E2",
        "to_id": "abstract-2023--acl-long--15:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We propose a method for formulating CCG as a recursive composition in a continuous vector space. Recent CCG supertagging and parsing models generally demonstrate high performance, yet rely on black-box neural architectures to implicitly model phrase structure dependencies. Instead, we leverage the method of holographic embeddings as a compositional operator to explicitly model the dependencies between words and phrase structures in the embedding space. Experimental results revealed that holographic composition effectively improves the supertagging accuracy to achieve state-of-the-art parsing performance when using a C&C parser. The proposed span-based parsing algorithm using holographic composition achieves performance comparable to state-of-the-art neural parsing with Transformers. Furthermore, our model can semantically and syntactically infill text at the phrase level due to the decomposability of holographic composition."
    }
  },
  {
    "id": "abstract-2023--acl-long--335",
    "result": [
      {
        "value": {
          "start": 523,
          "end": 566,
          "text": "new data sampling and evaluation strategies",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--335:E0"
      },
      {
        "value": {
          "start": 467,
          "end": 510,
          "text": "generalizability and reliability of results",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--335:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--335:E0",
        "to_id": "abstract-2023--acl-long--335:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Morphological inflection is a popular task in sub-word NLP with both practical and cognitive applications. For years now, state-of-the-art systems have reported high, but also highly variable, performance across data sets and languages. We investigate the causes of this high performance and high variability; we find several aspects of data set creation and evaluation which systematically inflate performance and obfuscate differences between languages. To improve generalizability and reliability of results, we propose new data sampling and evaluation strategies that better reflect likely use-cases. Using these new strategies, we make new observations on the generalization abilities of current inflection systems."
    }
  },
  {
    "id": "abstract-2023--acl-long--888",
    "result": [
      {
        "value": {
          "start": 472,
          "end": 475,
          "text": "INK",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--888:E0"
      },
      {
        "value": {
          "start": 955,
          "end": 972,
          "text": "inference speedup",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--888:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--888:E0",
        "to_id": "abstract-2023--acl-long--888:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Neural machine translation has achieved promising results on many translation tasks. However, previous studies have shown that neural models induce a non-smooth representation space, which harms its generalization results. Recently, kNN-MT has provided an effective paradigm to smooth the prediction based on neighbor representations during inference. Despite promising results, kNN-MT usually requires large inference overhead. We propose an effective training framework INK to directly smooth the representation space via adjusting representations of kNN neighbors with a small number of new parameters. The new parameters are then used to refresh the whole representation datastore to get new kNN knowledge asynchronously. This loop keeps running until convergence. Experiments on four benchmark datasets show that INK achieves average gains of 1.99 COMET and 1.0 BLEU, outperforming the state-of-the-art kNN-MT system with 0.02x memory space and 1.9x inference speedup."
    }
  },
  {
    "id": "abstract-2023--acl-long--240",
    "result": [
      {
        "value": {
          "start": 384,
          "end": 410,
          "text": "Multi-view Prompting (MVP)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--240:E0"
      },
      {
        "value": {
          "start": 1045,
          "end": 1073,
          "text": "state-of-the-art performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--240:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--240:E0",
        "to_id": "abstract-2023--acl-long--240:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Generative methods greatly promote aspect-based sentiment analysis via generating a sequence of sentiment elements in a specified format. However, existing studies usually predict sentiment elements in a fixed order, which ignores the effect of the interdependence of the elements in a sentiment tuple and the diversity of language expression on the results. In this work, we propose Multi-view Prompting (MVP) that aggregates sentiment elements generated in different orders, leveraging the intuition of human-like problem-solving processes from different views. Specifically, MVP introduces element order prompts to guide the language model to generate multiple sentiment tuples, each with a different element order, and then selects the most reasonable tuples by voting. MVP can naturally model multi-view and multi-task as permutations and combinations of elements, respectively, outperforming previous task-specific designed methods on multiple ABSA tasks with a single model. Extensive experiments show that MVP significantly advances the state-of-the-art performance on 10 datasets of 4 benchmark tasks, and performs quite effectively in low-resource settings. Detailed evaluation verified the effectiveness, flexibility, and cross-task transferability of MVP."
    }
  },
  {
    "id": "abstract-2023--acl-long--417",
    "result": [
      {
        "value": {
          "start": 670,
          "end": 693,
          "text": "adapter-based framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--417:E0"
      }
    ],
    "data": {
      "text": "Recent task-oriented dialog systems have had great success in building English-based personal assistants, but extending these systems to a global audience is challenging due to the need for annotated data in the target language. An alternative approach is to leverage existing data in a high-resource language to enable cross-lingual transfer in low-resource language models. However, this type of transfer has not been widely explored in natural language response generation. In this research, we investigate the use of state-of-the-art multilingual models such as mBART and T5 to facilitate zero-shot and few-shot transfer of code-switched responses. We propose a new adapter-based framework that allows for efficient transfer by learning task-specific representations and encapsulating source and target language representations. Our framework is able to successfully transfer language knowledge even when the target language corpus is limited. We present both quantitative and qualitative analyses to evaluate the effectiveness of our approach."
    }
  },
  {
    "id": "abstract-2023--acl-short--138",
    "result": [
      {
        "value": {
          "start": 276,
          "end": 282,
          "text": "MolXPT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--138:E0"
      },
      {
        "value": {
          "start": 825,
          "end": 854,
          "text": "molecular property prediction",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--138:E1"
      },
      {
        "value": {
          "start": 912,
          "end": 937,
          "text": "text-molecule translation",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--138:E2"
      },
      {
        "value": {
          "start": 996,
          "end": 1026,
          "text": "zero-shot molecular generation",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--138:E3"
      },
      {
        "from_id": "abstract-2023--acl-short--138:E0",
        "to_id": "abstract-2023--acl-short--138:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-short--138:E0",
        "to_id": "abstract-2023--acl-short--138:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2023--acl-short--138:E0",
        "to_id": "abstract-2023--acl-short--138:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Generative pre-trained Transformer (GPT) has demonstrates its great success in natural language processing and related techniques have been adapted into molecular modeling. Considering that text is the most important record for scientific discovery, in this paper, we propose MolXPT, a unified language model of text and molecules pre-trained on SMILES (a sequence representation of molecules) wrapped by text. Briefly, we detect the molecule names in each sequence and replace them to the corresponding SMILES. In this way, the SMILES could leverage the information from surrounding text, and vice versa. The above wrapped sequences, text sequences from PubMed and SMILES sequences from PubChem are all fed into a language model for pre-training. Experimental results demonstrate that MolXPT outperforms strong baselines of molecular property prediction on MoleculeNet, performs comparably to the best model in text-molecule translation while using less than half of its parameters, and enables zero-shot molecular generation without finetuning."
    }
  },
  {
    "id": "abstract-2023--acl-long--502",
    "result": [
      {
        "value": {
          "start": 563,
          "end": 582,
          "text": "learning with MixCE",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--502:E0"
      }
    ],
    "data": {
      "text": "Autoregressive language models are trained by minimizing the cross-entropy of the model distribution Q relative to the data distribution P – that is, minimizing the forward cross-entropy, which is equivalent to maximum likelihood estimation (MLE). We have observed that models trained in this way may “over-generalize”, in the sense that they produce non-human-like text. Moreover, we believe that reverse cross-entropy, i.e., the cross-entropy of P relative to Q, is a better reflection of how a human would evaluate text generated by a model. Hence, we propose learning with MixCE, an objective that mixes the forward and reverse cross-entropies. We evaluate models trained with this objective on synthetic data settings (where P is known) and real data, and show that the resulting models yield better generated text without complex decoding strategies."
    }
  },
  {
    "id": "abstract-2023--acl-short--76",
    "result": [
      {
        "value": {
          "start": 405,
          "end": 424,
          "text": "contemporary models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--76:E0"
      }
    ],
    "data": {
      "text": "We present PaRTE, a collection of 1,126 pairs of Recognizing Textual Entailment (RTE) examples to evaluate whether models are robust to paraphrasing. We posit that if RTE models understand language, their predictions should be consistent across inputs that share the same meaning. We use the evaluation set to determine if RTE models’ predictions change when examples are paraphrased. In our experiments, contemporary models change their predictions on 8-16% of paraphrased examples, indicating that there is still room for improvement."
    }
  },
  {
    "id": "abstract-2023--acl-short--85",
    "result": [
      {
        "value": {
          "start": 848,
          "end": 895,
          "text": "transfer from English in the same target domain",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--85:E0"
      },
      {
        "value": {
          "start": 933,
          "end": 941,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--85:E1"
      },
      {
        "value": {
          "start": 1053,
          "end": 1113,
          "text": "machine translation from English to other Nigerian languages",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--85:E2"
      },
      {
        "value": {
          "start": 1163,
          "end": 1187,
          "text": "cross-lingual evaluation",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--85:E3"
      },
      {
        "from_id": "abstract-2023--acl-short--85:E0",
        "to_id": "abstract-2023--acl-short--85:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-short--85:E2",
        "to_id": "abstract-2023--acl-short--85:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Africa has over 2000 indigenous languages but they are under-represented in NLP research due to lack of datasets. In recent years, there have been progress in developing labelled corpora for African languages. However, they are often available in a single domain and may not generalize to other domains. In this paper, we focus on the task of sentiment classification for cross-domain adaptation. We create a new dataset, Nollywood movie reviews for five languages widely spoken in Nigeria (English, Hausa, Igbo, Nigerian Pidgin, and Yoruba). We provide an extensive empirical evaluation using classical machine learning methods and pre-trained language models. By leveraging transfer learning, we compare the performance of cross-domain adaptation from Twitter domain, and cross-lingual adaptation from English language. Our evaluation shows that transfer from English in the same target domain leads to more than 5% improvement in accuracy compared to transfer from Twitter in the same language. To further mitigate the domain difference, we leverage machine translation from English to other Nigerian languages, which leads to a further improvement of 7% over cross-lingual evaluation. While machine translation to low-resource languages are often of low quality, our analysis shows that sentiment related words are often preserved."
    }
  },
  {
    "id": "abstract-2023--acl-demo--56",
    "result": [
      {
        "value": {
          "start": 743,
          "end": 757,
          "text": "Ranger toolkit",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-demo--56:E0"
      },
      {
        "value": {
          "start": 821,
          "end": 841,
          "text": "evaluation standards",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-demo--56:E1"
      },
      {
        "from_id": "abstract-2023--acl-demo--56:E0",
        "to_id": "abstract-2023--acl-demo--56:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In this paper, we introduce Ranger - a toolkit to facilitate the easy use of effect-size-based meta-analysis for multi-task evaluation in NLP and IR. We observed that our communities often face the challenge of aggregating results over incomparable metrics and scenarios, which makes conclusions and take-away messages less reliable. With Ranger, we aim to address this issue by providing a task-agnostic toolkit that combines the effect of a treatment on multiple tasks into one statistical evaluation, allowing for comparison of metrics and computation of an overall summary effect. Our toolkit produces publication-ready forest plots that enable clear communication of evaluation results over multiple tasks. Our goal with the ready-to-use Ranger toolkit is to promote robust, effect-size-based evaluation and improve evaluation standards in the community. We provide two case studies for common IR and NLP settings to highlight Ranger’s benefits."
    }
  },
  {
    "id": "abstract-2023--acl-short--59",
    "result": [
      {
        "value": {
          "start": 828,
          "end": 835,
          "text": "LM-CPPF",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--59:E0"
      }
    ],
    "data": {
      "text": "In recent years, there has been significant progress in developing pre-trained language models for NLP. However, these models often struggle when fine-tuned on small datasets. To address this issue, researchers have proposed various adaptation approaches. Prompt-based tuning is arguably the most common way, especially for larger models. Previous research shows that adding contrastive learning to prompt-based fine-tuning is effective as it helps the model generate embeddings that are more distinguishable between classes, and it can also be more sample-efficient as the model learns from positive and negative examples simultaneously. One of the most important components of contrastive learning is data augmentation, but unlike computer vision, effective data augmentation for NLP is still challenging. This paper proposes LM-CPPF, Contrastive Paraphrasing-guided Prompt-based Fine-tuning of Language Models, which leverages prompt-based few-shot paraphrasing using generative language models, especially large language models such as GPT-3 and OPT-175B, for data augmentation. Our experiments on multiple text classification benchmarks show that this augmentation method outperforms other methods, such as easy data augmentation, back translation, and multiple templates."
    }
  },
  {
    "id": "abstract-2023--acl-long--503",
    "result": [
      {
        "value": {
          "start": 1118,
          "end": 1121,
          "text": "KPE",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--503:E0"
      },
      {
        "value": {
          "start": 107,
          "end": 137,
          "text": "commonsense question answering",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--503:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--503:E0",
        "to_id": "abstract-2023--acl-long--503:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Commonsense question answering is important for making decisions about everyday matters. Although existing commonsense question answering works based on fully fine-tuned PLMs have achieved promising results, they suffer from prohibitive computation costs as well as poor interpretability. Some works improve the PLMs by incorporating knowledge to provide certain evidence, via elaborately designed GNN modules which require expertise. In this paper, we propose a simple knowledgeable parameter efficient tuning network to couple PLMs with external knowledge for commonsense question answering. Specifically, we design a trainable parameter-sharing adapter attached to a parameter-freezing PLM to incorporate knowledge at a small cost. The adapter is equipped with both entity- and query-related knowledge via two auxiliary knowledge-related tasks (i.e., span masking and relation discrimination). To make the adapter focus on the relevant knowledge, we design gating and attention mechanisms to respectively filter and fuse the query information from the PLM. Extensive experiments on two benchmark datasets show that KPE is parameter-efficient and can effectively incorporate knowledge for improving commonsense question answering."
    }
  },
  {
    "id": "abstract-2023--acl-short--57",
    "result": [
      {
        "value": {
          "start": 942,
          "end": 973,
          "text": "game-solving agents performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--57:E0"
      }
    ],
    "data": {
      "text": "The recent emergence of Neuro-Symbolic Agent (NeSA) approaches to natural language-based interactions calls for the investigation of model-based approaches. In contrast to model-free approaches, which existing NeSAs take, learning an explicit world model has an interesting potential especially in the explainability, which is one of the key selling points of NeSA. To learn useful world models, we leverage one of the recent neuro-symbolic architectures, Logical Neural Networks (LNN). Here, we describe a method that can learn neuro-symbolic world models on the TextWorld-Commonsense set of games. We then show how this can be improved further by taking inspiration from the concept of proprioception, but for conversation. This is done by enhancing the internal logic state with a memory of previous actions while also guiding future actions by augmenting the learned model with constraints based on this memory. This greatly improves the game-solving agents performance in a TextWorld setting, where the advantage over the baseline is an 85% average steps reduction and x2.3 average score."
    }
  },
  {
    "id": "abstract-2023--acl-long--587",
    "result": [
      {
        "value": {
          "start": 208,
          "end": 224,
          "text": "calibration step",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--587:E0"
      },
      {
        "value": {
          "start": 375,
          "end": 387,
          "text": "faithfulness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--587:E1"
      },
      {
        "value": {
          "start": 285,
          "end": 294,
          "text": "relevance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--587:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--587:E0",
        "to_id": "abstract-2023--acl-long--587:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--587:E0",
        "to_id": "abstract-2023--acl-long--587:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Summarization models often generate text that is poorly calibrated to quality metrics because they are trained to maximize the likelihood of a single reference (MLE). To address this, recent work has added a calibration step, which exposes a model to its own ranked outputs to improve relevance or, in a separate line of work, contrasts positive and negative sets to improve faithfulness. While effective, much of this work has focused onhowto generate and optimize these sets. Less is known aboutwhyone setup is more effective than another. In this work, we uncover the underlying characteristics of effective sets. For each training instance, we form a large, diverse pool of candidates and systematically vary the subsets used for calibration fine-tuning. Each selection strategy targets distinct aspects of the sets, such as lexical diversity or the size of the gap between positive and negatives. On three diverse scientific long-form summarization datasets (spanning biomedical, clinical, and chemical domains), we find, among others, that faithfulness calibration is optimal when the negative sets are extractive and more likely to be generated, whereas for relevance calibration, the metric margin between candidates should be maximized and surprise–the disagreement between model and metric defined candidate rankings–minimized."
    }
  },
  {
    "id": "abstract-2023--acl-long--528",
    "result": [
      {
        "value": {
          "start": 415,
          "end": 439,
          "text": "NeuroStructural Decoding",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--528:E0"
      }
    ],
    "data": {
      "text": "Text generation often involves producing coherent and grammatically correct texts that also satisfy a given set of semantic constraints. While most approaches for conditional text generation have primarily focused on lexical constraints, they often struggle to effectively incorporate syntactic constraints, which provide a richer language for approximating semantic constraints. We address this gap by introducing NeuroStructural Decoding, a new decoding algorithm that incorporates syntactic constraints to further improve the quality of the generated text. We build NeuroStructural Decoding on the NeuroLogic Decoding (Lu etal. 2021) algorithm, which enables language generation models to produce fluent text while satisfying complex lexical constraints. Our algorithm is powerful and scalable. It tracks lexico-syntactic constraints (e.g., we need to observe dog as subject and ball as object)during decoding by parsing the partial generations at each step. To this end, we adapt a dependency parser to generate parses for incomplete sentences. Our approach is evaluated on three different language generation tasks, and the results show improved performance in both lexical and syntactic metrics compared to previous methods. The results suggest this is a promising solution for integrating fine-grained controllable generation into the conventional beam search decoding."
    }
  },
  {
    "id": "abstract-2023--acl-short--68",
    "result": [
      {
        "value": {
          "start": 916,
          "end": 959,
          "text": "Prompt Pooling with Teacher Forcing (PP-TF)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--68:E0"
      },
      {
        "value": {
          "start": 1060,
          "end": 1098,
          "text": "21.54% improvement over Prompt Pooling",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--68:E1"
      },
      {
        "from_id": "abstract-2023--acl-short--68:E0",
        "to_id": "abstract-2023--acl-short--68:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Large-scale code generation models such as Copilot and CodeT5 have achieved impressive performance. However, libraries are upgraded or deprecated very frequently and re-training large-scale language models is computationally expensive. Therefore, Continual Learning (CL) is an important aspect that remains under-explored in the code domain. In this paper, we introduce a benchmark called CodeTask-CL that covers a wide range of tasks, including code generation, translation, summarization, and refinement, with different input and output programming languages. Next, on our CodeTask-CL benchmark, we compare popular CL techniques from NLP and Vision domains. We find that effective methods like Prompt Pooling (PP) suffer from catastrophic forgetting due to the unstable training of the prompt selection mechanism caused by stark distribution shifts in coding tasks. We address this issue with our proposed method, Prompt Pooling with Teacher Forcing (PP-TF), that stabilizes training by enforcing constraints on the prompt selection mechanism and leads to a 21.54% improvement over Prompt Pooling. Along with the benchmark, we establish a training pipeline that can be used for CL on code models, which we believe can motivate further development of CL methods for code models."
    }
  },
  {
    "id": "abstract-2023--acl-long--763",
    "result": [
      {
        "value": {
          "start": 930,
          "end": 984,
          "text": "abundance of tailored practice material and hypotheses",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--763:E0"
      }
    ],
    "data": {
      "text": "Many cognitive approaches to well-being, such as recognizing and reframing unhelpful thoughts, have received considerable empirical support over the past decades, yet still lack truly widespread adoption in self-help format. A barrier to that adoption is a lack of adequately specific and diverse dedicated practice material. This work examines whether current language models can be leveraged to both produce a virtually unlimited quantity of practice material illustrating standard unhelpful thought patterns matching specific given contexts, and generate suitable positive reframing proposals. We propose PATTERNREFRAME, a novel dataset of about 10k examples of thoughts containing unhelpful thought patterns conditioned on a given persona, accompanied by about 27k positive reframes. By using this dataset to train and/or evaluate current models, we show that existing models can already be powerful tools to help generate an abundance of tailored practice material and hypotheses, with no or minimal additional model training required."
    }
  },
  {
    "id": "abstract-2023--acl-srw--15",
    "result": [
      {
        "value": {
          "start": 166,
          "end": 187,
          "text": "syntactic information",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-srw--15:E0"
      },
      {
        "value": {
          "start": 911,
          "end": 943,
          "text": "understanding of ancient Chinese",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-srw--15:E1"
      },
      {
        "value": {
          "start": 501,
          "end": 548,
          "text": "confidence-based syntax encoding network (cSEN)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-srw--15:E2"
      },
      {
        "value": {
          "start": 1021,
          "end": 1036,
          "text": "noisy scenarios",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-srw--15:E3"
      },
      {
        "from_id": "abstract-2023--acl-srw--15:E0",
        "to_id": "abstract-2023--acl-srw--15:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-srw--15:E2",
        "to_id": "abstract-2023--acl-srw--15:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Despite the rapid development of neural-based models, syntax still plays a crucial role in modern natural language processing. However, few studies have incorporated syntactic information into ancient Chinese understanding tasks due to the lack of syntactic annotation. This paper explores the role of syntax in ancient Chinese understanding based on the noisy syntax trees from unsupervised derivation and modern Chinese syntax parsers. On top of that, we propose a novel syntax encoding component – confidence-based syntax encoding network (cSEN) to alleviate the side effects from the existing noise caused by unsupervised syntax derivation and the incompatibility between ancient and modern Chinese. Experiments on two typical ancient Chinese understanding tasks, ancient poetry theme classification and ancient-modern Chinese translation, demonstrate that syntactic information can effectively enhance the understanding of ancient Chinese over strong baselines, and that the proposed cSEN plays an important role in noisy scenarios."
    }
  },
  {
    "id": "abstract-2023--acl-long--584",
    "result": [
      {
        "value": {
          "start": 878,
          "end": 935,
          "text": "edge-enhanced joint document-level event extraction model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--584:E0"
      },
      {
        "value": {
          "start": 1314,
          "end": 1327,
          "text": "effectiveness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--584:E1"
      },
      {
        "value": {
          "start": 1350,
          "end": 1397,
          "text": "superiority over the state-of-the-art baselines",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--584:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--584:E0",
        "to_id": "abstract-2023--acl-long--584:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--584:E0",
        "to_id": "abstract-2023--acl-long--584:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We solve the challenging document-level event extraction problem by proposing a joint exaction methodology that can avoid inefficiency and error propagation issues in classic pipeline methods. Essentially, we address the three crucial limitations in existing studies. First, the autoregressive strategy of path expansion heavily relies on the orders of argument role. Second, the number of events in documents must be specified in advance. Last, unexpected errors usually exist when decoding events based on the entity-entity adjacency matrix. To address these issues, this paper designs a Token-Token Bidirectional Event Completed Graph (TT-BECG) in which the relation eType-Role1-Role2 serves as the edge type, precisely revealing which tokens play argument roles in an event of a specific event type. Exploiting the token-token adjacency matrix of the TT-BECG, we develop an edge-enhanced joint document-level event extraction model. Guided by the target token-token adjacency matrix, the predicted token-token adjacency matrix can be obtained during the model training. Then, extracted events and event records in a document are decoded based on the predicted matrix, including the graph structure and edge type decoding. Extensive experiments are conducted on two public datasets, and the results confirm the effectiveness of our method and its superiority over the state-of-the-art baselines."
    }
  },
  {
    "id": "abstract-2023--acl-long--630",
    "result": [
      {
        "value": {
          "start": 787,
          "end": 833,
          "text": "relevant, informative and defeasible questions",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--630:E0"
      }
    ],
    "data": {
      "text": "Context is everything, even in commonsense moral reasoning. Changing contexts can flip the moral judgment of an action; Lying to a friend is wrong in general, but may be morally acceptable if it is intended to protect their life. We present ClarifyDelphi, an interactive system that learns to ask clarification questions (e.g., why did you lie to your friend?) in order to elicit additional salient contexts of a social or moral situation. We posit that questions whose potential answers lead todivergingmoral judgments are the most informative. Thus, we propose a reinforcement learning framework with a defeasibility reward that aims to maximize the divergence between moral judgments of hypothetical answers to a question. Human evaluation demonstrates that our system generates more relevant, informative and defeasible questions compared to competitive baselines. Our work is ultimately inspired by studies in cognitive science that have investigated the flexibility in moral cognition (i.e., the diverse contexts in which moral rules can be bent), and we hope that research in this direction can assist both cognitive and computational investigations of moral judgments."
    }
  },
  {
    "id": "abstract-2023--acl-long--8",
    "result": [
      {
        "value": {
          "start": 149,
          "end": 153,
          "text": "ACLM",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--8:E0"
      }
    ],
    "data": {
      "text": "Complex Named Entity Recognition (NER) is the task of detecting linguistically complex named entities in low-context text. In this paper, we present ACLM Attention-map aware keyword selection for Conditional Language Model fine-tuning), a novel data augmentation approach based on conditional generation, to address the data scarcity problem in low-resource complex NER. ACLM alleviates the context-entity mismatch issue, a problem existing NER data augmentation techniques suffer from and often generates incoherent augmentations by placing complex named entities in the wrong context. ACLM builds on BART and is optimized on a novel text reconstruction or denoising task - we use selective masking (aided by attention maps) to retain the named entities and certain keywords in the input sentence that provide contextually relevant additional knowledge or hints about the named entities. Compared with other data augmentation strategies, ACLM can generate more diverse and coherent augmentations preserving the true word sense of complex entities in the sentence. We demonstrate the effectiveness of ACLM both qualitatively and quantitatively on monolingual, cross-lingual, and multilingual complex NER across various low-resource settings. ACLM outperforms all our neural baselines by a significant margin (1%-36%). In addition, we demonstrate the application of ACLM to other domains that suffer from data scarcity (e.g., biomedical). In practice, ACLM generates more effective and factual augmentations for these domains than prior methods."
    }
  },
  {
    "id": "abstract-2023--acl-long--328",
    "result": [
      {
        "value": {
          "start": 769,
          "end": 805,
          "text": "novel representation learning method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--328:E0"
      },
      {
        "value": {
          "start": 815,
          "end": 845,
          "text": "discriminative representations",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--328:E1"
      },
      {
        "value": {
          "start": 906,
          "end": 946,
          "text": "entity-aware contrastive learning method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--328:E2"
      },
      {
        "value": {
          "start": 1033,
          "end": 1069,
          "text": "distance-based relabeling strategies",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--328:E3"
      },
      {
        "value": {
          "start": 1198,
          "end": 1213,
          "text": "proposed method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--328:E4"
      },
      {
        "from_id": "abstract-2023--acl-long--328:E0",
        "to_id": "abstract-2023--acl-long--328:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "As the categories of named entities rapidly increase, the deployed NER models are required to keep updating toward recognizing more entity types, creating a demand for class-incremental learning for NER. Considering the privacy concerns and storage constraints, the standard paradigm for class-incremental NER updates the models with training data only annotated with the new classes, yet the entities from other entity classes are regarded as “Non-entity” (or “O”). In this work, we conduct an empirical study on the “Unlabeled Entity Problem” and find that it leads to severe confusion between “O” and entities, decreasing class discrimination of old classes and declining the model’s ability to learn new classes. To solve the Unlabeled Entity Problem, we propose a novel representation learning method to learn discriminative representations for the entity classes and “O”. Specifically, we propose an entity-aware contrastive learning method that adaptively detects entity clusters in “O”. Furthermore, we propose two effective distance-based relabeling strategies for better learning the old classes. We introduce a more realistic and challenging benchmark for class-incremental NER, and the proposed method achieves up to 10.62% improvement over the baseline methods."
    }
  },
  {
    "id": "abstract-2023--acl-long--908",
    "result": [
      {
        "value": {
          "start": 385,
          "end": 392,
          "text": "DEplain",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--908:E0"
      }
    ],
    "data": {
      "text": "Text simplification is an intralingual translation task in which documents, or sentences of a complex source text are simplified for a target audience. The success of automatic text simplification systems is highly dependent on the quality of parallel data used for training and evaluation. To advance sentence simplification and document simplification in German, this paper presents DEplain, a new dataset of parallel, professionally written and manually aligned simplifications in plain German “plain DE” or in German: “Einfache Sprache”. DEplain consists of a news-domain (approx. 500 document pairs, approx. 13k sentence pairs) and a web-domain corpus (approx. 150 aligned documents, approx. 2k aligned sentence pairs). In addition, we are building a web harvester and experimenting with automatic alignment methods to facilitate the integration of non-aligned and to be-published parallel documents. Using this approach, we are dynamically increasing the web-domain corpus, so it is currently extended to approx. 750 document pairs and approx. 3.5k aligned sentence pairs. We show that using DEplain to train a transformer-based seq2seq text simplification model can achieve promising results. We make available the corpus, the adapted alignment methods for German, the web harvester and the trained models here:https://github.com/rstodden/DEPlain."
    }
  },
  {
    "id": "abstract-2023--acl-long--793",
    "result": [
      {
        "value": {
          "start": 824,
          "end": 837,
          "text": "effectiveness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--793:E0"
      },
      {
        "value": {
          "start": 692,
          "end": 712,
          "text": "evaluation framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--793:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--793:E1",
        "to_id": "abstract-2023--acl-long--793:E0",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Existing controllable dialogue generation work focuses on the single-attribute control and lacks generalization capability to out-of-distribution multiple attribute combinations. In this paper, we explore the compositional generalization for multi-attribute controllable dialogue generation where a model can learn from seen attribute values and generalize to unseen combinations. We propose a prompt-based disentangled controllable dialogue generation model, DCG. It learns attribute concept composition by generating attribute-oriented prompt vectors and uses a disentanglement loss to disentangle different attributes for better generalization. Besides, we design a unified reference-free evaluation framework for multiple attributes with different levels of granularities. Experiment results on two benchmarks prove the effectiveness of our method and the evaluation metric."
    }
  },
  {
    "id": "abstract-2023--acl-srw--35",
    "result": [
      {
        "value": {
          "start": 1001,
          "end": 1027,
          "text": "downstream MMT performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-srw--35:E0"
      },
      {
        "value": {
          "start": 1128,
          "end": 1138,
          "text": "BLEU score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-srw--35:E1"
      }
    ],
    "data": {
      "text": "Pre-trained language models have achieved remarkable results on several NLP tasks. Most of them adopt masked language modeling to learn representations by randomly masking tokens and predicting them based on their context. However, this random selection of tokens to be masked is inefficient to learn some language patterns as it may not consider linguistic information that can be helpful for many NLP tasks, such as multimodal machine translation (MMT). Hence, we propose three novel masking strategies for cross-lingual visual pre-training - more informed visual masking, more informed textual masking, and more informed visual and textual masking - each one focusing on learning different linguistic patterns. We apply them to Vision Translation Language Modelling for video subtitles (Sato et al., 2022) and conduct extensive experiments on the Portuguese-English MMT task. The results show that our masking approaches yield significant improvements over the original random masking strategy for downstream MMT performance. Our models outperform the MMT baseline and we achieve state-of-the-art accuracy (52.70 in terms of BLEU score) on the How2 dataset, indicating that more informed masking helps in acquiring an understanding of specific language structures and has great potential for language understanding."
    }
  },
  {
    "id": "abstract-2023--acl-long--403",
    "result": [
      {
        "value": {
          "start": 455,
          "end": 498,
          "text": "multi-level knowledge distillation approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--403:E0"
      },
      {
        "value": {
          "start": 1389,
          "end": 1417,
          "text": "state-of-the-art performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--403:E1"
      },
      {
        "value": {
          "start": 213,
          "end": 218,
          "text": "model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--403:E2"
      },
      {
        "value": {
          "start": 1564,
          "end": 1588,
          "text": "exceeds human evaluators",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--403:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--403:E0",
        "to_id": "abstract-2023--acl-long--403:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--403:E2",
        "to_id": "abstract-2023--acl-long--403:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Self-supervised representation learning has proved to be a valuable component for out-of-distribution (OoD) detection with only the texts of in-distribution (ID) examples. These approaches either train a language model from scratch or fine-tune a pre-trained language model using ID examples, and then take the perplexity output by the language model as OoD scores. In this paper, we analyze the complementary characteristic of both methods and propose a multi-level knowledge distillation approach that integrates their strengths while mitigating their limitations. Specifically, we use a fine-tuned model as the teacher to teach a randomly initialized student model on the ID examples. Besides the prediction layer distillation, we present a similarity-based intermediate layer distillation method to thoroughly explore the representation space of the teacher model. In this way, the learned student can better represent the ID data manifold while gaining a stronger ability to map OoD examples outside the ID data manifold with the regularization inherited from pre-training. Besides, the student model sees only ID examples during parameter learning, further promoting more distinguishable features for OoD detection. We conduct extensive experiments over multiple benchmark datasets, i.e., CLINC150, SST, ROSTD, 20 NewsGroups, and AG News; showing that the proposed method yields new state-of-the-art performance. We also explore its application as an AIGC detector to distinguish answers generated by ChatGPT and human experts. It is observed that our model exceeds human evaluators in the pair-expert task on the Human ChatGPT Comparison Corpus."
    }
  },
  {
    "id": "abstract-2023--acl-long--204",
    "result": [
      {
        "value": {
          "start": 177,
          "end": 195,
          "text": "worst-case runtime",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--204:E0"
      }
    ],
    "data": {
      "text": "We present Earley’s (1970) context-free parsing algorithm as a deduction system, incorporating various known and new speed-ups. In particular, our presentation supports a known worst-case runtime improvement from Earley’s (1970)O(N3|G||R|), which is unworkable for the large grammars that arise in natural language processing, toO(N3|G|), which matches the complexity of CKY on a binarized version of the grammar G. Here N is the length of the sentence, |R| is the number of productions in G, and |G| is the total length of those productions. We also provide a version that achieves runtime ofO(N3|M|)with|M| ≤ |G|when the grammar is represented compactly as a single finite-state automaton M (this is partly novel). We carefully treat the generalization to semiring-weighted deduction, preprocessing the grammar like Stolcke (1995) to eliminate the possibility of deduction cycles, and further generalize Stolcke’s method to compute the weights of sentence prefixes. We also provide implementation details for efficient execution, ensuring that on a preprocessed grammar, the semiring-weighted versions of our methods have the same asymptotic runtime and space requirements as the unweighted methods, including sub-cubic runtime on some grammars."
    }
  },
  {
    "id": "abstract-2023--acl-long--741",
    "result": [
      {
        "value": {
          "start": 560,
          "end": 567,
          "text": "UniCoRN",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--741:E0"
      },
      {
        "value": {
          "start": 951,
          "end": 974,
          "text": "BLEU score on fMRI2text",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--741:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--741:E0",
        "to_id": "abstract-2023--acl-long--741:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Decoding text stimuli from cognitive signals (e.g. fMRI) enhances our understanding of the human language system, paving the way for building versatile Brain-Computer Interface. However, existing studies largely focus on decoding individual word-level fMRI volumes from a restricted vocabulary, which is far too idealized for real-world application. In this paper, we propose fMRI2text, the first open-vocabulary task aiming to bridge fMRI time series and human language. Furthermore, to explore the potential of this new task, we present a baseline solution, UniCoRN: the Unified Cognitive Signal ReconstructioN for Brain Decoding. By reconstructing both individual time points and time series, UniCoRN establishes a robust encoder for cognitive signals (fMRI & EEG). Leveraging a pre-trained language model as decoder, UniCoRN proves its efficacy in decoding coherent text from fMRI series across various split settings. Our model achieves a 34.77% BLEU score on fMRI2text, and a 37.04% BLEU when generalized to EEG-to-text decoding, thereby surpassing the former baseline. Experimental results indicate the feasibility of decoding consecutive fMRI volumes, and the effectiveness of decoding different cognitive signals using a unified structure."
    }
  },
  {
    "id": "abstract-2023--acl-long--752",
    "result": [
      {
        "value": {
          "start": 667,
          "end": 708,
          "text": "Topic Efficient StancE Detection (TESTED)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--752:E0"
      },
      {
        "value": {
          "start": 1156,
          "end": 1199,
          "text": "average of 3.5 F1 points increase in-domain",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--752:E1"
      },
      {
        "value": {
          "start": 1234,
          "end": 1278,
          "text": "averaged 10.2 F1 on out-of-domain evaluation",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--752:E2"
      },
      {
        "value": {
          "start": 749,
          "end": 767,
          "text": "sampling technique",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--752:E3"
      },
      {
        "value": {
          "start": 1452,
          "end": 1482,
          "text": "contrastive learning objective",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--752:E4"
      },
      {
        "value": {
          "start": 1506,
          "end": 1565,
          "text": "more pronounced segmentation of samples with varying labels",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--752:E5"
      },
      {
        "from_id": "abstract-2023--acl-long--752:E0",
        "to_id": "abstract-2023--acl-long--752:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--752:E0",
        "to_id": "abstract-2023--acl-long--752:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--752:E4",
        "to_id": "abstract-2023--acl-long--752:E5",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The task of Stance Detection is concerned with identifying the attitudes expressed by an author towards a target of interest. This task spans a variety of domains ranging from social media opinion identification to detecting the stance for a legal claim. However, the framing of the task varies within these domains in terms of the data collection protocol, the label dictionary and the number of available annotations. Furthermore, these stance annotations are significantly imbalanced on a per-topic and inter-topic basis. These make multi-domain stance detection challenging, requiring standardization and domain adaptation. To overcome this challenge, we propose Topic Efficient StancE Detection (TESTED), consisting of a topic-guided diversity sampling technique used for creating a multi-domain data efficient training set and a contrastive objective that is used for fine-tuning a stance classifier using the produced set. We evaluate the method on an existing benchmark of 16 datasets with in-domain, i.e. all topics seen and out-of-domain, i.e. unseen topics, experiments. The results show that the method outperforms the state-of-the-art with an average of 3.5 F1 points increase in-domain and is more generalizable with an averaged 10.2 F1 on out-of-domain evaluation while using <10% of the training data. We show that our sampling technique mitigates both inter- and per-topic class imbalances. Finally, our analysis demonstrates that the contrastive learning objective allows the model for a more pronounced segmentation of samples with varying labels."
    }
  },
  {
    "id": "abstract-2023--acl-long--249",
    "result": [
      {
        "value": {
          "start": 780,
          "end": 787,
          "text": "MiniMoE",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--249:E0"
      },
      {
        "value": {
          "start": 1047,
          "end": 1090,
          "text": "state-of-the-art performance at small FLOPs",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--249:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--249:E0",
        "to_id": "abstract-2023--acl-long--249:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Pretrained language models (LMs) have shown compelling performance on various downstream tasks, but unfortunately they require a tremendous amount of inference compute. Knowledge distillation finds a path to compress LMs to small ones with a teacher-student paradigm. However, when the capacity gap between the teacher and the student is large, a curse of capacity gap appears, invoking a deficiency in distilling LMs. While a few studies have been carried out to fill the gap, the curse is not yet well tackled. In this paper, we aim at lifting the curse of capacity gap via enlarging the capacity of the student without notably increasing the inference compute. Largely motivated by sparse activation regime of mixture of experts (MoE), we propose a mixture of minimal experts (MiniMoE), which imposes extra parameters to the student but introduces almost no additional inference compute. Experimental results on GLUE and CoNLL demonstrate the curse of capacity gap is lifted by the magic of MiniMoE to a large extent. MiniMoE also achieves the state-of-the-art performance at small FLOPs compared with a range of competitive baselines. With a compression rate as much as ~50×, MiniMoE preserves ~95% GLUE score of the teacher."
    }
  },
  {
    "id": "abstract-2023--acl-long--310",
    "result": [
      {
        "value": {
          "start": 362,
          "end": 366,
          "text": "DAAM",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--310:E0"
      },
      {
        "value": {
          "start": 1055,
          "end": 1077,
          "text": "presence of cohyponyms",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--310:E1"
      },
      {
        "value": {
          "start": 1086,
          "end": 1104,
          "text": "generation quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--310:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--310:E1",
        "to_id": "abstract-2023--acl-long--310:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Diffusion models are a milestone in text-to-image generation, but they remain poorly understood, lacking interpretability analyses. In this paper, we perform a text-image attribution analysis on Stable Diffusion, a recently open-sourced model. To produce attribution maps, we upscale and aggregate cross-attention maps in the denoising module, naming our method DAAM. We validate it by testing its segmentation ability on nouns, as well as its generalized attribution quality on all parts of speech, rated by humans. On two generated datasets, we attain a competitive 58.8-64.8 mIoU on noun segmentation and fair to good mean opinion scores (3.4-4.2) on generalized attribution. Then, we apply DAAM to study the role of syntax in the pixel space across head–dependent heat map interaction patterns for ten common dependency relations. We show that, for some relations, the head map consistently subsumes the dependent, while the opposite is true for others. Finally, we study several semantic phenomena, focusing on feature entanglement; we find that the presence of cohyponyms worsens generation quality by 9%, and descriptive adjectives attend too broadly. We are the first to interpret large diffusion models from a visuolinguistic perspective, which enables future research. Our code is athttps://github.com/castorini/daam."
    }
  },
  {
    "id": "abstract-2023--acl-long--636",
    "result": [
      {
        "value": {
          "start": 309,
          "end": 351,
          "text": "Counterfactual Active Learning (CounterAL)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--636:E0"
      },
      {
        "value": {
          "start": 1154,
          "end": 1169,
          "text": "IID performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--636:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--636:E0",
        "to_id": "abstract-2023--acl-long--636:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We study the out-of-distribution generalization of active learning that adaptively selects samples for annotation in learning the decision boundary of classification. Our empirical study finds that increasingly annotating seen samples may hardly benefit the generalization. To address the problem, we propose Counterfactual Active Learning (CounterAL) that empowers active learning with counterfactual thinking to bridge the seen samples with unseen cases. In addition to annotating factual samples, CounterAL requires annotators to answer counterfactual questions to construct counterfactual samples for training. To achieve CounterAL, we design a new acquisition strategy that selects the informative factual-counterfactual pairs for annotation; and a new training strategy that pushes the model update to focus on the discrepancy between factual and counterfactual samples. We evaluate CounterAL on multiple public datasets of sentiment analysis and natural language inference. The experiment results show that CounterAL requires fewer acquisition rounds and outperforms existing active learning methods by a large margin in OOD tests with comparable IID performance."
    }
  },
  {
    "id": "abstract-2023--acl-short--80",
    "result": [
      {
        "value": {
          "start": 850,
          "end": 862,
          "text": "PLL-word-l2r",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--80:E0"
      }
    ],
    "data": {
      "text": "Estimating the log-likelihood of a given sentence under an autoregressive language model is straightforward: one can simply apply the chain rule and sum the log-likelihood values for each successive token. However, for masked language models (MLMs), there is no direct way to estimate the log-likelihood of a sentence. To address this issue, Salazar et al. (2020) propose to estimate sentence pseudo-log-likelihood (PLL) scores, computed by successively masking each sentence token, retrieving its score using the rest of the sentence as context, and summing the resulting values. Here, we demonstrate that the original PLL method yields inflated scores for out-of-vocabulary words and propose an adapted metric, in which we mask not only the target token, but also all within-word tokens to the right of the target. We show that our adapted metric (PLL-word-l2r) outperforms both the original PLL metric and a PLL metric in which all within-word tokens are masked. In particular, it better satisfies theoretical desiderata and better correlates with scores from autoregressive models. Finally, we show that the choice of metric affects even tightly controlled, minimal pair evaluation benchmarks (such as BLiMP), underscoring the importance of selecting an appropriate scoring metric for evaluating MLM properties."
    }
  },
  {
    "id": "abstract-2023--acl-demo--47",
    "result": [
      {
        "value": {
          "start": 647,
          "end": 654,
          "text": "OpenICL",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-demo--47:E0"
      }
    ],
    "data": {
      "text": "In recent years, In-context Learning (ICL) has gained increasing attentionand emerged as the new paradigm for large language model (LLM) evaluation. Unlike traditional fine-tuning methods, ICL instead adapts the pre-trained models to unseen tasks without any parameter updates. However, the implementation of ICL is sophisticated due to the diverse retrieval and inference methods involved, as well as the varying pre-processing requirements for different models, datasets, and tasks. A unified and flexible framework for ICL is urgently needed to ease the implementation of the aforementioned components. To facilitate ICL research, we introduce OpenICL, an open-source toolkit for ICL and LLM evaluation. OpenICL is research-friendly with a highly flexible architecture that users can easily combine different components to suit their needs. It also provides various state-of-the-art retrieval and inference methods to streamline the process of adapting ICL to cutting-edge research. The effectiveness of OpenICL has been validated on a wide range of NLP tasks, including classification, QA, machine translation, and semantic parsing. As a side-product, we found OpenICL to be an efficient yet robust tool for LLMs evaluation. OpenICL is released athttps://github.com/Shark-NLP/OpenICL."
    }
  },
  {
    "id": "abstract-2023--acl-long--712",
    "result": [
      {
        "value": {
          "start": 436,
          "end": 451,
          "text": "Bayesian method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--712:E0"
      }
    ],
    "data": {
      "text": "We use both Bayesian and neural models to dissect a data set of Chinese learners’ pre- and post-interventional responses to two tests measuring their understanding of English prepositions. The results mostly replicate previous findings from frequentist analyses and newly reveal crucial interactions between student ability, task type, and stimulus sentence. Given the sparsity of the data as well as high diversity among learners, the Bayesian method proves most useful; but we also see potential in using language model probabilities as predictors of grammaticality and learnability."
    }
  },
  {
    "id": "abstract-2023--acl-long--395",
    "result": [
      {
        "value": {
          "start": 1044,
          "end": 1059,
          "text": "efficiency gain",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--395:E0"
      }
    ],
    "data": {
      "text": "Despite much success in natural language processing (NLP), pre-trained language models typically lead to a high computational cost during inference. Multi-exit is a mainstream approach to address this issue by making a trade-off between efficiency and accuracy, where the saving of computation comes from an early exit. However, whether such saving from early-exiting is robust remains unknown. Motivated by this, we first show that directly adapting existing adversarial attack approaches targeting model accuracy cannot significantly reduce inference efficiency. To this end, we propose a simple yet effective attacking framework, SAME, a novel slowdown attack framework on multi-exit models, which is specially tailored to reduce the efficiency of the multi-exit models. By leveraging the multi-exit models’ design characteristics, we utilize all internal predictions to guide the adversarial sample generation instead of merely considering the final prediction. Experiments on the GLUE benchmark show that SAME can effectively diminish the efficiency gain of various multi-exit models by 80% on average, convincingly validating its effectiveness and generalization ability."
    }
  },
  {
    "id": "abstract-2023--acl-long--597",
    "result": [
      {
        "value": {
          "start": 623,
          "end": 640,
          "text": "simple MLP models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--597:E0"
      },
      {
        "value": {
          "start": 801,
          "end": 842,
          "text": "scoring function and loss function design",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--597:E1"
      },
      {
        "value": {
          "start": 876,
          "end": 897,
          "text": "KGC model performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--597:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--597:E1",
        "to_id": "abstract-2023--acl-long--597:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Knowledge graphs (KGs) facilitate a wide variety of applications. Despite great efforts in creation and maintenance, even the largest KGs are far from complete. Hence, KG completion (KGC) has become one of the most crucial tasks for KG research. Recently, considerable literature in this space has centered around the use of Message Passing (Graph) Neural Networks (MPNNs), to learn powerful embeddings. The success of these methods is naturally attributed to the use of MPNNs over simpler multi-layer perceptron (MLP) models, given their additional message passing (MP) component. In this work, we find that surprisingly, simple MLP models are able to achieve comparable performance to MPNNs, suggesting that MP may not be as crucial as previously believed. With further exploration, we show careful scoring function and loss function design has a much stronger influence on KGC model performance. This suggests a conflation of scoring function design, loss function design, and MP in prior work, with promising insights regarding the scalability of state-of-the-art KGC methods today, as well as careful attention to more suitable MP designs for KGC tasks tomorrow."
    }
  },
  {
    "id": "abstract-2023--acl-long--500",
    "result": [
      {
        "value": {
          "start": 593,
          "end": 599,
          "text": "SPARTA",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--500:E0"
      }
    ],
    "data": {
      "text": "Conversational question generation aims to generate questions that depend on both context and conversation history. Conventional works utilizing deep learning have shown promising results, but heavily rely on the availability of large-scale annotated conversations. In this paper, we introduce a more realistic and less explored setting, Zero-shot Conversational Question Generation (ZeroCQG), which requires no human-labeled conversations for training. To solve ZeroCQG, we propose a multi-stage knowledge transfer framework, Synthesize, Prompt, and trAnsfer with pRe-Trained lAnguage model (SPARTA) to effectively leverage knowledge from single-turn question generation instances. To validate the zero-shot performance of SPARTA, we conduct extensive experiments on three conversational datasets: CoQA, QuAC, and DoQA by transferring knowledge from three single-turn datasets: MS MARCO, NewsQA, and SQuAD. The experimental results demonstrate the superior performance of our method. Specifically, SPARTA has achieved 14.81 BLEU-4 (88.2% absolute improvement compared to T5) in CoQA with knowledge transferred from SQuAD."
    }
  },
  {
    "id": "abstract-2023--acl-long--235",
    "result": [
      {
        "value": {
          "start": 510,
          "end": 530,
          "text": "exact match accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--235:E0"
      },
      {
        "value": {
          "start": 532,
          "end": 562,
          "text": "intent classification accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--235:E1"
      },
      {
        "value": {
          "start": 568,
          "end": 589,
          "text": "slot-filling F1 score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--235:E2"
      }
    ],
    "data": {
      "text": "We present the MASSIVE dataset–Multilingual Amazon Slu resource package (SLURP) for Slot-filling, Intent classification, and Virtual assistant Evaluation. MASSIVE contains 1M realistic, parallel, labeled virtual assistant utterances spanning 51 languages, 18 domains, 60 intents, and 55 slots. MASSIVE was created by tasking professional translators to localize the English-only SLURP dataset into 50 typologically diverse languages from 29 genera. We also present modeling results on XLM-R and mT5, including exact match accuracy, intent classification accuracy, and slot-filling F1 score. We have released our dataset, modeling code, and models publicly."
    }
  },
  {
    "id": "abstract-2023--acl-long--874",
    "result": [
      {
        "value": {
          "start": 777,
          "end": 788,
          "text": "joint model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--874:E0"
      }
    ],
    "data": {
      "text": "Implicit discourse relation classification is a challenging task due to the absence of discourse connectives. To overcome this issue, we design an end-to-end neural model to explicitly generate discourse connectives for the task, inspired by the annotation process of PDTB. Specifically, our model jointly learns to generate discourse connectives between arguments and predict discourse relations based on the arguments and the generated connectives. To prevent our relation classifier from being misled by poor connectives generated at the early stage of training while alleviating the discrepancy between training and inference, we adopt Scheduled Sampling to the joint learning. We evaluate our method on three benchmarks, PDTB 2.0, PDTB 3.0, and PCC. Results show that our joint model significantly outperforms various baselines on three datasets, demonstrating its superiority for the task."
    }
  },
  {
    "id": "abstract-2023--acl-long--527",
    "result": [
      {
        "value": {
          "start": 832,
          "end": 849,
          "text": "generative models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--527:E0"
      },
      {
        "value": {
          "start": 937,
          "end": 948,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--527:E1"
      },
      {
        "value": {
          "start": 746,
          "end": 762,
          "text": "guidance signals",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--527:E2"
      },
      {
        "value": {
          "start": 975,
          "end": 989,
          "text": "ROUGE-1 scores",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--527:E3"
      },
      {
        "value": {
          "start": 832,
          "end": 848,
          "text": "generative model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--527:E4"
      },
      {
        "value": {
          "start": 1185,
          "end": 1231,
          "text": "state-of-the-art performance on known hashtags",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--527:E5"
      },
      {
        "from_id": "abstract-2023--acl-long--527:E0",
        "to_id": "abstract-2023--acl-long--527:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--527:E2",
        "to_id": "abstract-2023--acl-long--527:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--527:E4",
        "to_id": "abstract-2023--acl-long--527:E5",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Short-form video hashtag recommendation (SVHR) aims to recommend hashtags to content creators from videos and corresponding descriptions. Most prior studies regard SVHR as a classification or ranking problem and select hashtags from a set of limited candidates. However, in reality, users can create new hashtags, and trending hashtags change rapidly over time on social media. Both of these properties cannot be easily modeled with classification approaches. To bridge this gap, we formulate SVHR as a generation task that better represents how hashtags are created naturally. Additionally, we propose the Guided Generative Model (GGM) where we augment the input features by retrieving relevant hashtags from a large-scale hashtag pool as extra guidance signals. Experimental results on two short-form video datasets show that our generative models outperform strong classification baselines, and the guidance signals further boost the performance by 8.11 and 2.17 absolute ROUGE-1 scores on average, respectively. We also perform extensive analyses including human evaluation, demonstrating that our generative model can create meaningful and relevant novel hashtags while achieving state-of-the-art performance on known hashtags"
    }
  },
  {
    "id": "abstract-2023--acl-long--46",
    "result": [
      {
        "value": {
          "start": 425,
          "end": 436,
          "text": "ColD Fusion",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--46:E0"
      },
      {
        "value": {
          "start": 976,
          "end": 1005,
          "text": "finetuning on unseen datasets",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--46:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--46:E0",
        "to_id": "abstract-2023--acl-long--46:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Pretraining has been shown to scale well with compute, data size and data diversity. Multitask learning trains on a mixture of supervised datasets and produces improved performance compared to self-supervised pretraining. Until now, massively multitask learning required simultaneous access to all datasets in the mixture and heavy compute resources that are only available to well-resourced teams. In this paper, we propose ColD Fusion, a method that provides the benefits of multitask learning but leverages distributed computation and requires limited communication and no sharing of data. Consequentially, ColD Fusion can create a synergistic loop, where finetuned models can be recycled to continually improve the pretrained model they are based on. We show that ColD Fusion yields comparable benefits to multitask training by producing a model that (a) attains strong performance on all of the datasets it was multitask trained on and (b) is a better starting point for finetuning on unseen datasets. We find ColD Fusion outperforms RoBERTa and even previous multitask models. Specifically, when training and testing on 35 diverse datasets, ColD Fusion-based model outperforms RoBERTa by 2.19 points on average without any changes to the architecture."
    }
  },
  {
    "id": "abstract-2023--acl-long--769",
    "result": [
      {
        "value": {
          "start": 720,
          "end": 728,
          "text": "FinTrust",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--769:E0"
      },
      {
        "value": {
          "start": 835,
          "end": 903,
          "text": "consistency of state-of-the-art NLP models for financial forecasting",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--769:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--769:E0",
        "to_id": "abstract-2023--acl-long--769:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Financial forecasting has been an important and active area of machine learning research, as even the most modest advantages in predictive accuracy can be parlayed into significant financial gains. Recent advances in natural language processing (NLP) bring the opportunity to leverage textual data, such as earnings reports of publicly traded companies, to predict the return rate for an asset. However, when dealing with such a sensitive task, the consistency of models – their invariance under meaning-preserving alternations in input – is a crucial property for building user trust. Despite this, current methods for financial forecasting do not take consistency into consideration. To address this issue, we propose FinTrust, an evaluation tool that assesses logical consistency in financial text. Using FinTrust, we show that the consistency of state-of-the-art NLP models for financial forecasting is poor. Our analysis of the performance degradation caused by meaning-preserving alternations suggests that current text-based methods are not suitable for robustly predicting market information."
    }
  },
  {
    "id": "abstract-2023--acl-short--62",
    "result": [
      {
        "value": {
          "start": 586,
          "end": 630,
          "text": "correctly retrieving global document context",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--62:E0"
      },
      {
        "value": {
          "start": 63,
          "end": 74,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--62:E1"
      },
      {
        "from_id": "abstract-2023--acl-short--62:E0",
        "to_id": "abstract-2023--acl-short--62:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Pre-trained transformer-based models have recently shown great performance when applied to Named Entity Recognition (NER). As the complexity of their self-attention mechanism prevents them from processing long documents at once, these models are usually applied in a sequential fashion. Such an approach unfortunately only incorporates local context and prevents leveraging global document context in long documents such as novels, which might hinder performance. In this article, we explore the impact of global document context, and its relationships with local context. We find that correctly retrieving global document context has a greater impact on performance than only leveraging local context, prompting for further research on how to better retrieve that context."
    }
  },
  {
    "id": "abstract-2023--acl-long--729",
    "result": [
      {
        "value": {
          "start": 656,
          "end": 663,
          "text": "DS-MOCE",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--729:E0"
      },
      {
        "value": {
          "start": 1087,
          "end": 1095,
          "text": "F1 score",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--729:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--729:E0",
        "to_id": "abstract-2023--acl-long--729:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "With the rapid growth of Massive Open Online Courses (MOOCs), it is expensive and time-consuming to extract high-quality knowledgeable concepts taught in the course by human effort to help learners grasp the essence of the course. In this paper, we propose to automatically extract course concepts using distant supervision to eliminate the heavy work of human annotations, which generates labels by matching them with an easily accessed dictionary. However, this matching process suffers from severe noisy and incomplete annotations because of the limited dictionary and diverse MOOCs. To tackle these challenges, we present a novel three-stage framework DS-MOCE, which leverages the power of pre-trained language models explicitly and implicitly and employs discipline-embedding models with a self-train strategy based on label generation refinement across different domains. We also provide an expert-labeled dataset spanning 20 academic disciplines. Experimental results demonstrate the superiority of DS-MOCE over the state-of-the-art distantly supervised methods (with 7% absolute F1 score improvement). Code and data are now available athttps://github.com/THU-KEG/MOOC-NER."
    }
  },
  {
    "id": "abstract-2023--acl-long--86",
    "result": [
      {
        "value": {
          "start": 306,
          "end": 346,
          "text": "novel non-autoregressive approach to GEC",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--86:E0"
      }
    ],
    "data": {
      "text": "Grammatical error correction (GEC) is an important NLP task that is currently usually solved with autoregressive sequence-to-sequence models. However, approaches of this class are inherently slow due to one-by-one token generation, so non-autoregressive alternatives are needed. In this work, we propose a novel non-autoregressive approach to GEC that decouples the architecture into a permutation network that outputs a self-attention weight matrix that can be used in beam search to find the best permutation of input tokens (with auxiliary <ins> tokens) and a decoder network based on a step-unrolled denoising autoencoder that fills in specific tokens. This allows us to find the token permutation after only one forward pass of the permutation network, avoiding autoregressive constructions. We show that the resulting network improves over previously known non-autoregressive methods for GEC and reaches the level of autoregressive methods that do not use language-specific synthetic data generation methods. Our results are supported by a comprehensive experimental validation on the ConLL-2014 and BEA datasets and an extensive ablation study that supports our architectural and algorithmic choices."
    }
  },
  {
    "id": "abstract-2023--acl-long--63",
    "result": [
      {
        "value": {
          "start": 1233,
          "end": 1270,
          "text": "pretrained bidirectional distillation",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--63:E0"
      },
      {
        "value": {
          "start": 1297,
          "end": 1328,
          "text": "machine translation performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--63:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--63:E0",
        "to_id": "abstract-2023--acl-long--63:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Knowledge transfer can boost neural machine translation (NMT), for example, by finetuning a pretrained masked language model (LM). However, it may suffer from the forgetting problem and the structural inconsistency between pretrained LMs and NMT models. Knowledge distillation (KD) may be a potential solution to alleviate these issues, but few studies have investigated language knowledge transfer from pretrained language models to NMT models through KD. In this paper, we propose Pretrained Bidirectional Distillation (PBD) for NMT, which aims to efficiently transfer bidirectional language knowledge from masked language pretraining to NMT models. Its advantages are reflected in efficiency and effectiveness through a globally defined and bidirectional context-aware distillation objective. Bidirectional language knowledge of the entire sequence is transferred to an NMT model concurrently during translation training. Specifically, we propose self-distilled masked language pretraining to obtain the PBD objective. We also design PBD losses to efficiently distill the language knowledge, in the form of token probabilities, to the encoder and decoder of an NMT model using the PBD objective. Extensive experiments reveal that pretrained bidirectional distillation can significantly improve machine translation performance and achieve competitive or even better results than previous pretrain-finetune or unified multilingual translation methods in supervised, unsupervised, and zero-shot scenarios. Empirically, it is concluded that pretrained bidirectional distillation is an effective and efficient method for transferring language knowledge from pretrained language models to NMT models."
    }
  },
  {
    "id": "abstract-2023--acl-long--200",
    "result": [
      {
        "value": {
          "start": 380,
          "end": 416,
          "text": "novel open-vocabulary language model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--200:E0"
      },
      {
        "value": {
          "start": 1064,
          "end": 1082,
          "text": "hierarchical model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--200:E1"
      }
    ],
    "data": {
      "text": "Current state-of-the-art models for natural language understanding require a preprocessing step to convert raw text into discrete tokens. This process known as tokenization relies on a pre-built vocabulary of words or sub-word morphemes. This fixed vocabulary limits the model’s robustness to spelling errors and its capacity to adapt to new domains. In this work, we introduce a novel open-vocabulary language model that adopts a hierarchical two-level approach: one at the word level and another at the sequence level. Concretely, we design an intra-word module that uses a shallow Transformer architecture to learn word representations from their characters, and a deep inter-word Transformer module that contextualizes each word representation by attending to the entire word sequence. Our model thus directly operates on character sequences with explicit awareness of word boundaries, but without biased sub-word or word-level vocabulary. Experiments on various downstream tasks show that our method outperforms strong baselines. We also demonstrate that our hierarchical model is robust to textual corruption and domain shift."
    }
  },
  {
    "id": "abstract-2023--acl-long--103",
    "result": [
      {
        "value": {
          "start": 505,
          "end": 508,
          "text": "SSR",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--103:E0"
      }
    ],
    "data": {
      "text": "Automating Cross-lingual Science Journalism (CSJ) aims to generate popular science summaries from English scientific texts for non-expert readers in their local language. We introduce CSJ as a downstream task of text simplification and cross-lingual scientific summarization to facilitate science journalists’ work. We analyze the performance of possible existing solutions as baselines for the CSJ task. Based on these findings, we propose to combine the three components - SELECT, SIMPLIFY and REWRITE (SSR) to produce cross-lingual simplified science summaries for non-expert readers. Our empirical evaluation on the Wikipedia dataset shows that SSR significantly outperforms the baselines for the CSJ task and can serve as a strong baseline for future work. We also perform an ablation study investigating the impact of individual components of SSR. Further, we analyze the performance of SSR on a high-quality, real-world CSJ dataset with human evaluation and in-depth analysis, demonstrating the superior performance of SSR for CSJ."
    }
  },
  {
    "id": "abstract-2023--acl-long--281",
    "result": [
      {
        "value": {
          "start": 409,
          "end": 417,
          "text": "INFINITY",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--281:E0"
      }
    ],
    "data": {
      "text": "Graph-to-text (G2T) generation and text-to-graph (T2G) triple extraction are two essential tasks for knowledge graphs. Existing unsupervised approaches become suitable candidates for jointly learning the two tasks due to their avoidance of using graph-text parallel data. However, they adopt multiple complex modules and still require entity information or relation type for training. To this end, we propose INFINITY, a simple yet effective unsupervised method with a unified pretrained language model that does not introduce external annotation tools or additional parallel information. It achieves fully unsupervised graph-text mutual conversion for the first time. Specifically, INFINITY treats both G2T and T2G as a bidirectional sequence generation task by fine-tuning only one pretrained seq2seq model. A novel back-translation-based framework is then designed to generate synthetic parallel data automatically. Besides, we investigate the impact of graph linearization and introduce the structure-aware fine-tuning strategy to alleviate possible performance deterioration via retaining structural information in graph sequences. As a fully unsupervised framework, INFINITY is empirically verified to outperform state-of-the-art baselines for G2T and T2G tasks. Additionally, we also devise a new training setting called cross learning for low-resource unsupervised information extraction."
    }
  },
  {
    "id": "abstract-2023--acl-long--439",
    "result": [
      {
        "value": {
          "start": 428,
          "end": 470,
          "text": "contrastive transfer pattern mining (CTPM)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--439:E0"
      },
      {
        "value": {
          "start": 560,
          "end": 578,
          "text": "performance of TST",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--439:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--439:E0",
        "to_id": "abstract-2023--acl-long--439:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Text style transfer (TST) is an important task in natural language generation, which aims to alter the stylistic attributes (e.g., sentiment) of a sentence and keep its semantic meaning unchanged. Most existing studies mainly focus on the transformation between styles, yet ignore that this transformation can be actually carried out via different hidden transfer patterns. To address this problem, we propose a novel approach, contrastive transfer pattern mining (CTPM), which automatically mines and utilizes inherent latent transfer patterns to improve the performance of TST. Specifically, we design an adaptive clustering module to automatically discover hidden transfer patterns from the data, and introduce contrastive learning based on the discovered patterns to obtain more accurate sentence representations, and thereby benefit the TST task. To the best of our knowledge, this is the first work that proposes the concept of transfer patterns in TST, and our approach can be applied in a plug-and-play manner to enhance other TST methods to further improve their performance. Extensive experiments on benchmark datasets verify the effectiveness and generality of our approach."
    }
  },
  {
    "id": "abstract-2023--acl-long--347",
    "result": [
      {
        "value": {
          "start": 459,
          "end": 465,
          "text": "APOLLO",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--347:E0"
      }
    ],
    "data": {
      "text": "Logical reasoning over text is an important ability that requires understanding the semantics of the text and reasoning through them to arrive at correct inferences. Prior works on pretraining language models to improve the logical reasoning ability require complex processing of training data (e.g., aligning symbolic knowledge to text), yielding task-specific data augmentation that is not easy to adapt to any general text corpus. In this work, we propose APOLLO, a simple adaptive pretraining approach to improve the logical reasoning skills of language models. We select a subset of Wikipedia for adaptive pretraining using a set of logical inference keywords as filter words. Further, we propose two self-supervised loss functions for training. First, we modify the masked language modeling loss only to mask specific parts-of-speech words that likely require higher-order reasoning to predict them. Second, we propose a sentence-level classification loss that teaches the model to distinguish between entailment and contradiction types of sentences. The proposed pretraining paradigm is both simple and independent of task formats. We demonstrate the effectiveness of APOLLO by comparing it with prior baselines on two logical reasoning datasets. APOLLO performs comparably on ReClor and outperforms baselines on LogiQA."
    }
  },
  {
    "id": "abstract-2023--acl-long--135",
    "result": [
      {
        "value": {
          "start": 1093,
          "end": 1104,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--135:E0"
      }
    ],
    "data": {
      "text": "The task of web information extraction is to extract target fields of an object from web pages, such as extracting the name, genre and actor from a movie page. Recent sequential modeling approaches have achieved state-of-the-art results on web information extraction. However, most of these methods only focus on extracting information from textual sources while ignoring the rich information from other modalities such as image and web layout. In this work, we propose a novel MUltimodal Structural Transformer (MUST) that incorporates multiple modalities for web information extraction. Concretely, we develop a structural encoder that jointly encodes the multimodal information based on the HTML structure of the web layout, where high-level DOM nodes, and low-level text and image tokens are introduced to represent the entire page. Structural attention patterns are designed to learn effective cross-modal embeddings for all DOM nodes and low-level tokens. An extensive set of experiments are conducted on WebSRC and Common Crawl benchmarks. Experimental results demonstrate the superior performance of MUST over several state-of-the-art baselines."
    }
  },
  {
    "id": "abstract-2023--acl-long--475",
    "result": [
      {
        "value": {
          "start": 1067,
          "end": 1095,
          "text": "state-of-the-art performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--475:E0"
      }
    ],
    "data": {
      "text": "To fully leverage the advantages of large-scale pre-trained language models (PLMs) on downstream tasks, it has become a ubiquitous adaptation paradigm to fine-tune the entire parameters of PLMs. However, this paradigm poses issues of inefficient updating and resource over-consuming for fine-tuning in data-scarce and resource-limited scenarios, because of the large scale of parameters in PLMs. To alleviate these concerns, in this paper, we propose a parameter-efficient fine-tuning method HiFi, that is, only the highly informative and strongly correlated attention heads for the specific task are fine-tuned. To search for those significant attention heads, we develop a novel framework to analyze the effectiveness of heads. Specifically, we first model the relationship between heads into a graph from two perspectives of information richness and correlation, and then apply PageRank algorithm to determine the relative importance of each head. Extensive experiments on the GLUE benchmark demonstrate the effectiveness of our method, and show that HiFi obtains state-of-the-art performance over the prior baselines."
    }
  },
  {
    "id": "abstract-2023--acl-long--871",
    "result": [
      {
        "value": {
          "start": 598,
          "end": 608,
          "text": "HyperMixer",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--871:E0"
      }
    ],
    "data": {
      "text": "Transformer-based architectures are the model of choice for natural language understanding, but they come at a significant cost, as they have quadratic complexity in the input length, require a lot of training data, and can be difficult to tune. In the pursuit of lower costs, we investigate simple MLP-based architectures. We find that existing architectures such as MLPMixer, which achieves token mixing through a static MLP applied to each feature independently, are too detached from the inductive biases required for natural language understanding. In this paper, we propose a simple variant, HyperMixer, which forms the token mixing MLP dynamically using hypernetworks. Empirically, we demonstrate that our model performs better than alternative MLP-based models, and on par with Transformers. In contrast to Transformers, HyperMixer achieves these results at substantially lower costs in terms of processing time, training data, and hyperparameter tuning."
    }
  },
  {
    "id": "abstract-2023--acl-long--383",
    "result": [
      {
        "value": {
          "start": 11,
          "end": 23,
          "text": "PairSpanBERT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--383:E0"
      }
    ],
    "data": {
      "text": "We present PairSpanBERT, a SpanBERT-based pre-trained model specialized for bridging resolution. To this end, we design a novel pre-training objective that aims to learn the contexts in which two mentions are implicitly linked to each other from a large amount of data automatically generated either heuristically or via distance supervision with a knowledge graph. Despite the noise inherent in the automatically generated data, we achieve the best results reported to date on three evaluation datasets for bridging resolution when replacing SpanBERT with PairSpanBERT in a state-of-the-art resolver that jointly performs entity coreference resolution and bridging resolution."
    }
  },
  {
    "id": "abstract-2023--acl-long--799",
    "result": [
      {
        "value": {
          "start": 844,
          "end": 858,
          "text": "clarifications",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--799:E0"
      }
    ],
    "data": {
      "text": "Code generation from text requires understanding the user’s intent from a natural languagedescription and generating an executable code snippet that satisfies this intent. While recent pretrained language models demonstrate remarkable performance for this task, these models fail when the given natural language description is under-specified. In this work, we introduce a novel and more realistic setup for this task. We hypothesize that the under-specification of a natural language description can be resolved by asking clarification questions. Therefore, we collect and introduce a new dataset named CodeClarQA containing pairs of natural language descriptions and code with created synthetic clarification questions and answers. The empirical results of our evaluation of pretrained language model performance on code generation show that clarifications result in more precisely generated code, as shown by the substantial improvement of model performance in all evaluation metrics. Alongside this, our task and dataset introduce new challenges to the community, including when and what clarification questions should be asked. Our code and dataset are available on GitHub."
    }
  },
  {
    "id": "abstract-2023--acl-long--692",
    "result": [
      {
        "value": {
          "start": 437,
          "end": 460,
          "text": "best-k search algorithm",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--692:E0"
      },
      {
        "value": {
          "start": 771,
          "end": 786,
          "text": "natural outputs",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--692:E1"
      },
      {
        "value": {
          "start": 851,
          "end": 863,
          "text": "text quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--692:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--692:E0",
        "to_id": "abstract-2023--acl-long--692:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--692:E0",
        "to_id": "abstract-2023--acl-long--692:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Modern natural language generation paradigms require a decoding strategy to obtain quality sequences out of the model. Beam search yields high-quality but low diversity outputs; stochastic approaches suffer from high variance and sometimes low quality. In this work, we propose a deterministic search algorithm balancing both quality and diversity. We first investigate the vanilla best-first search (BFS) algorithm and then propose the best-k search algorithm. Inspired by BFS, we greedily expand the top k nodes, instead of the first node, to boost efficiency and diversity. Upweighting recently discovered nodes accompanied by heap pruning ensures the completeness of the search procedure. Experiments on four NLG tasks show that best-k search yields more diverse and natural outputs compared to strong baselines, while our approach maintains high text quality. The proposed algorithm is parameter-free, lightweight, efficient, and easy-to-use."
    }
  },
  {
    "id": "abstract-2023--acl-long--140",
    "result": [
      {
        "value": {
          "start": 849,
          "end": 914,
          "text": "enrich query and passage representations with lexical information",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--140:E0"
      }
    ],
    "data": {
      "text": "Dual encoders are now the dominant architecture for dense retrieval. Yet, we have little understanding of how they represent text, and why this leads to good performance. In this work, we shed light on this question via distributions over the vocabulary. We propose to interpret the vector representations produced by dual encoders by projecting them into the model’s vocabulary space. We show that the resulting projections contain rich semantic information, and draw connection between them and sparse retrieval. We find that this view can offer an explanation for some of the failure cases of dense retrievers. For example, we observe that the inability of models to handle tail entities is correlated with a tendency of the token distributions to forget some of the tokens of those entities. We leverage this insight and propose a simple way to enrich query and passage representations with lexical information at inference time, and show that this significantly improves performance compared to the original model in zero-shot settings, and specifically on the BEIR benchmark."
    }
  },
  {
    "id": "abstract-2023--acl-long--657",
    "result": [
      {
        "value": {
          "start": 1063,
          "end": 1087,
          "text": "language family distance",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--657:E0"
      },
      {
        "value": {
          "start": 410,
          "end": 427,
          "text": "negative transfer",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--657:E1"
      },
      {
        "value": {
          "start": 1148,
          "end": 1174,
          "text": "conversational speech data",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--657:E2"
      },
      {
        "value": {
          "start": 7,
          "end": 27,
          "text": "language acquisition",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--657:E3"
      },
      {
        "value": {
          "start": 1232,
          "end": 1252,
          "text": "scripted speech data",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--657:E4"
      },
      {
        "from_id": "abstract-2023--acl-long--657:E0",
        "to_id": "abstract-2023--acl-long--657:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--657:E2",
        "to_id": "abstract-2023--acl-long--657:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--657:E4",
        "to_id": "abstract-2023--acl-long--657:E3",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Second language acquisition (SLA) research has extensively studied cross-linguistic transfer, the influence of linguistic structure of a speaker’s native language [L1] on the successful acquisition of a foreign language [L2]. Effects of such transfer can be positive (facilitating acquisition) or negative (impeding acquisition). We find that NLP literature has not given enough attention to the phenomenon of negative transfer. To understand patterns of both positive and negative transfer between L1 and L2, we model sequential second language acquisition in LMs. Further, we build a Mutlilingual Age Ordered CHILDES (MAO-CHILDES)—a dataset consisting of 5 typologically diverse languages, i.e., German, French, Polish, Indonesian, and Japanese—to understand the degree to which native Child-Directed Speech (CDS) [L1] can help or conflict with English language acquisition [L2]. To examine the impact of native CDS, we use the TILT-based cross lingual transfer learning approach established by Papadimitriou and Jurafsky (2020) and find that, as in human SLA, language family distance predicts more negative transfer. Additionally, we find that conversational speech data shows greater facilitation for language acquisition than scripted speech data. Our findings call for further research using our novel Transformer-based SLA models and we would like to encourage it by releasing our code, data, and models."
    }
  },
  {
    "id": "abstract-2023--acl-long--311",
    "result": [
      {
        "value": {
          "start": 248,
          "end": 267,
          "text": "zero-shot framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--311:E0"
      },
      {
        "value": {
          "start": 380,
          "end": 392,
          "text": "faithfulness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--311:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--311:E0",
        "to_id": "abstract-2023--acl-long--311:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Faithfully correcting factual errors is critical for maintaining the integrity of textual knowledge bases and preventing hallucinations in sequence-to-sequence models. Drawing on humans’ ability to identify and correct factual errors, we present a zero-shot framework that formulates questions about input claims, looks for correct answers in the given evidence, and assesses the faithfulness of each correction based on its consistency with the evidence. Our zero-shot framework outperforms fully-supervised approaches, as demonstrated by experiments on the FEVER and SciFact datasets, where our outputs are shown to be more faithful. More importantly, the decomposability nature of our framework inherently provides interpretability. Additionally, to reveal the most suitable metrics for evaluating factual error corrections, we analyze the correlation between commonly used metrics with human judgments in terms of three different dimensions regarding intelligibility and faithfulness."
    }
  },
  {
    "id": "abstract-2023--acl-long--546",
    "result": [
      {
        "value": {
          "start": 622,
          "end": 644,
          "text": "retrieval augmentation",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--546:E0"
      },
      {
        "value": {
          "start": 25,
          "end": 36,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--546:E1"
      },
      {
        "value": {
          "start": 953,
          "end": 968,
          "text": "inference costs",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--546:E2"
      },
      {
        "value": {
          "start": 725,
          "end": 758,
          "text": "memorization of popular knowledge",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--546:E3"
      },
      {
        "value": {
          "start": 793,
          "end": 838,
          "text": "memorization of factual knowledge in the tail",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--546:E4"
      },
      {
        "from_id": "abstract-2023--acl-long--546:E0",
        "to_id": "abstract-2023--acl-long--546:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--546:E0",
        "to_id": "abstract-2023--acl-long--546:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the difficulty of encoding a wealth of world knowledge in their parameters. This paper aims to understand LMs’ strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments on two open-domain entity-centric QA datasets: PopQA, our new dataset with 14k questions about long-tail entities, and EntityQuestions, a widely used open-domain QA dataset. We find that LMs struggle with less popular factual knowledge, and that retrieval augmentation helps significantly in these cases. Scaling, on the other hand, mainly improves memorization of popular knowledge, and fails to appreciably improve memorization of factual knowledge in the tail. Based on those findings, we devise a new method for retrieval-augmentation that improves performance and reduces inference costs by only retrieving non-parametric memories when necessary."
    }
  },
  {
    "id": "abstract-2023--acl-short--153",
    "result": [
      {
        "value": {
          "start": 603,
          "end": 632,
          "text": "unified cross-modal ST method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--153:E0"
      },
      {
        "value": {
          "start": 928,
          "end": 954,
          "text": "results on MuST-C datasets",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--153:E1"
      },
      {
        "from_id": "abstract-2023--acl-short--153:E0",
        "to_id": "abstract-2023--acl-short--153:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "A triple speech translation data comprises speech, transcription, and translation. In the end-to-end paradigm, text machine translation (MT) usually plays the role of a teacher model for the speech translation (ST) via knowledge distillation. Parameter sharing with the teacher is often adopted to construct the ST model architecture, however, the two modalities are independently fed and trained via different losses. This situation does not match ST’s properties across two modalities and also limits the upper bound of the performance. Inspired by the works of video Transformer, we propose a simple unified cross-modal ST method, which concatenates speech and text as the input, and builds a teacher that can utilize both cross-modal information simultaneously. Experimental results show that in our unified ST framework, models can effectively utilize the auxiliary information from speech and text, and achieve compelling results on MuST-C datasets."
    }
  },
  {
    "id": "abstract-2023--acl-long--451",
    "result": [
      {
        "value": {
          "start": 825,
          "end": 889,
          "text": "Observation-guided radiology Report Generation framework (ORGan)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--451:E0"
      },
      {
        "value": {
          "start": 1268,
          "end": 1280,
          "text": "text quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--451:E1"
      },
      {
        "value": {
          "start": 1285,
          "end": 1302,
          "text": "clinical efficacy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--451:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--451:E0",
        "to_id": "abstract-2023--acl-long--451:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--451:E0",
        "to_id": "abstract-2023--acl-long--451:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This paper explores the task of radiology report generation, which aims at generating free-text descriptions for a set of radiographs. One significant challenge of this task is how to correctly maintain the consistency between the images and the lengthy report. Previous research explored solving this issue through planning-based methods, which generate reports only based on high-level plans. However, these plans usually only contain the major observations from the radiographs (e.g., lung opacity), lacking much necessary information, such as the observation characteristics and preliminary clinical diagnoses. To address this problem, the system should also take the image information into account together with the textual plan and perform stronger reasoning during the generation process. In this paper, we propose an Observation-guided radiology Report Generation framework (ORGan). It first produces an observation plan and then feeds both the plan and radiographs for report generation, where an observation graph and a tree reasoning mechanism are adopted to precisely enrich the plan information by capturing the multi-formats of each observation. Experimental results demonstrate that our framework outperforms previous state-of-the-art methods regarding text quality and clinical efficacy."
    }
  },
  {
    "id": "abstract-2023--acl-long--153",
    "result": [
      {
        "value": {
          "start": 0,
          "end": 32,
          "text": "Chain-of-Thought (CoT) prompting",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--153:E0"
      },
      {
        "value": {
          "start": 62,
          "end": 92,
          "text": "multi-step reasoning abilities",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--153:E1"
      },
      {
        "value": {
          "start": 552,
          "end": 590,
          "text": "prompting with invalid reasoning steps",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--153:E2"
      },
      {
        "value": {
          "start": 450,
          "end": 461,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--153:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--153:E0",
        "to_id": "abstract-2023--acl-long--153:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--153:E2",
        "to_id": "abstract-2023--acl-long--153:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Chain-of-Thought (CoT) prompting can dramatically improve the multi-step reasoning abilities of large language models (LLMs). CoT explicitly encourages the LLM to generate intermediate rationales for solving a problem, by providing a series of reasoning steps in the demonstrations. Despite its success, there is still little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance. In this paper, we show that CoT reasoning is possible even with invalid demonstrations - prompting with invalid reasoning steps can achieve over 80-90% of the performance obtained using CoT under various metrics, while still generating coherent lines of reasoning during inference. Further experiments show that other aspects of the rationales, such as being relevant to the query and correctly ordering the reasoning steps, are much more important for effective CoT reasoning. Overall, these findings both deepen our understanding of CoT prompting, and open up new questions regarding LLMs’ capability to learn to reason in context."
    }
  },
  {
    "id": "abstract-2023--acl-long--516",
    "result": [
      {
        "value": {
          "start": 285,
          "end": 310,
          "text": "increasing size of models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--516:E0"
      },
      {
        "value": {
          "start": 977,
          "end": 991,
          "text": "LMs with tutor",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--516:E1"
      }
    ],
    "data": {
      "text": "Recent work has shown that large pretrained Language Models (LMs) can not only perform remarkably well on a range of Natural Language Processing (NLP) tasks but also start improving on reasoning tasks such as arithmetic induction, symbolic manipulation, and commonsense reasoning with increasing size of models. However, it is still unclear what the underlying capabilities of these LMs are. Surprisingly, we find that these models have limitations on certain basic symbolic manipulation tasks such as copy, reverse, and addition. When the total number of symbols or repeating symbols increases, the model performance drops quickly. We investigate the potential causes behind this phenomenon and examine a set of possible methods, including explicit positional markers, fine-grained computation steps, and LMs with callable programs. Experimental results show that none of these techniques can solve the simplest addition induction problem completely. In the end, we introduce LMs with tutor, which demonstrates every single step of teaching. LMs with tutor is able to deliver 100% accuracy in situations of OOD and repeating symbols, shedding new insights on the boundary of large LMs in induction."
    }
  },
  {
    "id": "abstract-2023--acl-long--16",
    "result": [
      {
        "value": {
          "start": 988,
          "end": 1015,
          "text": "Lottery Prompt Tuning (LPT)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--16:E0"
      },
      {
        "value": {
          "start": 1442,
          "end": 1470,
          "text": "state-of-the-art performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--16:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--16:E0",
        "to_id": "abstract-2023--acl-long--16:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Thanks to the recent success of Pre-trained Language Models (PLMs), it has become a promising research direction to develop a universal model (UIE) that can solve all typical information extraction tasks within one generative framework. Nonetheless, in real-world scenarios of UIE applications, new data of different IE tasks and domains usually come in a stream over time. A desirable UIE system should be capable of continually learning new tasks without forgetting old ones, thereby allowing knowledge and functionalities expansion without re-training the whole system. In this paper, we study the UIE system under a more challenging yet practical scenario, i.e., “lifelong learning” settings, to evaluate its abilities in three aspects, including knowledge sharing and expansion, catastrophic forgetting prevention, and rapid generalization on few-shot and unseen tasks. To achieve these three goals, we present a novel parameter- and deployment-efficient prompt tuning method namely Lottery Prompt Tuning (LPT).LPT freezes the PLM’s parameters and sequentially learns compact pruned prompt vectors for each task leveraging a binary prompt mask, while keeping the prompt parameters selected by the previous tasks insusceptible. Furthermore, we use a simple yet effective method to perform mask selection and show the powerful transferability of Lottery Prompts to novel tasks. Extensive experiments demonstrate that LPT consistently sets state-of-the-art performance on multiple lifelong learning settings of UIE, including task-incremental setting on seen tasks, few-shot adaptation, and zero-shot generalization on novel tasks."
    }
  },
  {
    "id": "abstract-2023--acl-long--273",
    "result": [
      {
        "value": {
          "start": 459,
          "end": 489,
          "text": "actively supervised clustering",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--273:E0"
      }
    ],
    "data": {
      "text": "Current clustering-based Open Relation Extraction (OpenRE) methods usually adopt a two-stage pipeline, which simultaneously learns relation representations and assignments in the first stage, then manually labels relation for each cluster. However, unsupervised objectives struggle to explicitly optimize clusters to align with relational semantics, and the number of clusters K has to be supplied in advance. In this paper, we present a novel setting, named actively supervised clustering for OpenRE. Our insight lies in that clustering learning and relation labeling can be performed simultaneously, which provides the necessary guidance for clustering without a significant increase in human effort. Along with this setting, we propose an active labeling strategy tailored for clustering. Instead of only focusing on improving the clustering of relations that have been discovered, our strategy is encouraged to discover new relations through diversity regularization. This is particularly beneficial for long-tail relations in the real world. Experimental results show that our method is able to discover almost all relational clusters in the data and improve the SOTA methods by 13.8% and 10.6%, on two datasets respectively."
    }
  },
  {
    "id": "abstract-2023--acl-long--853",
    "result": [
      {
        "value": {
          "start": 1008,
          "end": 1034,
          "text": "classification performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--853:E0"
      }
    ],
    "data": {
      "text": "Mental disorders affect millions of people worldwide and cause interference with their thinking and behavior. Through the past years, awareness created by health campaigns and other sources motivated the study of these disorders using information extracted from social media platforms. In this work, we aim to contribute to the study of these disorders and to the understanding of how mental problems reflect on social media. To achieve this goal, we propose a double-domain adaptation of a language model. First, we adapted the model to social media language, and then, we adapted it to the mental health domain. In both steps, we incorporated a lexical resource to guide the masking process of the language model and, therefore, to help it in paying more attention to words related to mental disorders. We have evaluated our model in the detection of signs of three major mental disorders: Anorexia, Self-harm, and Depression. Results are encouraging as they show that the proposed adaptation enhances the classification performance and yields competitive results against state-of-the-art methods."
    }
  },
  {
    "id": "abstract-2023--acl-long--743",
    "result": [
      {
        "value": {
          "start": 705,
          "end": 712,
          "text": "ShrinkE",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--743:E0"
      }
    ],
    "data": {
      "text": "Link prediction on knowledge graphs (KGs) has been extensively studied on binary relational KGs, wherein each fact is represented by a triple. A significant amount of important knowledge, however, is represented by hyper-relational facts where each fact is composed of a primal triple and a set of qualifiers comprising a key-value pair that allows for expressing more complicated semantics. Although some recent works have proposed to embed hyper-relational KGs, these methods fail to capture essential inference patterns of hyper-relational facts such as qualifier monotonicity, qualifier implication, and qualifier mutual exclusion, limiting their generalization capability. To unlock this, we present ShrinkE, a geometric hyper-relational KG embedding method aiming to explicitly model these patterns. ShrinkE models the primal triple as a spatial-functional transformation from the head into a relation-specific box. Each qualifier “shrinks” the box to narrow down the possible answer set and, thus, realizes qualifier monotonicity. The spatial relationships between the qualifier boxes allow for modeling core inference patterns of qualifiers such as implication and mutual exclusion. Experimental results demonstrate ShrinkE’s superiority on three benchmarks of hyper-relational KGs."
    }
  },
  {
    "id": "abstract-2023--acl-short--67",
    "result": [
      {
        "value": {
          "start": 14,
          "end": 66,
          "text": "novel unsupervised approach to subtitle segmentation",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--67:E0"
      },
      {
        "value": {
          "start": 305,
          "end": 326,
          "text": "segmentation accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--67:E1"
      },
      {
        "from_id": "abstract-2023--acl-short--67:E0",
        "to_id": "abstract-2023--acl-short--67:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We describe a novel unsupervised approach to subtitle segmentation, based on pretrained masked language models, where line endings and subtitle breaks are predicted according to the likelihood of punctuation to occur at candidate segmentation points. Our approach obtained competitive results in terms of segmentation accuracy across metrics, while also fully preserving the original text and complying with length constraints. Although supervised models trained on in-domain data and with access to source audio information can provide better segmentation accuracy, our approach is highly portable across languages and domains and may constitute a robust off-the-shelf solution for subtitle segmentation."
    }
  },
  {
    "id": "abstract-2023--acl-long--476",
    "result": [
      {
        "value": {
          "start": 400,
          "end": 405,
          "text": "CFSum",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--476:E0"
      }
    ],
    "data": {
      "text": "Multimodal summarization usually suffers from the problem that the contribution of the visual modality is unclear. Existing multimodal summarization approaches focus on designing the fusion methods of different modalities, while ignoring the adaptive conditions under which visual modalities are useful. Therefore, we propose a novel Coarse-to-Fine contribution network for multimodal Summarization (CFSum) to consider different contributions of images for summarization. First, to eliminate the interference of useless images, we propose a pre-filter module to abandon useless images. Second, to make accurate use of useful images, we propose two levels of visual complement modules, word level and phrase level. Specifically, image contributions are calculated and are adopted to guide the attention of both textual and visual modalities. Experimental results have shown that CFSum significantly outperforms multiple strong baselines on the standard benchmark. Furthermore, the analysis verifies that useful images can even help generate non-visual words which are implicitly represented in the image."
    }
  },
  {
    "id": "abstract-2023--acl-long--2",
    "result": [
      {
        "value": {
          "start": 1071,
          "end": 1091,
          "text": "combining the models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--2:E0"
      },
      {
        "value": {
          "start": 1096,
          "end": 1139,
          "text": "explaining the emergence of unsafe behavior",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--2:E1"
      },
      {
        "value": {
          "start": 1144,
          "end": 1164,
          "text": "detoxifying chatbots",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--2:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--2:E0",
        "to_id": "abstract-2023--acl-long--2:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--2:E0",
        "to_id": "abstract-2023--acl-long--2:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "One of the main challenges open-domain end-to-end dialogue systems, or chatbots, face is the prevalence of unsafe behavior, such as toxic languages and harmful suggestions. However, existing dialogue datasets do not provide enough annotation to explain and correct such unsafe behavior. In this work, we construct a new dataset called SafeConv for the research of conversational safety: (1) Besides the utterance-level safety labels, SafeConv also provides unsafe spans in an utterance, information able to indicate which words contribute to the detected unsafe behavior; (2) SafeConv provides safe alternative responses to continue the conversation when unsafe behavior detected, guiding the conversation to a gentle trajectory. By virtue of the comprehensive annotation of SafeConv, we benchmark three powerful models for the mitigation of conversational unsafe behavior, including a checker to detect unsafe utterances, a tagger to extract unsafe spans, and a rewriter to convert an unsafe response to a safe version. Moreover, we explore the huge benefits brought by combining the models for explaining the emergence of unsafe behavior and detoxifying chatbots. Experiments show that the detected unsafe behavior could be well explained with unsafe spans and popular chatbots could be detoxified by a huge extent. The dataset is available athttps://github.com/mianzhang/SafeConv."
    }
  },
  {
    "id": "abstract-2023--acl-long--900",
    "result": [
      {
        "value": {
          "start": 447,
          "end": 469,
          "text": "character-aware models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--900:E0"
      }
    ],
    "data": {
      "text": "Current image generation models struggle to reliably produce well-formed visual text. In this paper, we investigate a key contributing factor: popular text-to-image models lack character-level input features, making it much harder to predict a word’s visual makeup as a series of glyphs. To quantify this effect, we conduct a series of experiments comparing character-aware vs. character-blind text encoders. In the text-only domain, we find that character-aware models provide large gains on a novel spelling task (WikiSpell). Applying our learnings to the visual domain, we train a suite of image generation models, and show that character-aware variants outperform their character-blind counterparts across a range of novel text rendering tasks (our DrawText benchmark). Our models set a much higher state-of-the-art on visual spelling, with 30+ point accuracy gains over competitors on rare words, despite training on far fewer examples."
    }
  },
  {
    "id": "abstract-2023--acl-long--340",
    "result": [
      {
        "value": {
          "start": 810,
          "end": 860,
          "text": "novel framework for non-linguistic skill injection",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--340:E0"
      },
      {
        "value": {
          "start": 1058,
          "end": 1079,
          "text": "non-linguistic skills",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--340:E1"
      },
      {
        "value": {
          "start": 1087,
          "end": 1117,
          "text": "linguistic knowledge retention",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--340:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--340:E0",
        "to_id": "abstract-2023--acl-long--340:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--340:E0",
        "to_id": "abstract-2023--acl-long--340:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The field of Math-NLP has witnessed significant growth in recent years, motivated by the desire to expand LLM performance to the leaning of non-linguistic notions (numerals, and subsequently, arithmetic reasoning). However, non-linguistic skill injection typically comes at a cost for LLMs: it leads to catastrophic forgetting of core linguistic skills, a consequence that often remains unaddressed in the literature. As Math-NLP has been able to create LLMs that can closely approximate the mathematical skills of a grade schooler or the arithmetic reasoning skills of a calculator, the practicality of these models fail if they concomitantly shed their linguistic capabilities. In this work, we take a closer look into the phenomena of catastrophic forgetting as it pertains to LLMs and subsequently offer a novel framework for non-linguistic skill injection for LLMs based on information-theoretic interventions and skill-specific losses that enable the learning of strict arithmetic reasoning. Our model outperforms the state-of-the-art both on injected non-linguistic skills and on linguistic knowledge retention, and does so with a fraction of the non-linguistic training data (1/4) and zero additional synthetic linguistic training data."
    }
  },
  {
    "id": "abstract-2023--acl-long--829",
    "result": [
      {
        "value": {
          "start": 436,
          "end": 459,
          "text": "Multi-Model classifiers",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--829:E0"
      },
      {
        "value": {
          "start": 750,
          "end": 774,
          "text": "speed-accuracy trade-off",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--829:E1"
      },
      {
        "value": {
          "start": 1067,
          "end": 1087,
          "text": "speed-accuracy curve",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--829:E2"
      },
      {
        "value": {
          "start": 1271,
          "end": 1299,
          "text": "SWEET individual classifiers",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--829:E3"
      }
    ],
    "data": {
      "text": "Adaptive inference is a simple method for reducing inference costs. The method works by maintaining multiple classifiers of different capacities, and allocating resources to each test instance according to its difficulty. In this work, we compare the two main approaches for adaptive inference, Early-Exit and Multi-Model, when training data is limited. First, we observe that for models with the same architecture and size, individual Multi-Model classifiers outperform their Early-Exit counterparts by an average of 2.3%. We show that this gap is caused by Early-Exit classifiers sharing model parameters during training, resulting in conflicting gradient updates of model weights. We find that despite this gap, Early-Exit still provides a better speed-accuracy trade-off due to the overhead of the Multi-Model approach. To address these issues, we propose SWEET (Separating Weights for Early-Exit Transformers) an Early-Exit fine-tuning method that assigns each classifier its own set of unique model weights, not updated by other classifiers. We compare SWEET’s speed-accuracy curve to standard Early-Exit and Multi-Model baselines and find that it outperforms both methods at fast speeds while maintaining comparable scores to Early- Exit at slow speeds. Moreover, SWEET individual classifiers outperform Early-Exit ones by 1.1% on average. SWEET enjoys the benefits of both methods, paving the way for further reduction of inference costs in NLP."
    }
  },
  {
    "id": "abstract-2023--acl-long--851",
    "result": [
      {
        "value": {
          "start": 706,
          "end": 729,
          "text": "F1 score of coreference",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--851:E0"
      },
      {
        "value": {
          "start": 644,
          "end": 667,
          "text": "aggregated cache misses",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--851:E1"
      },
      {
        "value": {
          "start": 980,
          "end": 994,
          "text": "inference time",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--851:E2"
      }
    ],
    "data": {
      "text": "Recent works show the effectiveness of cache-based neural coreference resolution models on long documents. These models incrementally process a long document from left to right and extract relations between mentions and entities in a cache, resulting in much lower memory and computation cost compared to computing all mentions in parallel. However, they do not handle cache misses when high-quality entities are purged from the cache, which causes wrong assignments and leads to prediction errors. We propose a new hybrid cache that integrates two eviction policies to capture global and local entities separately, and effectively reduces the aggregated cache misses up to half as before, while improving F1 score of coreference by 0.7 5.7pt. As such, the hybrid policy can accelerate existing cache-based models and offer a new long document coreference resolution solution. Results show that our method outperforms existing methods on four benchmarks while saving up to 83% of inference time against non-cache-based models. Further, we achieve a new state-of-the-art on a long document coreference benchmark, LitBank."
    }
  },
  {
    "id": "abstract-2023--acl-srw--7",
    "result": [
      {
        "value": {
          "start": 489,
          "end": 556,
          "text": "semantic-aware dynamic retrospective-prospective reasoning approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-srw--7:E0"
      },
      {
        "value": {
          "start": 206,
          "end": 217,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-srw--7:E1"
      },
      {
        "from_id": "abstract-2023--acl-srw--7:E0",
        "to_id": "abstract-2023--acl-srw--7:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Event-Level Video Question Answering (EVQA) requires complex reasoning across video events to obtain the visual information needed to provide optimal answers. However, despite significant progress in model performance, few studies have focused on using the explicit semantic connections between the question and visual information especially at the event level. There is need for using such semantic connections to facilitate complex reasoning across video frames. Therefore, we propose a semantic-aware dynamic retrospective-prospective reasoning approach for video-based question answering. Specifically, we explicitly use the Semantic Role Labeling (SRL) structure of the question in the dynamic reasoning process where we decide to move to the next frame based on which part of the SRL structure (agent, verb, patient, etc.) of the question is being focused on. We conduct experiments on a benchmark EVQA dataset - TrafficQA. Results show that our proposed approach achieves superior performance compared to previous state-of-the-art models. Our code is publicly available athttps://github.com/lyuchenyang/Semantic-aware-VideoQA."
    }
  },
  {
    "id": "abstract-2023--acl-demo--43",
    "result": [
      {
        "value": {
          "start": 193,
          "end": 225,
          "text": "cloud based smart compose system",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-demo--43:E0"
      },
      {
        "value": {
          "start": 252,
          "end": 275,
          "text": "conversation efficiency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-demo--43:E1"
      },
      {
        "value": {
          "start": 538,
          "end": 560,
          "text": "novel phrase tokenizer",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-demo--43:E2"
      },
      {
        "value": {
          "start": 379,
          "end": 386,
          "text": "latency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-demo--43:E3"
      },
      {
        "from_id": "abstract-2023--acl-demo--43:E0",
        "to_id": "abstract-2023--acl-demo--43:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-demo--43:E2",
        "to_id": "abstract-2023--acl-demo--43:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Online conversation is a ubiquitous way to share information and connect everyone but repetitive idiomatic text typing takes users a lot of time. This paper demonstrates a simple yet effective cloud based smart compose system to improve human-to-human conversation efficiency. Heuristics from different perspectives are designed to achieve the best trade-off between quality and latency. From the modeling side, the decoder-only model exploited the previous turns of conversational history in a computation lightweight manner. Besides, a novel phrase tokenizer is proposed to reduce latency without losing the composing quality further. Additionally, the caching mechanism is applied to the serving framework. The demo video of the system is available athttps://youtu.be/U1KXkaqr60g.Weopen-sourced our phrase tokenizer inhttps://github.com/tensorflow/text."
    }
  },
  {
    "id": "abstract-2023--acl-long--390",
    "result": [
      {
        "value": {
          "start": 903,
          "end": 924,
          "text": "larger GPT-3 variants",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--390:E0"
      },
      {
        "value": {
          "start": 750,
          "end": 765,
          "text": "NER performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--390:E1"
      },
      {
        "value": {
          "start": 1434,
          "end": 1462,
          "text": "proposed GPT-3 configuration",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--390:E2"
      },
      {
        "value": {
          "start": 6,
          "end": 18,
          "text": "paraphrasing",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--390:E3"
      },
      {
        "value": {
          "start": 1409,
          "end": 1421,
          "text": "memorization",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--390:E4"
      },
      {
        "value": {
          "start": 1634,
          "end": 1665,
          "text": "mention replacement using GPT-3",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--390:E5"
      },
      {
        "from_id": "abstract-2023--acl-long--390:E0",
        "to_id": "abstract-2023--acl-long--390:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--390:E3",
        "to_id": "abstract-2023--acl-long--390:E4",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "While paraphrasing is a promising approach for data augmentation in classification tasks, its effect on named entity recognition (NER) is not investigated systematically due to the difficulty of span-level label preservation. In this paper, we utilize simple strategies to annotate entity spans in generations and compare established and novel methods of paraphrasing in NLP such as back translation, specialized encoder-decoder models such as Pegasus, and GPT-3 variants for their effectiveness in improving downstream performance for NER across different levels of gold annotations and paraphrasing strength on 5 datasets. We thoroughly explore the influence of paraphrasers, and dynamics between paraphrasing strength and gold dataset size on the NER performance with visualizations and statistical testing. We find that the choice of the paraphraser greatly impacts NER performance, with one of the larger GPT-3 variants exceedingly capable of generating high quality paraphrases, yielding statistically significant improvements in NER performance with increasing paraphrasing strength, while other paraphrasers show more mixed results. Additionally, inline auto annotations generated by larger GPT-3 are strictly better than heuristic based annotations. We also find diminishing benefits of paraphrasing as gold annotations increase for most datasets. Furthermore, while most paraphrasers promote entity memorization in NER, the proposed GPT-3 configuration performs most favorably among the compared paraphrasers when tested on unseen entities, with memorization reducing further with paraphrasing strength. Finally, we explore mention replacement using GPT-3, which provides additional benefits over base paraphrasing for specific datasets."
    }
  },
  {
    "id": "abstract-2023--acl-industry--55",
    "result": [
      {
        "value": {
          "start": 937,
          "end": 986,
          "text": "controlling FN instances with the proposed method",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-industry--55:E0"
      },
      {
        "value": {
          "start": 988,
          "end": 1002,
          "text": "IC performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-industry--55:E1"
      },
      {
        "from_id": "abstract-2023--acl-industry--55:E0",
        "to_id": "abstract-2023--acl-industry--55:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Item categorization (IC) aims to classify product descriptions into leaf nodes in a categorical taxonomy, which is a key technology used in a wide range of applications. Along with the fact that most datasets often has a long-tailed distribution, classification performances on tail labels tend to be poor due to scarce supervision, causing many issues in real-life applications. To address IC task’s long-tail issue, K-positive contrastive loss (KCL) is proposed on image classification task and can be applied on the IC task when using text-based contrastive learning, e.g., SimCSE. However, one shortcoming of using KCL has been neglected in previous research: false negative (FN) instances may harm the KCL’s representation learning. To address the FN issue in the KCL, we proposed to re-weight the positive pairs in the KCL loss with a regularization that the sum of weights should be constrained to K+1 as close as possible. After controlling FN instances with the proposed method, IC performance has been further improved and is superior to other LT-addressing methods."
    }
  },
  {
    "id": "abstract-2023--acl-long--558",
    "result": [
      {
        "value": {
          "start": 1209,
          "end": 1224,
          "text": "DiFaR framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--558:E0"
      }
    ],
    "data": {
      "text": "There has been a surge of interest in utilizing Knowledge Graphs (KGs) for various natural language processing/understanding tasks. The conventional mechanism to retrieve facts in KGs usually involves three steps: entity span detection, entity disambiguation, and relation classification. However, this approach requires additional labels for training each of the three subcomponents in addition to pairs of input texts and facts, and also may accumulate errors propagated from failures in previous steps. To tackle these limitations, we propose a simple knowledge retrieval framework, which directly retrieves facts from the KGs given the input text based on their representational similarities, which we refer to as Direct Fact Retrieval (DiFaR). Specifically, we first embed all facts in KGs onto a dense embedding space by using a language model trained by only pairs of input texts and facts, and then provide the nearest facts in response to the input text. Since the fact, consisting of only two entities and one relation, has little context to encode, we propose to further refine ranks of top-k retrieved facts with a reranker that contextualizes the input text and the fact jointly. We validate our DiFaR framework on multiple fact retrieval tasks, showing that it significantly outperforms relevant baselines that use the three-step approach."
    }
  },
  {
    "id": "abstract-2023--acl-long--296",
    "result": [
      {
        "value": {
          "start": 721,
          "end": 755,
          "text": "hybrid knowledge-transfer approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--296:E0"
      },
      {
        "value": {
          "start": 1096,
          "end": 1120,
          "text": "state-of-the-art results",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--296:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--296:E0",
        "to_id": "abstract-2023--acl-long--296:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In this paper, we address the Event Detection task under a zero-shot cross-lingual setting where a model is trained on a source language but evaluated on a distinct target language for which there is no labeled data available. Most recent efforts in this field follow a direct transfer approach in which the model is trained using language-invariant features and then directly applied to the target language. However, we argue that these methods fail to take advantage of the benefits of the data transfer approach where a cross-lingual model is trained on target-language data and is able to learn task-specific information from syntactical features or word-label relations in the target language. As such, we propose a hybrid knowledge-transfer approach that leverages a teacher-student framework where the teacher and student networks are trained following the direct and data transfer approaches, respectively. Our method is complemented by a hierarchical training-sample selection scheme designed to address the issue of noisy labels being generated by the teacher model. Our model achieves state-of-the-art results on 9 morphologically-diverse target languages across 3 distinct datasets, highlighting the importance of exploiting the benefits of hybrid transfer."
    }
  },
  {
    "id": "abstract-2023--acl-long--389",
    "result": [
      {
        "value": {
          "start": 1095,
          "end": 1108,
          "text": "effectiveness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--389:E0"
      }
    ],
    "data": {
      "text": "A curriculum is a planned sequence of learning materials and an effective one can make learning efficient and effective for both humans and machines. Recent studies developed effective data-driven curriculum learning approaches for training graph neural networks in language applications. However, existing curriculum learning approaches often employ a single criterion of difficulty in their training paradigms. In this paper, we propose a new perspective on curriculum learning by introducing a novel approach that builds on graph complexity formalisms (as difficulty criteria) and model competence during training. The model consists of a scheduling scheme which derives effective curricula by accounting for different views of sample difficulty and model competence during training. The proposed solution advances existing research in curriculum learning for graph neural networks with the ability to incorporate a fine-grained spectrum of graph difficulty criteria in their training paradigms. Experimental results on real-world link prediction and node classification tasks illustrate the effectiveness of the proposed approach."
    }
  },
  {
    "id": "abstract-2023--acl-long--872",
    "result": [
      {
        "value": {
          "start": 845,
          "end": 853,
          "text": "ASR-BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--872:E0"
      },
      {
        "value": {
          "start": 865,
          "end": 882,
          "text": "decoding speed-up",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--872:E1"
      },
      {
        "value": {
          "start": 901,
          "end": 917,
          "text": "proposed methods",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--872:E2"
      },
      {
        "value": {
          "start": 1002,
          "end": 1027,
          "text": "predicting discrete units",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--872:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--872:E3",
        "to_id": "abstract-2023--acl-long--872:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Direct speech-to-speech translation (S2ST), in which all components can be optimized jointly, is advantageous over cascaded approaches to achieve fast inference with a simplified pipeline. We present a novel two-pass direct S2ST architecture, UnitY, which first generates textual representations and predicts discrete acoustic units subsequently. We enhance the model performance by subword prediction in the first-pass decoder, advanced two-pass decoder architecture design and search strategy, and better training regularization. To leverage large amounts of unlabeled text data, we pre-train the first-pass text decoder based on the self-supervised denoising auto-encoding task. Experimental evaluations on benchmark datasets at various data scales demonstrate that UnitY outperforms a single-pass speech-to-unit translation model by 2.5-4.2 ASR-BLEU with 2.83x decoding speed-up. We show that the proposed methods boost the performance even when predicting spectrogram in the second pass. However, predicting discrete units achieves 2.51x decoding speed-up compared to that case."
    }
  },
  {
    "id": "abstract-2023--acl-long--762",
    "result": [
      {
        "value": {
          "start": 1240,
          "end": 1251,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--762:E0"
      }
    ],
    "data": {
      "text": "Verbal omissions are complex syntactic phenomena in VP coordination structures. They occur when verbs and (some of) their arguments are omitted from subsequent clauses after being explicitly stated in an initial clause. Recovering these omitted elements is necessary for accurate interpretation of the sentence, and while humans easily and intuitively fill in the missing information, state-of-the-art models continue to struggle with this task. Previous work is limited to small-scale datasets, synthetic data creation methods, and to resolution methods in the dependency-graph level. In this work we propose aconjunct resolutiontask that operates directly on the text and makes use of asplit-and-rephraseparadigm in order to recover the missing elements in the coordination structure. To this end, we first formulate a pragmatic framework of verbal omissions which describes the different types of omissions, and develop an automatic scalable collection method. Based on this method, we curate a large dataset, containing over 10K examples of naturally-occurring verbal omissions with crowd-sourced annotations of the resolved conjuncts. We train various neural baselines for this task, and show that while our best method obtains decent performance, it leaves ample space for improvement. We propose our dataset, metrics and models as a starting point for future research on this topic."
    }
  },
  {
    "id": "abstract-2023--acl-long--129",
    "result": [
      {
        "value": {
          "start": 183,
          "end": 188,
          "text": "Z-ICL",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--129:E0"
      },
      {
        "value": {
          "start": 886,
          "end": 914,
          "text": "zero-shot performance levels",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--129:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--129:E0",
        "to_id": "abstract-2023--acl-long--129:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Although large language models can be prompted for both zero- and few-shot learning, performance drops significantly when no demonstrations are available. In this paper, we introduce Z-ICL, a new zero-shot method that closes the gap by constructing pseudo-demonstrations for a given test input using a raw text corpus. Concretely, pseudo-demonstrations are constructed by (1) finding the nearest neighbors to the test input from the corpus and pairing them with random task labels, and (2) applying a set of techniques to reduce the amount of direct copying the model does from the resulting demonstrations. Evaluation on nine classification datasets shows that Z-ICL outperforms previous zero-shot methods by a significant margin, and is on par with in-context learning with labeled training data in the few-shot setting. Overall, Z-ICL provides a significantly higher estimate of the zero-shot performance levels of a model, and supports future efforts to develop better pseudo-demonstrations that further improve zero-shot results."
    }
  },
  {
    "id": "abstract-2023--acl-demo--51",
    "result": [
      {
        "value": {
          "start": 316,
          "end": 380,
          "text": "multilingual Knowledge Graph Question Answering (KGQA) technique",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-demo--51:E0"
      }
    ],
    "data": {
      "text": "Our research focuses on the most prevalent type of queries— simple questions —exemplified by questions like “What is the capital of France?”. These questions reference an entity such as “France”, which is directly connected (one hop) to the answer entity “Paris” in the underlying knowledge graph (KG). We propose a multilingual Knowledge Graph Question Answering (KGQA) technique that orders potential responses based on the distance between the question’s text embeddings and the answer’s graph embeddings. A system incorporating this novel method is also described in our work. Through comprehensive experimentation using various English and multilingual datasets and two KGs — Freebase and Wikidata — we illustrate the comparative advantage of the proposed method across diverse KG embeddings and languages. This edge is apparent even against robust baseline systems, including seq2seq QA models, search-based solutions and intricate rule-based pipelines. Interestingly, our research underscores that even advanced AI systems like ChatGPT encounter difficulties when tasked with answering simple questions. This finding emphasizes the relevance and effectiveness of our approach, which consistently outperforms such systems. We are making the source code and trained models from our study publicly accessible to promote further advancements in multilingual KGQA."
    }
  },
  {
    "id": "abstract-2023--acl-long--261",
    "result": [
      {
        "value": {
          "start": 997,
          "end": 1048,
          "text": "soft-sufficiency and soft-comprehensiveness metrics",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--261:E0"
      },
      {
        "value": {
          "start": 1074,
          "end": 1095,
          "text": "faithful explanations",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--261:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--261:E0",
        "to_id": "abstract-2023--acl-long--261:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Feature attribution methods (FAs) are popular approaches for providing insights into the model reasoning process of making predictions. The more faithful a FA is, the more accurately it reflects which parts of the input are more important for the prediction. Widely used faithfulness metrics, such as sufficiency and comprehensiveness use a hard erasure criterion, i.e. entirely removing or retaining the top most important tokens ranked by a given FA and observing the changes in predictive likelihood. However, this hard criterion ignores the importance of each individual token, treating them all equally for computing sufficiency and comprehensiveness. In this paper, we propose a simple yet effective soft erasure criterion. Instead of entirely removing or retaining tokens from the input, we randomly mask parts of the token vector representations proportionately to their FA importance. Extensive experiments across various natural language processing tasks and different FAs show that our soft-sufficiency and soft-comprehensiveness metrics consistently prefer more faithful explanations compared to hard sufficiency and comprehensiveness."
    }
  },
  {
    "id": "abstract-2023--acl-long--517",
    "result": [
      {
        "value": {
          "start": 1105,
          "end": 1118,
          "text": "EEL with TFRs",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--517:E0"
      },
      {
        "value": {
          "start": 1242,
          "end": 1275,
          "text": "performance on downstream metrics",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--517:E1"
      },
      {
        "value": {
          "start": 1189,
          "end": 1196,
          "text": "speedup",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--517:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--517:E0",
        "to_id": "abstract-2023--acl-long--517:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--517:E0",
        "to_id": "abstract-2023--acl-long--517:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Standard decoding approaches for conditional text generation tasks typically search for an output hypothesis with high model probability, but this may not yield the best hypothesis according to human judgments of quality. Reranking to optimize for “downstream” metrics can more closely optimize for quality, but many metrics of interest are computed with pre-trained language models, which are slow to apply to large numbers of hypotheses. We explore an approach for reranking hypotheses by using Transformers to efficiently encode lattices of generated outputs, a method we call EEL. With a single Transformer pass over the entire lattice, we can approximately compute a contextualized representation of each token as if it were only part of a single hypothesis in isolation. We combine this approach with a new class of token-factored rerankers (TFRs) that allow for efficient extraction of high reranker-scoring hypotheses from the lattice. Empirically, our approach incurs minimal degradation error compared to the exponentially slower approach of encoding each hypothesis individually. When applying EEL with TFRs across three text generation tasks, our results show both substantial speedup compared to naive reranking and often better performance on downstream metrics than comparable approaches."
    }
  },
  {
    "id": "abstract-2023--acl-long--685",
    "result": [
      {
        "value": {
          "start": 1252,
          "end": 1273,
          "text": "novel reward function",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--685:E0"
      },
      {
        "value": {
          "start": 1473,
          "end": 1496,
          "text": "usefulness and strength",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--685:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--685:E0",
        "to_id": "abstract-2023--acl-long--685:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The World Health Organization (WHO) has significantly emphasized the need for mental health care. The social stigma associated with mental illness prevents individuals from addressing their issues and getting assistance. In such a scenario, the relevance of online counseling has increased dramatically. The feelings and attitudes that a client and a counselor express towards each other result in a higher or lower counseling experience. A counselor should be friendly and gain clients’ trust to make them share their problems comfortably. Thus, it is essential for the counselor to adequately comprehend the client’s emotions and ensure client’s welfare, i.e. s/he should adapt and deal with the clients politely and empathetically to provide a pleasant, cordial and personalized experience. Motivated by this, in this work, we attempt to build a novel Polite and empAthetic counseLing conversational agent PAL to lay down the counseling support to substance addict and crime victims. To have client’s emotion-based polite and empathetic responses, two counseling datasets laying down the counseling support to substance addicts and crime victims are annotated. These annotated datasets are used to build PAL in a reinforcement learning framework. A novel reward function is formulated to ensure correct politeness and empathy preferences as per client’s emotions with naturalness and non-repetitiveness in responses. Thorough automatic and human evaluation showcase the usefulness and strength of the designed novel reward function. Our proposed system is scalable and can be easily modified with different modules of preference models as per need."
    }
  },
  {
    "id": "abstract-2023--acl-long--212",
    "result": [
      {
        "value": {
          "start": 995,
          "end": 1018,
          "text": "fine-tuning performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--212:E0"
      },
      {
        "value": {
          "start": 995,
          "end": 1032,
          "text": "fine-tuning performance of small PLMs",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--212:E1"
      },
      {
        "value": {
          "start": 954,
          "end": 966,
          "text": "our approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--212:E2"
      }
    ],
    "data": {
      "text": "By scaling the model size, large pre-trained language models (PLMs) have shown remarkable performance in various natural language processing tasks, mostly outperforming small PLMs by a large margin. However, due to the high computational cost, the huge number of parameters also restricts the applicability of large PLMs in real-world systems. In this paper, we focus on scaling up the parameters of PLMsonly duringfine-tuning, to benefit from the over-parameterization, while without increasingthe inference latency. Given a relatively small PLM, we over-parameterize it by employing a matrix product operator, an efficient and almost lossless decomposition method to factorize its contained parameter matrices into a set of higher-dimensional tensors.Considering the efficiency, we further propose both static and dynamic strategies to select the most important parameter matrices for over-parameterization.Extensive experiments have demonstrated that our approach can significantly boost the fine-tuning performance of small PLMs and even help small PLMs outperform3×parameterized larger ones.Our code is publicly available athttps://github.com/zfgao66/OPF."
    }
  },
  {
    "id": "abstract-2023--acl-long--28",
    "result": [
      {
        "value": {
          "start": 913,
          "end": 918,
          "text": "GraCe",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--28:E0"
      },
      {
        "value": {
          "start": 1355,
          "end": 1373,
          "text": "Is-Simile accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--28:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--28:E0",
        "to_id": "abstract-2023--acl-long--28:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Similes occur in the creative context of describing a concept (i.e., tenor) by making a literally false yet figuratively meaningful comparison to another (i.e., vehicle). Previous efforts form simile generation as a context-free generation task, focusing on simile-style transfer or writing a simile from a given prefix. However, generated texts under such settings might be undesirable, such as hardly meeting the simile definition (e.g., missing vehicle) or difficult to address certain preferences of content as humans wish (e.g., describe the color of apples through the simile). We believe that a simile could be more qualified and user-oriented if incorporated with pre-specified constraints. To this end, we introduce controllable simile generation (CSG), a new task that requires the model to generate a simile with multiple simile elements, e.g., context and vehicle. To facilitate this task, we present GraCe, including 61.3k simile-element annotated Chinese similes. Based on it, we propose a CSG model Similor to benchmark this task, including a vehicle retrieval module Scorer to obtain the explicable comparison for a given tenor in the vehicle-unknown situation. Both statistical and experimental analyses show that GraCe is of high quality beyond all other Chinese simile datasets, in terms of the number (8 vs. 3) of annotation elements, Is-Simile accuracy (98.9% vs. 78.7%), and increasing model-performance gains for both uncontrollable and controllable simile generation. Meanwhile, Similor can serve as a strong baseline for CSG, especially with Scorer, which beats model-based retrieval methods without any re-training."
    }
  },
  {
    "id": "abstract-2023--acl-long--381",
    "result": [
      {
        "value": {
          "start": 483,
          "end": 523,
          "text": "novel method with deep model compression",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--381:E0"
      },
      {
        "value": {
          "start": 970,
          "end": 991,
          "text": "quantifying ambiguity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--381:E1"
      },
      {
        "value": {
          "start": 1086,
          "end": 1096,
          "text": "model size",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--381:E2"
      },
      {
        "value": {
          "start": 1123,
          "end": 1130,
          "text": "latency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--381:E3"
      },
      {
        "from_id": "abstract-2023--acl-long--381:E0",
        "to_id": "abstract-2023--acl-long--381:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--381:E0",
        "to_id": "abstract-2023--acl-long--381:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--381:E0",
        "to_id": "abstract-2023--acl-long--381:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Natural language understanding (NLU) tasks face a non-trivial amount of ambiguous samples where veracity of their labels is debatable among annotators. NLU models should thus account for such ambiguity, but they approximate the human opinion distributions quite poorly and tend to produce over-confident predictions. To address this problem, we must consider how to exactly capture the degree of relationship between each sample and its candidate classes. In this work, we propose a novel method with deep model compression and show how such relationship can be accounted for. We see that more reasonably represented relationships can be discovered in the lower layers and that validation accuracies are converging at these layers, which naturally leads to layer pruning. We also see that distilling the relationship knowledge from a lower layer helps models produce better distribution. Experimental results demonstrate that our method makes substantial improvement on quantifying ambiguity without gold distribution labels. As positive side-effects, our method is found to reduce the model size significantly and improve latency, both attractive aspects of NLU products."
    }
  },
  {
    "id": "abstract-2023--acl-industry--29",
    "result": [
      {
        "value": {
          "start": 1671,
          "end": 1690,
          "text": "absolute recall@90P",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-industry--29:E0"
      }
    ],
    "data": {
      "text": "E-commerce websites (e.g. Amazon, Alibaba) have a plethora of structured and unstructured information (text and images) present on the product pages. Sellers often don’t label or mislabel values of the attributes (e.g. color, size etc.) for their products. Automatically identifying these attribute values from an eCommerce product page that contains both text and images is a challenging task, especially when the attribute value is not explicitly mentioned in the catalog. In this paper, we present a scalable solution for this problem where we pose attribute extraction problem as a question-answering task, which we solve using MXT, that consists of three key components: (i) MAG (Multimodal Adaptation Gate), (ii) Xception network, and (iii) T5 encoder-decoder. Our system consists of a generative model that generates attribute-values for a given product by using both textual and visual characteristics (e.g. images) of the product. We show that our system is capable of handling zero-shot attribute prediction (when attribute value is not seen in training data) and value-absent prediction (when attribute value is not mentioned in the text) which are missing in traditional classification-based and NER-based models respectively. We have trained our models using distant supervision, removing dependency on human labeling, thus making them practical for real-world applications. With this framework, we are able to train a single model for 1000s of (product-type, attribute) pairs, thus reducing the overhead of training and maintaining separate models. Extensive experiments on two real world datasets (total 57 attributes) show that our framework improves the absolute recall@90P by 10.16% and 6.9 from the existing state of the art models. In a popular e-commerce store, we have productionized our models that cater to 12K (product-type, attribute) pairs, and have extracted 150MM attribute values."
    }
  },
  {
    "id": "abstract-2023--acl-long--728",
    "result": [
      {
        "value": {
          "start": 418,
          "end": 423,
          "text": "mCLIP",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--728:E0"
      }
    ],
    "data": {
      "text": "Large-scale vision-language pretrained (VLP) models like CLIP have shown remarkable performance on various downstream cross-modal tasks. However, they are usually biased towards English due to the lack of sufficient non-English image-text pairs. Existing multilingual VLP methods often learn retrieval-inefficient single-stream models by translation-augmented non-English image-text pairs. In this paper, we introduce mCLIP, a retrieval-efficient dual-stream multilingual VLP model, trained by aligning the CLIP model and a Multilingual Text Encoder (MTE) through a novel Triangle Cross-modal Knowledge Distillation (TriKD) method. It is parameter-efficient as only two light projectors on the top of them are updated during distillation. Furthermore, to enhance the token- and sentence-level multilingual representation of the MTE, we propose to train it with machine translation and contrastive learning jointly before the TriKD to provide a better initialization. Empirical results show that mCLIP achieves new state-of-the-art performance for both zero-shot and finetuned multilingual image-text retrieval task."
    }
  },
  {
    "id": "abstract-2023--acl-short--98",
    "result": [
      {
        "value": {
          "start": 530,
          "end": 554,
          "text": "reranking-based approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--98:E0"
      }
    ],
    "data": {
      "text": "Recognizing flat, overlapped and discontinuous entities uniformly has been paid increasing attention. Among these works, Seq2Seq formulation prevails for its flexibility and effectiveness. It arranges the output entities into a specific target sequence. However, it introduces bias by assigning all the probability mass to the observed sequence. To alleviate the bias, previous works either augment the data with possible sequences or resort to other formulations. In this paper, we stick to the Seq2Seq formulation and propose a reranking-based approach. It redistributes the likelihood among candidate sequences depending on their performance via a contrastive loss. Extensive experiments show that our simple yet effective method consistently boosts the baseline, and yields competitive or better results compared with the state-of-the-art methods on 8 widely-used datasets for Named Entity Recognition."
    }
  },
  {
    "id": "abstract-2023--acl-long--178",
    "result": [
      {
        "value": {
          "start": 544,
          "end": 553,
          "text": "InfoMetIC",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--178:E0"
      },
      {
        "value": {
          "start": 868,
          "end": 901,
          "text": "correlation with human judgements",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--178:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--178:E0",
        "to_id": "abstract-2023--acl-long--178:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Automatic image captioning evaluation is critical for benchmarking and promoting advances in image captioning research. Existing metrics only provide a single score to measure caption qualities, which are less explainable and informative. Instead, we humans can easily identify the problems of captions in details, e.g., which words are inaccurate and which salient objects are not described, and then rate the caption quality. To support such informative feedback, we propose an Informative Metric for Reference-free Image Caption evaluation (InfoMetIC). Given an image and a caption, InfoMetIC is able to report incorrect words and unmentioned image regions at fine-grained level, and also provide a text precision score, a vision recall score and an overall quality score at coarse-grained level. The coarse-grained score of InfoMetIC achieves significantly better correlation with human judgements than existing metrics on multiple benchmarks. We also construct a token-level evaluation dataset and demonstrate the effectiveness of InfoMetIC in fine-grained evaluation. Our code and datasets are publicly available athttps://github.com/HAWLYQ/InfoMetIC."
    }
  },
  {
    "id": "abstract-2023--acl-short--70",
    "result": [
      {
        "value": {
          "start": 296,
          "end": 323,
          "text": "counterfactual conditionals",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--70:E0"
      },
      {
        "value": {
          "start": 538,
          "end": 564,
          "text": "counterfactual predictions",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--70:E1"
      },
      {
        "from_id": "abstract-2023--acl-short--70:E0",
        "to_id": "abstract-2023--acl-short--70:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on the understanding of real world. We tease these factors apart by leveraging counterfactual conditionals, which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from five pre-trained language models. We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge—however, we also find that for most models this effect appears largely to be driven by simple lexical cues. When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we find that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors."
    }
  },
  {
    "id": "abstract-2023--acl-long--754",
    "result": [
      {
        "value": {
          "start": 362,
          "end": 375,
          "text": "Self-Instruct",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--754:E0"
      }
    ],
    "data": {
      "text": "Large “instruction-tuned” language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning."
    }
  },
  {
    "id": "abstract-2023--acl-long--34",
    "result": [
      {
        "value": {
          "start": 562,
          "end": 588,
          "text": "diversification approaches",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--34:E0"
      },
      {
        "value": {
          "start": 602,
          "end": 616,
          "text": "data diversity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--34:E1"
      },
      {
        "value": {
          "start": 642,
          "end": 655,
          "text": "data accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--34:E2"
      },
      {
        "value": {
          "start": 784,
          "end": 806,
          "text": "label replacement (LR)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--34:E3"
      },
      {
        "value": {
          "start": 1034,
          "end": 1051,
          "text": "absolute accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--34:E4"
      },
      {
        "value": {
          "start": 845,
          "end": 874,
          "text": "out-of-scope filtering (OOSF)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--34:E5"
      },
      {
        "value": {
          "start": 1290,
          "end": 1304,
          "text": "model accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--34:E6"
      },
      {
        "from_id": "abstract-2023--acl-long--34:E0",
        "to_id": "abstract-2023--acl-long--34:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--34:E0",
        "to_id": "abstract-2023--acl-long--34:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--34:E3",
        "to_id": "abstract-2023--acl-long--34:E4",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--34:E5",
        "to_id": "abstract-2023--acl-long--34:E6",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Large language models (LLMs) can be used to generate text data for training and evaluating other models. However, creating high-quality datasets with LLMs can be challenging. In this work, we explore human-AI partnerships to facilitate high diversity and accuracy in LLM-based text data generation. We first examine two approaches to diversify text generation: 1) logit suppression, which minimizes the generation of languages that have already been frequently generated, and 2) temperature sampling, which flattens the token sampling probability. We found that diversification approaches can increase data diversity but often at the cost of data accuracy (i.e., text and labels being appropriate for the target domain). To address this issue, we examined two human interventions, 1) label replacement (LR), correcting misaligned labels, and 2) out-of-scope filtering (OOSF), removing instances that are out of the user’s domain of interest or to which no considered label applies. With oracle studies, we found that LR increases the absolute accuracy of models trained with diversified datasets by 14.4%. Moreover, we found that some models trained with data generated with LR interventions outperformed LLM-based few-shot classification. In contrast, OOSF was not effective in increasing model accuracy, implying the need for future work in human-in-the-loop text data generation."
    }
  },
  {
    "id": "abstract-2023--acl-long--553",
    "result": [
      {
        "value": {
          "start": 740,
          "end": 755,
          "text": "SimOAP strategy",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--553:E0"
      }
    ],
    "data": {
      "text": "Language models trained on large-scale corpora can generate remarkably fluent results in open-domain dialogue. However, for the persona-based dialogue generation task, consistency and coherence are also key factors, which are great challenges for language models. Existing works mainly focus on valuable data filtering, model structure modifying, or objective function designing, while their improvements are limited and hard to generalize to all types of pre-trained language models. However, we find that language models can produce consistent and coherent responses if we consider enough generations. Thus, the problems lay in large-scale response generation and target response selection. In this work, a simple but effective two-stage SimOAP strategy is proposed, i.e., over-sampling and post-evaluation. The over-sampling stage takes large-scale responses from existing trained models efficiently via off-the-shelf distilling and compressing methods, and the post-evaluation stage selects a good response based on multiple well-designed evaluation metrics from large-scale candidates. Experimental results show that the proposed plug-in SimOAP strategy improves the backbone models and outperforms the baseline strategies in both automatic and human evaluations."
    }
  },
  {
    "id": "abstract-2023--acl-long--282",
    "result": [
      {
        "value": {
          "start": 308,
          "end": 312,
          "text": "RSMI",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--282:E0"
      },
      {
        "value": {
          "start": 256,
          "end": 278,
          "text": "adversarial robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--282:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--282:E0",
        "to_id": "abstract-2023--acl-long--282:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Large-scale pre-trained language models have shown outstanding performance in a variety of NLP tasks. However, they are also known to be significantly brittle against specifically crafted adversarial examples, leading to increasing interest in probing the adversarial robustness of NLP systems. We introduce RSMI, a novel two-stage framework that combines randomized smoothing (RS) with masked inference (MI) to improve the adversarial robustness of NLP systems. RS transforms a classifier into a smoothed classifier to obtain robust representations, whereas MI forces a model to exploit the surrounding context of a masked token in an input sequence. RSMI improves adversarial robustness by 2 to 3 times over existing state-of-the-art methods on benchmark datasets. We also perform in-depth qualitative analysis to validate the effectiveness of the different stages of RSMI and probe the impact of its components through extensive ablations. By empirically proving the stability of RSMI, we put it forward as a practical method to robustly train large-scale NLP models. Our code and datasets are available athttps://github.com/Han8931/rsmi_nlp"
    }
  },
  {
    "id": "abstract-2023--acl-srw--22",
    "result": [
      {
        "value": {
          "start": 831,
          "end": 858,
          "text": "memorizing multiple objects",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-srw--22:E0"
      },
      {
        "value": {
          "start": 367,
          "end": 375,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-srw--22:E1"
      },
      {
        "value": {
          "start": 1152,
          "end": 1186,
          "text": "generalizing the retrieval ability",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-srw--22:E2"
      },
      {
        "value": {
          "start": 1199,
          "end": 1210,
          "text": "enumeration",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-srw--22:E3"
      },
      {
        "from_id": "abstract-2023--acl-srw--22:E0",
        "to_id": "abstract-2023--acl-srw--22:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-srw--22:E2",
        "to_id": "abstract-2023--acl-srw--22:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "It has been suggested that pretrained language models can be viewed as knowledge bases. One of the prerequisites for using language models as knowledge bases is how accurately they can store and retrieve world knowledge. It is already revealed that language models can store much 1-to-1 relational knowledge, such as ”country and its capital,” with high memorization accuracy. On the other hand, world knowledge includes not only 1-to-1 but also 1-to-N relational knowledge, such as ”parent and children.”However, it is not clear how accurately language models can handle 1-to-N relational knowledge. To investigate language models’ abilities toward 1-to-N relational knowledge, we start by designing the problem settings. Specifically, we organize the character of 1-to-N relational knowledge and define two essential skills: (i) memorizing multiple objects individually and (ii) retrieving multiple stored objects without excesses or deficiencies at once. We inspect LMs’ ability to handle 1-to-N relational knowledge on the controlled synthesized data. As a result, we report that it is possible to memorize multiple objects with high accuracy, but generalizing the retrieval ability (expressly, enumeration) is challenging."
    }
  },
  {
    "id": "abstract-2023--acl-industry--30",
    "result": [
      {
        "value": {
          "start": 849,
          "end": 897,
          "text": "new framework for consistent text categorization",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-industry--30:E0"
      },
      {
        "value": {
          "start": 926,
          "end": 945,
          "text": "model’s consistency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-industry--30:E1"
      },
      {
        "from_id": "abstract-2023--acl-industry--30:E0",
        "to_id": "abstract-2023--acl-industry--30:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The categorization of massive e-Commerce data is a crucial, well-studied task, which is prevalent in industrial settings. In this work, we aim to improve an existing product categorization model that is already in use by a major web company, serving multiple applications. At its core, the product categorization model is a text classification model that takes a product title as an input and outputs the most suitable category out of thousands of available candidates. Upon a closer inspection, we found inconsistencies in the labeling of similar items. For example, minor modifications of the product title pertaining to colors or measurements majorly impacted the model’s output. This phenomenon can negatively affect downstream recommendation or search applications, leading to a sub-optimal user experience. To address this issue, we propose a new framework for consistent text categorization. Our goal is to improve the model’s consistency while maintaining its production-level performance. We use a semi-supervised approach for data augmentation and presents two different methods for utilizing unlabeled samples. One method relies directly on existing catalogs, while the other uses a generative model. We compare the pros and cons of each approach and present our experimental results."
    }
  },
  {
    "id": "abstract-2023--acl-short--104",
    "result": [
      {
        "value": {
          "start": 213,
          "end": 229,
          "text": "stability of IFs",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--104:E0"
      }
    ],
    "data": {
      "text": "Influence functions (IFs) are a powerful tool for detecting anomalous examples in large scale datasets. However, they are unstable when applied to deep networks. In this paper, we provide an explanation for the instability of IFs and develop a solution to this problem. We show that IFs are unreliable when the two data points belong to two different classes. Our solution leverages class information to improve the stability of IFs.Extensive experiments show that our modification significantly improves the performance and stability of IFs while incurring no additional computational cost."
    }
  },
  {
    "id": "abstract-2023--acl-short--120",
    "result": [
      {
        "value": {
          "start": 496,
          "end": 514,
          "text": "prefix-propagation",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--120:E0"
      },
      {
        "value": {
          "start": 819,
          "end": 830,
          "text": "calibration",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--120:E1"
      },
      {
        "from_id": "abstract-2023--acl-short--120:E0",
        "to_id": "abstract-2023--acl-short--120:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Parameter-efficient tuning aims to mitigate the large memory requirements of adapting pretrained language models for downstream tasks. For example, one popular method, prefix-tuning, prepends trainable tokens to sequences while freezing the rest of the model’s parameters. Although such models attain comparable performance with fine-tuning when applied to sequences with short to moderate lengths, we show their inferior performance when modelling long sequences. To bridge this gap, we propose prefix-propagation, a simple but effective approach that conditions prefixes on previous hidden states. We empirically demonstrate that prefix-propagation outperforms prefix-tuning across long-document tasks, while using 50% fewer parameters. To further investigate the proposed architecture, we also show its advantage in calibration, and perform additional study on its relationship with kernel attention. To the best of our knowledge, this work is the first to focus on parameter-efficient learning for long-sequence language tasks."
    }
  },
  {
    "id": "abstract-2023--acl-long--1",
    "result": [
      {
        "value": {
          "start": 1357,
          "end": 1368,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--1:E0"
      },
      {
        "value": {
          "start": 1437,
          "end": 1459,
          "text": "generalization ability",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--1:E1"
      }
    ],
    "data": {
      "text": "User simulators are agents designed to imitate human users; recent advances have found that Task-oriented Dialogue (ToD) systems optimized toward a user simulator could better satisfy the need of human users. However, this might result in a sub-optimal ToD system if it is tailored to only onead hocuser simulator, since human users can behave differently. In this paper, we propose a framework called MUST to optimize ToD systems via leveraging Multiple User SimulaTors. The main challenges of implementing MUST fall in 1) how to adaptively determine which user simulator to interact with the ToD system at each optimization step, since the ToD system might be over-fitted to some specific user simulators, and simultaneously under-fitted to some others; 2) how to avoid catastrophic forgetting of the adaption for a simulator that is not selected for several consecutive optimization steps.To tackle these challenges, we formulate MUST as a Multi-armed bandits (MAB) problem and provide a method called MUSTadaptivethat balancesi) theboosting adaptionfor adaptive interactions between different user simulators and the ToD system andii) theuniform adaptionto avoid the catastrophic forgetting issue.With both automatic evaluations and human evaluations, our experimental results on MultiWOZ show that the dialogue system trained by MUST achieves a better performance than those trained by a single user simulator. It also has a better generalization ability when testing with unseen user simulators."
    }
  },
  {
    "id": "abstract-2023--acl-long--469",
    "result": [
      {
        "value": {
          "start": 934,
          "end": 977,
          "text": "state-of-the-art or competitive performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--469:E0"
      }
    ],
    "data": {
      "text": "We present a simple and unified approach for both continuous and discontinuous constituency parsing via autoregressive span selection. Constituency parsing aims to produce a set of non-crossing spans so that they can form a constituency parse tree. We sort gold spans using a predefined order and leverage a pointer network to autoregressively select spans by that order. To deal with discontinuous spans, we consecutively select their subspans from left to right, label all but last subspans with special discontinuous labels and the last subspan as the whole discontinuous spans’ labels. We use simple heuristic to output valid trees so that our approach is able to predict all possible continuous and discontinuous constituency trees without sacrificing data coverage and without the need to use expensive chart-based parsing algorithms. Experiments on multiple continuous and discontinuous benchmarks show that our model achieves state-of-the-art or competitive performance."
    }
  },
  {
    "id": "abstract-2023--acl-long--205",
    "result": [
      {
        "value": {
          "start": 1075,
          "end": 1082,
          "text": "MathGPT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--205:E0"
      }
    ],
    "data": {
      "text": "Mathematical language in scientific communications and educational scenarios is important yet relatively understudied compared to natural languages. Recent works on mathematical language focus either on representing stand-alone mathematical expressions, especially in their natural tree format, or mathematical reasoning in pre-trained natural language models. Existing works on jointly modeling and generating natural and mathematical languages simply treat mathematical expressions as text, without accounting for the rigid structural properties of mathematical expressions. In this paper, we propose a series of modifications to existing language models to jointly represent and generate text and math: representing mathematical expressions as sequences of node tokens in their operator tree format, using math symbol and tree position embeddings to preserve the semantic and structural properties of mathematical expressions, and using a constrained decoding method to generate mathematically valid expressions. We ground our modifications in GPT-2, resulting in a model MathGPT, and demonstrate that it outperforms baselines on mathematical expression generation tasks."
    }
  },
  {
    "id": "abstract-2023--acl-long--174",
    "result": [
      {
        "value": {
          "start": 556,
          "end": 567,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--174:E0"
      }
    ],
    "data": {
      "text": "Establishing retrieval-based dialogue systems that can select appropriate responses from the pre-built index has gained increasing attention. Recent common practice is to construct a two-stage pipeline with a fast retriever (e.g., bi-encoder) for first-stage recall followed by a smart response reranker (e.g., cross-encoder) for precise ranking. However, existing studies either optimize the retriever and reranker in independent ways, or distill the knowledge from a pre-trained reranker into the retriever in an asynchronous way, leading to sub-optimal performance of both modules. Thus, an open question remains about how to train them for a better combination of the best of both worlds. To this end, we present a cooperative training of the response retriever and the reranker whose parameters are dynamically optimized by the ground-truth labels as well as list-wise supervision signals from each other. As a result, the two modules can learn from each other and evolve together throughout the training. Experimental results on two benchmarks demonstrate the superiority of our method."
    }
  },
  {
    "id": "abstract-2023--acl-long--572",
    "result": [
      {
        "value": {
          "start": 258,
          "end": 278,
          "text": "linear decomposition",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--572:E0"
      },
      {
        "value": {
          "start": 639,
          "end": 677,
          "text": "efficiency and fidelity of explanation",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--572:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--572:E0",
        "to_id": "abstract-2023--acl-long--572:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In recent years, deep neural networks (DNNs) have achieved state-of-the-art performance on a wide range of tasks. However, limitations in interpretability have hindered their applications in the real world. This work proposes to interpret neural networks by linear decomposition and finds that the ReLU-activated Transformer can be considered as a linear model on a single input. We further leverage the linearity of the model and propose a linear decomposition of the model output to generate local explanations. Our evaluation of sentiment classification and machine translation shows that our method achieves competitive performance in efficiency and fidelity of explanation. In addition, we demonstrate the potential of our approach in applications with examples of error analysis on multiple tasks."
    }
  },
  {
    "id": "abstract-2023--acl-srw--26",
    "result": [
      {
        "value": {
          "start": 301,
          "end": 311,
          "text": "new corpus",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-srw--26:E0"
      },
      {
        "value": {
          "start": 441,
          "end": 467,
          "text": "classification performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-srw--26:E1"
      },
      {
        "value": {
          "start": 496,
          "end": 505,
          "text": "less data",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-srw--26:E2"
      },
      {
        "from_id": "abstract-2023--acl-srw--26:E0",
        "to_id": "abstract-2023--acl-srw--26:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-srw--26:E2",
        "to_id": "abstract-2023--acl-srw--26:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Numerous studies found that the linguistic properties of a person’s native language affect the cognitive processing of other languages. However, only one study has shown that it was possible to identify the native language based on eye-tracking records of natural L2 reading using machine learning. A new corpus allows us to replicate these results on a more interrelated and larger set of native languages. Our results show that comparable classification performance is maintained despite using less data. However, analysis shows that the correlation between L2 eye movements and native language similarity may be more complex than the original study found."
    }
  },
  {
    "id": "abstract-2023--acl-long--96",
    "result": [
      {
        "value": {
          "start": 600,
          "end": 609,
          "text": "Supporter",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--96:E0"
      },
      {
        "value": {
          "start": 379,
          "end": 407,
          "text": "positive emotion elicitation",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--96:E1"
      },
      {
        "value": {
          "start": 690,
          "end": 708,
          "text": "dialogue coherence",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--96:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--96:E0",
        "to_id": "abstract-2023--acl-long--96:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--96:E0",
        "to_id": "abstract-2023--acl-long--96:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Emotional support conversation (ESC) aims to provide emotional support (ES) to improve one’s mental state. Existing works stay at fitting grounded responses and responding strategies (e.g.,question), which ignore the effect on ES and lack explicit goals to guide emotional positive transition. To this end, we introduce a new paradigm to formalize multi-turn ESC as a process of positive emotion elicitation. Addressing this task requires finely adjusting the elicitation intensity in ES as the conversation progresses while maintaining conversational goals like coherence. In this paper, we propose Supporter, a mixture-of-expert-based reinforcement learning model, and well design ES and dialogue coherence rewards to guide policy’s learning for responding. Experiments verify the superiority of Supporter in achieving positive emotion elicitation during responding while maintaining conversational goals including coherence."
    }
  },
  {
    "id": "abstract-2023--acl-long--479",
    "result": [
      {
        "value": {
          "start": 359,
          "end": 372,
          "text": "AV-TranSpeech",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--479:E0"
      },
      {
        "value": {
          "start": 855,
          "end": 879,
          "text": "cross-modal distillation",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--479:E1"
      }
    ],
    "data": {
      "text": "Direct speech-to-speech translation (S2ST) aims to convert speech from one language into another, and has demonstrated significant progress to date. Despite the recent success, current S2ST models still suffer from distinct degradation in noisy environments and fail to translate visual speech (i.e., the movement of lips and teeth). In this work, we present AV-TranSpeech, the first audio-visual speech-to-speech (AV-S2ST) translation model without relying on intermediate text. AV-TranSpeech complements the audio stream with visual information to promote system robustness and opens up a host of practical applications: dictation or dubbing archival films. To mitigate the data scarcity with limited parallel AV-S2ST data, we 1) explore self-supervised pre-training with unlabeled audio-visual data to learn contextual representation, and 2) introduce cross-modal distillation with S2ST models trained on the audio-only corpus to further reduce the requirements of visual data. Experimental results on two language pairs demonstrate that AV-TranSpeech outperforms audio-only models under all settings regardless of the type of noise. With low-resource audio-visual data (10h, 30h), cross-modal distillation yields an improvement of 7.6 BLEU on average compared with baselines. Audio samples are available athttps://AV-TranSpeech.github.io/."
    }
  },
  {
    "id": "abstract-2023--acl-long--745",
    "result": [
      {
        "value": {
          "start": 209,
          "end": 214,
          "text": "EDAtt",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--745:E0"
      },
      {
        "value": {
          "start": 1033,
          "end": 1060,
          "text": "computational-aware latency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--745:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--745:E0",
        "to_id": "abstract-2023--acl-long--745:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "In simultaneous speech translation (SimulST), effective policies that determine when to write partial translations are crucial to reach high output quality with low latency. Towards this objective, we propose EDAtt (Encoder-Decoder Attention), an adaptive policy that exploits the attention patterns between audio source and target textual translation to guide an offline-trained ST model during simultaneous inference. EDAtt exploits the attention scores modeling the audio-translation relation to decide whether to emit a partial hypothesis or wait for more audio input. This is done under the assumption that, if attention is focused towards the most recently received speech segments, the information they provide can be insufficient to generate the hypothesis (indicating that the system has to wait for additional audio input). Results on en->de, es show that EDAtt yields better results compared to the SimulST state of the art, with gains respectively up to 7 and 4 BLEU points for the two languages, and with a reduction in computational-aware latency up to 1.4s and 0.7s compared to existing SimulST policies applied to offline-trained models."
    }
  },
  {
    "id": "abstract-2023--acl-industry--33",
    "result": [
      {
        "value": {
          "start": 34,
          "end": 42,
          "text": "EvolveMT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-industry--33:E0"
      },
      {
        "value": {
          "start": 806,
          "end": 826,
          "text": "translation accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-industry--33:E1"
      },
      {
        "value": {
          "start": 718,
          "end": 722,
          "text": "cost",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-industry--33:E2"
      },
      {
        "from_id": "abstract-2023--acl-industry--33:E0",
        "to_id": "abstract-2023--acl-industry--33:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-industry--33:E0",
        "to_id": "abstract-2023--acl-industry--33:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This work proposes a method named EvolveMT for the efficient combination of multiple machine translation (MT) engines. The method selects the output from one engine for each segment, using online learning techniques to predict the most appropriate system for each translation request. A neural quality estimation metric supervises the method without requiring reference translations. The method’s online learning capability enables it to adapt to changes in the domain or MT engines dynamically, eliminating the requirement for retraining. The method selects a subset of translation engines to be called based on the source sentence features. The degree of exploration is configurable according to the desired quality-cost trade-off. Results from custom datasets demonstrate that EvolveMT achieves similar translation accuracy at a lower cost than selecting the best translation of each segment from all translations using an MT quality estimator. To the best of our knowledge, EvolveMT is the first MT system that adapts itself after deployment to incoming translation requests from the production environment without needing costly retraining on human feedback."
    }
  },
  {
    "id": "abstract-2023--acl-long--428",
    "result": [
      {
        "value": {
          "start": 1165,
          "end": 1197,
          "text": "adding Entity-Linking objectives",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--428:E0"
      }
    ],
    "data": {
      "text": "Extracting structured and grounded fact triples from raw text is a fundamental task in Information Extraction (IE). Existing IE datasets are typically collected from Wikipedia articles, using hyperlinks to link entities to the Wikidata knowledge base. However, models trained only on Wikipedia have limitations when applied to web domains, which often contain noisy text or text that does not have any factual information. We present WebIE, the first large-scale, entity-linked closed IE dataset consisting of 1.6M sentences automatically collected from the English Common Crawl corpus. WebIE also includes negative examples, i.e. sentences without fact triples, to better reflect the data on the web. We annotate ~25K triples from WebIE through crowdsourcing and introduce mWebIE, a translation of the annotated set in four other languages: French, Spanish, Portuguese, and Hindi. We evaluate the in-domain, out-of-domain, and zero-shot cross-lingual performance of generative IE models and find models trained on WebIE show better generalisability. We also propose three training strategies that use entity linking as an auxiliary task. Our experiments show that adding Entity-Linking objectives improves the faithfulness of our generative IE models."
    }
  },
  {
    "id": "abstract-2023--acl-long--844",
    "result": [
      {
        "value": {
          "start": 1234,
          "end": 1260,
          "text": "fine-tuned language models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--844:E0"
      },
      {
        "value": {
          "start": 1301,
          "end": 1328,
          "text": "summary factual consistency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--844:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--844:E0",
        "to_id": "abstract-2023--acl-long--844:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Despite the recent progress in language generation models, their outputs may not always meet user expectations. In this work, we study whether informational feedback in natural language can be leveraged to improve generation quality and user preference alignment. To this end, we consider factual consistency in summarization, the quality that the summary should only contain information supported by the input documents, as the user-expected preference. We collect a high-quality dataset, DeFacto, containing human demonstrations and informational natural language feedback consisting of corrective instructions, edited summaries, and explanations with respect to the factual consistency of the summary. Using our dataset, we study three natural language generation tasks: (1) editing a summary by following the human feedback, (2) generating human feedback for editing the original summary, and (3) revising the initial summary to correct factual errors by generating both the human feedback and edited summary. We show that DeFacto can provide factually consistent human-edited summaries and further insights into summarization factual consistency thanks to its informational natural language feedback. We further demonstrate that fine-tuned language models can leverage our dataset to improve the summary factual consistency, while large language models lack the zero-shot learning ability in our proposed tasks that require controllable text generation."
    }
  },
  {
    "id": "abstract-2023--acl-long--869",
    "result": [
      {
        "value": {
          "start": 391,
          "end": 399,
          "text": "NPPrompt",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--869:E0"
      },
      {
        "value": {
          "start": 1072,
          "end": 1103,
          "text": "accuracy on text classification",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--869:E1"
      },
      {
        "value": {
          "start": 1121,
          "end": 1135,
          "text": "GLUE benchmark",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--869:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--869:E0",
        "to_id": "abstract-2023--acl-long--869:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--869:E0",
        "to_id": "abstract-2023--acl-long--869:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "How can we extend a pre-trained model to many language understanding tasks, without labeled or additional unlabeled data? Pre-trained language models (PLMs) have been effective for a wide range of NLP tasks. However, existing approaches either require fine-tuning on downstream labeled datasets or manually constructing proper prompts. In this paper, we propose nonparametric prompting PLM (NPPrompt) for fully zero-shot language understanding. Unlike previous methods, NPPrompt uses only pre-trained language models and does not require any labeled data or additional raw corpus for further fine-tuning, nor does it rely on humans to construct a comprehensive set of prompt label words. We evaluate NPPrompt against previous major few-shot and zero-shot learning methods on diverse NLP tasks: including text classification, text entailment, similar text retrieval, paraphrasing, and multiple-choice question answering. Experimental results demonstrate that our NPPrompt outperforms the previous best fully zero-shot method by big margins, with absolute gains of 12.8% in accuracy on text classification and 15.6% on the GLUE benchmark. Our source code is available athttps://anonymous.4open.science/r/NPPrompt."
    }
  },
  {
    "id": "abstract-2023--acl-long--19",
    "result": [
      {
        "value": {
          "start": 569,
          "end": 622,
          "text": "AMR-based Path Aggregation Relational Network (APARN)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--19:E0"
      },
      {
        "value": {
          "start": 1089,
          "end": 1103,
          "text": "F1 improvement",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--19:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--19:E0",
        "to_id": "abstract-2023--acl-long--19:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment classification task. Many recent works have used dependency trees to extract the relation between aspects and contexts and have achieved significant improvements. However, further improvement is limited due to the potential mismatch between the dependency tree as a syntactic structure and the sentiment classification as a semantic task. To alleviate this gap, we replace the syntactic dependency tree with the semantic structure named Abstract Meaning Representation (AMR) and propose a model called AMR-based Path Aggregation Relational Network (APARN) to take full advantage of semantic structures. In particular, we design the path aggregator and the relation-enhanced self-attention mechanism that complement each other. The path aggregator extracts semantic features from AMRs under the guidance of sentence information, while the relation-enhanced self-attention mechanism in turn improves sentence features with refined semantic information. Experimental results on four public datasets demonstrate 1.13% average F1 improvement of APARN in ABSA when compared with state-of-the-art baselines."
    }
  },
  {
    "id": "abstract-2023--acl-long--376",
    "result": [
      {
        "value": {
          "start": 504,
          "end": 571,
          "text": "multimodal back-translation using diffusion-based generative models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--376:E0"
      }
    ],
    "data": {
      "text": "We revisit the multimodal entity and relation extraction from a translation point of view. Special attention is paid on the misalignment issue in text-image datasets which may mislead the learning. We are motivated by the fact that the cross-modal misalignment is a similar problem of cross-lingual divergence issue in machine translation. The problem can then be transformed and existing solutions can be borrowed by treating a text and its paired image as the translation to each other. We implement a multimodal back-translation using diffusion-based generative models for pseudo-paralleled pairs and a divergence estimator by constructing a high-resource corpora as a bridge for low-resource learners. Fine-grained confidence scores are generated to indicate both types and degrees of alignments with which better representations are obtained. The method has been validated in the experiments by outperforming 14 state-of-the-art methods in both entity and relation extraction tasks. The source code is available athttps://github.com/thecharm/TMR."
    }
  },
  {
    "id": "abstract-2023--acl-long--321",
    "result": [
      {
        "value": {
          "start": 467,
          "end": 486,
          "text": "datastore retrieval",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--321:E0"
      },
      {
        "value": {
          "start": 501,
          "end": 520,
          "text": "translation quality",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--321:E1"
      }
    ],
    "data": {
      "text": "k-Nearest neighbor machine translation (kNN-MT) has attracted increasing attention due to its ability to non-parametrically adapt to new translation domains. By using an upstream NMT model to traverse the downstream training corpus, it is equipped with a datastore containing vectorized key-value pairs, which are retrieved during inference to benefit translation.However, there often exists a significant gap between upstream and downstream domains, which hurts the datastore retrieval and the final translation quality.To deal with this issue, we propose a novel approach to boost the datastore retrieval ofkNN-MT by reconstructing the original datastore.Concretely, we design a reviser to revise the key representations, making them better fit for the downstream domain. The reviser is trained using the collected semantically-related key-queries pairs, and optimized by two proposed losses: one is the key-queries semantic distance ensuring each revised key representation is semantically related to its corresponding queries, and the other is an L2-norm loss encouraging revised key representations to effectively retain the knowledge learned by the upstream NMT model. Extensive experiments on domain adaptation tasks demonstrate that our method can effectively boost the datastore retrieval and translation quality ofkNN-MT.Our code is available athttps://github.com/DeepLearnXMU/Revised-knn-mt."
    }
  },
  {
    "id": "abstract-2023--acl-srw--36",
    "result": [
      {
        "value": {
          "start": 464,
          "end": 502,
          "text": "early, linguistically inspired methods",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-srw--36:E0"
      }
    ],
    "data": {
      "text": "Procedural knowledge understanding (PKU) underlies the ability to infer goal-step relations. The task of Visual Goal–Step Inference addresses this ability in the multimodal domain. It requires to identify images that represent the steps towards achieving a textually expressed goal. The best existing methods encode texts and images either with independent encoders, or with object-level multimodal encoders using blackbox transformers. This stands in contrast to early, linguistically inspired methods for event representations, which focus on capturing the most crucial information, namely actions and the participants, to learn stereotypical event sequences and hence procedural knowledge. In this work, we study various methods and their effects on PKU of injecting the early shallow event representations to nowadays multimodal deep learning-based models. We find that the early, linguistically inspired methods for representing event knowledge does contribute to understand procedures in combination with modern vision-and-language models. In the future, we are going to explore more complex structure of events and study how to exploit it on top of large language models."
    }
  },
  {
    "id": "abstract-2023--acl-long--777",
    "result": [
      {
        "value": {
          "start": 571,
          "end": 623,
          "text": "unsupervised retrieval method-based pipeline U-CREAT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--777:E0"
      },
      {
        "value": {
          "start": 759,
          "end": 770,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--777:E1"
      },
      {
        "value": {
          "start": 1030,
          "end": 1058,
          "text": "state-of-the-art performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--777:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--777:E0",
        "to_id": "abstract-2023--acl-long--777:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--777:E0",
        "to_id": "abstract-2023--acl-long--777:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The task of Prior Case Retrieval (PCR) in the legal domain is about automatically citing relevant (based on facts and precedence) prior legal cases in a given query case. To further promote research in PCR, in this paper, we propose a new large benchmark (in English) for the PCR task: IL-PCR (Indian Legal Prior Case Retrieval) corpus. Given the complex nature of case relevance and the long size of legal documents, BM25 remains a strong baseline for ranking the cited prior documents. In this work, we explore the role of events in legal case retrieval and propose an unsupervised retrieval method-based pipeline U-CREAT (Unsupervised Case Retrieval using Events Extraction). We find that the proposed unsupervised retrieval method significantly increases performance compared to BM25 and makes retrieval faster by a considerable margin, making it applicable to real-time case retrieval systems. Our proposed system is generic, we show that it generalizes across two different legal systems (Indian and Canadian), and it shows state-of-the-art performance on the benchmarks for both the legal systems (IL-PCR and COLIEE corpora)."
    }
  },
  {
    "id": "abstract-2023--acl-long--192",
    "result": [
      {
        "value": {
          "start": 604,
          "end": 640,
          "text": "TIT model with a multimodal codebook",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--192:E0"
      },
      {
        "value": {
          "start": 1062,
          "end": 1075,
          "text": "effectiveness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--192:E1"
      },
      {
        "value": {
          "start": 783,
          "end": 813,
          "text": "multi-stage training framework",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--192:E2"
      },
      {
        "from_id": "abstract-2023--acl-long--192:E0",
        "to_id": "abstract-2023--acl-long--192:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-long--192:E2",
        "to_id": "abstract-2023--acl-long--192:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Text image translation (TIT) aims to translate the source texts embedded in the image to target translations, which has a wide range of applications and thus has important research value. However, current studies on TIT are confronted with two main bottlenecks: 1) this task lacks a publicly available TIT dataset, 2) dominant models are constructed in a cascaded manner, which tends to suffer from the error propagation of optical character recognition (OCR). In this work, we first annotate a Chinese-English TIT dataset named OCRMT30K, providing convenience for subsequent studies. Then, we propose a TIT model with a multimodal codebook, which is able to associate the image with relevant texts, providing useful supplementary information for translation. Moreover, we present a multi-stage training framework involving text machine translation, image-text alignment, and TIT tasks, which fully exploits additional bilingual texts, OCR dataset and our OCRMT30K dataset to train our model. Extensive experiments and in-depth analyses strongly demonstrate the effectiveness of our proposed model and training framework."
    }
  },
  {
    "id": "abstract-2023--acl-short--1",
    "result": [
      {
        "value": {
          "start": 505,
          "end": 533,
          "text": "ignoring the marginalization",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-short--1:E0"
      },
      {
        "value": {
          "start": 796,
          "end": 817,
          "text": "gap in log-likelihood",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--1:E1"
      },
      {
        "from_id": "abstract-2023--acl-short--1:E0",
        "to_id": "abstract-2023--acl-short--1:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Autoregressive language models (LMs) map token sequences to probabilities. The usual practice for computing the probability of any character string (e.g. English sentences) is to first transform it into a sequence of tokens that is scored by the model. However, there are exponentially many token sequences that represent any given string. To truly compute the probability of a string one should marginalize over all tokenizations, which is typically intractable. Here, we analyze whether the practice of ignoring the marginalization is justified. To this end, we devise an importance-sampling-based algorithm that allows us to compute estimates of the marginal probabilities and compare them to the default procedure in a range of state-of-the-art models and datasets. Our results show that the gap in log-likelihood is no larger than 0.5% in most cases, but that it becomes more pronounced for data with long complex words."
    }
  },
  {
    "id": "abstract-2023--acl-srw--49",
    "result": [
      {
        "value": {
          "start": 1232,
          "end": 1321,
          "text": "training on linguistic variants of problem statements and voting on candidate predictions",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-srw--49:E0"
      },
      {
        "value": {
          "start": 11,
          "end": 33,
          "text": "mathematical reasoning",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-srw--49:E1"
      },
      {
        "value": {
          "start": 1361,
          "end": 1384,
          "text": "robustness of the model",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-srw--49:E2"
      },
      {
        "from_id": "abstract-2023--acl-srw--49:E0",
        "to_id": "abstract-2023--acl-srw--49:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2023--acl-srw--49:E0",
        "to_id": "abstract-2023--acl-srw--49:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "The art of mathematical reasoning stands as a fundamental pillar of intellectual progress and is a central catalyst in cultivating human ingenuity. Researchers have recently published a plethora of works centered around the task of solving Math Word Problems (MWP) — a crucial stride towards general AI. These existing models are susceptible to dependency on shallow heuristics and spurious correlations to derive the solution expressions. In order to ameliorate this issue, in this paper, we propose a framework for MWP solvers based on the generation of linguistic variants of the problem text. The approach involves solving each of the variant problems and electing the predicted expression with the majority of the votes. We use DeBERTa (Decoding-enhanced BERT with disentangled attention) as the encoder to leverage its rich textual representations and enhanced mask decoder to construct the solution expressions. Furthermore, we introduce a challenging dataset, ParaMAWPS, consisting of paraphrased, adversarial, and inverse variants of selectively sampled MWPs from the benchmark Mawps dataset. We extensively experiment on this dataset along with other benchmark datasets using some baseline MWP solver models. We show that training on linguistic variants of problem statements and voting on candidate predictions improve the mathematical reasoning and robustness of the model. We make our code and data publicly available."
    }
  },
  {
    "id": "abstract-2023--acl-demo--25",
    "result": [
      {
        "value": {
          "start": 505,
          "end": 508,
          "text": "XMD",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-demo--25:E0"
      },
      {
        "value": {
          "start": 1009,
          "end": 1053,
          "text": "OOD performance on text classification tasks",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-demo--25:E1"
      },
      {
        "from_id": "abstract-2023--acl-demo--25:E0",
        "to_id": "abstract-2023--acl-demo--25:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "NLP models are susceptible to learning spurious biases (i.e., bugs) that work on some datasets but do not properly reflect the underlying task. Explanation-based model debugging aims to resolve spurious biases by showing human users explanations of model behavior, asking users to give feedback on the behavior, thenusing the feedback to update the model. While existing model debugging methods have shown promise, their prototype-level implementations provide limited practical utility. Thus, we propose XMD: the first open-source, end-to-end framework for explanation-based model debugging. Given task- or instance-level explanations,users can flexibly provide various forms of feedback via an intuitive, web-based UI. After receiving user feedback, XMD automatically updates the model in real time, by regularizing the model so that its explanationsalign with the user feedback. The new model can then be easily deployed into real-world applications via Hugging Face. Using XMD, we can improve the model’s OOD performance on text classification tasks by up to 18%."
    }
  },
  {
    "id": "abstract-2023--acl-long--635",
    "result": [
      {
        "value": {
          "start": 617,
          "end": 621,
          "text": "TEAM",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--635:E0"
      }
    ],
    "data": {
      "text": "Multimodal Sarcasm Explanation (MuSE) is a new yet challenging task, which aims to generate a natural language sentence for a multimodal social post (an image as well as its caption) to explain why it contains sarcasm. Although the existing pioneer study has achieved great success with the BART backbone, it overlooks the gap between the visual feature space and the decoder semantic space, the object-level metadata of the image, as well as the potential external knowledge. To solve these limitations, in this work, we propose a novel mulTi-source sEmantic grAph-based Multimodal sarcasm explanation scheme, named TEAM. In particular, TEAM extracts the object-level semantic meta-data instead of the traditional global visual features from the input image. Meanwhile, TEAM resorts to ConceptNet to obtain the external related knowledge concepts for the input text and the extracted object meta-data. Thereafter, TEAM introduces a multi-source semantic graph that comprehensively characterize the multi-source (i.e., caption, object meta-data, external knowledge) semantic relations to facilitate the sarcasm reasoning. Extensive experiments on a public released dataset MORE verify the superiority of our model over cutting-edge methods."
    }
  },
  {
    "id": "abstract-2023--acl-long--131",
    "result": [
      {
        "value": {
          "start": 851,
          "end": 879,
          "text": "state-of-the-art performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--131:E0"
      }
    ],
    "data": {
      "text": "Simultaneous machine translation (SiMT) presents a unique challenge as it requires generating target tokens before the source sentence is fully consumed. This can lead to the hallucination problem, where target tokens are generated without support from the source sentence. The prefix-to-prefix training data used to train SiMT models are not always parallel, due to divergent word order between the source and target languages, and can contribute to the problem. In this paper, we propose a novel approach that leverages traditional translation models as teachers and employs a two-stage beam search algorithm to generate monotonic yet accurate reference translations for sequence-level knowledge distillation. Experimental results demonstrate the significant improvements achieved by our approach over multiple strong SiMT baselines, leading to new state-of-the-art performance across various language pairs. Notably, when evaluated on a monotonic version of the WMT15 De-En test set, which includes references generated in a more monotonic style by professional translators, our approach achieves even more substantial improvement over the baselines. The source code and data are publicly available for further exploration."
    }
  },
  {
    "id": "abstract-2023--acl-short--124",
    "result": [
      {
        "value": {
          "start": 802,
          "end": 810,
          "text": "96.4 LAS",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--124:E0"
      },
      {
        "value": {
          "start": 815,
          "end": 823,
          "text": "97.4 UAS",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--124:E1"
      },
      {
        "value": {
          "start": 943,
          "end": 967,
          "text": "computational efficiency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-short--124:E2"
      }
    ],
    "data": {
      "text": "We introduce a novel dependency parser, the hexatagger, that constructs dependency trees by tagging the words in a sentence with elements from a finite set of possible tags. In contrast to many approaches to dependency parsing, our approach is fully parallelizable at training time, i.e., the structure-building actions needed to build a dependency parse can be predicted in parallel to each other. Additionally, exact decoding is linear in time and space complexity. Furthermore, we derive a probabilistic dependency parser that predicts hexatags using no more than a linear model with features from a pretrained language model, i.e., we forsake a bespoke architecture explicitly designed for the task. Despite the generality and simplicity of our approach, we achieve state-of-the-art performance of 96.4 LAS and 97.4 UAS on the Penn Treebank test set. Additionally, our parser’s linear time complexity and parallelism significantly improve computational efficiency, with a roughly 10-times speed-up over previous state-of-the-art models during decoding."
    }
  },
  {
    "id": "abstract-2023--acl-long--242",
    "result": [
      {
        "value": {
          "start": 216,
          "end": 244,
          "text": "explanation-based finetuning",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--242:E0"
      },
      {
        "value": {
          "start": 801,
          "end": 809,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--242:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--242:E0",
        "to_id": "abstract-2023--acl-long--242:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Large Language Models (LLMs) are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task, leading to poor generalization on out-of-distribution data. We propose explanation-based finetuning as a general approach to mitigate LLMs’ reliance on spurious correlations. Unlike standard finetuning where the model only predicts the answer given the input, we finetune the model to additionally generate a free-text explanation supporting its answer. To evaluate our method, we finetune the model on artificially constructed training sets containing different types of spurious cues, and test it on a test set without these cues. Compared to standard finetuning, our method makes GPT-3 (davinci) remarkably more robust against spurious cues in terms of accuracy drop across four classification tasks: ComVE (+1.2), CREAK (+9.1), e-SNLI (+15.4), and SBIC (+6.5). The efficacy generalizes across multiple model families and scales, with greater gains for larger models. Finally, our method also works well with explanations generated by the model, implying its applicability to more datasets without human-written explanations."
    }
  },
  {
    "id": "abstract-2023--acl-long--136",
    "result": [
      {
        "value": {
          "start": 488,
          "end": 524,
          "text": "augmentation-adapted retriever (AAR)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--136:E0"
      },
      {
        "value": {
          "start": 725,
          "end": 770,
          "text": "zero-shot generalization of larger target LMs",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--136:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--136:E0",
        "to_id": "abstract-2023--acl-long--136:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Retrieval augmentation can aid language models (LMs) in knowledge-intensive tasks by supplying them with external information. Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target LMs that may not be known beforehand or are unable to be fine-tuned together. To retrieve useful documents for unseen target LMs, we propose augmentation-adapted retriever (AAR), which learns LM’s preferences obtained from a known source LM. Experiments on the MMLU and PopQA datasets demonstrate that our AAR trained with a small source LM is able to significantly improve the zero-shot generalization of larger target LMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates that the preferences of different LMs overlap, enabling AAR trained with a single source LM to serve as a generic plug-in for various target LMs. Our code is open-sourced athttps://github.com/OpenMatch/Augmentation-Adapted-Retriever."
    }
  },
  {
    "id": "abstract-2023--acl-long--39",
    "result": [
      {
        "value": {
          "start": 327,
          "end": 348,
          "text": "modality independence",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2023--acl-long--39:E0"
      },
      {
        "value": {
          "start": 371,
          "end": 388,
          "text": "model performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2023--acl-long--39:E1"
      },
      {
        "from_id": "abstract-2023--acl-long--39:E0",
        "to_id": "abstract-2023--acl-long--39:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Multi-modal emotion recognition has gained increasing attention in recent years due to its widespread applications and the advances in multi-modal learning approaches. However, previous studies primarily focus on developing models that exploit the unification of multiple modalities. In this paper, we propose that maintaining modality independence is beneficial for the model performance. According to this principle, we construct a dataset, and devise a multi-modal transformer model. The new dataset, CHinese Emotion Recognition dataset with Modality-wise Annotions, abbreviated as CHERMA, provides uni-modal labels for each individual modality, and multi-modal labels for all modalities jointly observed. The model consists of uni-modal transformer modules that learn representations for each modality, and a multi-modal transformer module that fuses all modalities. All the modules are supervised by their corresponding labels separately, and the forward information flow is uni-directionally from the uni-modal modules to the multi-modal module. The supervision strategy and the model architecture guarantee each individual modality learns its representation independently, and meanwhile the multi-modal module aggregates all information. Extensive empirical results demonstrate that our proposed scheme outperforms state-of-the-art alternatives, corroborating the importance of modality independence in multi-modal emotion recognition. The dataset and codes are availabel athttps://github.com/sunjunaimer/LFMIM"
    }
  },
  {
    "id": "abstract-2020--acl-main--34",
    "result": [
      {
        "value": {
          "start": 473,
          "end": 495,
          "text": "content word-aware NMT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--34:E0"
      },
      {
        "value": {
          "start": 519,
          "end": 530,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--34:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--34:E0",
        "to_id": "abstract-2020--acl-main--34:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Neural machine translation (NMT) encodes the source sentence in a universal way to generate the target sentence word-by-word. However, NMT does not consider the importance of word in the sentence meaning, for example, some words (i.e., content words) express more important meaning than others (i.e., function words). To address this limitation, we first utilize word frequency information to distinguish between content and function words in a sentence, and then design a content word-aware NMT to improve translation performance. Empirical results on the WMT14 English-to-German, WMT14 English-to-French, and WMT17 Chinese-to-English translation tasks show that the proposed methods can significantly improve the performance of Transformer-based NMT."
    }
  },
  {
    "id": "abstract-2021--acl-long--303",
    "result": [
      {
        "value": {
          "start": 924,
          "end": 928,
          "text": "ABCD",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--303:E0"
      },
      {
        "value": {
          "start": 949,
          "end": 960,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--303:E1"
      },
      {
        "value": {
          "start": 549,
          "end": 596,
          "text": "Accept, Break, Copy or Drop elements of a graph",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--303:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--303:E0",
        "to_id": "abstract-2021--acl-long--303:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--303:E2",
        "to_id": "abstract-2021--acl-long--303:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Atomic clauses are fundamental text units for understanding complex sentences. Identifying the atomic sentences within complex sentences is important for applications such as summarization, argument mining, discourse analysis, discourse parsing, and question answering. Previous work mainly relies on rule-based methods dependent on parsing. We propose a new task to decompose each complex sentence into simple sentences derived from the tensed clauses in the source, and a novel problem formulation as a graph edit task. Our neural model learns to Accept, Break, Copy or Drop elements of a graph that combines word adjacency and grammatical dependencies. The full processing pipeline includes modules for graph construction, graph editing, and sentence generation from the output graph. We introduce DeSSE, a new dataset designed to train and evaluate complex sentence decomposition, and MinWiki, a subset of MinWikiSplit. ABCD achieves comparable performance as two parsing baselines on MinWiki. On DeSSE, which has a more even balance of complex sentence types, our model achieves higher accuracy on the number of atomic sentences than an encoder-decoder baseline. Results include a detailed error analysis."
    }
  },
  {
    "id": "P10-1049",
    "result": [
      {
        "value": {
          "start": 265,
          "end": 289,
          "text": "leaving-one-out approach",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1049:E0"
      },
      {
        "value": {
          "start": 750,
          "end": 754,
          "text": "BLEU",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1049:E1"
      },
      {
        "value": {
          "start": 778,
          "end": 795,
          "text": "phrase table size",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1049:E2"
      },
      {
        "from_id": "P10-1049:E0",
        "to_id": "P10-1049:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P10-1049:E0",
        "to_id": "P10-1049:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Several attempts have been made to learn phrase translation probabilities for phrase-based statistical machine translation that go beyond pure counting of phrases in word-aligned training data. Most approaches report problems with over-fitting. We describe a novel leaving-one-out approach to prevent over-fitting that allows us to train phrase models that show improved translation performance on the WMT08 Europarl German-English task. In contrast to most previous work where phrase models were trained separately from other models used in translation, we include all components such as single word lexica and reordering models in training. Using this consistent training of phrase models we are able to achieve improvements of up to 1.4 points in BLEU. As a side effect, the phrase table size is reduced by more than 80%."
    }
  },
  {
    "id": "abstract-2021--acl-long--265",
    "result": [
      {
        "value": {
          "start": 151,
          "end": 226,
          "text": "introduce denoising word alignment as a new cross-lingual pre-training task",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--265:E0"
      },
      {
        "value": {
          "start": 829,
          "end": 839,
          "text": "error rate",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--265:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--265:E0",
        "to_id": "abstract-2021--acl-long--265:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "The cross-lingual language models are typically pretrained with masked language modeling on multilingual text or parallel sentences. In this paper, we introduce denoising word alignment as a new cross-lingual pre-training task. Specifically, the model first self-label word alignments for parallel sentences. Then we randomly mask tokens in a bitext pair. Given a masked token, the model uses a pointer network to predict the aligned token in the other language. We alternately perform the above two steps in an expectation-maximization manner. Experimental results show that our method improves cross-lingual transferability on various datasets, especially on the token-level tasks, such as question answering, and structured prediction. Moreover, the model can serve as a pretrained word aligner, which achieves reasonably low error rate on the alignment benchmarks. The code and pretrained parameters are available at github.com/CZWin32768/XLM-Align."
    }
  },
  {
    "id": "abstract-2021--acl-long--206",
    "result": [
      {
        "value": {
          "start": 493,
          "end": 536,
          "text": "Automated Concatenation of Embeddings (ACE)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--206:E0"
      },
      {
        "value": {
          "start": 1320,
          "end": 1331,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--206:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--206:E0",
        "to_id": "abstract-2021--acl-long--206:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Pretrained contextualized embeddings are powerful word representations for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of embeddings. However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward. We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in all the evaluations."
    }
  },
  {
    "id": "P11-1097",
    "result": [
      {
        "value": {
          "start": 225,
          "end": 271,
          "text": "submit a token with context to a search engine",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1097:E0"
      },
      {
        "value": {
          "start": 413,
          "end": 424,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1097:E1"
      },
      {
        "value": {
          "start": 276,
          "end": 344,
          "text": "use similar contexts in the search results as additional information",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1097:E2"
      },
      {
        "from_id": "P11-1097:E0",
        "to_id": "P11-1097:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "P11-1097:E2",
        "to_id": "P11-1097:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We use search engine results to address a particularly difficult cross-domain language processing task, the adaptation of named entity recognition (NER) from news text to web queries. The key novelty of the method is that we submit a token with context to a search engine and use similar contexts in the search results as additional information for correctly classifying the token. We achieve strong gains in NER performance on news, in-domain and out-of-domain, and on web queries."
    }
  },
  {
    "id": "abstract-2021--acl-long--217",
    "result": [
      {
        "value": {
          "start": 276,
          "end": 286,
          "text": "Text2Event",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--217:E0"
      },
      {
        "value": {
          "start": 799,
          "end": 810,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--217:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--217:E0",
        "to_id": "abstract-2021--acl-long--217:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event. Traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks. In this paper, we propose Text2Event, a sequence-to-structure generation paradigm that can directly extract events from the text in an end-to-end manner. Specifically, we design a sequence-to-structure network for unified event extraction, a constrained decoding algorithm for event knowledge injection during inference, and a curriculum learning algorithm for efficient model learning. Experimental results show that, by uniformly modeling all tasks in a single model and universally predicting different labels, our method can achieve competitive performance using only record-level annotations in both supervised learning and transfer learning settings."
    }
  },
  {
    "id": "abstract-2021--acl-long--421",
    "result": [
      {
        "value": {
          "start": 920,
          "end": 940,
          "text": "gradient information",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--421:E0"
      },
      {
        "value": {
          "start": 967,
          "end": 987,
          "text": "transfer performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--421:E1"
      },
      {
        "value": {
          "start": 633,
          "end": 637,
          "text": "CdKD",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--421:E2"
      },
      {
        "value": {
          "start": 976,
          "end": 987,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--421:E3"
      },
      {
        "from_id": "abstract-2021--acl-long--421:E0",
        "to_id": "abstract-2021--acl-long--421:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--421:E2",
        "to_id": "abstract-2021--acl-long--421:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Unsupervised Domain Adaptation (UDA) aims to transfer the knowledge of source domain to the unlabeled target domain. Existing methods typically require to learn to adapt the target model by exploiting the source data and sharing the network architecture across domains. However, this pipeline makes the source data risky and is inflexible for deploying the target model. This paper tackles a novel setting where only a trained source model is available and different network architectures can be adapted for target domain in terms of deployment environments. We propose a generic framework named Cross-domain Knowledge Distillation (CdKD) without needing any source data. CdKD matches the joint distributions between a trained source model and a set of target data during distilling the knowledge from the source model to the target domain. As a type of important knowledge in the source domain, for the first time, the gradient information is exploited to boost the transfer performance. Experiments on cross-domain text classification demonstrate that CdKD achieves superior performance, which verifies the effectiveness in this novel setting."
    }
  },
  {
    "id": "abstract-2021--acl-long--279",
    "result": [
      {
        "value": {
          "start": 863,
          "end": 914,
          "text": "morphologically-informed vocabulary of input tokens",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--279:E0"
      },
      {
        "value": {
          "start": 796,
          "end": 831,
          "text": "generalization capabilities of PLMs",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--279:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--279:E0",
        "to_id": "abstract-2021--acl-long--279:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "How does the input segmentation of pretrained language models (PLMs) affect their interpretations of complex words? We present the first study investigating this question, taking BERT as the example PLM and focusing on its semantic representations of English derivatives. We show that PLMs can be interpreted as serial dual-route models, i.e., the meanings of complex words are either stored or else need to be computed from the subwords, which implies that maximally meaningful input tokens should allow for the best generalization on new words. This hypothesis is confirmed by a series of semantic probing tasks on which DelBERT (Derivation leveraging BERT), a model with derivational input segmentation, substantially outperforms BERT with WordPiece segmentation. Our results suggest that the generalization capabilities of PLMs could be further improved if a morphologically-informed vocabulary of input tokens were used."
    }
  },
  {
    "id": "abstract-2021--acl-long--70",
    "result": [
      {
        "value": {
          "start": 641,
          "end": 654,
          "text": "long contexts",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--70:E0"
      },
      {
        "value": {
          "start": 742,
          "end": 752,
          "text": "perplexity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--70:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--70:E0",
        "to_id": "abstract-2021--acl-long--70:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Transformer-based language models benefit from conditioning on contexts of hundreds to thousands of previous tokens. What aspects of these contexts contribute to accurate model prediction? We describe a series of experiments that measure usable information by selectively ablating lexical and structural information in transformer language models trained on English Wikipedia. In both mid- and long-range contexts, we find that several extremely destructive context manipulations—including shuffling word order within sentences and deleting all words other than nouns—remove less than 15% of the usable information. Our results suggest that long contexts, but not their detailed syntactic and propositional content, are important for the low perplexity of current transformer language models."
    }
  },
  {
    "id": "abstract-2021--acl-long--285",
    "result": [
      {
        "value": {
          "start": 503,
          "end": 511,
          "text": "MPC-BERT",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--285:E0"
      },
      {
        "value": {
          "start": 1227,
          "end": 1238,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--285:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--285:E0",
        "to_id": "abstract-2021--acl-long--285:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Recently, various neural models for multi-party conversation (MPC) have achieved impressive improvements on a variety of tasks such as addressee recognition, speaker identification and response prediction. However, these existing methods on MPC usually represent interlocutors and utterances individually and ignore the inherent complicated structure in MPC which may provide crucial interlocutor and utterance semantics and would enhance the conversation understanding process. To this end, we present MPC-BERT, a pre-trained model for MPC understanding that considers learning who says what to whom in a unified model with several elaborated self-supervised tasks. Particularly, these tasks can be generally categorized into (1) interlocutor structure modeling including reply-to utterance recognition, identical speaker searching and pointer consistency distinction, and (2) utterance semantics modeling including masked shared utterance restoration and shared node detection. We evaluate MPC-BERT on three downstream tasks including addressee recognition, speaker identification and response selection. Experimental results show that MPC-BERT outperforms previous methods by large margins and achieves new state-of-the-art performance on all three downstream tasks at two benchmarks."
    }
  },
  {
    "id": "abstract-2020--acl-main--736",
    "result": [
      {
        "value": {
          "start": 776,
          "end": 813,
          "text": "models the different features jointly",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--736:E0"
      },
      {
        "value": {
          "start": 1004,
          "end": 1018,
          "text": "relative error",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--736:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--736:E0",
        "to_id": "abstract-2020--acl-main--736:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "The written forms of Semitic languages are both highly ambiguous and morphologically rich: a word can have multiple interpretations and is one of many inflected forms of the same concept or lemma. This is further exacerbated for dialectal content, which is more prone to noise and lacks a standard orthography. The morphological features can be lexicalized, like lemmas and diacritized forms, or non-lexicalized, like gender, number, and part-of-speech tags, among others. Joint modeling of the lexicalized and non-lexicalized features can identify more intricate morphological patterns, which provide better context modeling, and further disambiguate ambiguous lexical choices. However, the different modeling granularity can make joint modeling more difficult. Our approach models the different features jointly, whether lexicalized (on the character-level), or non-lexicalized (on the word-level). We use Arabic as a test case, and achieve state-of-the-art results for Modern Standard Arabic with 20% relative error reduction, and Egyptian Arabic with 11% relative error reduction."
    }
  },
  {
    "id": "P11-1034",
    "result": [
      {
        "value": {
          "start": 811,
          "end": 869,
          "text": "incorporating dialogue structure in the graph-based method",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1034:E0"
      },
      {
        "value": {
          "start": 905,
          "end": 916,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1034:E1"
      },
      {
        "from_id": "P11-1034:E0",
        "to_id": "P11-1034:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This paper presents a pilot study of opinion summarization on conversations. We create a corpus containing extractive and abstractive summaries of speaker's opinion towards a given topic using 88 telephone conversations. We adopt two methods to perform extractive summarization. The first one is a sentence-ranking method that linearly combines scores measured from different aspects including topic relevance, subjectivity, and sentence importance. The second one is a graph-based method, which incorporates topic and sentiment information, as well as additional information about sentence-to-sentence relations extracted based on dialogue structure. Our evaluation results show that both methods significantly outperform the baseline approach that extracts the longest utterances. In particular, we find that incorporating dialogue structure in the graph-based method contributes to the improved system performance."
    }
  },
  {
    "id": "abstract-2021--acl-long--289",
    "result": [
      {
        "value": {
          "start": 1023,
          "end": 1057,
          "text": "generative structural supervisions",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--289:E0"
      },
      {
        "value": {
          "start": 260,
          "end": 274,
          "text": "generalization",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--289:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--289:E0",
        "to_id": "abstract-2021--acl-long--289:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Transformer-based language models pre-trained on large amounts of text data have proven remarkably successful in learning generic transferable linguistic representations. Here we study whether structural guidance leads to more human-like systematic linguistic generalization in Transformer language models without resorting to pre-training on very large amounts of data. We explore two general ideas. The “Generative Parsing” idea jointly models the incremental parse and word sequence as part of the same sequence modeling task. The “Structural Scaffold” idea guides the language model’s representation via additional structure loss that separately predicts the incremental constituency parse. We train the proposed models along with a vanilla Transformer language model baseline on a 14 million-token and a 46 million-token subset of the BLLIP dataset, and evaluate models’ syntactic generalization performances on SG Test Suites and sized BLiMP. Experiment results across two benchmarks suggest converging evidence that generative structural supervisions can induce more robust and humanlike linguistic generalization in Transformer language models without the need for data intensive pre-training."
    }
  },
  {
    "id": "abstract-2021--acl-long--35",
    "result": [
      {
        "value": {
          "start": 971,
          "end": 1011,
          "text": "two-stage extract-then-generate baseline",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--35:E0"
      },
      {
        "value": {
          "start": 1061,
          "end": 1068,
          "text": "ROUGE-L",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--35:E1"
      },
      {
        "value": {
          "start": 1108,
          "end": 1119,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--35:E2"
      },
      {
        "from_id": "abstract-2021--acl-long--35:E0",
        "to_id": "abstract-2021--acl-long--35:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--35:E0",
        "to_id": "abstract-2021--acl-long--35:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Short textual descriptions of entities provide summaries of their key attributes and have been shown to be useful sources of background knowledge for tasks such as entity linking and question answering. However, generating entity descriptions, especially for new and long-tail entities, can be challenging since relevant information is often scattered across multiple sources with varied content and style. We introduce DESCGEN: given mentions spread over multiple documents, the goal is to generate an entity summary description. DESCGEN consists of 37K entity descriptions from Wikipedia and Fandom, each paired with nine evidence documents on average. The documents were collected using a combination of entity linking and hyperlinks into the entity pages, which together provide high-quality distant supervision. Compared to other multi-document summarization tasks, our task is entity-centric, more abstractive, and covers a wide range of domains. We also propose a two-stage extract-then-generate baseline and show that there exists a large gap (19.9% in ROUGE-L) between state-of-art models and human performance, suggesting that the data will support significant future work."
    }
  },
  {
    "id": "abstract-2021--acl-long--462",
    "result": [
      {
        "value": {
          "start": 26,
          "end": 59,
          "text": "Shallow Aggressive Decoding (SAD)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--462:E0"
      },
      {
        "value": {
          "start": 1041,
          "end": 1045,
          "text": "F0.5",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--462:E1"
      },
      {
        "value": {
          "start": 1041,
          "end": 1045,
          "text": "F0.5",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--462:E2"
      },
      {
        "value": {
          "start": 842,
          "end": 859,
          "text": "inference speedup",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--462:E3"
      },
      {
        "from_id": "abstract-2021--acl-long--462:E0",
        "to_id": "abstract-2021--acl-long--462:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--462:E0",
        "to_id": "abstract-2021--acl-long--462:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--462:E0",
        "to_id": "abstract-2021--acl-long--462:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "In this paper, we propose Shallow Aggressive Decoding (SAD) to improve the online inference efficiency of the Transformer for instantaneous Grammatical Error Correction (GEC). SAD optimizes the online inference efficiency for GEC by two innovations: 1) it aggressively decodes as many tokens as possible in parallel instead of always decoding only one token in each step to improve computational parallelism; 2) it uses a shallow decoder instead of the conventional Transformer architecture with balanced encoder-decoder depth to reduce the computational cost during inference. Experiments in both English and Chinese GEC benchmarks show that aggressive decoding could yield identical predictions to greedy decoding but with significant speedup for online inference. Its combination with the shallow decoder could offer an even higher online inference speedup over the powerful Transformer baseline without quality loss. Not only does our approach allow a single model to achieve the state-of-the-art results in English GEC benchmarks: 66.4 F0.5 in the CoNLL-14 and 72.9 F0.5 in the BEA-19 test set with an almost 10x online inference speedup over the Transformer-big model, but also it is easily adapted to other languages. Our code is available at https://github.com/AutoTemp/Shallow-Aggressive-Decoding."
    }
  },
  {
    "id": "P10-1031",
    "result": [
      {
        "value": {
          "start": 345,
          "end": 352,
          "text": "OntoUSP",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1031:E0"
      },
      {
        "value": {
          "start": 832,
          "end": 838,
          "text": "recall",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1031:E1"
      },
      {
        "from_id": "P10-1031:E0",
        "to_id": "P10-1031:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Extracting knowledge from unstructured text is a long-standing goal of NLP. Although learning approaches to many of its subtasks have been developed (e.g., parsing, taxonomy induction, information extraction), all end-to-end solutions to date require heavy supervision and/or manual engineering, limiting their scope and scalability. We present OntoUSP, a system that induces and populates a probabilistic ontology using only dependency-parsed text as input. OntoUSP builds on the USP unsupervised semantic parser by jointly forming ISA and IS-PART hierarchies of lambda-form clusters. The ISA hierarchy allows more general knowledge to be learned, and the use of smoothing for parameter estimation. We evaluate OntoUSP by using it to extract a knowledge base from biomedical abstracts and answer questions. OntoUSP improves on the recall of USP by 47% and greatly outperforms previous state-of-the-art approaches."
    }
  },
  {
    "id": "abstract-2020--acl-main--548",
    "result": [
      {
        "value": {
          "start": 409,
          "end": 458,
          "text": "the Negative Binomial-Neural Topic Model (NB-NTM)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--548:E0"
      },
      {
        "value": {
          "start": 857,
          "end": 867,
          "text": "perplexity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--548:E1"
      },
      {
        "value": {
          "start": 872,
          "end": 887,
          "text": "topic coherence",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--548:E2"
      },
      {
        "value": {
          "start": 463,
          "end": 519,
          "text": "the Gamma Negative Binomial-Neural Topic Model (GNB-NTM)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--548:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--548:E0",
        "to_id": "abstract-2020--acl-main--548:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--548:E0",
        "to_id": "abstract-2020--acl-main--548:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--548:E3",
        "to_id": "abstract-2020--acl-main--548:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--548:E3",
        "to_id": "abstract-2020--acl-main--548:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Mixed counting models that use the negative binomial distribution as the prior can well model over-dispersed and hierarchically dependent random variables; thus they have attracted much attention in mining dispersed document topics. However, the existing parameter inference method like Monte Carlo sampling is quite time-consuming. In this paper, we propose two efficient neural mixed counting models, i.e., the Negative Binomial-Neural Topic Model (NB-NTM) and the Gamma Negative Binomial-Neural Topic Model (GNB-NTM) for dispersed topic discovery. Neural variational inference algorithms are developed to infer model parameters by using the reparameterization of Gamma distribution and the Gaussian approximation of Poisson distribution. Experiments on real-world datasets indicate that our models outperform state-of-the-art baseline models in terms of perplexity and topic coherence. The results also validate that both NB-NTM and GNB-NTM can produce explainable intermediate variables by generating dispersed proportions of document topics."
    }
  },
  {
    "id": "abstract-2020--acl-main--595",
    "result": [
      {
        "value": {
          "start": 367,
          "end": 418,
          "text": "couples distant annotation and adversarial training",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--595:E0"
      },
      {
        "value": {
          "start": 992,
          "end": 1002,
          "text": "robustness",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--595:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--595:E0",
        "to_id": "abstract-2020--acl-main--595:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Fully supervised neural approaches have achieved significant progress in the task of Chinese word segmentation (CWS). Nevertheless, the performance of supervised models always drops gravely if the domain shifts due to the distribution gap across domains and the out of vocabulary (OOV) problem. In order to simultaneously alleviate the issues, this paper intuitively couples distant annotation and adversarial training for cross-domain CWS. 1) We rethink the essence of “Chinese words” and design an automatic distant annotation mechanism, which does not need any supervision or pre-defined dictionaries on the target domain. The method could effectively explore domain-specific words and distantly annotate the raw texts for the target domain. 2) We further develop a sentence-level adversarial training procedure to perform noise reduction and maximum utilization of the source domain information. Experiments on multiple real-world datasets across various domains show the superiority and robustness of our model, significantly outperforming previous state-of-the-arts cross-domain CWS methods."
    }
  },
  {
    "id": "abstract-2021--acl-long--233",
    "result": [
      {
        "value": {
          "start": 229,
          "end": 296,
          "text": "Pre-trained masked Language model with Misspelled knowledgE (PLOME)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--233:E0"
      },
      {
        "value": {
          "start": 914,
          "end": 925,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--233:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--233:E0",
        "to_id": "abstract-2021--acl-long--233:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Chinese spelling correction (CSC) is a task to detect and correct spelling errors in texts. CSC is essentially a linguistic problem, thus the ability of language understanding is crucial to this task. In this paper, we propose a Pre-trained masked Language model with Misspelled knowledgE (PLOME) for CSC, which jointly learns how to understand language and correct spelling errors. To this end, PLOME masks the chosen tokens with similar characters according to a confusion set rather than the fixed token “[MASK]” as in BERT. Besides character prediction, PLOME also introduces pronunciation prediction to learn the misspelled knowledge on phonic level. Moreover, phonological and visual similarity knowledge is important to this task. PLOME utilizes GRU networks to model such knowledge based on characters’ phonics and strokes. Experiments are conducted on widely used benchmarks. Our method achieves superior performance against state-of-the-art approaches by a remarkable margin. We release the source code and pre-trained model for further use by the community (https://github.com/liushulinle/PLOME)."
    }
  },
  {
    "id": "abstract-2020--acl-main--5",
    "result": [
      {
        "value": {
          "start": 293,
          "end": 353,
          "text": "Dialogue State Tracking with Slot Connections (DST-SC) model",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--5:E0"
      },
      {
        "value": {
          "start": 738,
          "end": 749,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--5:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--5:E0",
        "to_id": "abstract-2020--acl-main--5:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recent proposed approaches have made promising progress in dialogue state tracking (DST). However, in multi-domain scenarios, ellipsis and reference are frequently adopted by users to express values that have been mentioned by slots from other domains. To handle these phenomena, we propose a Dialogue State Tracking with Slot Connections (DST-SC) model to explicitly consider slot correlations across different domains. Given a target slot, the slot connecting mechanism in DST-SC can infer its source slot and copy the source slot value directly, thus significantly reducing the difficulty of learning and reasoning. Experimental results verify the benefits of explicit slot connection modeling, and our model achieves state-of-the-art performance on MultiWOZ 2.0 and MultiWOZ 2.1 datasets."
    }
  },
  {
    "id": "abstract-2020--acl-main--73",
    "result": [
      {
        "value": {
          "start": 281,
          "end": 311,
          "text": "autoencoding variational Bayes",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--73:E0"
      },
      {
        "value": {
          "start": 337,
          "end": 348,
          "text": "scalability",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--73:E1"
      },
      {
        "value": {
          "start": 374,
          "end": 385,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--73:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--73:E0",
        "to_id": "abstract-2020--acl-main--73:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--73:E0",
        "to_id": "abstract-2020--acl-main--73:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "This paper presents a tree-structured neural topic model, which has a topic distribution over a tree with an infinite number of branches. Our model parameterizes an unbounded ancestral and fraternal topic distribution by applying doubly-recurrent neural networks. With the help of autoencoding variational Bayes, our model improves data scalability and achieves competitive performance when inducing latent topics and tree structures, as compared to a prior tree-structured topic model (Blei et al., 2010). This work extends the tree-structured topic model such that it can be incorporated with neural models for downstream tasks."
    }
  },
  {
    "id": "abstract-2021--acl-long--227",
    "result": [
      {
        "value": {
          "start": 349,
          "end": 358,
          "text": "ERNIE-Doc",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--227:E0"
      },
      {
        "value": {
          "start": 969,
          "end": 979,
          "text": "perplexity",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--227:E1"
      },
      {
        "from_id": "abstract-2021--acl-long--227:E0",
        "to_id": "abstract-2021--acl-long--227:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Transformers are not suited for processing long documents, due to their quadratically increasing memory and time consumption. Simply truncating a long document or applying the sparse attention mechanism will incur the context fragmentation problem or lead to an inferior modeling capability against comparable model sizes. In this paper, we propose ERNIE-Doc, a document-level language pretraining model based on Recurrence Transformers. Two well-designed techniques, namely the retrospective feed mechanism and the enhanced recurrence mechanism, enable ERNIE-Doc, which has a much longer effective context length, to capture the contextual information of a complete document. We pretrain ERNIE-Doc to explicitly learn the relationships among segments with an additional document-aware segment-reordering objective. Various experiments were conducted on both English and Chinese document-level tasks. ERNIE-Doc improved the state-of-the-art language modeling result of perplexity to 16.8 on WikiText-103. Moreover, it outperformed competitive pretraining models by a large margin on most language understanding tasks, such as text classification and question answering."
    }
  },
  {
    "id": "abstract-2020--acl-main--605",
    "result": [
      {
        "value": {
          "start": 645,
          "end": 674,
          "text": "neural graph rewriting system",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--605:E0"
      },
      {
        "value": {
          "start": 841,
          "end": 849,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--605:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--605:E0",
        "to_id": "abstract-2020--acl-main--605:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "We propose variable-in-situ logico-semantic graphs to bridge the gap between semantic graph and logical form parsing. The new type of graph-based meaning representation allows us to include analysis for scope-related phenomena, such as quantification, negation and modality, in a way that is consistent with the state-of-the-art underspecification approach. Moreover, the well-formedness of such a graph is clear, since model-theoretic interpretation is available. We demonstrate the effectiveness of this new perspective by developing a new state-of-the-art semantic parser for English Resource Semantics. At the core of this parser is a novel neural graph rewriting system which combines the strengths of Hyperedge Replacement Grammar, a knowledge-intensive model, and Graph Neural Networks, a data-intensive model. Our parser achieves an accuracy of 92.39% in terms of elementary dependency match, which is a 2.88 point improvement over the best data-driven model in the literature. The output of our parser is highly coherent: at least 91% graphs are valid, in that they allow at least one sound scope-resolved logical form."
    }
  },
  {
    "id": "abstract-2020--acl-main--214",
    "result": [
      {
        "value": {
          "start": 1136,
          "end": 1170,
          "text": "Fine-tuning MAG-BERT and MAG-XLNet",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--214:E0"
      },
      {
        "value": {
          "start": 112,
          "end": 123,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--214:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--214:E0",
        "to_id": "abstract-2020--acl-main--214:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Recent Transformer-based contextual word representations, including BERT and XLNet, have shown state-of-the-art performance in multiple disciplines within NLP. Fine-tuning the trained contextual models on task-specific datasets has been the key to achieving superior performance downstream. While fine-tuning these pre-trained models is straightforward for lexical applications (applications with only language modality), it is not trivial for multimodal language (a growing area in NLP focused on modeling face-to-face communication). More specifically, this is due to the fact that pre-trained models don’t have the necessary components to accept two extra modalities of vision and acoustic. In this paper, we proposed an attachment to BERT and XLNet called Multimodal Adaptation Gate (MAG). MAG allows BERT and XLNet to accept multimodal nonverbal data during fine-tuning. It does so by generating a shift to internal representation of BERT and XLNet; a shift that is conditioned on the visual and acoustic modalities. In our experiments, we study the commonly used CMU-MOSI and CMU-MOSEI datasets for multimodal sentiment analysis. Fine-tuning MAG-BERT and MAG-XLNet significantly boosts the sentiment analysis performance over previous baselines as well as language-only fine-tuning of BERT and XLNet. On the CMU-MOSI dataset, MAG-XLNet achieves human-level multimodal sentiment analysis performance for the first time in the NLP community."
    }
  },
  {
    "id": "abstract-2021--acl-long--16",
    "result": [
      {
        "value": {
          "start": 705,
          "end": 737,
          "text": "token-level early-exit mechanism",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--16:E0"
      },
      {
        "value": {
          "start": 235,
          "end": 253,
          "text": "computational cost",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--16:E1"
      },
      {
        "value": {
          "start": 590,
          "end": 615,
          "text": "sentence-level early-exit",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2021--acl-long--16:E2"
      },
      {
        "value": {
          "start": 1208,
          "end": 1222,
          "text": "inference cost",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--16:E3"
      },
      {
        "value": {
          "start": 5,
          "end": 16,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2021--acl-long--16:E4"
      },
      {
        "from_id": "abstract-2021--acl-long--16:E0",
        "to_id": "abstract-2021--acl-long--16:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--16:E2",
        "to_id": "abstract-2021--acl-long--16:E3",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      },
      {
        "from_id": "abstract-2021--acl-long--16:E2",
        "to_id": "abstract-2021--acl-long--16:E4",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "Both performance and efficiency are crucial factors for sequence labeling tasks in many real-world scenarios. Although the pre-trained models (PTMs) have significantly improved the performance of various sequence labeling tasks, their computational cost is expensive. To alleviate this problem, we extend the recent successful early-exit mechanism to accelerate the inference of PTMs for sequence labeling tasks. However, existing early-exit mechanisms are specifically designed for sequence-level tasks, rather than sequence labeling. In this paper, we first propose a simple extension of sentence-level early-exit for sequence labeling tasks. To further reduce the computational cost, we also propose a token-level early-exit mechanism that allows partial tokens to exit early at different layers. Considering the local dependency inherent in sequence labeling, we employed a window-based criterion to decide for a token whether or not to exit. The token-level early-exit brings the gap between training and inference, so we introduce an extra self-sampling fine-tuning stage to alleviate it. The extensive experiments on three popular sequence labeling tasks show that our approach can save up to 66%∼75% inference cost with minimal performance degradation. Compared with competitive compressed models such as DistilBERT, our approach can achieve better performance under the same speed-up ratios of 2×, 3×, and 4×."
    }
  },
  {
    "id": "P10-1004",
    "result": [
      {
        "value": {
          "start": 11,
          "end": 33,
          "text": "an efficient algorithm",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1004:E0"
      },
      {
        "value": {
          "start": 60,
          "end": 68,
          "text": "readings",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1004:E1"
      },
      {
        "from_id": "P10-1004:E0",
        "to_id": "P10-1004:E1",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "We present an efficient algorithm for computing the weakest readings of semantically ambiguous sentences. A corpus-based evaluation with a large-scale grammar shows that our algorithm reduces over 80% of sentences to one or two readings, in negligible runtime, and thus makes it possible to work with semantic representations derived by deep large-scale grammars."
    }
  },
  {
    "id": "abstract-2020--acl-main--499",
    "result": [
      {
        "value": {
          "start": 236,
          "end": 277,
          "text": "integrating logic rules and neural models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--499:E0"
      },
      {
        "value": {
          "start": 171,
          "end": 179,
          "text": "accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--499:E1"
      },
      {
        "value": {
          "start": 184,
          "end": 195,
          "text": "consistency",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--499:E2"
      },
      {
        "value": {
          "start": 763,
          "end": 774,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--499:E3"
      },
      {
        "from_id": "abstract-2020--acl-main--499:E0",
        "to_id": "abstract-2020--acl-main--499:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--499:E0",
        "to_id": "abstract-2020--acl-main--499:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--499:E0",
        "to_id": "abstract-2020--acl-main--499:E3",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Many natural language questions require qualitative, quantitative or logical comparisons between two entities or events. This paper addresses the problem of improving the accuracy and consistency of responses to comparison questions by integrating logic rules and neural models. Our method leverages logical and linguistic knowledge to augment labeled training data and then uses a consistency-based regularizer to train the model. Improving the global consistency of predictions, our approach achieves large improvements over previous methods in a variety of question answering (QA) tasks, including multiple-choice qualitative reasoning, cause-effect reasoning, and extractive machine reading comprehension. In particular, our method significantly improves the performance of RoBERTa-based models by 1-5% across datasets. We advance state of the art by around 5-8% on WIQA and QuaRel and reduce consistency violations by 58% on HotpotQA. We further demonstrate that our approach can learn effectively from limited data."
    }
  },
  {
    "id": "P11-1048",
    "result": [
      {
        "value": {
          "start": 250,
          "end": 308,
          "text": "a single model with both supertagging and parsing features",
          "labels": [
            "Operation"
          ]
        },
        "id": "P11-1048:E0"
      },
      {
        "value": {
          "start": 701,
          "end": 710,
          "text": "F-measure",
          "labels": [
            "Effect"
          ]
        },
        "id": "P11-1048:E1"
      },
      {
        "from_id": "P11-1048:E0",
        "to_id": "P11-1048:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Via an oracle experiment, we show that the upper bound on accuracy of a CCG parser is significantly lowered when its search space is pruned using a supertagger, though the supertagger also prunes many bad parses. Inspired by this analysis, we design a single model with both supertagging and parsing features, rather than separating them into distinct models chained together in a pipeline. To overcome the resulting increase in complexity, we experiment with both belief propagation and dual decomposition approaches to inference, the first empirical comparison of these algorithms that we are aware of on a structured natural language processing problem. On CCGbank we achieve a labelled dependency F-measure of 88.8% on gold POS tags, and 86.7% on automatic part-of-speeoch tags, the best reported results for this task."
    }
  },
  {
    "id": "abstract-2020--acl-main--102",
    "result": [
      {
        "value": {
          "start": 20,
          "end": 60,
          "text": "Dynamic Memory Induction Networks (DMIN)",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--102:E0"
      },
      {
        "value": {
          "start": 473,
          "end": 484,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--102:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--102:E0",
        "to_id": "abstract-2020--acl-main--102:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "This paper proposes Dynamic Memory Induction Networks (DMIN) for few-short text classification. The model develops a dynamic routing mechanism over static memory, enabling it to better adapt to unseen classes, a critical capability for few-short classification. The model also expands the induction process with supervised learning weights and query information to enhance the generalization ability of meta-learning. The proposed model brings forward the state-of-the-art performance significantly by 2~4% improvement on the miniRCV1 and ODIC datasets. Detailed analysis is further performed to show how the proposed network achieves the new performance."
    }
  },
  {
    "id": "abstract-2020--acl-main--127",
    "result": [
      {
        "value": {
          "start": 178,
          "end": 190,
          "text": "PR-Embedding",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--127:E0"
      },
      {
        "value": {
          "start": 800,
          "end": 832,
          "text": "quality of the selected response",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--127:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--127:E0",
        "to_id": "abstract-2020--acl-main--127:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Human conversations contain many types of information, e.g., knowledge, common sense, and language habits. In this paper, we propose a conversational word embedding method named PR-Embedding, which utilizes the conversation pairs <post, reply> to learn word embedding. Different from previous works, PR-Embedding uses the vectors from two different semantic spaces to represent the words in post and reply.To catch the information among the pair, we first introduce the word alignment model from statistical machine translation to generate the cross-sentence window, then train the embedding on word-level and sentence-level.We evaluate the method on single-turn and multi-turn response selection tasks for retrieval-based dialog systems.The experiment results show that PR-Embedding can improve the quality of the selected response."
    }
  },
  {
    "id": "abstract-2020--acl-main--336",
    "result": [
      {
        "value": {
          "start": 473,
          "end": 495,
          "text": "applying meta learning",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--336:E0"
      },
      {
        "value": {
          "start": 644,
          "end": 655,
          "text": "performance",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--336:E1"
      },
      {
        "value": {
          "start": 706,
          "end": 718,
          "text": "over-fitting",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--336:E2"
      },
      {
        "from_id": "abstract-2020--acl-main--336:E0",
        "to_id": "abstract-2020--acl-main--336:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "abstract-2020--acl-main--336:E0",
        "to_id": "abstract-2020--acl-main--336:E2",
        "type": "relation",
        "labels": [
          "Negative"
        ]
      }
    ],
    "data": {
      "text": "Hypernymy detection, a.k.a, lexical entailment, is a fundamental sub-task of many natural language understanding tasks. Previous explorations mostly focus on monolingual hypernymy detection on high-resource languages, e.g., English, but few investigate the low-resource scenarios. This paper addresses the problem of low-resource hypernymy detection by combining high-resource languages. We extensively compare three joint training paradigms and for the first time propose applying meta learning to relieve the low-resource issue. Experiments demonstrate the superiority of our method among the three settings, which substantially improves the performance of extremely low-resource languages by preventing over-fitting on small datasets."
    }
  },
  {
    "id": "P10-1035",
    "result": [
      {
        "value": {
          "start": 406,
          "end": 417,
          "text": "PCFG parser",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1035:E0"
      },
      {
        "value": {
          "start": 496,
          "end": 517,
          "text": "supertagging accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1035:E1"
      },
      {
        "value": {
          "start": 519,
          "end": 536,
          "text": "PARSEVAL measures",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1035:E2"
      },
      {
        "value": {
          "start": 541,
          "end": 560,
          "text": "dependency accuracy",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1035:E3"
      },
      {
        "from_id": "P10-1035:E0",
        "to_id": "P10-1035:E1",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "P10-1035:E0",
        "to_id": "P10-1035:E2",
        "type": "relation",
        "labels": [
          "Other"
        ]
      },
      {
        "from_id": "P10-1035:E0",
        "to_id": "P10-1035:E3",
        "type": "relation",
        "labels": [
          "Other"
        ]
      }
    ],
    "data": {
      "text": "The definition of combinatory categorial grammar (CCG) in the literature varies quite a bit from author to author. However, the differences between the definitions are important in terms of the language classes of each CCG. We prove that a wide range of CCGs are strongly context-free, including the CCG of CCG-bank and of the parser of Clark and Curran (2007). In light of these new results, we train the PCFG parser of Petrov and Klein (2007) on CCGbank and achieve state of the art results in supertagging accuracy, PARSEVAL measures and dependency accuracy."
    }
  },
  {
    "id": "abstract-2020--acl-main--617",
    "result": [
      {
        "value": {
          "start": 473,
          "end": 514,
          "text": "a class of hyperbolic KG embedding models",
          "labels": [
            "Operation"
          ]
        },
        "id": "abstract-2020--acl-main--617:E0"
      },
      {
        "value": {
          "start": 838,
          "end": 864,
          "text": "mean reciprocal rank (MRR)",
          "labels": [
            "Effect"
          ]
        },
        "id": "abstract-2020--acl-main--617:E1"
      },
      {
        "from_id": "abstract-2020--acl-main--617:E0",
        "to_id": "abstract-2020--acl-main--617:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "Knowledge graph (KG) embeddings learn low- dimensional representations of entities and relations to predict missing facts. KGs often exhibit hierarchical and logical patterns which must be preserved in the embedding space. For hierarchical data, hyperbolic embedding methods have shown promise for high-fidelity and parsimonious representations. However, existing hyperbolic embedding methods do not account for the rich logical patterns in KGs. In this work, we introduce a class of hyperbolic KG embedding models that simultaneously capture hierarchical and logical patterns. Our approach combines hyperbolic reflections and rotations with attention to model complex relational patterns. Experimental results on standard KG benchmarks show that our method improves over previous Euclidean- and hyperbolic-based efforts by up to 6.1% in mean reciprocal rank (MRR) in low dimensions. Furthermore, we observe that different geometric transformations capture different types of relations while attention- based transformations generalize to multiple relations. In high dimensions, our approach yields new state-of-the-art MRRs of 49.6% on WN18RR and 57.7% on YAGO3-10."
    }
  },
  {
    "id": "P10-1017",
    "result": [
      {
        "value": {
          "start": 33,
          "end": 62,
          "text": "hierarchical search algorithm",
          "labels": [
            "Operation"
          ]
        },
        "id": "P10-1017:E0"
      },
      {
        "value": {
          "start": 523,
          "end": 532,
          "text": "F-measure",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1017:E1"
      },
      {
        "value": {
          "start": 549,
          "end": 553,
          "text": "Bleu",
          "labels": [
            "Effect"
          ]
        },
        "id": "P10-1017:E2"
      },
      {
        "from_id": "P10-1017:E0",
        "to_id": "P10-1017:E1",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      },
      {
        "from_id": "P10-1017:E0",
        "to_id": "P10-1017:E2",
        "type": "relation",
        "labels": [
          "Positive"
        ]
      }
    ],
    "data": {
      "text": "We present a simple yet powerful hierarchical search algorithm for automatic word alignment. Our algorithm induces a forest of alignments from which we can efficiently extract a ranked k-best list. We score a given alignment within the forest with a flexible, linear discriminative model incorporating hundreds of features, and trained on a relatively small amount of annotated data. We report results on Arabic-English word alignment and translation tasks. Our model outperforms a GIZA++ Model-4 baseline by 6.3 points in F-measure, yielding a 1.1 Bleu score increase over a state-of-the-art syntax-based machine translation system."
    }
  }
]